import os
import json
import glob
import torch
import pickle
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ==========================================
# 1. è¨­å®šå€
# ==========================================
# è¼¸å…¥è³‡æ–™å¤¾ (å‰›å‰› Qwen è·‘å®Œçš„ JSON ä½ç½®)
INPUT_DIR = "/home/maxwell/data/nlp/final_project/dataset/WattBot2025/local_llm_processed"
# è¼¸å‡ºè³‡æ–™å¤¾ (å­˜ Embedding çš„ä½ç½®)
OUTPUT_DIR = "/home/maxwell/data/nlp/final_project/dataset/WattBot2025/embeddings"

# ä½¿ç”¨çš„ Embedding æ¨¡å‹ (æ¨è–¦ BAAI/bge-m3ï¼Œæ”¯æ´é•·æ–‡æœ¬èˆ‡å¤šèªè¨€)
MODEL_NAME = "BAAI/bge-m3"

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ==========================================
# 2. Encoder é¡åˆ¥
# ==========================================
class WattBotEncoder:
    def __init__(self, model_name=MODEL_NAME):
        print(f"ğŸš€ æ­£åœ¨è¼‰å…¥ Embedding æ¨¡å‹: {model_name}...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = SentenceTransformer(model_name, device=device)
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ (Device: {device})")

    def encode(self, texts):
        """
        å°‡æ–‡å­—åˆ—è¡¨è½‰ç‚ºå‘é‡
        Output: Numpy array
        """
        # normalize_embeddings=True å°æ–¼è¨ˆç®— Cosine Similarity å¾ˆé‡è¦
        return self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False, batch_size=32)

# ==========================================
# 3. è³‡æ–™è™•ç†å‡½å¼ (åŒ…å«éŒ¯èª¤ä¿®å¾©)
# ==========================================
def process_file(json_file_path, encoder):
    filename = os.path.basename(json_file_path)
    doc_id = os.path.splitext(filename)[0]
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            print(f"âš ï¸ [Skip] ç„¡æ³•è®€å– JSON: {filename}")
            return None

    # ç”¨ä¾†æ”¶é›†é€™ä¸€ä»½æ–‡ä»¶è£¡æ‰€æœ‰è¦ embedding çš„æ–‡å­—
    chunks_text = []
    chunks_metadata = []

    for item in data:
        # --- ğŸ›¡ï¸ é—œéµä¿®å¾©ï¼šå®‰å…¨æª¢æŸ¥ ---
        content = item.get('content')
        caption = item.get('caption')
        
        # 1. å¦‚æœ content æ˜¯ Noneï¼Œçµ¦å®ƒç©ºå­—ä¸²ï¼Œé¿å… AttributeError
        if content is None:
            content = ""
        
        # 2. ç¢ºä¿æ˜¯å­—ä¸²å‹æ…‹ (æœ‰æ™‚å€™å¯èƒ½æ˜¯æ•¸å­—æˆ–ç‰©ä»¶)
        if not isinstance(content, str):
            content = str(content)
            
        # 3. å»é™¤ç©ºç™½
        text_to_embed = content.strip()
        
        # 4. å¦‚æœæ˜¯è¡¨æ ¼æˆ–åœ–ç‰‡ï¼ŒæŠŠ Caption åŠ é€²å»ä¸€èµ· embeddingï¼Œå¢åŠ æª¢ç´¢æº–ç¢ºåº¦
        if caption and isinstance(caption, str) and caption.strip():
            text_to_embed = f"{caption.strip()}\n{text_to_embed}"

        # 5. å¦‚æœæœ€å¾Œæ–‡å­—æ˜¯ç©ºçš„ï¼ˆä¾‹å¦‚æœ‰äº›åœ–ç‰‡æè¿°å¤±æ•—ï¼‰ï¼Œå°±è·³é
        if not text_to_embed:
            continue
            
        chunks_text.append(text_to_embed)
        chunks_metadata.append(item)

    if not chunks_text:
        return None

    # --- æ‰¹é‡é€²è¡Œ Encoding ---
    # é€™æ¨£æ¯”ä¸€ç­†ä¸€ç­† encode å¿«éå¸¸å¤š
    embeddings = encoder.encode(chunks_text)

    # --- æ•´åˆçµæœ ---
    processed_results = []
    for i, meta in enumerate(chunks_metadata):
        # æŠŠå‘é‡å­˜é€²å» (è½‰æˆ List æ–¹ä¾¿å­˜ JSON/Pickle)
        meta['embedding'] = embeddings[i].tolist()
        # ç‚ºäº†ç¢ºèªæˆ‘å€‘ embed äº†ä»€éº¼ï¼ŒæŠŠçµ„åˆå¥½çš„æ–‡å­—ä¹Ÿå­˜å›å»
        meta['embedded_text'] = chunks_text[i] 
        processed_results.append(meta)

    return processed_results

# ==========================================
# 4. ä¸»ç¨‹å¼
# ==========================================
def main():
    # 1. åˆå§‹åŒ– Encoder
    encoder = WattBotEncoder()

    # 2. æŠ“å–æ‰€æœ‰ JSON
    json_files = glob.glob(os.path.join(INPUT_DIR, "*.json"))
    print(f"ğŸ“‚ æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆï¼Œé–‹å§‹ Embedding...")

    all_documents_data = []

    # 3. è™•ç†æ¯å€‹æª”æ¡ˆ
    for json_file in tqdm(json_files, desc="Encoding Files"):
        result = process_file(json_file, encoder)
        if result:
            all_documents_data.extend(result)

    # 4. å­˜æª” (å„²å­˜ç‚º Pickleï¼Œå› ç‚ºè®€å¯«å‘é‡æœ€å¿«)
    output_pkl = os.path.join(OUTPUT_DIR, "corpus_embeddings.pkl")
    print(f"ğŸ’¾ æ­£åœ¨å„²å­˜ {len(all_documents_data)} ç­†å‘é‡è³‡æ–™åˆ° {output_pkl} ...")
    
    with open(output_pkl, 'wb') as f:
        pickle.dump(all_documents_data, f)

    # 5. é¡å¤–å­˜ä¸€ä»½ JSONL (æ–¹ä¾¿äººé¡æª¢æŸ¥ï¼Œä¸å«å‘é‡ä»¥å…æª”æ¡ˆå¤ªå¤§)
    output_jsonl = os.path.join(OUTPUT_DIR, "corpus_text_only.jsonl")
    with open(output_jsonl, 'w', encoding='utf-8') as f:
        for entry in all_documents_data:
            # è¤‡è£½ä¸€ä»½ä¸å« embedding çš„è³‡æ–™
            entry_copy = entry.copy()
            del entry_copy['embedding']
            f.write(json.dumps(entry_copy, ensure_ascii=False) + '\n')

    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"   - å‘é‡æª”: {output_pkl}")
    print(f"   - æ–‡å­—æª”: {output_jsonl}")

if __name__ == "__main__":
    main()