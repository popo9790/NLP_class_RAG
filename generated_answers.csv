id,answer,answer_value,answer_unit,supporting_materials,explanation,ref_id,ref_url
q001,4%,4,percent,"The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].",Quote,"['luccioni2025a', 'han2024']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2412.06288']"
q002,Unable to answer with confidence based on the provided documents.,is_blank,cars,is_blank,"The provided documents do not contain information about the estimated amount of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid in 2023, nor do they provide a direct equivalent of this specific project's CO2e avoidance to cars taken off the road.","['amazon2023', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q004,Unable to answer with confidence based on the provided documents.,is_blank,data centers,is_blank,"The provided documents state that AWS invested in on-site water treatment systems in 2023 to enable recycling more water for cooling, but they do not specify the number of data centers where this practice began in that year.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q005,463 kg per GPU,463,kg/GPU,"Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO₂eq per 8x server node, equal 463 kg per GPU.",Quote,"['luccioni2024', 'morrison2025']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2503.05804']"
q006,400x,400,multiplier,"cottier2024: ""Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.""; li2025a: ""In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.""","GPT-4's amortized hardware and energy cost is $40 million. The FLM-101B model was trained within a $100,000 budget. To find the factor, divide $40,000,000 by $100,000, which equals 400.",['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q007,Approximately 2 tCO2e,2,tCO2e,"strubell2019 - ""Air travel, 1 passenger, NY<>SF & 1984"", ""Table 3 lists CO2 emissions and estimated cost of training the models described in §2.1.""","According to Table 3 in Strubell et al. (2019), a single passenger round trip via air travel between New York and San Francisco generates 1984 kg of CO2e. To convert kilograms to metric tons (tCO2e), divide by 1000: 1984 kg / 1000 = 1.984 tCO2e. As the question asks for an 'approximate' value, 1.984 tCO2e is rounded to 2 tCO2e.","['patterson2021', 'han2024']","['https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2104.10350']"
q008,44.41,44.41,score,Table 6: Performance of the three stages of FLM on Open LLM.,Table 6 in li2025a shows the performance of the three stages of FLM on Open LLM. The row for '101B' (representing FLM-101B) shows an 'Average' score of 44.41.,['li2025a'],['https://arxiv.org/pdf/2309.03852']
q010,"More than 6,750 fold",6750,fold,"Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.",Quote,"['wu2021b', 'patterson2021']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2104.10350']"
q011,Unable to answer with confidence based on the provided documents.,is_blank,days,is_blank,"To calculate the training time, the total computational requirement in FLOPs for GPT-3 is needed. The formula '6 × parameters × tokens' is provided in the documents (cottier2024) for estimating compute for GPT-3 variants. The number of parameters for GPT-3 (175 billion) is also mentioned (patterson2021). However, the total number of tokens GPT-3 was trained on is not explicitly stated in the provided documents. Without the number of tokens, the total FLOPs cannot be calculated, and thus the training time cannot be determined.","['cottier2024', 'patterson2021']","['https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2405.21015']"
q012,0.864 kWh,0.864,kWh,"Request freq. (req / s) | GPU Power Usage (kWh) | Carbon Emissions (g CO2eq) | Water consump. (L) | Seconds per 100 req. | # Inf. for CO2 equiv. w/ training

Llama 3.2 1B | ∞ | 0.003 | 1.0 | 0.004 | 1.38 | 258 bil.
8 | 0.036 | 12.0 | 0.054 | 12.64 | 21.5 bil.
1 | 0.16 | 53.1 | 0.238 | 100.58 | 4.83 bil.","The table in morrison2025 provides the 'GPU Power Usage (kWh)' for the Llama 3.2 1B model at different request frequencies. For an 8 req/s frequency, the GPU Power Usage is 0.036 kWh per 100 requests. The question asks for the usage for 2400 prompts. Therefore, the calculation is (2400 prompts / 100 prompts) * 0.036 kWh = 24 * 0.036 kWh = 0.864 kWh.","['chung2025', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598']"
q013,"13,000 tons",13000,tons,"The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The document states the total permitted annual emission limits for data center diesel generators in Northern Virginia, including permits issued since January 1, 2023, are approximately 13,000 tons of NOx.",['han2024'],['https://arxiv.org/pdf/2412.06288']
q014,90%,90,percent,"we show that our model, namely FLM-101B, trained with our growth strategy under a budget of $100K, reaches 80% of the baselines' performances with only 10% of their floating-point operations.","The FLM-101B model, trained with a growth strategy, used only 10% of the floating-point operations (computational cost) compared to baselines (from-scratch approach). This implies a 90% saving in computational cost, and by extension, training time.","['li2025a', 'amazon2023']","['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf', 'https://arxiv.org/pdf/2309.03852']"
q015,Unable to answer with confidence based on the provided documents.,is_blank,deaths,is_blank,"The provided documents discuss the public health impact and costs associated with U.S. data centers and air pollutants, including premature mortality from various sources. However, they do not explicitly state a projected number of premature deaths in 2030 caused specifically by scope-2 pollutants from U.S. data centers alone. The texts mention a general increase in public health costs for U.S. data centers by 2028 and describe methods for calculating health impacts, but they do not provide the exact numerical answer for premature deaths in 2030.",['han2024'],['https://arxiv.org/pdf/2412.06288']
q016,Unable to answer with confidence based on the provided documents.,is_blank,days,"The largest training runs (e.g., 6 billion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only trained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here).","The document states that the 6 billion parameter model was 'only trained for 13% of a full run'. However, the duration of this 13% partial run is not provided in the text. Therefore, the full training run duration cannot be calculated.","['cottier2024', 'wu2021a']","['https://arxiv.org/pdf/2111.00364', 'https://arxiv.org/pdf/2405.21015']"
q017,Command-R Plus,Command-R Plus,is_blank,"Model | Organization | GPU Energy for 1k Queries (Wh) | Task
bert-tiny-finetuned-squadv2 | mrm8488 | 0.06 | Extractive QA
GIST-all-MiniLM-L6-v2 | avsolatorio | 0.11 | Sentence Similarity
dynamic_tinybert | Intel | 0.21 | Extractive QA
distilbert-imdb | lvwerra | 0.22 | Text Classification
question_answering_v2 | Falconsai | 0.23 | Extractive QA
Resnet 18 | Microsoft | 0.30 | Image Classification
yolos-tiny | hustvl | 1.00 | Object Detection
Vision Perceiver Conv | Google | 2.64 | Image Classification
SFR-Embedding-Mistral | Salesforce | 5.22 | Sentence Similarity
yolos-base | hustvl | 7.98 | Object Detection
Gemma 7B | Google | 18.90 | Text Generation	T5 11b | Google | 27.79 | Text Classification
phi-4 | Microsoft | 28.74 | Text Generation	T5 11b | Google | 178.13 | Extractive QA
Mitsua Diffusion One | Mitsua | 186.81 | Image Generation
Mixtral 8x7B | Mistral | 615.39 | Text Generation
Stable Diffusion XL Base | Stability AI | 1,639.85 | Image Generation
Llama 3 70B | Meta | 1,719.66 | Text Generation
Qwen2.5 72B | Qwen | 1,869.55 | Text Generation
Command-R Plus | Cohere | 3,426.38 | Text Generation","According to the table titled ""Model | Organization | GPU Energy for 1k Queries (Wh) | Task"" in the provided document from luccioni2025c, the 'Command-R Plus' model has the highest GPU energy consumption for 1,000 queries, with a value of 3,426.38 Wh.","['zschache2025', 'han2024', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2508.14170 ']"
q018,2014,2014,year,"The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.",Quote,['stone2022'],['https://arxiv.org/pdf/2211.06318']
q019,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,is_blank,"['luccioni2025a', 'amazon2023']","['https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q020,520 MWh,520,MWh,Model | Organization | Energy Consumption (MWh) | GHG Emissions (tCO2e) BLOOM | Big Science | 520 | 30,The table in luccioni2025c directly states the energy consumption for the BLOOM model as 520 MWh.,"['luccioni2024', 'morrison2025']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2503.05804']"
q021,0.1%,0.1,percent,The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.,Quote,"['patterson2021', 'shen2024', 'wu2021a']","['https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2404.07413', 'https://arxiv.org/pdf/2111.00364']"
q022,8 experts,8,experts,"The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer. Such that the model has approximately two times the computation compared to a 1B model. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss is set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.","The document explicitly states that for the JetMoE-8B, the 'number of experts' is set to 8 for every layer in the MoE architecture.","['xia2024', 'shen2024']","['https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2404.07413']"
q023,Unable to answer with confidence based on the provided documents.,is_blank,second,is_blank,"The provided documents discuss execution time breakdowns and utilization for various models and GPUs, including BlackMamba on NVIDIA A40. However, they do not explicitly state the total execution time for a dense BlackMamba model with a batch size of 30 on an NVIDIA A40-48 GB GPU.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q024,28.22 zettaFLOPs,28.22,zettaFLOPs,The data ratio of FLM-101B is 53.5% : 46.5% for English and Chinese. The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,Quote,['li2025a'],['https://arxiv.org/pdf/2309.03852']
q025,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,"The documents discuss energy-efficient inference for text classification and local inference optimization, but none specifically report on a hardware processor used for *energy-efficient local inference in financial sentiment classification*.","['patterson2021', 'zschache2025']","['https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2508.14170 ']"
q026,88 models,88,models,"We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency. We identify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and tasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and ensuing carbon emissions.","The 'Power Hungry Processing' paper (Luccioni et al., 2024) explicitly states that their study analyzed 88 machine learning models.","['luccioni2024', 'luccioni2025c', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2506.15572']"
q027,Unable to answer with confidence based on the provided documents.,is_blank,multiplier,is_blank,is_blank,"['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q028,1.2 to 4 times larger,"[1.2,4]",multiplier,"We estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.",The text states that 'the ratio of total compute to final training run compute ranges from 1.2x to 4x'.,['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q029,196 MWh,196,MWh,"Table 1: We developed our models in five groups, based on parameter count and architecture: less than 1 billion, 1 billion, 7 billion, and 13 billion parameters...","The table in the supporting material 'morrison2025' provides 'Total MWh' for different model parameter groups. For the '7B' parameter group, which is the closest to a 6.1 billion parameter model, the 'Total MWh' is listed as 196.","['zschache2025', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2508.14170 ']"
q030,Jevons' Paradox,Jevons' Paradox,is_blank,"Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries.",The paper 'From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate' explicitly states that Jevons' Paradox explains how efficiency gains in AI may paradoxically lead to increased consumption and thus not result in net environmental benefits.,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548']
q031,4.2 – 6.6 billion cubic meters,4.2-6.6,billion cubic meters,"More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.",Quote,['li2025b'],['https://arxiv.org/pdf/2304.03271']
q032,FALSE,0,is_blank,schwartz2019,"The document states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost', directly contradicting the statement that Red AI is on the decline.",['schwartz2019'],['https://arxiv.org/pdf/1907.10597']
q033,21.92 days,21.92,days,Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.; Table 2 (within text): | Params (billion)| Tensor Parallel Size| Pipeline Parallel Size| Data Parallel Size| Number of GPUs| Batch Size| teraFLOP/s per GPU| FLOPs Utilization| |---|---|---|---|---|---|---|---| |16| 2| 1| 96| 192| 2304| 162| 51.90%| |51| 4| 2| 24| 192| 2304| 160| 51.30%| |101| 4| 4| 12| 192| 2160| 165| 52.88%|,"According to Table 4, the FLM-101B model required 1.01e5 GPU hours (101,000 GPU hours). Table 2 states that the 101 billion parameter model used 192 GPUs. To calculate the wall-clock time, divide the total GPU hours by the number of GPUs: 101,000 GPU hours / 192 GPUs = 526.04 hours. Converting hours to days: 526.04 hours / 24 hours/day = 21.918 days, which rounds to 21.92 days. The document explicitly states that FLM-101B was trained using a growth strategy.",['li2025a'],['https://arxiv.org/pdf/2309.03852']
q034,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,"The provided documents do not contain specific information about the GPU utilization percentage for a majority of Facebook's model experimentation workflows. While 'wu2021a' mentions an assumed average utilization of 30-60% over the 3- to 5-year lifetime for servers, this is a general figure for overall server utilization and not specific to GPU utilization in experimentation, nor does it refer to a 'majority' being over 80% capacity.","['chung2025', 'wu2021a']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2111.00364']"
q035,1287 MWh,1287,MWh,"GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse language model at the time [Bro20]. It achieves strong performance on many NLP datasets. A winner of the best paper award at NeurIPS 2020, this 8-month-old paper already has ~700 citations and made mainstream media headlines.13 It is now available for commercial use. One potential energy benefit of a large language model like GPT-3 is that they exhibit few-shot generalization, which means that they don’t need to be retrained for every new task like smaller models [Wan20]. Its estimated carbon emissions due to training are 552 tCO2e and its energy consumption is 1287 MWh.14 Table 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.",The document 'patterson2021' explicitly states that GPT-3's energy consumption due to training is 1287 MWh.,"['li2025b', 'patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q036,AI Energy Score,AI Energy Score,is_blank,"These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.",Quote,"['stone2022', 'han2024']","['https://arxiv.org/pdf/2211.06318', 'https://arxiv.org/pdf/2412.06288']"
q037,Unable to answer with confidence based on the provided documents.,is_blank,microseconds,is_blank,"The provided documents describe that figures in the paper 'xia2024' show execution time breakdowns and kernel-level latency. For example, 'The figure clearly shows that matrix multiplication (W1, W2, and W3) is the largest component of the MoE layer for both BlackMamba and Mixtral. As batch size and sparsity increase, so does computational demand, prolonging matrix multiplication latency.' (id: xia2024) mentions that matrix multiplication is the longest component. Also, 'High-level and layer-level latency breakdown results shown in Fig. 4 and 5 can be obtained by running: python3 mamba_latency.py ./profile_data/blackmamba/latency > mamba_latency_breakdown.txt' (id: xia2024) indicates that such data exists. However, the actual numerical values for execution times of specific kernels in the MoE layer for a dense BlackMamba model with a batch size of 30 on an NVIDIA A40-48 GB GPU are not presented within the text snippets of the retrieved documents. Without access to the figures (Fig. 4 and 5) or the data outputted by the mentioned script, the specific execution time for the longest kernel cannot be determined.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q038,2 experts,2,experts,"The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The document states that for JetMoE-8B, the 'top-k' value is set to 2 for every layer. This is also confirmed in Table 1.",['shen2024'],['https://arxiv.org/pdf/2404.07413']
q039,FALSE,0,is_blank,"schwartz2019: ""The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].""","The provided document states that the increase in computations from 2012 to 2018 was an estimated 300,000x, not 200,000x. Therefore, the statement is false.","['dodge2022', 'luccioni2025a', 'luccioni2023']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2302.08476']"
q040,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,The provided documents do not contain information about the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic.,"['li2025a', 'patterson2021']","['https://arxiv.org/pdf/2309.03852', 'https://arxiv.org/pdf/2104.10350']"
q041,Unable to answer with confidence based on the provided documents.,is_blank,data centers,is_blank,"The provided documents state that in 2023, 100% of the electricity consumed by Amazon's global operations, including all data centers, was matched with renewable energy sources. However, the documents do not specify the exact number of AWS data center *regions* that achieved this. It is a company-wide statement for all data centers, not a regional count.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q042,69 years,69,years,The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,"The field of Artificial Intelligence was officially established in 1956. To find its approximate age in 2025, calculate 2025 - 1956 = 69 years.","['stone2022', 'luccioni2025c', 'luccioni2025b']","['https://arxiv.org/pdf/2211.06318', 'https://arxiv.org/pdf/2504.00797', 'https://arxiv.org/pdf/2506.15572']"
q043,Neural Architecture Search (NAS),Neural Architecture Search,is_blank,"The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. In the case of the latter, they estimated that the NAS approach... could yield 626,155 pounds (284 metric tons) CO₂-equivalent GHG emissions (CO₂e), or about five times the emissions of a car during its lifetime, including fuel. ... Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.",The 'five cars' estimate from the 2019 Strubell et al. study is based on the energy consumption of Neural Architecture Search (NAS). This process is described as a 'large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.',['wu2021a'],['https://arxiv.org/pdf/2111.00364']
q044,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,is_blank,"['chung2025', 'samsi2024']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2505.06371']"
q045,Unable to answer with confidence based on the provided documents.,is_blank,samples,is_blank,"The provided documents discuss fine-tuning Mixtral and BlackMamba models and their maximum batch sizes on NVIDIA A40 (48GB) GPUs. However, the exact maximum batch size for BlackMamba with a sparse setup on the GSM8K dataset is not explicitly stated. A table-like structure provides values for 'CS' and 'MATH' datasets but not 'GSM8K'.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q046,Unable to answer with confidence based on the provided documents.,is_blank,GW,"At the end of 2023, Amazon had 270 rooftop solar projects at our facilities around the globe. We brought 50 new on-site solar energy systems online in 2023, for a total capacity of 58 MW. These on-site solar energy systems are estimated to generate 123,000 MWh annually
—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.The Baldy Mesa Solar and Storage Project in Adelanto, California (developed and operated by AES), represents one of the solar projects that we added to our portfolio that includes storage capacity.","The document states that Amazon added a solar and storage project and is planning to add more storage capacity in 2024 and beyond. However, it only provides the *solar* generation capacity for 2023 (58 MW) and does not specify the total energy *storage* capacity in gigawatts (GW) for that year.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q047,thousands of transatlantic flights,thousands,flights,"A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights and consume water equivalent to the annual drinking needs of millions of people.",The document states that the annual carbon emissions from GPT-4o inference are comparable to 'thousands of transatlantic flights'.,"['luccioni2023', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2302.08476']"
q048,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,is_blank,"['luccioni2025a', 'luccioni2025b', 'han2024']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2504.00797']"
q049,Unable to answer with confidence based on the provided documents.,is_blank,PUE,is_blank,"The provided documents discuss Power Usage Effectiveness (PUE) values for data centers in general (e.g., 1.58 for 2018, 1.59 for 2020, 1.1 for state-of-the-art facilities, or specific company PUEs like Meta's 1.08 in 2023), but none of them state a global average PUE specifically for 'AI-dedicated' data centers in 2023.","['li2025b', 'ebert2024', 'han2024']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2412.06288']"
q050,2 billion parameters,2B,parameters,"JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",Quote,['shen2024'],['https://arxiv.org/pdf/2404.07413']
q051,14 tCO2e,14,tCO2e,"Model | Organization | Energy Consumption (MWh) | GHG Emissions (tCO2e) OLMo 20M²⁶ | Ai2 | 0.8 | 0.3 CodeGen 350M⁶² | Salesforce | 71 | 6 Llama 7B⁶³ | Meta | 356 | 14 BLOOM¹¹ | Big Science | 520 | 30 T5¹⁶ | Google | 85.7 | 47 OLMo 2 13B²⁶ | Ai2 | 157 | 101 Gemma 2B + 9B⁶⁴ | Google | ? | 131 GPT-3¹⁶ | OpenAI | 1,287 | 552 Llama 4 Scout⁶⁵ | Meta | 3,500 | 1,354 Llama 3 70B⁶⁶ | Meta | ? | 1,900 Llama 3.1 405B³⁵ | Meta | ? | 8,930 Max/Min Variance: | | 4,375 | 29,767",Table,"['khan2025', 'rubei2025']","['https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2501.05899']"
q052,Unable to answer with confidence based on the provided documents.,is_blank,electric delivery vans,is_blank,"The document states that Amazon's U.S. fleet included 11,800 electric delivery vans from Rivian in 2023, ""up from more than 2,600 in 2022."" This indicates an increase of 9,200 Rivian vans in the U.S. fleet during 2023 (11,800 - 2,600 = 9,200). However, the question asks for the total number of vans ""added in total across 2022 and 2023."" The document does not provide a specific number for vans added during 2022, nor does it provide global figures for the number of vans added in 2022 and 2023. Therefore, the total number of electric delivery vans added across both years cannot be confidently determined.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q053,FALSE,0,is_blank,"Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The provided document explicitly states that operational environmental impacts include GHG emissions from energy sources used to power model training and deployment, including servers and data center cooling. This contradicts the statement in the question.","['chung2025', 'khan2025', 'morrison2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2503.05804']"
q055,Unable to answer with confidence based on the provided documents.,is_blank,Wh,"In contrast, large-scale and reasoning models such as o3, DeepSeek-R1 (DS), and DeepSeek-V3 (DS) exhibit substantially higher footprints.","The document mentions 'o3' as a large-scale reasoning model with substantially higher footprints for long queries, but only provides specific numerical values (water consumption and CO2e) for DeepSeek-R1, not for 'o3' itself. The energy consumption in Wh for 'o3' is not specified.","['chung2025', 'zschache2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2508.14170 ']"
q056,1956,1956,year,The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,Quote,['stone2022'],['https://arxiv.org/pdf/2211.06318']
q057,Unable to answer with confidence based on the provided documents.,is_blank,WUE,is_blank,"The provided documents mention Google's environmental reports for 2024 and general water consumption trends (e.g., a 20% uptick between 2021 and 2022, general scope-1 water efficiency values of 1.2 L/kWh for withdrawal and 1.0 L/kWh for consumption for data center operators including Google). However, none of the documents explicitly state the average Water Use Effectiveness (WUE) specifically for Google's AI-dedicated data centers in 2024.","['luccioni2025a', 'li2025b', 'morrison2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2503.05804']"
q058,TRUE,1,is_blank,"Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].",The provided text directly states that approximately 770 million people worldwide do not have access to a stable supply of electricity.,"['wu2021b', 'amazon2023', 'han2024']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2412.06288', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q059,Unable to answer with confidence based on the provided documents.,is_blank,joules per token,samsi2024,"The provided documents mention that 'Figure 4 shows energy costs for maximum generation length 512' for LLaMA 65B, and discusses 'energy per second (Watts)' and 'energy per tokens'. However, the specific numerical value for energy consumed per token in joules per token is not explicitly stated in the text. To provide an answer, I would need to analyze Figure 4, which is not provided as text.",['samsi2024'],['https://arxiv.org/pdf/2310.03003']
q060,15%,15,percent,"By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.",Quote,['wu2021a'],['https://arxiv.org/pdf/2111.00364']
q061,FALSE,0,is_blank,"The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale. Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—in fact, many of the emissions reductions on a global scale require individual, societal and political shifts.",The document explicitly states that 'The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed' and that 'Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding.',['luccioni2025c'],['https://arxiv.org/pdf/2506.15572']
q063,TRUE,1,is_blank,"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The document explicitly states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy'.","['patterson2021', 'wu2021a', 'luccioni2024']","['https://arxiv.org/pdf/2111.00364', 'https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2104.10350']"
q064,"$25,000",25000,USD,"Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.",Quote,"['cottier2024', 'schwartz2019']","['https://arxiv.org/pdf/1907.10597', 'https://arxiv.org/pdf/2405.21015']"
q065,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents describe the overall execution time breakdown, mentioning the MoE layer as the costliest, and various hardware and software characteristics. However, they do not provide specific data on the percentage of running time taken by the 'optimizer stage' during BlackMamba sparse fine-tuning with an NVIDIA A40-48GB GPU at a batch size of 1. Figures 4 and 5, which might contain such details, are not directly accessible in the text.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q066,83 MWh,83,MWh,luccioni2024,"Flan-T5-xxl uses 0.083 kWh for 1,000 queries. This means it uses 0.083 / 1000 = 0.000083 kWh per query. For 1 billion (1,000,000,000) queries per day, the total daily energy consumption would be 0.000083 kWh/query * 1,000,000,000 queries/day = 83,000 kWh/day. Converting this to MWh (1 MWh = 1000 kWh), we get 83,000 kWh / 1000 = 83 MWh.","['luccioni2025c', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2506.15572']"
q067,Unable to answer with confidence based on the provided documents.,is_blank,PUE,is_blank,"The provided documents mention global or US average PUE for years like 2018 and 2020, and a general average of 1.58, but none explicitly state the average global data center PUE for 2023. While one document references a Statista link titled 'Average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide' for 2025, the actual value for 2023 is not present in the provided text.","['luccioni2025a', 'ebert2024', 'han2024']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2412.06288']"
q068,Unable to answer with confidence based on the provided documents.,is_blank,wind turbines,is_blank,"The provided documents discuss Microsoft's general sustainability efforts, renewable energy purchases, and data center efficiency, often mentioning wind and solar power. However, none of the documents specify the exact number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. The information is not granular enough to answer this specific question.","['ebert2024', 'jegham2025']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2505.09598']"
q069,29–49%,"[29,49]",percent,"Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total.",Quote,['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q070,17 people,17,people,"The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.",The provided text explicitly states that the inaugural 2015 Study Panel was a 'seventeen-member Study Panel'.,['stone2022'],['https://arxiv.org/pdf/2211.06318']
q071,74%,74,percent,"Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.",The document explicitly states that the manufacturing carbon cost accounts for 74% of the total footprint of client devices.,"['amazon2023', 'wu2021a']","['https://arxiv.org/pdf/2111.00364', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q072,FALSE,0,is_blank,"jegham2025: ""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.""","The provided text offers a direct counter-example: ""GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes."" This shows that a smaller model (fewer parameters) can, under certain conditions (like older hardware), consume more energy than a larger model, disproving the statement that more parameters *always* lead to more energy consumption during inference.","['fernandez2025', 'luccioni2024']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2504.17674']"
q073,FALSE,0,is_blank,"Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future.",The 'stone2022' document explicitly states that the Study Panel 'found no cause for concern that AI is an imminent threat to humankind'.,['stone2022'],['https://arxiv.org/pdf/2211.06318']
q074,Unable to answer with confidence based on the provided documents.,is_blank,tCO2e,Public disclosures of inference-level energy and carbon data remain limited...,"The documents mention per-query energy estimates for OpenAI models (e.g., 0.34 Wh/query for GPT-4o) and discuss the general lack of transparency regarding AI's environmental impact. However, none of the provided documents offer a total number of OpenAI API requests for January 2024, nor a direct figure for total CO2e emissions from these requests for that specific month. Therefore, it is not possible to calculate or state the total metric tons of CO2 emitted by OpenAI's API requests in January 2024 with confidence.","['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q076,"Meta's Llama 3 family of models emitted 11,390 tCO2e during pre-training, which is over 40 times the 'five cars' estimate of 284 tCO2e.",11390,tCO2e,"Meta reports that their Llama 3 family of models emitted 11,390 tons CO₂e or over 40x the “five cars” estimate. [...] the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO₂-equivalent GHG emissions (CO₂e), or about five times the emissions of a car during its lifetime.","The document explicitly states the GHG emissions for Meta's Llama 3 family of models as 11,390 tCO2e during pre-training. It also defines the 'five cars' estimate as 284 tCO2e and directly compares the Llama 3 emissions as 'over 40x' that amount.","['khan2025', 'morrison2025']","['https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2503.05804']"
q077,2.9x,2.9,multiplier,Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.,Quote,['wu2021a'],['https://arxiv.org/pdf/2111.00364']
q079,Unable to answer with confidence based on the provided documents.,is_blank,miles,is_blank,is_blank,"['amazon2023', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q080,TRUE,1,is_blank,"The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.",The provided text explicitly states that AlphaGo 'beat the human Go champion in a five-game match'.,"['stone2022', 'schwartz2019']","['https://arxiv.org/pdf/2211.06318', 'https://arxiv.org/pdf/1907.10597']"
q081,Continuous batching,Continuous batching,is_blank,"To improve GPU utilization in LLM decoding, continuous batching is widely adopted [16, 20, 46]. By processing multiple inputs concurrently, the model parameters in GPU memory can be reused, making the workload more computation-intensive.","The document states that 'continuous batching is widely adopted' to 'improve GPU utilization' by 'processing multiple inputs concurrently', which aligns with dynamically replacing completed requests to reduce idle GPU time.","['griggs2024', 'jegham2025']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2505.09598']"
q082,60 H100 GPU hours,60,H100 GPU hours,The entire alignment process takes 60 H100 GPU hours.,"The document 'shen2024' explicitly states that 'The entire alignment process takes 60 H100 GPU hours', which includes the dDPO fine-tuning mentioned in the same context, and the question specifies the 'entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning'. Therefore, this value covers both processes.","['xia2024', 'shen2024']","['https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2404.07413']"
q083,25.3%,25.3,percent,"kim2025: ""Experiments on AWS GPU instances show that ... KV cache offloading saves up to 20.19% for offline workloads.""","InferSave aims for cost efficiency, often by optimizing KV cache offloading. The document states that KV cache offloading saves up to 20.19% for offline workloads. If InferSave achieves this saving compared to the Max-Performance policy (which prioritizes performance and does not explicitly account for such cost-saving offloading), then if InferSave's cost is C, Max-Performance's cost is C / (1 - 0.2019). To find how much more expensive Max-Performance is, we calculate ( (C / 0.7981) - C ) / C * 100 = (1 / 0.7981 - 1) * 100 = (1.25297 - 1) * 100 = 25.297%, which rounds to 25.3%.",['kim2025'],['https://arxiv.org/pdf/2504.11816']
q084,3416.8 g CO2eq,3416.8,g CO2eq,"luccioni2024: ""whereas the least efficient image generation model uses as much energy as 522 smartphone charges (11.49 kWh)"", ""all of our experiments were run in the same compute region (AWS’s us-west-2), which is based in Oregon and has an average carbon intensity of 297.6 grams of CO2eq per kWh4""","The document luccioni2024 states that the least efficient image generation model uses 11.49 kWh of energy for 1,000 inferences. The experiments were conducted in the AWS us-west-2 region (Oregon), which has a carbon intensity of 297.6 grams of CO2eq per kWh. To find the CO2eq emissions, multiply the energy consumption by the carbon intensity: 11.49 kWh * 297.6 gCO2eq/kWh = 3416.784 gCO2eq. Rounded to one decimal place, this is 3416.8 gCO2eq.","['luccioni2023', 'luccioni2024', 'amazon2023']","['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf', 'https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2302.08476']"
q085,"0.06 to 3,426.38 Wh","[0.06,3426.38]",Wh,"Model | Organization | GPU Energy for 1k Queries (Wh) | Task
bert-tiny-finetuned-squadv2	mrm8488	0.06	Extractive QA
GIST-all-MiniLM-L6-v2	avsolatorio	0.11	Sentence Similarity
dynamic_tinybert	Intel	0.21	Extractive QA
distilbert-imdb	lvwerra	0.22	Text Classification
question_answering_v2	Falconsai	0.23	Extractive QA
Resnet 18	Microsoft	0.30	Image Classification	yolos-tiny	hustvl	1.00	Object Detection
Vision Perceiver Conv	Google	2.64	Image Classification
SFR-Embedding-Mistral	Salesforce	5.22	Sentence Similarity
yolos-base	hustvl	7.98	Object Detection
Gemma 7B	Google	18.90	Text Generation	T5 11b	Google	27.79	Text Classification
phi-4	Microsoft	28.74	Text Generation	T5 11b	Google	178.13	Extractive QA
Mitsua Diffusion One	Mitsua	186.81	Image Generation
Mixtral 8x7B	Mistral	615.39	Text Generation
Stable Diffusion XL Base	Stability AI	1,639.85	Image Generation
Llama 3 70B	Meta	1,719.66	Text Generation
Qwen2.5 72B	Qwen	1,869.55	Text Generation
Command-R Plus	Cohere	3,426.38	Text Generation
Max/Min Variance:	57,106","The table in 'luccioni2025c' lists the 'GPU Energy for 1k Queries (Wh)' for various models. The minimum value in this column is 0.06 Wh (for 'bert-tiny-finetuned-squadv2') and the maximum value is 3,426.38 Wh (for 'Command-R Plus').","['zschache2025', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2508.14170 ']"
q086,FALSE,0,is_blank,We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist),"The document explicitly states that researchers do not believe a universal approach to AI ethics and sustainability can exist, and that there is no one-size-fits-all solution.",['luccioni2025b'],['https://arxiv.org/pdf/2504.00797']
q087,0.429 kg of CO2e/KWh,0.429,kg of CO2e/KWh,The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].,"The document 'patterson2021' explicitly states the gross carbon intensity of energy according to the U.S. average mix as 0.429 kg of CO2e/KWh, with a citation to USE21 (US Energy Information Administration, 2021).","['patterson2021', 'amazon2023', 'morrison2025']","['https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2503.05804', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q088,Hivemind,Hivemind,is_blank,"Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training. The advantage of Hivemind for geo-distributed training comes from cumulating different techniques, such as Delayed Parameter Updates [36], big-batch training [44] and aggressive communication quantization [16]. All of these combined reduce time and frequency of the communication rounds, which in turn makes training on heterogeneous devices and low-bandwidth networks possible. This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers?",The document 'erben2023' explicitly states that 'Hivemind is a PyTorch-based framework... that runs in a decentralized fashion' and is used to enable 'distributed spot training' across 'different data centers and cloud providers.',['erben2023'],['https://arxiv.org/pdf/2306.03163']
q089,usable transparency,usable transparency,is_blank,"Mandating transparency for already deployed AI systems can help establish audits, red teaming efforts and AI energy score ratings to raise users’ awareness around the impacts of the systems they use [29], contributing to what can be termed ""usable transparency"" [144].","The document introduces ""usable transparency"" as a term for increasing user awareness about the impacts of AI systems, aligning with the question's description of expanding transparency to include socio-technical aspects and environmental/societal footprints.","['luccioni2025c', 'luccioni2025b']","['https://arxiv.org/pdf/2506.15572', 'https://arxiv.org/pdf/2504.00797']"
q090,A traditional linear model using pre-trained sentence embeddings.,traditional linear model using pre-trained sentence embeddings,is_blank,The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.,"Quote from zschache2025, which discusses experiments on German public administration texts. It explicitly states that ""The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.""","['patterson2021', 'zschache2025', 'luccioni2025b']","['https://arxiv.org/pdf/2508.14170 ', 'https://arxiv.org/pdf/2504.00797', 'https://arxiv.org/pdf/2104.10350']"
q092,Lamina,Lamina,is_blank,"We build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation and solves the related challenges.","The document by Chen et al. states, ""We build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation."" This directly answers the question.","['chen2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2405.01814']"
q093,Unable to answer with confidence based on the provided documents.,is_blank,parameters,is_blank,The provided documents mention T5 as a large NLP model but do not specify the number of parameters for the largest T5 model.,"['chen2024', 'li2025a', 'schwartz2019']","['https://arxiv.org/pdf/2405.01814', 'https://arxiv.org/pdf/1907.10597', 'https://arxiv.org/pdf/2309.03852']"
q094,8 billion parameters,8,billion parameters,Table with P_total for JetMoE-8B,The table under 'Hyper-parameters' (id 1.1114617586135864) in the document 'shen2024' lists 'P_total' for the JetMoE-8B model as '8B'.,"['shen2024', 'schwartz2019']","['https://arxiv.org/pdf/2404.07413', 'https://arxiv.org/pdf/1907.10597']"
q095,20%,20,percent,"Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].",The document explicitly states that 'Google observed a 20% uptick' in water consumption between 2021 and 2022.,"['luccioni2025a', 'amazon2023', 'han2024']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2412.06288', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q096,Carbon Intensity,Carbon Intensity,is_blank,"MetricUnitDefinitionReferenceCarbon Dioxide Equivalent (CO2e)Metric tons (tCO2e)A measure of greenhouse gases expressed as CO2 equivalentIPCC, GHG ProtocolCarbon IntensitygCO2/kWhCO2 emissions per unit of electricity consumedInternational Energy Agency",The document 'khan2025' explicitly defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed'.,"['khan2025', 'patterson2021']","['https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2104.10350']"
q097,52.88%,52.88,percent,"li2025a, Table from snippet: |Params (billion)| Tensor Parallel Size| Pipeline Parallel Size| Data Parallel Size| Number of GPUs| Batch Size| teraFLOP/s per GPU| FLOPs Utilization| |---|---|---|---|---|---|---|---| |16| 2| 1| 96| 192| 2304| 162| 51.90%| |51| 4| 2| 24| 192| 2304| 160| 51.30%| |101| 4| 4| 12| 192| 2160| 165| 52.88%|","The provided table in li2025a details the training configurations for different FLM model sizes. For the FLM-101B model, which represents the final growth stage, the 'FLOPs Utilization' is listed as 52.88%.",['li2025a'],['https://arxiv.org/pdf/2309.03852']
q098,$40M,40000000,USD,We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.,Quote,"['strubell2019', 'cottier2024']","['https://arxiv.org/pdf/1906.02243', 'https://arxiv.org/pdf/2405.21015']"
q099,810x,810,multiplier,"Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800×. Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7×. In addition to caching, deploying LM across GPU-based specialized AI hardware unlocks an additional 10.1× energy efficiency improvement. Finally, algorithmic optimizations provide an additional 12× energy efficiency reduction.","The document explicitly states that ""Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×."" It also breaks down these optimizations: platform-level caching (6.7x), GPU acceleration (10.1x), and algorithmic optimization (12x). Multiplying these individual factors (6.7 * 10.1 * 12 = 812.04) confirms the approximate 810x reduction in operational carbon footprint compared to a CPU server baseline.","['patterson2021', 'wu2021a']","['https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2111.00364']"
q100,0.64,0.64,multiplier,erben2023,"The document states that for the four continents experiment (C-4), NLP experienced a 36% drop in throughput compared to the intra-zone A-4 runs. This means it achieved 100% - 36% = 64% of the local throughput, or 0.64 as a fraction.","['erben2023', 'patterson2021']","['https://arxiv.org/pdf/2306.03163', 'https://arxiv.org/pdf/2104.10350']"
q101,3.5 billion liters,3.5,billion liters,"By the end of 2023, AWS had invested in 15 water replenishment activities, returning water to the communities where it operates across 12 of our global infrastructure regions. These investments have expanded AWS replenishment activities to three new countries, bringing the total to 10: Australia, Brazil, India, Indonesia, Ireland, South Africa, Spain, Sweden, the UK, and the U.S. These activities help recharge groundwater, build wetlands, improve water quality, and reduce water loss in utility systems. In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.",The document explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023.,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q103,TRUE,1,is_blank,Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,Quote,['rubei2025'],['https://arxiv.org/pdf/2501.05899']
q104,Unable to answer with confidence based on the provided documents.,is_blank,GPUs,is_blank,"The provided documents do not contain information on the number of data center GPUs NVIDIA shipped in 2024. One document mentions 3.76 million data-center GPUs shipped in 2023, but no equivalent figure is available for 2024.","['luccioni2025a', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.16548']"
q107,44%,44,percent,"Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.",Quote,['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q108,1.08,1.08,PUE,han2024,The document states that 'Meta’s 2023 PUE of 1.08'.,"['luccioni2025a', 'ebert2024']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2501.16548']"
q109,ETAIROS,ETAIROS,is_blank,"From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]",Quote,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797']
q110,$30M,30,million USD,We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.,Quote,['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q111,FALSE,0,is_blank,"For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.; The current AI Act overlooks key environmental factors related to AI systems; Inclusion of environmental risks in assessments: Clarify that environmental risks must be part of risk assessments for AI providers.; Environmental Risk Assessments: Providers should be required to include environmental risks in their risk assessments. The language in Art. 1(1) and Recital 1, as well as Art. 9 and 55 AI Act should be clarified to reflect this.","The provided documents, particularly 'ebert2024', state that the authors *argue* for and *propose* the inclusion of environmental risks in mandatory risk assessments under the AI Act, and that the Act *overlooks* key environmental factors. Phrases like 'We argue that these measures should also consider environmental risks' and 'Providers should be required to include environmental risks' indicate that this is a policy recommendation or an interpretation, not an explicit current requirement within the Act itself. Therefore, the statement that the AI Act *requires* providers to include environmental risks is false.",['ebert2024'],['https://arxiv.org/pdf/2410.06681']
q112,9 µg/m³,9,µg/m³,the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9 μg/m3,Quote,"['wu2021a', 'han2024']","['https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2111.00364']"
q113,115 books,115,books,"For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].",Quote,"['luccioni2025a', 'amazon2023']","['https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q114,200x,200,multiplier,"The public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities.",The document explicitly states that the per-household health burden in disadvantaged communities could be 200 times more than in less-impacted communities.,['han2024'],['https://arxiv.org/pdf/2412.06288']
q115,Unable to answer with confidence based on the provided documents.,is_blank,Wh,is_blank,"The provided documents do not contain information about the energy consumption of the DS Llama 70B model for inference on the FKTG dataset. The tables in the documents list energy consumption for datasets such as 'news', 'yelp', 'tomatoes', and 'emotion', but not 'FKTG'.","['samsi2024', 'zschache2025']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2508.14170 ']"
q116,Unable to answer with confidence based on the provided documents.,is_blank,parameters,is_blank,"The provided documents refer to a 2022 paper by Dodge et al. that measured electricity and carbon emissions for training language models, but they do not specify the total number of parameters for any large language model analyzed in that paper.","['samsi2024', 'zschache2025']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2508.14170 ']"
q117,Jevons' Paradox,Jevons' Paradox,is_blank,"jegham2025: ""As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand."" luccioni2025a: ""Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries"" luccioni2025a: ""This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.""","The documents consistently refer to 'Jevons' Paradox' as the phenomenon where improvements in efficiency lead to increased usage and, paradoxically, an overall increase in resource consumption rather than a decrease.","['luccioni2025a', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.16548']"
q118,Unable to answer with confidence based on the provided documents.,is_blank,multiplier,is_blank,"The documents provide the energy consumption for GPT-3 training (1287 MWh), but a direct energy consumption value in MWh for Meena's training runs is not explicitly stated. One document mentions Meena's carbon footprint equivalence to miles driven, but this cannot be directly converted to MWh without additional information.","['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q119,1.35 kWh,1.35,kWh,"Text-based tasks are, all things considered, more energy-efficient than image-based tasks, with image classification requiring less energy (median of 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH) requiring more than text classification (0.0023 kWh).","The provided text from the 2024 Luccioni et al. study, in the context of discussing findings from 'Table 2', states that 'image generation (1.35 kWh)' is the energy required for 1,000 inferences.","['chung2025', 'luccioni2024', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2311.16863']"
q120,"36,156 lbs",36156,lbs,"Table 1 in Strubell et al. 2019 states 'American life, avg, 1 year & 36,156'",The document explicitly states the CO2e for an average American life in one year.,"['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q121,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,"For example, several counties in West Virginia are among the most affected, because many coal-fired power plants in West Virginia are supplying electricity to data centers in the neighboring state of Virginia [4,5]. The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31,103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value.",The document mentions that 'several counties in West Virginia are among the most affected' but does not specify which particular county in West Virginia is projected to have the highest per-household health cost in 2030.,['han2024'],['https://arxiv.org/pdf/2412.06288']
q122,Unable to answer with confidence based on the provided documents.,is_blank,multiplier,is_blank,"The provided documents mention a general reduction in carbon emissions (e.g., 'up to 45% after optimization' in khan2025) but do not provide specific data or a multiplier for 'Mistral-small' for the financial sentiment classification task after optimization. While 'khan2025' refers to 'Table III' for these reductions, the table itself is not present in the provided text.","['li2025a', 'khan2025', 'luccioni2025c']","['https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2506.15572', 'https://arxiv.org/pdf/2309.03852']"
q123,Unable to answer with confidence based on the provided documents.,is_blank,kWh,is_blank,"The 'Power Hungry Processing' study (luccioni2024) mentions that it estimated training and fine-tuning costs for BLOOMz models and presented these numbers in 'Table 5'. However, 'Table 5' itself, which would contain the combined training and fine-tuning energy costs for BLOOMz-7B, is not provided in the extracted document snippets. The only table present lists energy consumption for '1,000 inferences', which is not the requested training and fine-tuning cost.","['luccioni2024', 'patterson2021']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2104.10350']"
q125,101 billion parameters,101,billion parameters,Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference. |Model| GPT-3 | Gopher | PaLM | GLM-130B | Llama-2 | FLM-101B |Params| 175B | 280B | 540B | 130B | 70B | 101B,Table 4 in li2025a explicitly states that the FLM-101B model has 101B parameters.,"['li2025a', 'schwartz2019']","['https://arxiv.org/pdf/1907.10597', 'https://arxiv.org/pdf/2309.03852']"
q126,1.96 billion inferences,1960000000,inferences,"morrison2025, Table 1; luccioni2024","According to Morrison et al. (2025), a 7B parameter model (OLMo 7B), which is comparable in size to a 6.1B model, requires 196 MWh (196,000 kWh) for a full training run. Luccioni et al. (2024) report that the BLOOMz-7B model consumes 1.0 × 10−4 kWh per inference. Therefore, to match the training energy cost of 196,000 kWh, approximately 196,000 kWh / (1.0 × 10−4 kWh/inference) = 1,960,000,000 inferences are needed.",['luccioni2024'],['https://arxiv.org/pdf/2311.16863']
q127,754.66 kWh,754.66,kWh,"In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.",Quote,"['chung2025', 'zschache2025', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2508.14170 ']"
q128,Unable to answer with confidence based on the provided documents.,is_blank,inferences,"The document states that the total energy used or carbon emitted during training was not reported for BLOOMz models in their original papers, but that these numbers were estimated and added to fine-tuning costs. It further states, 'We present these numbers, alongside the average energy consumption per inference, in Table 5.' However, the provided 'Table 5' (the markdown table in the document snippet) only lists energy consumption per 1,000 inferences, not the initial energy cost of training and fine-tuning. Therefore, the information required to calculate the total number of inferences to reach parity is not available in the provided text.","The document explicitly states that the total energy cost of training and fine-tuning for BLOOMz models was estimated and meant to be presented alongside inference costs in Table 5. However, the provided Table 5 only contains the energy consumption per 1,000 inferences (0.104 kWh for BLOOMz-7B) and not the total training and fine-tuning energy cost. Without the total training and fine-tuning energy cost, it is impossible to calculate how many inferences are needed to reach parity.",['luccioni2024'],['https://arxiv.org/pdf/2311.16863']
q129,FKTG-dataset,FKTG-dataset,is_blank,"The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).",The document explicitly states that the data from the German nuclear waste site selection process was published as the 'FKTG-dataset'.,"['chung2025', 'zschache2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2508.14170 ']"
q130,Unable to answer with confidence based on the provided documents.,is_blank,liters,is_blank,"The provided documents discuss water consumption related to various LLMs, including Llama models, but none give a specific total annual freshwater consumption value for Meta's Llama 3 inference serving clusters in 2024. Information available includes per-query water consumption, water consumption for model training/development (for a series of models, not specifically Llama 3 inference), or overall water efficiency (L/kWh) for Meta's entire data center fleet in a different year (2023), without the corresponding total energy consumption for Llama 3 inference in 2024 to make a calculation.","['chung2025', 'rubei2025', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.05899']"
q131,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents discuss NVIDIA GPU manufacturing, embodied carbon, and the mining of rare earth metals, but none of them contain information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Luccioni et al. (2025a) explicitly states that it is ""impossible to carry out a full life cycle analysis of the hundreds of thousands of GPUs that are designed and manufactured each year"" due to lack of granular data from manufacturers like TSMC.","['luccioni2025a', 'jegham2025', 'morrison2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2503.05804']"
q132,Unable to answer with confidence based on the provided documents.,is_blank,passengers,is_blank,"The provided documents state the CO2e for the Evolved Transformer NAS but do not contain information on the CO2e emissions for a round trip flight between San Francisco and New York for a single passenger, which is needed to make the comparison.",['patterson2021'],['https://arxiv.org/pdf/2104.10350']
q133,83.5%,83.5,percent,"By the first quarter of 2025, over 70% of notable models provided no environmental impact disclosures... For example, according to May 2025 data from the API platform OpenRouter, these undisclosed models accounted for 83.5% of total LLM token usage.",Quote,"['khan2025', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2504.06307']"
q134,1,1,A100_80GB_GPU,Table 2: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.,"The document categorizes LLMs into hardware classes based on model size. LLaMA-13B falls into the 'Micro' class (7–20B parameters), for which the document assigns 1 GPU for deployment on A100 systems. A 13 billion parameter model at FP16 precision would require approximately 26 GB of memory (13B * 2 bytes/parameter), which fits within a single 80GB A100 GPU.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q136,Unable to answer with confidence based on the provided documents.,is_blank,metric tons,is_blank,"The provided documents refer to Figure 2 in 'Measuring the Carbon Intensity of AI in Cloud Instances' (dodge2022) for the emissions of a 6.1 billion parameter Transformer (6B Transf.). The text states that 'the 6 billion parameter LM is only trained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here.' However, the actual numerical values for the reported emissions (either min, max, or average) from Figure 2 are not available in the provided text. Without access to the visual information in Figure 2, it is impossible to determine the estimated range of CO2 emissions for either the partial or the complete training run.","['dodge2022', 'luccioni2023', 'morrison2025']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2302.08476']"
q137,Unable to answer with confidence based on the provided documents.,is_blank,tCO2e,is_blank,is_blank,"['li2025a', 'dodge2022', 'patterson2021']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2309.03852']"
q138,24%,24,percent,Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.,Quote,['griggs2024'],['https://arxiv.org/pdf/2404.14527']
q140,4.63 USD per hour,4.63,USD per hour,| H100 | H20 | TPU v6e [7] |---|---|---|| BF16 TFLOPs | 989 | 148 | 918 | Memory capacity | 80 GB | 96 GB | 32 GB | Memory bandwidth | 3.35 TB/s | 4.0 TB/s | 1.64 TB/s | Power rating | 700 W | 400 W | unlisted | Inter-chip bandwidth | 450 GB/s | 450 GB/s | 448 GB/s | Network bandwidth | 400 Gbps | 400 Gbps | 200 Gbps | Price per chip [2] | $11.06/hr | $4.63/hr * | $2.70/hr,"The table in Chen et al. (2024) lists the 'Price per chip' for H20 as $4.63/hr. It should be noted that the document's year is 2024, not 2025 as in the question, but it provides the exact information requested.","['cottier2024', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2405.21015']"
q141,FALSE,0,is_blank,"Despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant information.","The document explicitly states that a very small percentage of authors were willing to share relevant information, indicating that obtaining data often requires direct contact and is not typically automatic or readily available without outreach.","['luccioni2023', 'luccioni2025c']","['https://arxiv.org/pdf/2506.15572', 'https://arxiv.org/pdf/2302.08476']"
q142,Unable to answer with confidence based on the provided documents.,is_blank,percent,We show the results in Table 2. It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.,"The document states that the 'total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.' This indicates a highly variable cost and refers specifically to 'training data center locations,' not the overall 'data centers' total electricity cost' for all U.S. data centers in 2023. Without access to 'Table 2' or a broader average, a confident, specific percentage cannot be provided for all data centers.",['han2024'],['https://arxiv.org/pdf/2412.06288']
q143,Unable to answer with confidence based on the provided documents.,is_blank,A100_80GB_GPU,is_blank,"The provided documents mention NVIDIA A100 80GB GPUs and LLaMA-7B inference, but they do not specify the model size of LLaMA-7B in a way that allows calculating the minimum number of A100 80GB GPUs required for inference without compression or quantization.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q144,TRUE,1,is_blank,"Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization... Table III shows significant reductions in carbon emissions across all models, with some achieving up to 45% after optimization.",The document explicitly states that quantization can reduce carbon emissions by up to 45%.,"['khan2025', 'zschache2025']","['https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2508.14170 ']"
q145,75 answers,75,answers,"As we discussed in Section 3, despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant",The document states that 'only 15% of authors from our initial sample of 500 were willing to share relevant' information. Calculating 15% of 500 gives 75.,"['luccioni2023', 'luccioni2024']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2302.08476']"
q147,3.33 USD per hour,3.33,USD per hour,"Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","Calculated by dividing the total training budget ($100,000) by the total H100 GPU hours (30,000 hours).","['shen2024', 'han2024']","['https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2404.07413']"
q148,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents state that the health cost for training a Llama-3.1 scale model in Iowa is $2.5 million. However, the electricity cost for training this model in Iowa is not provided, making it impossible to calculate the requested percentage.","['morrison2025', 'han2024']","['https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2503.05804']"
q149,1.25T tokens,1.25T,tokens,"Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.",Quote,['shen2024'],['https://arxiv.org/pdf/2404.07413']
q150,Unable to answer with confidence based on the provided documents.,is_blank,projects,is_blank,"The documents mention '513 Global renewable energy projects' and 'our Moray West offshore wind farm in Scotland' (which is in the UK) as of January 2024, but do not provide a specific total number of announced Amazon Renewable Energy Projects in the United Kingdom.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q151,55.7%,55.7,percent,Table 1: Amazon Workforce (All Levels),Table 1 in the Amazon 2023 Sustainability Report shows the percentage of men in Amazon's workforce (all levels) in 2023 was 55.7%.,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q152,99%,99,percent,Apple reports that its supply chain accounts for 99% of its total water footprint [23].,Quote,"['wu2021b', 'li2025b']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2304.03271']"
q154,Unable to answer with confidence based on the provided documents.,is_blank,seconds,is_blank,"The documents describe an analytical model for estimating maximum batch size and throughput for Mixtral and BlackMamba fine-tuning, including discussions on execution time breakdown. However, specific execution time values for a sparse BlackMamba model with a batch size of 84 on an NVIDIA A40-48GB GPU are not provided. A table in the documents shows the maximum batch size for a sparse BlackMamba model as 20 for the CS dataset and 8 for the MATH dataset, suggesting that a batch size of 84 might exceed supported capacities or was not evaluated and reported.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q155,granularity metric,granularity,is_blank,"This paper analyzes multi- and hybrid-cloud training in a decentralized fashion on spot instances. We define the lower bounds of model sizes that can be scaled cost-efficiently using the granularity metric to estimate their suitability for distributed training in low-bandwidth, high-latency situations. We show that training on multiple cloud providers and four continents still scales with additional compute resources.","The document explicitly states: ""We define the lower bounds of model sizes that can be scaled cost-efficiently using the granularity metric to estimate their suitability for distributed training in low-bandwidth, high-latency situations."" It also links granularity to ""an almost equal part in communication and calculation"" for NLP tasks across continents.",['erben2023'],['https://arxiv.org/pdf/2306.03163']
q156,Unable to answer with confidence based on the provided documents.,is_blank,times,is_blank,"The provided documents mention that Microsoft employees have fought against the company's oil contracts and that Microsoft's AI push imperils climate goals due to increased carbon emissions. Specifically, references are made to articles like 'Microsoft’s ambitious climate goal forgets about its oil contracts' and 'Microsoft’s Hypocrisy on AI'. However, none of the retrieved text snippets contain the specific numerical comparison of a single deal with Exxon Mobil to the company's yearly carbon removal targets.","['wu2021b', 'luccioni2025a', 'luccioni2025b']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2504.00797']"
q157,Water withdrawal,Water withdrawal,is_blank,"Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].",Quote,"['li2025b', 'amazon2023']","['https://arxiv.org/pdf/2304.03271', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q159,Every five years,5,years,"As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.",Quote,['stone2022'],['https://arxiv.org/pdf/2211.06318']
q160,25 devices,25,devices,"At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The text explicitly states 'every US household has an average of 25 connected devices' and cites 'Deloitte, 2021', which matches the question's criteria for the year and types of devices.","['wu2021b', 'luccioni2025a']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2501.16548']"
q161,0.8 to 3500 MWh,"[0.8,3500]",MWh,"patterson2021, luccioni2025c, morrison2025","From luccioni2025c, OLMo 20M was trained with 0.8 MWh and Llama 4 Scout was trained with 3,500 MWh. Other models mentioned in patterson2021 and morrison2025 fall within this range (e.g., GShard-600B at 24 MWh, Switch Transformer at 179 MWh, GPT-3 at 1287 MWh, BLOOM at 520 MWh).","['zschache2025', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2508.14170 ']"
q162,FALSE,0,is_blank,"IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.",The provided text explicitly states that IBM’s Watson program 'beat human contenders to win the Jeopardy challenge in 2011'.,"['wu2021b', 'shen2024', 'stone2022']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2211.06318', 'https://arxiv.org/pdf/2404.07413']"
q163,10-50 queries,10-50,queries,"Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].",Quote,"['li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598']"
q165,6.681 score,6.681,score,"| Model | MT-Bench Score |
|---|---|
| GPT-4 | 9.014 |
| GPT-3.5-turbo | 7.995 |
| Claude-v1 | 7.923 |
| JetMoE-8B-chat | 6.681 |
| Llama-2-13b-chat | 6.650 |
| Vicuna-13b-v1.3 | 6.413 |
| Wizardlm-13b | 6.353 |
| Llama-2-7b-chat | 6.269 |
","The table titled 'MT-Bench Score' in the document lists the MT-Bench scores for various models, including JetMoE-8B-chat and Llama-2-13b-chat. JetMoE-8B-chat achieved a score of 6.681, while Llama-2-13b-chat achieved 6.650, indicating that JetMoE-8B-chat surpassed it.",['shen2024'],['https://arxiv.org/pdf/2404.07413']
q167,10 to 50 responses,"[10,50]",responses,"""Additionally, GPT-3 needs to -drink- (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed."", ""More specifically, we consider a medium-sized request, each with approximately=800 words of input and 150 - 300 words of output [30]. """,The provided text states that one 500ml bottle of water is consumed for roughly 10 to 50 medium-length responses of GPT-3.,"['li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598']"
q168,77%,77,percent,"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.",Quote,['griggs2024'],['https://arxiv.org/pdf/2404.14527']
q169,2 A100 80GB GPUs,2,A100_80GB_GPUs,LLaMA-65B: 130.1 GB (chen2024); A100-80G: 80 GB Memory (griggs2024),"The LLaMA-65B model has a size of 130.1 GB (chen2024). An NVIDIA A100-80G GPU has 80 GB of memory (griggs2024). To run the LLaMA-65B model, you would need 130.1 GB / 80 GB per GPU = 1.62625 GPUs. Since GPUs cannot be fractional, at least 2 A100 80GB GPUs are required.","['xia2024', 'griggs2024', 'chen2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2405.01814', 'https://arxiv.org/pdf/2408.04693']"
q171,"More than 10,000 round trips",10000,round trips,"Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.",Quote,"['luccioni2025c', 'han2024']","['https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2506.15572']"
q172,80-90%,"[80,90]",percent,"luccioni2024: According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28]. [28] George Leopold. 2019. AWS to Offer NVIDIA’s T4 GPUs for AI Inferencing.","According to a 2019 statement related to AWS offering NVIDIA T4 GPUs for AI inferencing, inference processing was estimated to account for 80-90% of the total ML cloud computing demand.","['xia2024', 'fernandez2025', 'cottier2024']","['https://arxiv.org/pdf/2405.21015', 'https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2504.17674']"
q173,178.97 kg CO2eq,178.97,kg CO2eq,luccioni2024,"The document 'Power Hungry Processing:⚡Watts⚡Driving the Cost of AI Deployment?' (luccioni2024) states: 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.'","['luccioni2023', 'amazon2023']","['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf', 'https://arxiv.org/pdf/2302.08476']"
q174,FALSE,0,is_blank,"LLMs and VLMs consume significantly less power than the GPU's TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU's compute resources. VLMs show slightly higher power consumption than LLMs due to its additional modality encoder, which is compute-intensive. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. Solid bars are energy measurements, whereas dimmed bars behind each solid bar are estimations based on the GPU’s TDP, with numbers showing the ratio of overestimation. Note the log scale Y-axis.","The document explicitly states that estimations based on GPU's TDP lead to 'overestimation' for many models, particularly LLMs, because they don't fully utilize the GPU's compute resources. This indicates that using TDP as a direct estimate of energy consumption is not reliable or accurate.",['chung2025'],['https://arxiv.org/pdf/2505.06371']
q175,FALSE,0,is_blank,"For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The document explicitly states that 'GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries'. This contradicts the statement in the question, making it False.","['chung2025', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598']"
q176,Unable to answer with confidence based on the provided documents.,is_blank,queries/sec,is_blank,"The provided documents discuss throughput for Mixtral models, including a table for A40 GPU (xia2024, Table II). While A100-40GB is mentioned as a GPU used for validation (xia2024, score 1.310858964920044), and figures are described that show throughput for different batch sizes, the specific ground truth throughput for a dense Mixtral-CS model on an A100-40GB GPU at a batch size of 1 queries/sec is not explicitly provided in the text or tables.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q177,FALSE,0,is_blank,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.","The document explicitly states that the direct release of environmental information peaked in 2022 and then experienced a ""notable reversal"" and ""dramatically reducing"" trend, with the majority of models having ""no disclosure"" by Q1 2025, which contradicts the statement that the trend continued to increase.",['luccioni2025c'],['https://arxiv.org/pdf/2506.15572']
q178,7.516 USD per hour,7.516,USD per hour,"We calculate this by comparing RunPod
’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.",The document 'griggs2024' explicitly states the calculation for the normalized on-demand hourly price for an H100 GPU: (4.69/2.29) × 3.67 = $7.516.,"['griggs2024', 'cottier2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2405.21015']"
q179,Unable to answer with confidence based on the provided documents.,is_blank,liters of water,"While no official information is available on the resource consumption, some subsequent models like GPT-4 could consume substantially more energy and water than GPT-3 for processing the same request [31, 32].","The provided documents discuss water consumption in data centers and for training/inference of various AI models, including some estimates for GPT-3 and GPT-4o inference. However, there is no specific information or data provided that quantifies the liters of water used for cooling during OpenAI's GPT-4 *training run*. The documents explicitly state that official information on resource consumption for models like GPT-4 is unavailable and that the data center industry suffers from a lack of transparency regarding water usage.","['morrison2025', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2503.05804']"
q180,$7.34,7.34,USD per hour,Table 1 in griggs2024,"According to Table 1 in the griggs2024 document, the on-demand price for a single NVIDIA A100-80G (SXM) GPU is $3.67 per hour. To run the model using 2 NVIDIA A100 GPUs, the hourly cost would be 2 * $3.67 = $7.34.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q181,Unable to answer with confidence based on the provided documents.,is_blank,multiplier,wu2021a,"While 'wu2021a' mentions a '1000x model size increase for GPT3-based language translation tasks', it does not specify the corresponding BLEU score increase, nor does it provide a relationship or graph from which to infer the model size needed for a BLEU score increase from 5 to 40.","['luccioni2023', 'strubell2019', 'patterson2021']","['https://arxiv.org/pdf/1906.02243', 'https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2302.08476']"
q182,"Approximately 713,489 miles",713489,miles,"patterson2021: '[Str19] estimates the CO2e for the neural architecture search (NAS) to find the more-efficient Evolved Transformer architecture done by [So19] at Google as 626,155 pounds (284 tCO2e).'; dodge2022: 'one mile driven is estimated to emit 3.98 × 10−4 metric tons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline)'","Strubell et al. estimated the CO2e for the neural architecture search (NAS) of the Evolved Transformer at 626,155 pounds. A recent study estimates that one mile driven by an average US passenger vehicle emits 3.98 × 10−4 metric tons of CO2e. Converting metric tons to pounds (1 metric ton = 2204.62 lbs), one mile driven is approximately 0.8776 lbs of CO2e. Dividing the total estimated CO2e (626,155 lbs) by the CO2e per mile (0.8776 lbs/mile) yields approximately 713,489 miles.","['dodge2022', 'patterson2021']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2104.10350']"
q183,"60,609.6 MWh",60609.6,MWh,luccioni2024,"The BLOOMz-7B model consumes 1.0 × 10−4 kWh per inference. If 606,096 downloads each result in 1 million inferences, the total number of inferences is 606,096 * 1,000,000 = 606,096,000,000 inferences. The total energy consumed in kWh is 606,096,000,000 inferences * (1.0 × 10−4 kWh/inference) = 60,609,600 kWh. Converting this to MWh (1 MWh = 1000 kWh), the total energy is 60,609,600 / 1000 = 60,609.6 MWh.",['luccioni2024'],['https://arxiv.org/pdf/2311.16863']
q184,"30,000 H100 GPU hours",30000,H100 GPU hours,"Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.",Quote,"['xia2024', 'chung2025', 'strubell2019']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/1906.02243', 'https://arxiv.org/pdf/2408.04693']"
q185,1 billion dollars,1,billion USD,The trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027,"The document explicitly states that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027, based on the trend of growing development costs.",['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q186,Unable to answer with confidence based on the provided documents.,is_blank,FLOPS,is_blank,"The provided documents discuss the cost of training GPT-3, GPU hours, and energy consumption, and even mention FLOPs for 'smaller versions of GPT-3' (4.5e22 FLOPs in cottier2024) but none of the provided text explicitly states the *total* number of floating point operations (FLOPs) used to train the full GPT-3 model as published by OpenAI.","['li2025a', 'cottier2024']","['https://arxiv.org/pdf/2405.21015', 'https://arxiv.org/pdf/2309.03852']"
q187,5 NVIDIA V100 32GB GPUs,5,V100_32GB_GPUs,"chen2024: Table 3 lists LLaMA-65B model size as 130.1 GB. zschache2025: Table in section 6.2 states ""4 x NVIDIA V100 (32GB)"" under GPU per node, indicating a single V100 GPU has 32GB memory.","The LLaMA-65B model size is 130.1 GB (chen2024, Table 3). Each NVIDIA V100 GPU has 32GB of memory (zschache2025, Table in section 6.2). To run the model, the total memory required is 130.1 GB. The number of GPUs needed is 130.1 GB / 32 GB/GPU = 4.065625. Since a partial GPU cannot be used, we round up to the next whole number, which is 5 GPUs.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q188,28.22 zettaFLOPs,28.22,zettaFLOPs,"li2025a: | Model | Cost (zettaFLOPs) | Average | ARC | HellaSwag | MMLU | TruthfulQA |
|-------|-------------------|---------|-----|----------|-------|-------------|
| Llama-2 (13B) | 201.37 (±28.77) | 58.66 | 59.39 | 82.13 | 55.77 | 37.38 |
| Llama-2 (7B) | 106.60 (±15.23) | 54.32 | 53.07 | 78.59 | 46.87 | 38.76 |
| Llama (13B) | 94.81 (±13.54) | 56.08 | 56.23 | 80.93 | 47.67 | 39.48 |
| Llama (7B) | 49.54 (±7.08) | 49.72 | 51.02 | 77.82 | 35.71 | 34.33 |
| GLM-130B | 210.80 | 48.11 | 42.15 | 67.91 | 42.59 | 39.80 |

| FLM-101B | 28.22 | 43.94 | 39.76 | 66.23 | 28.30* | 41.47 |","The table in li2025a explicitly lists the computational cost for FLM-101B as 28.22 zettaFLOPs. The paper describes FLM-101B as the result of a growth strategy, where models of 16B, 51B, and 101B parameters are trained sequentially, implying the 101B is the final stage.","['li2025a', 'erben2023']","['https://arxiv.org/pdf/2306.03163', 'https://arxiv.org/pdf/2309.03852']"
q189,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The documents mention AlexNet 2012 in the context of ImageNet classification, but none of the provided texts explicitly state its top-1 accuracy on ImageNet.","['schwartz2019', 'wu2021a']","['https://arxiv.org/pdf/1907.10597', 'https://arxiv.org/pdf/2111.00364']"
q190,192,192,GPUs,We fix our budget to be $100K with 192 A800 GPUs.,The document states that the FLM-101B model was trained with 192 A800 GPUs.,"['li2025a', 'samsi2024']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2309.03852']"
q191,Unable to answer with confidence based on the provided documents.,is_blank,lifetimes,"luccioni2025c: ""estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO₂-equivalent GHG emissions (CO₂e), or about five times the emissions of a car during its lifetime, including fuel."" dodge2022: ""Attention was first drawn to the environmental impact of AI research by the seminal work of Strubell et al. [42], which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars."" patterson2021: ""[Str19] estimates the CO2e for the neural architecture search (NAS) to find the more-efficient Evolved Transformer architecture done by [So19] at Google as 626,155 pounds (284 tCO2e).""","The estimated CO2 emissions for Neural Architecture Search (NAS) to train a Transformer-based model for machine translation are stated to be 626,155 pounds (284 metric tons) CO2e. This amount is consistently compared to ""about five times the emissions of a car during its lifetime"" across multiple documents. However, the provided documents do not contain information or a conversion factor to relate ""car lifetimes"" to ""average American human lifetimes"" in terms of CO2 emissions. Therefore, the second part of the question cannot be answered with confidence using the given information.","['patterson2021', 'luccioni2025c']","['https://arxiv.org/pdf/2506.15572', 'https://arxiv.org/pdf/2104.10350']"
q192,Unable to answer with confidence based on the provided documents.,is_blank,hours,is_blank,is_blank,"['xia2024', 'erben2023']","['https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2306.03163']"
q193,"47,400 metric tons of CO2e",47400,metric tons,"These on-site solar energy systems are estimated to generate 123,000 MWh annually
—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The provided text explicitly states that Amazon's on-site solar energy systems avoid 'roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.'",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q194,"The vLLM library, running on a Ray cluster.","vLLM library, Ray cluster",is_blank,"LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations. If a model is too large to be deployed on a single GPU, the model weights are distributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing nodes are needed, the model is split into two parts and executed sequentially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model part on the first node and then the model part on the second node.",Quote,"['fernandez2025', 'shen2024', 'samsi2024']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2404.07413', 'https://arxiv.org/pdf/2504.17674']"
q195,1.95x,1.95,multiplier,Model Duration (s) Energy consumed (Wh) single double ratio single double ratio Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95,"The table in the supporting materials explicitly shows the energy consumed (Wh) for Llama 3.1 70B in 'single' (48.60 Wh) and 'double' (94.88 Wh) node deployments, with a calculated 'ratio' of 1.95.","['chung2025', 'samsi2024']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2505.06371']"
q196,Unable to answer with confidence based on the provided documents.,is_blank,gallons of water,is_blank,"The documents provide some information on water consumption for AI models (e.g., GPT-3 queries, GPT-4o annual consumption projections for 2025) and general data center water usage, but none directly state the water consumed per ChatGPT user session specifically for 2023.","['amazon2023', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q197,"35,000 U.S. homes",35000,homes,jegham2025,"The document states that 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes'.","['jegham2025', 'morrison2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2503.05804']"
q198,34%,34,percent,"Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].",Quote,"['wu2021b', 'luccioni2025a', 'amazon2023']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q199,FALSE,0,is_blank,zschache2025: Dataset news yelp Model Energy (Wh) Accuracy Energy (Wh) Accuracy Linear BoW <0.01 0.65 <0.01 0.36 Linear Tf-idf <0.01 0.65 <0.01 0.34 Linear Embedding <0.01 0.83 0.04 0.43 XGBoost BoW <0.01 0.48 <0.01 0.31 XGBoost Tf-idf <0.01 0.52 <0.01 0.29 XGBoost Embedding 0.03 0.74 0.01 0.40 Llama 3.1 8B 4.31 0.71 4.73 0.58 Llama 3.1 70B 34.15 0.88 36.71 0.67 Qwen 2.5 7B 4.21 0.01 4.52 0.60 Qwen 2.5 72B 33.75 0.79 38.20 0.68 Phi 3.5 Mini 3.30 0.53 15.55 0.58 Phi 3.5 MoE 8.53 0.78 8.32 0.58 Jamba Mini 1.5 9.34 0.78 11.45 0.56 DS Llama 8B 60.58 0.82 97.18 0.62 DS Llama 70B 483.73 0.83 707.03 0.67 DS Qwen 14B 113.81 0.83 177.41 0.63 DS Qwen 32B 271.92 0.83 358.62 0.63,"According to the table in zschache2025, for the 'yelp' dataset (sentiment analysis), traditional models like Linear and XGBoost achieved accuracies ranging from 0.29 to 0.43. In contrast, Large Language Models (LLMs) such as Llama, Qwen, Phi, Jamba, and DS Llama achieved accuracies ranging from 0.56 to 0.68. The LLM accuracies are significantly higher than the traditional model accuracies, indicating they are not comparable.","['fernandez2025', 'luccioni2024']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2504.17674']"
q201,1.11,1.11,is_blank,"The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.",Quote,"['patterson2021', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2104.10350']"
q204,255.5 billion queries,255.5,billion queries,"jegham2025, abstract","The abstract of the Jegham et al. (2025) paper states: 'Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes...'. To estimate the total annual queries for 2025, the daily estimate of 700 million queries is multiplied by 365 days: 700,000,000 * 365 = 255,500,000,000, which is 255.5 billion queries.","['luccioni2024', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2311.16863']"
q205,53.0 score,53.0,score,Table in shen2024: '| OpenLLM Leaderboard Avg. | 51.0 | 51.1 | 46.4 | 53.0 |' for JetMoE model.,"The table in the provided document 'shen2024' lists the performance of various models, including 'JetMoE', on different benchmarks. Under the 'OpenLLM Leaderboard Avg.' row, the score for 'JetMoE' is explicitly stated as 53.0.","['chung2025', 'shen2024']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2404.07413']"
q206,Unable to answer with confidence based on the provided documents.,is_blank,training runs,is_blank,is_blank,"['luccioni2025a', 'cottier2024', 'morrison2025']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2405.21015']"
q208,FALSE,0,is_blank,"ebert2024: ""Elimination of open-source exemption: Remove the exemption that allows open-source models to bypass reporting obligations.""ebert2024: ""The first climate-related obligation in the AI Act concerns transparency and reporting, however not with respect to data centers, but concerning providers of high-risk and GPAI models [4].""ebert2024: ""For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9).""ebert2024: ""Energy Reporting for High-Risk AI: Replace computational resource reporting with direct energy consumption disclosures for High-Risk AI systems.""","While an 'open-source exemption' generally allows open-source models to bypass reporting obligations, this exemption does not apply if the open-source general-purpose AI model poses systemic risk. For 'providers of GPAI models with systemic risk and providers of HRAI systems', the AI Act 'mandates risk assessment and mitigation' and includes 'transparency and reporting obligations'. The document further implies that for such models, reporting on 'computational resources' is currently required, even if direct energy consumption disclosures are only a proposal. Therefore, if an open-source model poses systemic risk, it is not 'fully exempt from reporting their energy consumption' (at least indirectly through computational resources), making the statement FALSE.","['luccioni2025a', 'ebert2024']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2501.16548']"
q209,1.59,1.59,PUE,"The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.",Quote,"['ebert2024', 'jegham2025']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2505.09598']"
q210,5.312 GB,5.312,GB,"For example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.",Quote,['kim2025'],['https://arxiv.org/pdf/2504.11816']
q212,29-49%,"[29,49]",percent,"Breaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. This motivates further research on the scaling of R&D labor with computing power.","The document explicitly states that for the selected frontier models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs, including equity, constituted 29–49% of the total amortized model development cost.",['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q213,CodeCarbon,CodeCarbon,is_blank,"Third, energy consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine’s total energy use (Bouza et al., 2023).",The document explicitly states that 'energy consumption was measured using CodeCarbon'.,"['chung2025', 'zschache2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2508.14170 ']"
q214,53%,53,percent,"Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on the energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural network architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when addressing this subject47, stating that the true figures remain unknown.",The document explicitly states that '53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search'.,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572']
q216,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,"Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.",Quote,['kim2025'],['https://arxiv.org/pdf/2504.11816']
q217,TRUE,1,is_blank,"In order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary the number of GPUs used (Figure 9). Because the amount of computation executed is the same regardless of the number of GPUs, energy consumption should ideally be constant. Indeed, energy consumption barely changes when scaling from one GPU (no communication) to two, but when scaling further, energy consumption significantly increases. This is because, while the amount of computation decreases for each GPU, additional communication time between the GPUs offsets the reduction in computation time. Since communication time increases with the number of GPUs, using too many GPUs can lead to slowdowns in executing the same amount of computation and increase energy consumption.","The provided text explains that when scaling up the number of GPUs (which is analogous to increasing shards for a sharded model like LLaMA-65B), energy consumption significantly increases due to additional communication time between GPUs. This communication overhead offsets any reduction in computation per GPU, leading to higher overall energy consumption for the same amount of work (a response).",['samsi2024'],['https://arxiv.org/pdf/2310.03003']
q218,0.0022 kL,0.0022,kL,"We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO₂eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams12 and produces about 63 H100s.13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO₂eq per GPU.","The document explicitly states that mining rare earth metals for a single H100 GPU adds ""an additional 2.2 liters consumed"". To convert liters to kiloliters, divide by 1000: 2.2 L / 1000 = 0.0022 kL.","['jegham2025', 'morrison2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2503.05804']"
q219,FALSE,0,is_blank,"The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The document states that the open-source exemption from reporting obligations *should be removed*, implying that such an exemption *currently exists* under the rules the paper is discussing. Therefore, under current EU rules, open-source models are exempt from reporting their energy consumption.","['luccioni2025a', 'ebert2024']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2501.16548']"
q220,30%,30,percent,"In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].","The document states that in 2020, Amazon, Google, Facebook (Meta), and Microsoft collectively accounted for 30% of the cumulative total of renewable energy capacities purchased by corporations globally, which includes Power Purchase Agreements.","['luccioni2025a', 'amazon2023']","['https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q222,Unable to answer with confidence based on the provided documents.,is_blank,USD,is_blank,"The document states that the total annual public health burden of U.S. data centers could exceed $20 billion by 2028, and that the cost from 2023 to 2028 is 'potentially tripling'. However, it does not provide a specific, definitive total public health cost for U.S. data centers in 2023.",['han2024'],['https://arxiv.org/pdf/2412.06288']
q223,Unable to answer with confidence based on the provided documents.,is_blank,multiplier,is_blank,"The provided documents state that GPT-4.1 nano is among the most resource-efficient systems for long-form prompts, while o3 exhibits 'substantially higher footprints.' However, specific energy consumption values (e.g., in Wh) for either model for a long prompt are not explicitly provided in the text or directly extractable from the descriptions of figures. The document mentions carbon emissions and water consumption for GPT-4.1 nano but not its energy consumption in Wh, which is needed to calculate the requested ratio for energy consumption.","['li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598']"
q224,Mélange achieved cost reductions in the range of 15% to 77% compared to single-GPU baselines.,"[15,77]",percent,"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting. Table from griggs2024: Rate (req/s) | Solver | L4 | A10G | A100 | H100 | Norm. Cost ($/hr) | Savings... 1 | H100-only | ... | 77.25%... 1 | A10G-only | ... | 15.35%","The document states that Mélange reduces deployment costs by 'up to 77% in conversational settings'. The 'conversational settings' refer to the Arena dataset. While the specific SLO of 120ms is not explicitly tied to a singular percentage, the general range of savings observed in the detailed cost tables (which are applicable to the Arena dataset, as confirmed by other text) spans from approximately 15.35% (A10G-only at 1 req/s) to 77.25% (H100-only at 1 req/s). Therefore, a range of 15% to 77% is appropriate.",['griggs2024'],['https://arxiv.org/pdf/2404.14527']
q225,Unable to answer with confidence based on the provided documents.,is_blank,tCO2e,is_blank,The provided documents do not contain information about the total estimated net carbon emissions for the pre-training of FLM-101B.,"['luccioni2023', 'patterson2021', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2302.08476', 'https://arxiv.org/pdf/2104.10350']"
q226,Unable to answer with confidence based on the provided documents.,is_blank,seconds,"The provided documents mention 'Fig. 4. Execution time breakdown.' and discuss fine-tuning Mixtral models on NVIDIA A40 GPUs, including the availability of latency breakdown results. However, the exact numerical value for the total execution time of a sparse Mixtral model with a batch size of 1 fine-tuned on an NVIDIA A40-48 GB GPU is not explicitly stated in the text snippets. To obtain this value, access to the actual Figure 4 or additional data would be required.","The document 'xia2024' mentions a 'Fig. 4. Execution time breakdown.' for Mixtral fine-tuning on NVIDIA A40 GPUs and that latency breakdown results can be obtained. However, the specific numerical value for the *total execution time* for a sparse Mixtral model with a batch size of 1 on an A40-48 GB GPU is not explicitly present in the provided text excerpts. Without the actual figure or more detailed numerical data in the text, it's not possible to confidently extract the answer.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q227,FALSE,0,is_blank,"The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. For example, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The document explicitly states that 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities'. It further highlights that 'the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities'.",['han2024'],['https://arxiv.org/pdf/2412.06288']
q228,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,"The provided documents include a reference to a 2019 paper by Sun et al. on CPU and GPU design trends ('wu2021b'). However, the content of this paper or any other document does not elaborate on the specific claim regarding GPU theoretical performance per watt doubling approximately every 3-4 years as of 2019 product data. Therefore, the information needed to confidently answer True or False is not present.","['wu2021b', 'cottier2024', 'morrison2025']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2405.21015']"
q229,Ollama,Ollama,is_blank,"We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The document states that 4-bit quantization was applied through Ollama, which is described as an open-source platform supporting edge computing and privacy-centric deployments, directly answering the question.","['griggs2024', 'khan2025', 'zschache2025']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2504.06307', 'https://arxiv.org/pdf/2508.14170 ']"
q232,Backblaze,Backblaze,is_blank,"First, loading data from Backblaze costs $0.01/GB from anywhere in the world, which gives us a rate of $0.144/h for the CV and $0.083/h for the NLP experiments. Even when CV throughput is less than half of the NLP model (Figure 10a), images are much larger than text, resulting in a higher data rate. While this is close to the spot instance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time costs until the entire dataset is downloaded and retrieved from the disk cache, assuming large enough local storage.",The document explicitly states that data was loaded from Backblaze for experiments involving spot instances.,"['erben2023', 'kim2025']","['https://arxiv.org/pdf/2504.11816', 'https://arxiv.org/pdf/2306.03163']"
q233,TRUE,1,is_blank,"Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime. This makes the duration of computations a valuable proxy measure for energy consumption in settings where the latter cannot be traced.",The document explicitly states that 'the energy consumption during inference is shown to highly correlate with the model’s runtime' and that 'the relation between duration and energy is approximately linear.',"['zschache2025', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2508.14170 ']"
q234,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,[78] U.S. Congress. 2024. S.3732 - Artificial Intelligence Environmental Impacts Act of 2024. https://www.congress.gov/bill/118th-congress/senate-bill/3732/ Accessed: 2025-03-18.,"The provided document mentions the 'Artificial Intelligence Environmental Impacts Act of 2024' and its bill number (S.3732), but it does not specify the name of the U.S. Senator who introduced the bill.",['ebert2024'],['https://arxiv.org/pdf/2410.06681']
q235,$11.06/hr,11.06,USD per hour,"| H100 | H20 | TPU v6e [7] 

|---|---|---|
| BF16 TFLOPs | 989 | 148 | 918 
| Memory capacity | 80 GB | 96 GB | 32 GB 
| Memory bandwidth | 3.35 TB/s | 4.0 TB/s | 1.64 TB/s 
| Power rating | 700 W | 400 W | unlisted 
| Inter-chip bandwidth | 450 GB/s | 450 GB/s | 448 GB/s 
| Network bandwidth | 400 Gbps | 400 Gbps | 200 Gbps 
| Price per chip [2] | $11.06/hr | $4.63/hr * | $2.70/hr",Table,"['cottier2024', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2405.21015']"
q236,4 years,4,years,"Morrison et al. (2025) state: ""Internally, we assume a 4 year lifespan for our GPUs..."" Cottier et al. (2024) also provide a data point that suggests an ""expected hardware lifetime would be 3.7 years"" based on observed failure rates of NVIDIA A100 GPUs.",Morrison et al. (2025) directly state their assumed GPU lifespan. Cottier et al. (2024) provide a calculated expected lifetime from a different perspective (hardware failures) which is very close to the assumed lifespan.,"['cottier2024', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2405.21015']"
q237,Unable to answer with confidence based on the provided documents.,is_blank,V100_32GB_GPUs,is_blank,"The retrieved documents confirm that NVIDIA V100 GPUs exist with 32GB of memory and that LLaMA models up to 13 billion parameters are relevant. However, the documents do not provide the specific memory footprint in gigabytes (GB) for the LLaMA-13B model. Without this crucial information, it is not possible to calculate the number of GPUs required, even with the condition of running without compression or quantization (which still requires knowing the base precision, e.g., FP16 or FP32, to convert parameters to GB).","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q238,"Google's Gemma family of language models emitted 1247.61 tCO2e during pre-training, which is over 4 times the 'five cars' estimate of 284 tCO2e.",1247.61,tCO2e,luccioni2025c,"The document 'luccioni2025c' states: 'Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO₂e,³⁴ over 4x the estimate that forms the basis for the “five cars” number'. The same document identifies the 'five cars' estimate as 284 metric tons (tCO2e).","['luccioni2023', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2302.08476']"
q239,Unable to answer with confidence based on the provided documents.,is_blank,hours,is_blank,"The document states that ELMo was trained on ""3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours)"". However, the question asks for the training time on ""3 NVIDIA GTX 1080 Ti GPUs"". The GTX 1080 and GTX 1080 Ti are distinct GPU models with different specifications, and the provided text does not offer information for the 'Ti' variant. Therefore, the exact answer cannot be confidently extracted.","['strubell2019', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/1906.02243']"
q240,3.1 L/kWh,3.1,L/kWh,"For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.",Quote,"['li2025b', 'morrison2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2503.05804']"
q241,1.10-1.11 PUE,"[1.10,1.11]",PUE,"The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. We use Google Georgia datacenter’s PUE from the period in which the search computation was run (1.10 in Table 4)","The document, published in April 2021, states the PUE for Google's Iowa datacenter as 1.11 and Google's Georgia datacenter as 1.10. These values are specific reported PUEs for Google's data centers around the 2021 timeframe, which are considered hyperscale.","['wu2021b', 'patterson2021']","['https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2104.10350']"
q242,up to 96%,96,percent,"Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.",Quote,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q243,$3460,3460,USD,"For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.",Quote,['xia2024'],['https://arxiv.org/pdf/2408.04693']
q244,50-70%,50-70,percent,"GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].",Quote,"['fernandez2025', 'chung2025', 'jegham2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2504.17674']"
q245,96,96,H100 GPUs,We conduct training on a cluster containing 12 nodes and 96 H100s.,Quote,"['shen2024', 'zschache2025']","['https://arxiv.org/pdf/2404.07413', 'https://arxiv.org/pdf/2508.14170 ']"
q247,Unable to answer with confidence based on the provided documents.,is_blank,Watts,is_blank,"The provided documents state that GPU power consumption is logged at sub-second intervals for a single node during training (morrison2025), but they do not contain specific data for the average GPU power during the first 300 logging steps of OLMo 2 7B training. The tables in morrison2025 provide total MWh for training, not average power for specific logging steps.","['chung2025', 'zschache2025', 'morrison2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2508.14170 ']"
q248,"11,023 lbs",11023,lbs,strubell2019,"Table 3 in Strubell et al. (2019) lists 'Human life, avg, 1 year' as having 11,023 CO2e emissions. The units within this table are implicitly in pounds based on common reporting practices in similar contexts within the paper, and the overall context of the paper which discusses carbon emissions in various units including pounds.","['patterson2021', 'amazon2023', 'morrison2025']","['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q249,1.25x,1.25,multiplier,"As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. Faster response rates and inference are likely due to the fact that the number of computations...","The document states that for LLaMA 13B, there is a '1.25 times increase' in performance metrics like 'words per second, tokens per second, and responses per second' on A100 GPUs compared to V100 GPUs. Although the sentence uses the term 'inference latency', the accompanying metrics (words per second, tokens per second, responses per second) are measures of throughput, and the overall context indicates A100 outperforms V100 with 'Faster response rates and inference'. Therefore, '1.25 times increase' refers to an increase in throughput, which is a speedup.","['griggs2024', 'samsi2024']","['https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2404.14527']"
q250,0.42 Wh,0.42,Wh,"Our framework estimates 0.42 Wh (±0.13 Wh) for a short GPT-4o prompt (0.37 Wh without datacenter overhead), within 19% of Altman’s figure.",The document explicitly states that their framework estimates 0.42 Wh for a short GPT-4o prompt.,"['luccioni2025c', 'jegham2025']","['https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2506.15572']"
q251,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents mention that InferSave can achieve cost savings for online workloads and refer to figures and tables that compare InferSave with Max-Performance for different SLOs (e.g., 100 TPS and 200 TPS). However, the specific data for an online workload with a 400 TPS SLO, including the exact costs for Max-Performance and InferSave's top choice, is not present in the text. Therefore, a percentage difference cannot be calculated.","['griggs2024', 'kim2025']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2504.11816']"
q252,A V100 or A30 GPU,V100 or A30,is_blank,"For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The document states directly that for models generating a single token per inference, V100 or A30 GPUs are more efficient.","['chung2025', 'patterson2021', 'zschache2025']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2508.14170 ']"
q254,TRUE,1,is_blank,"Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.","The document explicitly states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice'.",['schwartz2019'],['https://arxiv.org/pdf/1907.10597']
q255,Unable to answer with confidence based on the provided documents.,is_blank,metric tons,is_blank,"The document 'luccioni2025a' cites 'The global e-waste monitor 2024', which would likely contain the requested information. However, the provided text snippet from 'luccioni2025a' only includes the citation and not the specific data point for the total amount of electronic waste generated worldwide in 2022.","['luccioni2025a', 'amazon2023']","['https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']"
q256,Unable to answer with confidence based on the provided documents.,is_blank,Watts,is_blank,"The average power for the TPU v2 is stated as 208 Watts in 'patterson2021'. However, the documents provided do not explicitly state the average system power per processor for the V100 GPU. While 'luccioni2023' mentions the V100 as a popular piece of hardware and discusses a range of TDPs (180W to 450W for GTX 1080 and TPU v3 respectively), it does not specify the V100's TDP or average power. Therefore, a confident calculation of the difference cannot be made.","['luccioni2023', 'cottier2024', 'patterson2021']","['https://arxiv.org/pdf/2405.21015', 'https://arxiv.org/pdf/2302.08476', 'https://arxiv.org/pdf/2104.10350']"
q257,"700,000 liters",700000,liters,"For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.",Quote,"['luccioni2025a', 'li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.16548']"
q258,20x,20,multiplier,"Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20×",Quote,['wu2021a'],['https://arxiv.org/pdf/2111.00364']
q259,o3-mini,o3-mini,is_blank,"As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These results suggest that downsizing reasoning models can yield meaningful sustainability gains without compromising performance.",The document states that 'o3-mini achieved the highest cross-efficiency score (0.884)' in the eco-efficiency analysis.,['jegham2025'],['https://arxiv.org/pdf/2505.09598']
q260,TRUE,1,is_blank,"We must go beyond efficiency optimization and build a sustainable ecosystem to achieve environmentally-sustainable computing. For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020]. Modular system design will enable component-level upgrades without having to decommission the system at its entirety, reducing overall electronic waste and the environmental footprint [Fairphone].","The document explicitly states that the current average lifetime for cell phones is less than 3 years, and discusses this in the context of reducing electronic waste and environmental footprint.",['wu2021b'],['https://arxiv.org/pdf/2108.06738']
q261,TRUE,1,is_blank,"CV’s per-GPU speedup (speedup / GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively. We used Google Cloud [5] for all experiments in this section, as they were the first to give us access to all necessary zones. ... We decided to use the n1-standard-8 template with eight cores, 30 GB memory, and a T4 GPU...","The document 'erben2023' states that CV's per-GPU speedup is 'almost linear' with values like (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, respectively. This context is within experiments conducted on Google Cloud using T4 GPUs, which are referred to as 'intra-zone' in the experimental design.","['xia2024', 'kim2025', 'wu2021a']","['https://arxiv.org/pdf/2504.11816', 'https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2111.00364']"
q264,Unable to answer with confidence based on the provided documents.,is_blank,tokens,is_blank,"The provided documents discuss the FLM-101B model's architecture, training cost, and performance, but do not specify its context window size or maximum sequence length.",['li2025a'],['https://arxiv.org/pdf/2309.03852']
q265,TRUE,1,is_blank,"Generally, LLMs and VLMs consume significantly less power than the GPU's TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU's compute resources. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding. It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.","The documents explicitly state that LLMs consume significantly less power than diffusion models during inference because LLM decoding is memory-intensive, has low compute-intensity, and is bottlenecked by VRAM bandwidth, leading to underutilization of GPU compute resources and thus lower power draw. In contrast, diffusion models are more compute-intensive and consume near maximum GPU power.",['chung2025'],['https://arxiv.org/pdf/2505.06371']
q266,31.6%,31.6,percent,Table 2: People Managers,"Table 2 in the Amazon 2023 Sustainability Report provides a breakdown of People Managers by gender. For the year 2023, the table explicitly states that 31.6% of People Managers identified as women.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q267,61–76%,"[61,76]",percent,"On the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while energy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.","The document states that 'With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.' This directly provides the percentage range for hardware cost when equity is excluded.",['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q268,FALSE,0,is_blank,"However, the impact on performance metrics such as accuracy, F1 score, recall, and precision varies. While the reduction in carbon footprint is consistent, performance trade-offs are evident, with some metrics experiencing marginal improvements","The document 'khan2025' states that the impact on performance metrics like accuracy and F1 score 'varies' and that 'performance trade-offs are evident, with some metrics experiencing marginal improvements'. This indicates that accuracy and F1 scores do not 'always' improve after optimization.",['khan2025'],['https://arxiv.org/pdf/2504.06307']
q269,0.93 lbs/kWh,0.93,lbs/kWh,Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018.,"The document states that the US average CO2 per kilowatt hour, as calculated by the U.S. Environmental Protection Agency (EPA) in 2018, was 0.423 kg per KWh. To convert kilograms to pounds, we use the conversion factor 1 kg = 2.20462 lbs. Therefore, 0.423 kg/KWh * 2.20462 lbs/kg = 0.93255966 lbs/KWh, which rounds to 0.93 lbs/kWh.","['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q270,Unable to answer with confidence based on the provided documents.,is_blank,TWh,is_blank,"The documents mention that electricity demand from data centers, heavily driven by AI, will more than double by 2026 and surpass Canada's national power use. However, they do not provide a specific projected range of electricity consumption in TWh for global AI in 2027.","['luccioni2025a', 'luccioni2024']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2311.16863']"
q271,Unable to answer with confidence based on the provided documents.,is_blank,packages,is_blank,"The document states that Amazon delivered more than 680 million packages globally using over 24,000 electric delivery vehicles in 2023. It also mentions deploying more than 3,000 electric delivery vehicles in Europe, including 300 Rivian vans. However, it does not provide a specific number of packages delivered by EVs in Europe.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q273,"1,920,000 tokens",1920000,tokens,"Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload consists of 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request are 128 + 512 = 640 tokens. Therefore, the total number of tokens processed for the entire workload is 640 tokens/request * 3000 requests = 1,920,000 tokens.","['chen2024', 'kim2025', 'morrison2025']","['https://arxiv.org/pdf/2405.01814', 'https://arxiv.org/pdf/2504.11816', 'https://arxiv.org/pdf/2503.05804']"
q274,FALSE,0,is_blank,"The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration [37]) and water consumption [49].","The provided document explicitly states that the AI Act omits indirect emissions from AI applications, including those used for oil and gas exploration, from its reporting requirements.","['ebert2024', 'luccioni2025c']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2506.15572']"
q275,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided text mentions that a figure in the Dodge et al. (2022) paper shows the CO2 emissions decrease in percentage for different regions for DenseNet 201 under Flexible Start optimization. However, the specific figure or table content detailing the maximum percentage reduction for the West US region is not included in the provided snippets, preventing a confident answer.",['dodge2022'],['https://arxiv.org/pdf/2206.05229']
q276,1453.5 times,1453.5,times,"Table 2: Estimated mean and standard deviation of energy (in kWh) and carbon emissions (in gCO2eq) per 1,000 inferences across all models for each task.","According to Table 2 in the Luccioni et al. (2024) study, the mean energy for 1,000 inferences for text classification is 0.002 kWh, and for image generation, it is 2.907 kWh. Dividing 2.907 by 0.002 gives a factor of 1453.5.",['luccioni2024'],['https://arxiv.org/pdf/2311.16863']
q277,FALSE,0,is_blank,Dataset news yelp Model Energy (Wh) Accuracy Energy (Wh) Accuracy Linear BoW <0.01 0.65 <0.01 0.36 Linear Tf-idf <0.01 0.65 <0.01 0.34 Linear Embedding <0.01 0.83 0.04 0.43 XGBoost BoW <0.01 0.48 <0.01 0.31 XGBoost Tf-idf <0.01 0.52 <0.01 0.29 XGBoost Embedding 0.03 0.74 0.01 0.40 Llama 3.1 8B 4.31 0.71 4.73 0.58 Llama 3.1 70B 34.15 0.88 36.71 0.67 Qwen 2.5 7B 4.21 0.01 4.52 0.60 Qwen 2.5 72B 33.75 0.79 38.20 0.68 Phi 3.5 Mini 3.30 0.53 15.55 0.58 Phi 3.5 MoE 8.53 0.78 8.32 0.58 Jamba Mini 1.5 9.34 0.78 11.45 0.56 DS Llama 8B 60.58 0.82 97.18 0.62 DS Llama 70B 483.73 0.83 707.03 0.67 DS Qwen 14B 113.81 0.83 177.41 0.63 DS Qwen 32B 271.92 0.83 358.62 0.63,"According to the table, for the 'yelp' dataset (sentiment analysis), traditional models (Linear, XGBoost) show accuracies ranging from 0.29 to 0.43. Large Language Models (LLMs) like Llama, Qwen, Phi, Jamba, and DS Llama/Qwen, show accuracies ranging from 0.56 to 0.68. The LLMs consistently achieve significantly higher accuracy than traditional models, indicating their performance is not comparable.","['fernandez2025', 'luccioni2024']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2504.17674']"
q279,Unable to answer with confidence based on the provided documents.,is_blank,projects,is_blank,"The document states that 'As of January 2024, Amazon had announced: 513 Global renewable energy projects' and mentions '243 Utility-scale wind and solar projects', but does not specify the number of projects specifically in the United States for that date.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q281,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents state that Amazon, including AWS, matched 100% of its electricity consumption with renewable energy sources in 2023, and 90% in 2022. However, no information is available for 2018.",['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf']
q283,"At the granularity of a single, whole generation response to a request.","single, whole generation response",to a request,"Our goal is to measure and report energy consumption at a granularity that is neither too coarse (as it only provides limited insight into the runtime behavior of the service) nor too fine (as it may miss important higher-level insights relevant to the service). Also aligned with our goal of representing real-world deployments (Section 2.2), our approach is to mainly report energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video).","The document explicitly states the goal of measuring and reporting energy consumption at a granularity that is 'neither too coarse nor too fine', and then specifies this as 'the granularity of a single, whole generation response to a request' for generative AI models.","['chung2025', 'luccioni2025c']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2506.15572']"
q284,74%,74,percent,dodge2022,The provided table within the document 'dodge2022' explicitly states that the GPU accounts for 74% of the total electricity consumption in the experiment where a BERT-base model was trained.,"['dodge2022', 'jegham2025']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2505.09598']"
q285,4 NVIDIA A100-80GB GPUs,4,NVIDIA A100-80GB GPUs,Table 5: Hardware Configurations for Evaluation,"The document chen2024, in its Table 5 under 'Serving Performance', indicates that serving the LLaMA3-70B model with the vLLM system requires 4 H100 GPUs. Given that both H100 and A100 GPUs are commonly available with 80GB memory capacities, and LLaMA3-70B is comparable to Llama2-70b, it is reasonable to infer that 4 NVIDIA A100-80GB GPUs would be required for serving.","['xia2024', 'zschache2025']","['https://arxiv.org/pdf/2408.04693', 'https://arxiv.org/pdf/2508.14170 ']"
q286,25.8%,25.8,percent,"Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. Along with other efficiency optimization at-scale, this has translated into 25.8% operational energy footprint reduction over the two-year period.",The document explicitly states that Facebook achieved a '25.8% operational energy footprint reduction over the two-year period' through efficiency optimizations.,['wu2021a'],['https://arxiv.org/pdf/2111.00364']
q287,Unable to answer with confidence based on the provided documents.,is_blank,kilometers of fiberoptic cable,is_blank,"The provided documents discuss AI infrastructure, energy consumption, and costs, but none contain specific information about the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023.","['luccioni2025a', 'amazon2023', 'wu2021a']","['https://arxiv.org/pdf/2501.16548', 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf', 'https://arxiv.org/pdf/2111.00364']"
q288,800 million USD,800000000,USD,"For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.",The document explicitly states the hardware acquisition cost for training GPT-4 as $800M.,['cottier2024'],['https://arxiv.org/pdf/2405.21015']
q289,FALSE,0,is_blank,"The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The provided text clearly states that 'Sustainable AI' encompasses 'both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves'. This indicates it is not *only* about climate-positive applications, making the statement false.","['luccioni2025a', 'luccioni2025b']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2504.00797']"
q290,Unable to answer with confidence based on the provided documents.,is_blank,samples,is_blank,"The document mentions that the authors found the maximum batch size for LLM models on NVIDIA A100 (40GB) and that Figure 13 correlates projected maximum batch sizes with experimented ground truth. However, the specific ground truth value for Mixtral on an A100-40GB GPU is not explicitly stated in the provided text snippets, nor is the content of Figure 13 transcribed.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q291,Swapping,Swapping,is_blank,"In Figure 8, we compare the energy consumption per generation of the two preemption mechanisms with the Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size configuration and causing preemption. It can be seen that when the server is overloaded, Swapping consistently consumes less energy.",Quote,"['chung2025', 'ebert2024']","['https://arxiv.org/pdf/2505.06371', 'https://arxiv.org/pdf/2410.06681']"
q292,48%,48,percent,"For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]",The provided text directly states that Google reported a 48% increase in GHG emissions since 2019 in its 2024 environmental sustainability report.,"['luccioni2025a', 'patterson2021', 'han2024']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2412.06288', 'https://arxiv.org/pdf/2104.10350']"
q293,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents mention McKinsey & Company in several references ([chung2025], [han2024], [morrison2025]), often citing reports like ""Investing in the rising data center economy"" or ""How data centers and the energy sector can sate AI’s hunger for power"". While these documents discuss the increasing energy demands of data centers and AI, they do not contain a specific projection from McKinsey stating the percentage of U.S. national electricity consumption that data centers are anticipated to account for in 2030.","['dodge2022', 'han2024']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2412.06288']"
q294,11.4%,11.4,percent,Model | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge P&R | 9.5% | 11.0% | 11.4% | 2.0% | 2.8% | 3.1% | 11.0% | 11.0% | 10.8% | 11.4% | 11.3%,"The provided table, which reports the decrease in CO2 emissions for various models and optimization strategies, shows that the 'Pause and Resume' (P&R) optimization for the '6B Transf.' model yields an 11.4% emissions decrease. This is the maximum value found among the provided snippets for this specific model and optimization.",['dodge2022'],['https://arxiv.org/pdf/2206.05229']
q295,70%,70,percent,"Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",Quote,['shen2024'],['https://arxiv.org/pdf/2404.07413']
q298,Unable to answer with confidence based on the provided documents.,is_blank,lbs CO2e,is_blank,"The provided documents refer to the Strubell et al. (2019) paper as a 'seminal 2019 study' that quantifies carbon emissions for NLP models, stating 'Table 1 lists CO2 emissions and estimated cost of training the models described in §2.1.' However, the actual content of 'Table 1' or a direct quote of the carbon footprint for the BERT large language model in lbs CO2e from this study is not present in the provided text snippets.","['rubei2025', 'luccioni2024', 'patterson2021']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2501.05899']"
q299,1287 MWh,1287,MWh,GPT-3... Its estimated carbon emissions due to training are 552 tCO2e and its energy consumption is 1287 MWh.,Quote from document `patterson2021` explicitly states the energy consumption for training GPT-3.,"['li2025b', 'cottier2024', 'patterson2021']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2104.10350', 'https://arxiv.org/pdf/2405.21015']"
q300,TRUE,1,is_blank,"The execution time for the MoE layer encompasses both the forward and backward passes during fine-tuning. Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.",The document states that the MoE layer is the 'costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.' This directly supports the statement in the question.,['xia2024'],['https://arxiv.org/pdf/2408.04693']
q301,Unable to answer with confidence based on the provided documents.,is_blank,samples,is_blank,"The document 'xia2024' mentions that Figure 13 shows the maximum batch sizes for fine-tuning Mixtral on different NVIDIA GPUs, including the A40 (48GB). However, the content of Figure 13 is not provided in the text. The document explicitly states: 'Due to space limitations, we only show the projection of Mixtral model' for GPU memory capacities of 100GB and 120GB, but not for the A40. Therefore, the specific maximum batch size for Mixtral with a dense setup on the Hellaswag dataset using an NVIDIA A40 GPU with 48 GB memory is not available in the provided text.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q302,FALSE,0,is_blank,"Comparing the A-3 and C-3 experiments with three local versus three fully remote GPUs, CV is only 5% slower, while NLP suffers a 34% drop in throughput (Figure 9a) and does not even reach the baseline single GPU performance (A-1). The peak egress for each region was 318, 258, and 237 Mb/s for the US, EU, and ASIA, respectively. Since our bandwidth measurements were 210 and 130 Mb/s from the US to the EU and ASIA, respectively (Table 3), this suggests that the averaging was done over the US node and not an N-to-N all-reduce (a detailed analysis of how averaging affects bandwidths is discussed in Section 6). Thus, the limiting factor was the US-ASIA connection at 130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend continues with the C-4 run, which adds AUS as a continent with one additional VM. As we know from the transatlantic experiments (B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a).","The document states that for intercontinental training, CV models were 5% slower with three GPUs and 9% slower with four continents (four GPUs) compared to local training. Neither of these matches 'only 7%'. Therefore, the statement is false.",['erben2023'],['https://arxiv.org/pdf/2306.03163']
q303,Unable to answer with confidence based on the provided documents.,is_blank,hectares,is_blank,"The provided documents discuss the environmental impacts of AI and data centers, including energy and water consumption, carbon footprint, and land use in a general sense. However, none of the documents contain specific data regarding the number of hectares of land occupied by new AI data centers globally in 2022.","['luccioni2025a', 'luccioni2025b']","['https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2504.00797']"
q305,0.32g CO2eq,0.32,g CO2eq,"for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.",Quote,"['luccioni2023', 'patterson2021', 'luccioni2024']","['https://arxiv.org/pdf/2302.08476', 'https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2104.10350']"
q307,110 thousands of grams,110,thousands of grams,"Figure 2: CO2 Grams Emitted, BERT Language Modeling (from dodge2022)","Figure 2 displays the range of CO2 emissions for various models, including 'BERT Language Modeling'. By visually estimating from the logarithmic scale on the y-axis, the maximum emissions for 'BERT Language Modeling' are approximately 1.2 x 10^5 grams, and the minimum emissions are approximately 1.0 x 10^4 grams. The approximate range is calculated as 120,000 grams - 10,000 grams = 110,000 grams. Expressed in thousands of grams, this is 110.",['dodge2022'],['https://arxiv.org/pdf/2206.05229']
q308,2022,2022,year,"The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.",Quote,"['dodge2022', 'luccioni2025a', 'luccioni2025c']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2501.16548', 'https://arxiv.org/pdf/2506.15572']"
q309,5 days,5,days,"Table 1: We developed our models in five groups, based on parameter count and architecture: less than 1 billion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active and 7 billion total parameters. We found that ~70% of our developmental environmental impact came from developing the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth of gasoline, and equal to about 7 and a half years of water used by the average person in the United States.","The table 'Model | Power Usage (MWh) | Carbon Emissions (tCO₂eq) | Equiv. to... (energy usage, 1 home, U.S.) | Water Consumption (kL) | Equiv. to... (water usage, 1 person, U.S.)' in the 'morrison2025' document explicitly lists 'OLMo 60M†' with an equivalent water usage of '5 days' for one person in the U.S.",['morrison2025'],['https://arxiv.org/pdf/2503.05804']
q310,Unable to answer with confidence based on the provided documents.,is_blank,liters of freshwater,is_blank,"The provided documents discuss general water consumption by data centers and some AI models (like GPT-3, GPT-4o), and mentions DeepMind's AI reducing Google Data Centre cooling bills. However, there is no specific information regarding the freshwater consumption of Google's DeepMind AlphaFold servers in 2023.","['luccioni2025a', 'li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598', 'https://arxiv.org/pdf/2501.16548']"
q311,FALSE,0,is_blank,A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.,"The document explicitly states that adding compute resources to accelerate MoE layers is a way to *reduce* the cost of LLM fine-tuning, not increase it.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q312,Unable to answer with confidence based on the provided documents.,is_blank,kWh,is_blank,The provided documents do not contain information about the energy consumption for training the FLM-101B model.,"['patterson2021', 'zschache2025']","['https://arxiv.org/pdf/2508.14170 ', 'https://arxiv.org/pdf/2104.10350']"
q313,More than $20 billion,20,billion USD,"Our findings suggest that the total annual public health burden of U.S. data centers could exceed $20 billion by 2028, approaching or even surpassing the impacts of on-road vehicle emissions in California. Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.",The document explicitly states that the total annual public health burden of U.S. data centers is projected to exceed $20 billion by 2028.,['han2024'],['https://arxiv.org/pdf/2412.06288']
q314,Unable to answer with confidence based on the provided documents.,is_blank,USD,is_blank,"The provided documents discuss fine-tuning Mixtral models and the use of NVIDIA A40-48GB GPUs. It also mentions that GSM8K was used for evaluation of arithmetic reasoning. However, the documents do not provide an estimated total cost for fine-tuning a Mixtral model specifically on the GSM8K dataset using an NVIDIA A40-48GB GPU. Cost estimates are given for other datasets (MATH, OpenOrca) and other GPUs (H100), but not the exact combination requested.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q315,Unable to answer with confidence based on the provided documents.,is_blank,samples,"The provided documents state that experiments were conducted using an NVIDIA A40 GPU and that batch sizes were manually set and varied for profiling (e.g., batch sizes 1, 10, and 32 are mentioned in the context of figures). While the paper mentions finding the 'maximum batch size' for various GPUs including the A40, and projects maximum batch sizes for hypothetical future GPUs (e.g., 28 for 100GB, 35 for 120GB), it does not explicitly state the specific maximum batch size for the A40-48 GB GPU for the longest-running MoE layer within the provided text.","The documents indicate that the MoE layer is the longest-running layer, and experiments were performed on an NVIDIA A40-48GB. However, while the paper mentions that varying batch sizes were used for profiling and an analytical model was developed to find optimal coefficients and maximum batch sizes, the specific 'batch size (in samples) of the longest-running MoE layer' for the A40-48GB is not explicitly stated as a single value in the provided text. Batch sizes of 1, 10, and 32 are mentioned as examples used in figure analysis, but not as 'the' batch size.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q317,Unable to answer with confidence based on the provided documents.,is_blank,seconds,is_blank,"The provided documents describe that the 'xia2024' paper characterizes fine-tuning of sparse Mixture-of-Experts (MoE) LLM models, including Mixtral, on a single GPU like the NVIDIA A40. It also mentions an 'analytical model to estimate the cost of LLM fine-tuning' and refers to figures that show 'execution time breakdown' (Fig. 4) and 'maximum batch size' (Fig. 13). However, the specific total execution time in seconds for a sparse Mixtral model fine-tuned on an NVIDIA A40-48GB with a batch size of 10 is not explicitly stated in the text content of the provided documents. The figures themselves are not included as text, and their descriptions do not contain this precise numerical answer.",['xia2024'],['https://arxiv.org/pdf/2408.04693']
q318,FALSE,0,is_blank,We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. ... it substantially under-represents the actual energy consumption since it measures just a single component.,The document explicitly states that GPU-level power consumption monitoring is not recommended for overall energy measurements because it significantly under-represents the actual energy consumption by only measuring a single component.,"['ebert2024', 'jegham2025']","['https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2505.09598']"
q319,Unable to answer with confidence based on the provided documents.,is_blank,percent,is_blank,"The provided documents refer to the Luccioni et al. 2022/2023 paper on BLOOM's carbon footprint and mention it covers a lifecycle approach, but they do not explicitly state the percentage of the BLOOM model's overall emissions that training accounted for. They mention inference can be a significant portion over time, but no specific breakdown for training's share of overall emissions is given.","['luccioni2024', 'zschache2025', 'luccioni2025b']","['https://arxiv.org/pdf/2311.16863', 'https://arxiv.org/pdf/2504.00797', 'https://arxiv.org/pdf/2508.14170 ']"
q320,1 V100 32GB GPU,1,V100_32GB_GPU,"The LLaMA-7B model has approximately 7 billion parameters. Assuming FP16 precision (2 bytes per parameter), the model size is 7B * 2 bytes/parameter = 14 GB. Documents state that an NVIDIA V100 GPU has 32GB of memory. Since 14 GB (LLaMA-7B) is less than 32 GB (1x V100 32GB GPU), a single GPU is sufficient.","To determine the number of GPUs needed, we first estimate the model size of LLaMA-7B. Based on common practice for LLMs and the given context for LLaMA-33B (64.7 GB, which aligns with 33B parameters * 2 bytes/parameter for FP16), LLaMA-7B with 7 billion parameters would require approximately 7 * 2 = 14 GB of memory in FP16. The documents confirm that an NVIDIA V100 32GB GPU has 32GB of memory. Since 14 GB is less than 32 GB, one NVIDIA V100 32GB GPU is sufficient to run LLaMA-7B inference without compression or quantization.","['xia2024', 'griggs2024']","['https://arxiv.org/pdf/2404.14527', 'https://arxiv.org/pdf/2408.04693']"
q321,9.629 requests,9.629,requests,"Table 1: Estimate of GPT-3's operational water consumption footprint. '*' denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.",Table 1 in li2025b directly provides the number of requests for a 500ml bottle of water for GPT-3 in Arizona as 9.629.,"['li2025b', 'jegham2025']","['https://arxiv.org/pdf/2304.03271', 'https://arxiv.org/pdf/2505.09598']"
q322,8.30 metric tons,8.30,metric tons,"One average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)",Quote,"['patterson2021', 'morrison2025']","['https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2104.10350']"
q323,27.8 score,27.8,score,shen2024,Table lookup,['shen2024'],['https://arxiv.org/pdf/2404.07413']
