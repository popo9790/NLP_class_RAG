import os
import json
import torch
import re
import glob
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import fitz  # PyMuPDF
from PIL import Image
import io

# å˜—è©¦åŒ¯å…¥ json_repairï¼Œé€™æ˜¯è™•ç† LLM çˆ› JSON çš„ç¥å™¨
try:
    import json_repair
    HAS_JSON_REPAIR = True
except ImportError:
    HAS_JSON_REPAIR = False
    print("âš ï¸ å»ºè­°åŸ·è¡Œ 'pip install json_repair' ä»¥ç²å¾—æœ€å¼·çš„å®¹éŒ¯èƒ½åŠ›ï¼")

# ==========================================
# 1. è¨­å®šå€ & æ¨¡å‹è¼‰å…¥
# ==========================================
# è¨­å®šè³‡æ–™è·¯å¾‘
BASE_DIR = "/home/maxwell/data/nlp/final_project/dataset/WattBot2025"
PDF_DIR = os.path.join(BASE_DIR, "pdf")
OUTPUT_DIR = os.path.join(BASE_DIR, "local_llm_processed")

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

print("ğŸš€ [Init] æ­£åœ¨è¼‰å…¥ Qwen2-VL-7B-Instruct åˆ° RTX 5090...")

# è¼‰å…¥æ¨¡å‹
# ä½¿ç”¨ bfloat16 ä»¥ç¯€çœé¡¯å­˜ä¸¦åŠ é€Ÿ (5090 å®Œç¾æ”¯æ´)
# ä½¿ç”¨ flash_attention_2 é€²è¡Œæ¥µè‡´åŠ é€Ÿ
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    device_map="auto"
)

# è¼‰å…¥è™•ç†å™¨
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

print("âœ… [Init] æ¨¡å‹è¼‰å…¥å®Œæˆï¼æº–å‚™é–‹å§‹ GPU é‹ç®—ã€‚")

# ==========================================
# 2. å®šç¾© Prompt (é‡å° Qwen å„ªåŒ–)
# ==========================================
SYSTEM_PROMPT = """
You are an expert in digitizing academic documents. Analyze this image of a paper page.
Extract ALL content into a structured JSON list following the reading order (Left column first, then Right column).

**Extraction Rules:**
1. **Reading Order:** Strictly follow the logical reading order of a scientific paper.
2. **Structure:** Extract Text, Headers, Tables, and Figures.
3. **Noise:** Ignore running headers, page numbers, and decorative lines.

**Output Format (Strict JSON List of Objects):**
[
  {"type": "header", "content": "Section Title (e.g., 1. Introduction)"},
  {"type": "text", "content": "Full text of the paragraph..."},
  {"type": "table", "caption": "Table 1: Title...", "content": "Markdown representation of the table"},
  {"type": "figure", "caption": "Figure 1: Title...", "content": "Detailed description of the image content/trends"}
]

Return ONLY the valid JSON string. Do not add markdown code blocks (```json).
"""

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½ (æœ¬åœ°æ¨è«–)
# ==========================================
def extract_with_local_vlm(page_image):
    """
    ä½¿ç”¨ Qwen2-VL é€²è¡Œæœ¬åœ°æ¨è«–
    """
    # å»ºæ§‹è¨Šæ¯æ ¼å¼
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": page_image, # ç›´æ¥å‚³å…¥ PIL Image
                },
                {"type": "text", "text": SYSTEM_PROMPT},
            ],
        }
    ]

    # 1. é è™•ç†è¼¸å…¥
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    
    # ç§»è‡³ GPU
    inputs = inputs.to("cuda")

    # 2. ç”Ÿæˆ (Inference)
    # max_new_tokens è¨­ç‚º 4096 ç¢ºä¿é•·æ–‡æœ¬ (å¦‚è¡¨æ ¼) ä¸æœƒè¢«åˆ‡æ–·
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=4096)
    
    # 3. è§£ç¢¼è¼¸å‡º
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    return output_text

def parse_json_output(text):
    """
    å¼·å£¯çš„ JSON è§£æå™¨ (v2.0)ï¼š
    1. ä½¿ç”¨ Regex æŠ“å– JSON å€å¡Š
    2. ä½¿ç”¨ json_repair è‡ªå‹•ä¿®å¾©æ ¼å¼éŒ¯èª¤
    """
    text = text.strip()
    
    # --- é˜²è­· 1: ä½¿ç”¨ Regex æŠ“å–æœ€å¤–å±¤çš„ List [...] ---
    # é˜²æ­¢æ¨¡å‹åœ¨ JSON å‰å¾Œè¬›å»¢è©±ï¼Œæˆ–è€…æ²’å¯« markdown block
    pattern = r"\[\s*\{.*\}\s*\]"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        text = match.group()
    else:
        # å¦‚æœ Regex æ²’æŠ“åˆ°å®Œæ•´çš„ï¼Œå˜—è©¦å¾ç¬¬ä¸€å€‹ '[' æŠ“åˆ°æœ€å¾Œ
        start_idx = text.find('[')
        if start_idx != -1:
            text = text[start_idx:]

    # --- é˜²è­· 2: å˜—è©¦è§£æ ---
    try:
        # å…ˆè©¦æ¨™æº–è§£æ
        return json.loads(text)
    except json.JSONDecodeError as e:
        # --- é˜²è­· 3: å¦‚æœå¤±æ•—ï¼Œä½¿ç”¨ json_repair ---
        if HAS_JSON_REPAIR:
            try:
                # json_repair å¯ä»¥ä¿®è£œæœªé–‰åˆçš„å¼•è™Ÿã€ç¼ºå°‘çš„é€—è™Ÿç­‰å¸¸è¦‹ LLM éŒ¯èª¤
                repaired_obj = json_repair.loads(text)
                print(f"   ğŸ”§ JSON æ ¼å¼æœ‰èª¤ (Raw len: {len(text)})ï¼Œå·²è‡ªå‹•ä¿®å¾©ï¼")
                return repaired_obj
            except Exception:
                pass # å¦‚æœé€£ repair éƒ½å¤±æ•—ï¼Œå°±å¾€ä¸‹èµ°
        
        print(f"   âš ï¸ JSON è§£æåš´é‡å¤±æ•— (Raw text length: {len(text)})")
        # å›å‚³åŒ…å«åŸå§‹æ–‡å­—çš„éŒ¯èª¤ç‰©ä»¶ï¼Œé¿å…è³‡æ–™éºå¤±
        return [{
            "type": "error_parsing", 
            "content": "JSON Parsing Failed",
            "raw_content": text
        }]

def process_pdf(pdf_path):
    doc_id = os.path.splitext(os.path.basename(pdf_path))[0]
    output_json_path = os.path.join(OUTPUT_DIR, f"{doc_id}.json")
    
    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œæ”¯æ´ä¸­æ–·å¾ŒçºŒå‚³
    if os.path.exists(output_json_path):
        print(f"â© {doc_id} å·²å­˜åœ¨ï¼Œè·³éã€‚")
        return

    print(f"ğŸš€ é–‹å§‹è™•ç†: {doc_id}")
    
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"âŒ ç„¡æ³•é–‹å•Ÿ: {e}")
        return

    all_data = []
    global_idx = 0  # å…¨åŸŸ ID è¨ˆæ•¸å™¨

    for page_num, page in enumerate(doc):
        current_page = page_num + 1
        print(f"   -> Page {current_page}/{len(doc)}...", end="", flush=True)

        # 1. è½‰åœ–ç‰‡ (300 DPI å° OCR å¾ˆé‡è¦)
        pix = page.get_pixmap(dpi=300)
        img = Image.open(io.BytesIO(pix.tobytes("png")))

        # 2. æœ¬åœ°æ¨è«– (é€Ÿåº¦å–æ±ºæ–¼ä½ çš„ GPUï¼Œ5090 æ‡‰è©²éå¸¸å¿«)
        try:
            raw_output = extract_with_local_vlm(img)
            extracted_items = parse_json_output(raw_output)
            
            if extracted_items:
                # è¨ˆç®—æœ‰æ•ˆå€å¡Š (æ’é™¤ error_parsing)
                valid_items = [x for x in extracted_items if x.get('type') != 'error_parsing']
                print(f" âœ… æŠ“åˆ° {len(valid_items)} å€‹å€å¡Š")
                
                # è£œä¸Š metadata
                for item in extracted_items:
                    # å¦‚æœæ˜¯è§£æéŒ¯èª¤ï¼Œä¿ç•™å®ƒä»¥ä¾¿ Debugï¼Œä½†ä¸çµ¦ id
                    if item.get('type') != 'error_parsing':
                        item['id'] = global_idx
                        global_idx += 1
                    
                    item['doc_id'] = doc_id
                    item['page'] = current_page
                    all_data.append(item)
            else:
                print(f" âš ï¸ ç©ºå…§å®¹")
                
        except Exception as e:
            print(f" âŒ GPU Inference Error: {e}")

        # ä¸éœ€è¦ sleepï¼Œå› ç‚ºæ˜¯æœ¬åœ°é‹ç®—

    # å­˜æª”
    with open(output_json_path, "w", encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ å®Œæˆï¼å­˜æª”è‡³: {output_json_path}\n")

# ==========================================
# 4. åŸ·è¡Œ (æ‰¹é‡è™•ç†)
# ==========================================
if __name__ == "__main__":
    # ä½¿ç”¨ glob æŠ“å–è³‡æ–™å¤¾ä¸‹æ‰€æœ‰ .pdf æª”æ¡ˆ
    pdf_files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    
    if not pdf_files:
        print(f"âŒ åœ¨ {PDF_DIR} æ‰¾ä¸åˆ°ä»»ä½• PDF æª”æ¡ˆã€‚")
    else:
        print(f"ğŸ“‚ æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆï¼Œ5090 å¼•æ“å…¨é–‹ï¼")
        print("="*50)
        
        for i, pdf_path in enumerate(pdf_files):
            filename = os.path.basename(pdf_path)
            print(f"ğŸ”„ [{i+1}/{len(pdf_files)}] æ­£åœ¨è™•ç†: {filename}")
            process_pdf(pdf_path)
            print("-" * 50)
            
        print("âœ… æ‰€æœ‰ PDF è™•ç†å®Œæˆï¼")