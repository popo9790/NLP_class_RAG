{"type": "section", "content": "Power Hungry Processing:⚡Watts⚡Driving the Cost of AI Deployment?\n\nALEXANDRA SASHA LUCCIONI and YACINE JERNITE, Hugging Face, Canada/USA\n\nEMMA STRUBELL, Carnegie Mellon University, Allen Institute for AI, USA", "doc_id": "luccioni2024", "page": 1, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Power Hungry Processing:⚡Watts⚡Driving the Cost of AI Deployment?\n\nALEXANDRA SASHA LUCCIONI and YACINE JERNITE, Hugging Face, Canada/USA\n\nEMMA STRUBELL, Carnegie Mellon University, Allen Institute for AI, USA", "original_types": ["text", "header"], "id": 0}
{"type": "figure", "content": "Fig. 1. The tasks examined in our study and the average quantity of carbon emissions they produced (in g of CO2eq) for 1,000 queries. N.B. The y axis is in logarithmic scale.", "doc_id": "luccioni2024", "page": 1, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 1. The tasks examined in our study and the average quantity of carbon emissions they produced (in g of CO2eq) for 1,000 queries. N.B. The y axis is in logarithmic scale.", "id": 1}
{"type": "section", "content": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.", "doc_id": "luccioni2024", "page": 1, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.", "original_types": ["text"], "id": 2}
{"type": "section", "content": "1. INTRODUCTION", "doc_id": "luccioni2024", "page": 2, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "1. INTRODUCTION", "original_types": ["header"], "id": 3}
{"type": "section", "content": "Understanding the environmental impacts of different industries is an important first step towards developing effective strategies to mitigate those impacts. For newer industries such as information and communication technologies (ICT) of which Artificial Intelligence (AI) and Machine Learning (ML) are considered to be a part of, more work is needed to understand the extent of their environmental impacts and the factors that influence it. Between 2017 and 2021, the electricity used by Meta, Amazon, Microsoft, and Google, the main providers of commercially-available cloud compute, more than doubled [22]. According to the most recent figures available, global data centre electricity consumption has grown by 20-40% annually in recent years, reaching 1-1.3% of global electricity demand and contributing 1% of energy-related greenhouse gas emissions in 2022 [21]. However the contribution of the AI sector specifically towards these figures is unclear. \nRecent work documenting the environmental impacts of ML has focused largely on quantifying the operational energy and carbon required to perform the training phase of the ML model life cycle [12, 30, 41, 49] due to the relative ease of measuring per-model energy use for that phase and the impressive quantity of energy required to perform a single training run [41, 49]. Yet, other phases of the ML model life cycle, such as inference, stand to impact the environment just as much, or more, than training due to the computational resources required to deploy modern models at scale. While inference on a single example requires much less computation than that required to train the same model, inference happens far more frequently than model training — as many as billions of times a day for a model powering a popular user-facing product such as Google Translate.1 Yet, in-depth work quantifying the costs of model inference and deployment is limited and their environmental impacts, in terms of energy and carbon as well as water and mining of rare earth minerals, have yet to be estimated. According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by Meta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the remainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training [40]. Given the increasing ubiquity of AI model deployment, it is crucial to go beyond these high-level statistics to get a better idea of the energy requirements and carbon emissions of model inference for different models and tasks. In particular, looking at inference rather than training leads to drastically different conclusions when considering the multi-purpose (or “general-purpose”) aspect specifically. Training a single model for multiple tasks can indeed be more energy-efficient when considering training costs only, but these gains can easily be lost and even reversed over the course of the model’s lifetime, given how much inference is carried out when these models are deployed in user-facing applications like chat and web search. \nTo help shed light on this issue, we perform an extensive study measuring the amount of energy required to deploy various ML models and architectures, including large language models (LLMs)- as such, our study is, to our knowledge, the first to focus solely on the inference phase of the ML model life cycle. We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency. We identify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and tasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and ensuing carbon emissions. By painting a more detailed picture of widely varying energy requirements for ML model", "doc_id": "luccioni2024", "page": 2, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Understanding the environmental impacts of different industries is an important first step towards developing effective strategies to mitigate those impacts. For newer industries such as information and communication technologies (ICT) of which Artificial Intelligence (AI) and Machine Learning (ML) are considered to be a part of, more work is needed to understand the extent of their environmental impacts and the factors that influence it. Between 2017 and 2021, the electricity used by Meta, Amazon, Microsoft, and Google, the main providers of commercially-available cloud compute, more than doubled [22]. According to the most recent figures available, global data centre electricity consumption has grown by 20-40% annually in recent years, reaching 1-1.3% of global electricity demand and contributing 1% of energy-related greenhouse gas emissions in 2022 [21]. However the contribution of the AI sector specifically towards these figures is unclear. \nRecent work documenting the environmental impacts of ML has focused largely on quantifying the operational energy and carbon required to perform the training phase of the ML model life cycle [12, 30, 41, 49] due to the relative ease of measuring per-model energy use for that phase and the impressive quantity of energy required to perform a single training run [41, 49]. Yet, other phases of the ML model life cycle, such as inference, stand to impact the environment just as much, or more, than training due to the computational resources required to deploy modern models at scale. While inference on a single example requires much less computation than that required to train the same model, inference happens far more frequently than model training — as many as billions of times a day for a model powering a popular user-facing product such as Google Translate.1 Yet, in-depth work quantifying the costs of model inference and deployment is limited and their environmental impacts, in terms of energy and carbon as well as water and mining of rare earth minerals, have yet to be estimated. According to AWS, the largest global cloud provider, inference is estimated to make up 80 to 90% of total ML cloud computing demand [2, 28], whereas a 2021 publication by Meta attributed approximately one-third of their internal end-to-end ML carbon footprint to model inference, with the remainder produced by data management, storage, and training [57]; similarly, a 2022 study from Google attributed 60% of its ML energy use to inference, compared to 40% for training [40]. Given the increasing ubiquity of AI model deployment, it is crucial to go beyond these high-level statistics to get a better idea of the energy requirements and carbon emissions of model inference for different models and tasks. In particular, looking at inference rather than training leads to drastically different conclusions when considering the multi-purpose (or “general-purpose”) aspect specifically. Training a single model for multiple tasks can indeed be more energy-efficient when considering training costs only, but these gains can easily be lost and even reversed over the course of the model’s lifetime, given how much inference is carried out when these models are deployed in user-facing applications like chat and web search. \nTo help shed light on this issue, we perform an extensive study measuring the amount of energy required to deploy various ML models and architectures, including large language models (LLMs)- as such, our study is, to our knowledge, the first to focus solely on the inference phase of the ML model life cycle. We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency. We identify orders-of-magnitude differences in the amount of energy required per inference across models, modalities and tasks and shine light on an important trade-off between the benefit of multi-purpose systems, their energy cost, and ensuing carbon emissions. By painting a more detailed picture of widely varying energy requirements for ML model", "id": 4}
{"type": "section", "content": "2 PREVIOUS WORK\n\nEstimating the energy and emissions of ML models has remains a relatively under-explored topic, albeit one that has been gathering traction since Strubell et al’s seminal article quantifying the energy and carbon emissions of a variety of then-large NLP models [2019]. Since then, most studies have focused on estimating the energy consumed and carbon emitted during the training phase of neural networks – this includes studies by Patterson et al. [2022, 2021], who compared different models and analyzed factors influencing their emissions. There have also been studies of specific model architectures, e.g. BLOOM [31] and Nour [27], which carried out in-depth analyses of the different steps in the models’ life cycle and their relative contribution towards the final quantity of carbon emissions. Given the increasing deployment of ML models in the cloud, several studies have therefore looked at cloud-specific ways to reduce the emissions of ML models such as delayed scheduling, workload elasticity and choosing the least carbon-intensive electricity available Chien et al. [6], Dodge et al. [12], Hanafy et al. [19].\n\nDespite these empirical studies, there is currently a lack of standardized methodology for quantifying and comparing the energy consumption and carbon emissions of ML models. There are several tools that exist, such as Code Carbon [47], MLCO2 [26] and LLMCarbon [13], all of which adopt different approaches and output different results (see [1] for a detailed comparison). It is therefore difficult to systematically compare the carbon footprints of different models. Existing tools and studies have also largely focused on the dynamic power consumption (i.e. the electricity necessary for powering hardware) and its resulting emissions. However, there have been several proposals to also take into account the embodied emissions of ML models (i.e. the emissions that can be attributed to the manufacturing of computing equipment) into carbon emissions estimates. This has been impeded by a lack of transparency from the designers of common computing hardware such as GPUs, although recent estimates have revealed that the embodied carbon footprint of an LLM trained and deployed on Meta’s compute cluster constitutes up to 50% of its carbon footprint [57]. While the majority of existing work has been focused on ML model training given that it is a more tractable part of the model life cycle (i.e. it is most often carried out over a set period of time on a specific compute instance), model inference has started to also become the subject of scholarship [6, 11]. Luccioni et al.’s study of BLOOM was the first of its kind to look at the specific energy costs related to deploying an LLM [31] and found that, over time, this can represent a significant portion of a model’s overall carbon footprint.", "doc_id": "luccioni2024", "page": 3, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "2 PREVIOUS WORK\n\nEstimating the energy and emissions of ML models has remains a relatively under-explored topic, albeit one that has been gathering traction since Strubell et al’s seminal article quantifying the energy and carbon emissions of a variety of then-large NLP models [2019]. Since then, most studies have focused on estimating the energy consumed and carbon emitted during the training phase of neural networks – this includes studies by Patterson et al. [2022, 2021], who compared different models and analyzed factors influencing their emissions. There have also been studies of specific model architectures, e.g. BLOOM [31] and Nour [27], which carried out in-depth analyses of the different steps in the models’ life cycle and their relative contribution towards the final quantity of carbon emissions. Given the increasing deployment of ML models in the cloud, several studies have therefore looked at cloud-specific ways to reduce the emissions of ML models such as delayed scheduling, workload elasticity and choosing the least carbon-intensive electricity available Chien et al. [6], Dodge et al. [12], Hanafy et al. [19].\n\nDespite these empirical studies, there is currently a lack of standardized methodology for quantifying and comparing the energy consumption and carbon emissions of ML models. There are several tools that exist, such as Code Carbon [47], MLCO2 [26] and LLMCarbon [13], all of which adopt different approaches and output different results (see [1] for a detailed comparison). It is therefore difficult to systematically compare the carbon footprints of different models. Existing tools and studies have also largely focused on the dynamic power consumption (i.e. the electricity necessary for powering hardware) and its resulting emissions. However, there have been several proposals to also take into account the embodied emissions of ML models (i.e. the emissions that can be attributed to the manufacturing of computing equipment) into carbon emissions estimates. This has been impeded by a lack of transparency from the designers of common computing hardware such as GPUs, although recent estimates have revealed that the embodied carbon footprint of an LLM trained and deployed on Meta’s compute cluster constitutes up to 50% of its carbon footprint [57]. While the majority of existing work has been focused on ML model training given that it is a more tractable part of the model life cycle (i.e. it is most often carried out over a set period of time on a specific compute instance), model inference has started to also become the subject of scholarship [6, 11]. Luccioni et al.’s study of BLOOM was the first of its kind to look at the specific energy costs related to deploying an LLM [31] and found that, over time, this can represent a significant portion of a model’s overall carbon footprint.", "original_types": ["text", "header"], "id": 5}
{"type": "section", "content": "3.1 Task and dataset selection\n\nAs the starting point of our study, we chose 10 ML tasks from 5 different modalities: Text-to-category (text classification, token classification, extractive question answering), Text-to-text (masked language modeling, text generation, summarization), Image-to-category (image classification, object detection), Image-to-text (image captioning) and Text-to-image (image generation). These tasks were chosen because they are common in both Natural Language Processing and Computer Vision, allowing us to explore multiple modalities, and include several multimodal tasks (i.e. image captioning and image generation), allowing us to explore the nexus between several modalities as well. To test each of the tasks listed above, we chose three of the most downloaded datasets from the Hugging Face Hub. We present the tasks and their corresponding datasets in Table 1.", "doc_id": "luccioni2024", "page": 4, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "3.1 Task and dataset selection\n\nAs the starting point of our study, we chose 10 ML tasks from 5 different modalities: Text-to-category (text classification, token classification, extractive question answering), Text-to-text (masked language modeling, text generation, summarization), Image-to-category (image classification, object detection), Image-to-text (image captioning) and Text-to-image (image generation). These tasks were chosen because they are common in both Natural Language Processing and Computer Vision, allowing us to explore multiple modalities, and include several multimodal tasks (i.e. image captioning and image generation), allowing us to explore the nexus between several modalities as well. To test each of the tasks listed above, we chose three of the most downloaded datasets from the Hugging Face Hub. We present the tasks and their corresponding datasets in Table 1.", "original_types": ["text", "header"], "id": 6}
{"type": "table", "content": "Table 1. A list of the tasks and datasets used in our study.\n| Task | Datasets | Task | Datasets |\n| --- | --- | --- | --- |\n| image classification | CIFAR 10 [25] | question answering | SQuAD[44] \nCIFAR 100 [25] | SQuAD v2 [43] \nImageNet 1K [45] | SciQ [23] |\n\n| image captioning | Visual Genome [24] | summarization | SAMSum [15] \nRedCaps [10] | CNN-Daily Mail [20] \nCOCO [29] | XSum [35] |\n\n| image generation | DiffusionDB [54] | text classification | IMDB [32] \nImageReward [58] | Rotten Tomatoes [39] \nSD Prompts [46] | SST 2 [48] |\n\n| masked language modeling | BookCorpus [59] | text generation | WikiText [33] \nC4 [42] | BookCorpus [59] \nOSCAR [37] |\n\n| object detection | Visual Genome [24] | token classification | ReCoRD [53] \nCPPE-5 [9] | WikiANN [38] \nCOCO [29] | CoNLL 2003 [50]", "doc_id": "luccioni2024", "page": 4, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 1. A list of the tasks and datasets used in our study.\n| Task | Datasets | Task | Datasets |\n| --- | --- | --- | --- |\n| image classification | CIFAR 10 [25] | question answering | SQuAD[44] \nCIFAR 100 [25] | SQuAD v2 [43] \nImageNet 1K [45] | SciQ [23] |\n\n| image captioning | Visual Genome [24] | summarization | SAMSum [15] \nRedCaps [10] | CNN-Daily Mail [20] \nCOCO [29] | XSum [35] |\n\n| image generation | DiffusionDB [54] | text classification | IMDB [32] \nImageReward [58] | Rotten Tomatoes [39] \nSD Prompts [46] | SST 2 [48] |\n\n| masked language modeling | BookCorpus [59] | text generation | WikiText [33] \nC4 [42] | BookCorpus [59] \nOSCAR [37] |\n\n| object detection | Visual Genome [24] | token classification | ReCoRD [53] \nCPPE-5 [9] | WikiANN [38] \nCOCO [29] | CoNLL 2003 [50]", "id": 7}
{"type": "section", "content": "Multi-Purpose Models\n\nIn addition to the task-specific models listed above, we also selected 8 multi-purpose models to analyze on different tasks – models that were specifically trained to perform well in various different application settings. We chose 4 sequence-to-sequence models of different sizes from the Flan-T5 family [8] (base, large, xl and xxl) and 4 decoder-only models from the BLOOMz family [34]: BLOOMz-560M, BLOOMz-1B, BLOOMz-3B and BLOOMz-7B. We tested these on a subset of the tasks to allow a comparison of multi-purpose generative models with individual task-specific systems in terms of their energy consumption and emissions: question answering, text classification and summarization. We selected these three tasks because we were able to find a set of models that were capable of carrying them out with a unified model architecture (which wasn’t possible for all tasks, especially ones that involved multiple modalities.) We prompted these 8 models in a zero-shot setting that was constant across models, e.g. \"Summarize the following text: [text]. Summary:\" on the same 1,000 samples as the fine-tuned models, also repeating each experiment ten times to measure the significance of results.\n\nWe ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference 3. Given that all of our experiments were run in the same compute region (AWS’s us-west-2), which is based in Oregon and has an average carbon intensity of 297.6 grams of CO2eq per kWh4, this means that both the energy consumed during inference and the carbon emitted are correlated; we will therefore plot one or the other depending on which aspect of our results we are discussing. While the energy consumed during inference will remain similar for models deployed on A100 GPUs in other compute regions, the carbon emissions will vary depending on the source of energy used in the region – it is therefore helpful to report both energy and carbon separately to allow for meaningful comparisons across regions and hardware. We provide all the code used for our experiments in our GitHub repository, alongside the logs produced by Code Carbon, which not only provides the total energy consumed but also a more fine-grained breakdown by hardware component (GPU, CPU and RAM), which can be used to carry out further analyses. In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.\n\n4 RESULTS", "doc_id": "luccioni2024", "page": 5, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Multi-Purpose Models\n\nIn addition to the task-specific models listed above, we also selected 8 multi-purpose models to analyze on different tasks – models that were specifically trained to perform well in various different application settings. We chose 4 sequence-to-sequence models of different sizes from the Flan-T5 family [8] (base, large, xl and xxl) and 4 decoder-only models from the BLOOMz family [34]: BLOOMz-560M, BLOOMz-1B, BLOOMz-3B and BLOOMz-7B. We tested these on a subset of the tasks to allow a comparison of multi-purpose generative models with individual task-specific systems in terms of their energy consumption and emissions: question answering, text classification and summarization. We selected these three tasks because we were able to find a set of models that were capable of carrying them out with a unified model architecture (which wasn’t possible for all tasks, especially ones that involved multiple modalities.) We prompted these 8 models in a zero-shot setting that was constant across models, e.g. \"Summarize the following text: [text]. Summary:\" on the same 1,000 samples as the fine-tuned models, also repeating each experiment ten times to measure the significance of results.\n\nWe ran all of our experiments on a node of 8 NVIDIA A100-SXM4-80GB GPUs hosted on Amazon Web Services, and used the Code Carbon package [47] to measure both the energy consumed and the carbon emitted during inference 3. Given that all of our experiments were run in the same compute region (AWS’s us-west-2), which is based in Oregon and has an average carbon intensity of 297.6 grams of CO2eq per kWh4, this means that both the energy consumed during inference and the carbon emitted are correlated; we will therefore plot one or the other depending on which aspect of our results we are discussing. While the energy consumed during inference will remain similar for models deployed on A100 GPUs in other compute regions, the carbon emissions will vary depending on the source of energy used in the region – it is therefore helpful to report both energy and carbon separately to allow for meaningful comparisons across regions and hardware. We provide all the code used for our experiments in our GitHub repository, alongside the logs produced by Code Carbon, which not only provides the total energy consumed but also a more fine-grained breakdown by hardware component (GPU, CPU and RAM), which can be used to carry out further analyses. In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.\n\n4 RESULTS", "original_types": ["text", "header"], "id": 8}
{"type": "section", "content": "We present our results in the subsections below: in Section 4.1, we analyze the range of energy used and carbon emitted for each task for task-specific models. In Section 4.2, we shift our focus to multi-purpose (i.e. ‘zero-shot’ models), looking at the variation between different sizes and architectures of multi-purpose models and the difference in the energy consumption and emissions between task-specific and multi-purpose models. In Section 4.3, we carry out a comparison between model training and inference costs for models of different sizes, calculating when parity is reached.\n\n4.1 Task-specific model analysis\n\nWe start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety of tasks. Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for 1,000 inferences and its standard deviation. We can see that classification tasks for both images and text are on the lower end of the spectrum in terms of emissions (ranging between 0.002 and 0.007 kWh for 1,000 inferences), whereas", "doc_id": "luccioni2024", "page": 5, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "We present our results in the subsections below: in Section 4.1, we analyze the range of energy used and carbon emitted for each task for task-specific models. In Section 4.2, we shift our focus to multi-purpose (i.e. ‘zero-shot’ models), looking at the variation between different sizes and architectures of multi-purpose models and the difference in the energy consumption and emissions between task-specific and multi-purpose models. In Section 4.3, we carry out a comparison between model training and inference costs for models of different sizes, calculating when parity is reached.\n\n4.1 Task-specific model analysis\n\nWe start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety of tasks. Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for 1,000 inferences and its standard deviation. We can see that classification tasks for both images and text are on the lower end of the spectrum in terms of emissions (ranging between 0.002 and 0.007 kWh for 1,000 inferences), whereas", "original_types": ["text", "header"], "id": 9}
{"type": "section", "content": "generative tasks such as text generation and summarization use, on average, over 10 times more energy for the same number of inferences (around 0.05 kWh for 1,000 inferences), and multimodal tasks such as image captioning and image generation are on the highest end of the spectrum (0.06-2.9 kWh for 1,000 inferences). Text-based tasks are, all things considered, more energy-efficient than image-based tasks, with image classification requiring less energy (median of 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH) requiring more than text classification (0.0023 kWh). For comparison, charging the average smartphone requires 0.022 kWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full smartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as 522 smartphone charges (11.49 kWh), or around half a charge per image generation –, although there is also a large variation between image generation models, depending on the size of image that they generate.", "doc_id": "luccioni2024", "page": 6, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "generative tasks such as text generation and summarization use, on average, over 10 times more energy for the same number of inferences (around 0.05 kWh for 1,000 inferences), and multimodal tasks such as image captioning and image generation are on the highest end of the spectrum (0.06-2.9 kWh for 1,000 inferences). Text-based tasks are, all things considered, more energy-efficient than image-based tasks, with image classification requiring less energy (median of 0.0068 kWh for 1,000 inferences) than image generation (1.35 kWh) and, conversely, text generation (0.042 KwH) requiring more than text classification (0.0023 kWh). For comparison, charging the average smartphone requires 0.022 kWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full smartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as 522 smartphone charges (11.49 kWh), or around half a charge per image generation –, although there is also a large variation between image generation models, depending on the size of image that they generate.", "original_types": ["text"], "id": 10}
{"type": "table", "content": "Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.\n| task | mean | std |\n|---|---|---|\n| text classification | 0.002 | 0.001 |\n| extractive QA | 0.003 | 0.001 |\n| masked language modeling | 0.003 | 0.001 |\n| token classification | 0.004 | 0.002 |\n| image classification | 0.007 | 0.001 |\n| object detection | 0.038 | 0.02 |\n| text generation | 0.047 | 0.03 |\n| summarization | 0.049 | 0.01 |\n| image captioning | 0.063 | 0.02 |\n| image generation | 2.907 | 3.31 |\n", "doc_id": "luccioni2024", "page": 6, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.\n| task | mean | std |\n|---|---|---|\n| text classification | 0.002 | 0.001 |\n| extractive QA | 0.003 | 0.001 |\n| masked language modeling | 0.003 | 0.001 |\n| token classification | 0.004 | 0.002 |\n| image classification | 0.007 | 0.001 |\n| object detection | 0.038 | 0.02 |\n| text generation | 0.047 | 0.03 |\n| summarization | 0.049 | 0.01 |\n| image captioning | 0.063 | 0.02 |\n| image generation | 2.907 | 3.31 |\n", "id": 11}
{"type": "figure", "content": "Fig. 2. The 5 modalities examined in our study, with the number of parameters of each model on the x axis and the average amount of carbon emitted for 1000 inferences on the y axis. NB: Both axes are in logarithmic scale.", "doc_id": "luccioni2024", "page": 7, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 2. The 5 modalities examined in our study, with the number of parameters of each model on the x axis and the average amount of carbon emitted for 1000 inferences on the y axis. NB: Both axes are in logarithmic scale.", "id": 12}
{"type": "section", "content": "4.2 The environmental cost of multi-purpose systems\n\nThe second part of our analysis examines multi-task models of two types: decoder only, from the BLOOMz family, and sequence-to-sequence models from the FLAN-T5 family, with the goal of comparing energy intensity and carbon emissions of models with differing numbers of parameters when applied to different tasks. To address this question, we selected a subset of 3 tasks – text classification, extractive question answering, and summarization – given their diversity and broad applicability in a variety of settings, and compare the 8 zero-shot models of different sizes, based on the same 3 datasets per task as described in Table 1.\n\n4.2.1 Emissions of task-specific and multi-task architectures\n\nTo start our analysis, we examined how the choice of model and architecture type impacts emissions given a specific task and dataset. For this analysis, we took the same 8 task-specific models described in Section 3.2 and compared their emissions to the 8 multi-purpose models described above.", "doc_id": "luccioni2024", "page": 8, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "4.2 The environmental cost of multi-purpose systems\n\nThe second part of our analysis examines multi-task models of two types: decoder only, from the BLOOMz family, and sequence-to-sequence models from the FLAN-T5 family, with the goal of comparing energy intensity and carbon emissions of models with differing numbers of parameters when applied to different tasks. To address this question, we selected a subset of 3 tasks – text classification, extractive question answering, and summarization – given their diversity and broad applicability in a variety of settings, and compare the 8 zero-shot models of different sizes, based on the same 3 datasets per task as described in Table 1.\n\n4.2.1 Emissions of task-specific and multi-task architectures\n\nTo start our analysis, we examined how the choice of model and architecture type impacts emissions given a specific task and dataset. For this analysis, we took the same 8 task-specific models described in Section 3.2 and compared their emissions to the 8 multi-purpose models described above.", "original_types": ["text", "header"], "id": 13}
{"type": "section", "content": "generate 4-10g of CO2eq for 1,000 inferences, while multi-purpose models emit 20-30g for the same task. The difference appears to mostly come from model size – all of the task-specific summarization models we looked at were 600 million parameters at most, compared to the larger multi-purpose architectures, which attained the 11 billion parameters.\n\nWe also carry out an evaluation of both the task-specific and multi-purpose models examined in our study to ensure that they have comparable performance. For task-specific models, we used the evaluate library [52] and the LM Evaluation Harness [14] for zero-shot models. Fundamentally speaking, it is hard to compare task-specific and multi-purpose models using the same metrics, given that task-specific models have a much more constrained decision space (e.g. two classes in the case of binary text classification), whereas multi-purpose models have a large output vocabulary to choose from, and are dependent upon the prompt schema and prompting strategy used. However, by utilizing two standardized packages (evaluate and lm-evaluation-harness) and keeping the prompting approach stable across zero-shot models, we endeavor to standardize our evaluation approach as much as possible.", "doc_id": "luccioni2024", "page": 9, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "generate 4-10g of CO2eq for 1,000 inferences, while multi-purpose models emit 20-30g for the same task. The difference appears to mostly come from model size – all of the task-specific summarization models we looked at were 600 million parameters at most, compared to the larger multi-purpose architectures, which attained the 11 billion parameters.\n\nWe also carry out an evaluation of both the task-specific and multi-purpose models examined in our study to ensure that they have comparable performance. For task-specific models, we used the evaluate library [52] and the LM Evaluation Harness [14] for zero-shot models. Fundamentally speaking, it is hard to compare task-specific and multi-purpose models using the same metrics, given that task-specific models have a much more constrained decision space (e.g. two classes in the case of binary text classification), whereas multi-purpose models have a large output vocabulary to choose from, and are dependent upon the prompt schema and prompting strategy used. However, by utilizing two standardized packages (evaluate and lm-evaluation-harness) and keeping the prompting approach stable across zero-shot models, we endeavor to standardize our evaluation approach as much as possible.", "original_types": ["text"], "id": 14}
{"type": "figure", "content": "Fig. 4. Model size, measured in number of parameters (x axis, logarithmic scale) and text classification accuracy (y axis), with dot size indicating the quantity of emissions (logarithmic scale.)", "doc_id": "luccioni2024", "page": 9, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 4. Model size, measured in number of parameters (x axis, logarithmic scale) and text classification accuracy (y axis), with dot size indicating the quantity of emissions (logarithmic scale.)", "id": 15}
{"type": "section", "content": "We hone in on one specific task, text classification, in Figure 4, which illustrates the relationship between model size (x axis, in logarithmic scale), accuracy (y axis) and emissions (dot size, in logarithmic scale). Among task-specific encoder models, we observe that accuracy varies more widely, i.e. there are several smaller models of similar size and comparably small amounts of carbon emissions, with widely varying levels of accuracy. The multi-purpose models vary less in terms of accuracy, having higher average accuracy overall. Both sequence-to-sequence and decoder-only models produce comparable amounts of emissions (several orders of magnitude more than task-specific models).We can see that mid-size multi-purpose models (in the 3B parameter range) may have slightly better accuracy compared to both larger and smaller models. However, given the many caveats and specificities involved in multi-purpose LLM evaluation, this difference may not be significant. We present the full results of our evaluation, which include the other 2 tasks, in Section B in the Supplementary Materials.", "doc_id": "luccioni2024", "page": 9, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "We hone in on one specific task, text classification, in Figure 4, which illustrates the relationship between model size (x axis, in logarithmic scale), accuracy (y axis) and emissions (dot size, in logarithmic scale). Among task-specific encoder models, we observe that accuracy varies more widely, i.e. there are several smaller models of similar size and comparably small amounts of carbon emissions, with widely varying levels of accuracy. The multi-purpose models vary less in terms of accuracy, having higher average accuracy overall. Both sequence-to-sequence and decoder-only models produce comparable amounts of emissions (several orders of magnitude more than task-specific models).We can see that mid-size multi-purpose models (in the 3B parameter range) may have slightly better accuracy compared to both larger and smaller models. However, given the many caveats and specificities involved in multi-purpose LLM evaluation, this difference may not be significant. We present the full results of our evaluation, which include the other 2 tasks, in Section B in the Supplementary Materials.", "original_types": ["text"], "id": 16}
{"type": "section", "content": "Differences within multi-purpose architectures.\n\nBeyond the differences between task-specific and multi-purpose models generally, we also observed variation within the multi-purpose models that we examined. We present our results in Table 3; in it, we can observe that on a per-architecture basis (i.e. within the family of decoder-only models and the family of sequence-to-sequence models), size and emissions are correlated, with smaller models emitting less carbon and using less energy. However, sequence-to-sequence models are more efficient than their decoder-only counterparts when models of the same size are compared: for instance, Flan-T5-XL and BLOOMz-3B are both of a similar size (around 3B parameters), but the former generates, on average, 2 grams of emissions less for 1,000 inferences than the latter. This difference holds when comparing Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we tested (11 billion), yet it has lower emissions (11.48g on average) compared to the smaller BLOOMz-7B. Comparing the models on a per-task basis in Figure 5, we can see the same pattern for zero-shot models as for task-specific ones, with text classification a less carbon-intensive task compared to question answering, and summarization the most intensive one of the three. The spread between the tasks is smaller for sequence-to-sequence models (indicated with dots in Figure 5), whereas for decoder-only models (indicated with crosses), the difference between the different tasks is more significant.", "doc_id": "luccioni2024", "page": 10, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Differences within multi-purpose architectures.\n\nBeyond the differences between task-specific and multi-purpose models generally, we also observed variation within the multi-purpose models that we examined. We present our results in Table 3; in it, we can observe that on a per-architecture basis (i.e. within the family of decoder-only models and the family of sequence-to-sequence models), size and emissions are correlated, with smaller models emitting less carbon and using less energy. However, sequence-to-sequence models are more efficient than their decoder-only counterparts when models of the same size are compared: for instance, Flan-T5-XL and BLOOMz-3B are both of a similar size (around 3B parameters), but the former generates, on average, 2 grams of emissions less for 1,000 inferences than the latter. This difference holds when comparing Flan-T5-XXL, which is the biggest model in terms of parameter count in the multi-purpose models that we tested (11 billion), yet it has lower emissions (11.48g on average) compared to the smaller BLOOMz-7B. Comparing the models on a per-task basis in Figure 5, we can see the same pattern for zero-shot models as for task-specific ones, with text classification a less carbon-intensive task compared to question answering, and summarization the most intensive one of the three. The spread between the tasks is smaller for sequence-to-sequence models (indicated with dots in Figure 5), whereas for decoder-only models (indicated with crosses), the difference between the different tasks is more significant.", "original_types": ["text", "header"], "id": 17}
{"type": "table", "content": "Table 3. Zero-shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of emissions (in g of CO2eq) and average energy usage (in kWh) for 1,000 inferences.\n```markdown\n| seq2seq models | decoder-only models |\n|----------------|---------------------|\n| model name     | number of parameters | emissions (g CO2eq) | energy (kWh) | model name     | number of parameters | emissions (g CO2eq) | energy (kWh) |\n|----------------|---------------------|---------------------|--------------|----------------|---------------------|---------------------|--------------|\n| Flan-T5-base   | 222M                | 3.67                | 0.026         | BLOOMz-560M   | 559M                | 7.5                | 0.054         |\n| Flan-T5-large  | 750M                | 7.68                | 0.055         | BLOOMz-1B     | 1.7B                | 8.66               | 0.062         |\n| Flan-T5-xl     | 2.8B                | 8.08                | 0.058         | BLOOMz-3B     | 3B                  | 10.17              | 0.073         |\n| Flan-T5-xxl    | 11B                 | 11.48               | 0.083         | BLOOMz-7B     | 7B                  | 14.46              | 0.104         |\n```", "doc_id": "luccioni2024", "page": 10, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 3. Zero-shot models in our analysis with their architecture type, model size (in number of parameters), average quantity of emissions (in g of CO2eq) and average energy usage (in kWh) for 1,000 inferences.\n```markdown\n| seq2seq models | decoder-only models |\n|----------------|---------------------|\n| model name     | number of parameters | emissions (g CO2eq) | energy (kWh) | model name     | number of parameters | emissions (g CO2eq) | energy (kWh) |\n|----------------|---------------------|---------------------|--------------|----------------|---------------------|---------------------|--------------|\n| Flan-T5-base   | 222M                | 3.67                | 0.026         | BLOOMz-560M   | 559M                | 7.5                | 0.054         |\n| Flan-T5-large  | 750M                | 7.68                | 0.055         | BLOOMz-1B     | 1.7B                | 8.66               | 0.062         |\n| Flan-T5-xl     | 2.8B                | 8.08                | 0.058         | BLOOMz-3B     | 3B                  | 10.17              | 0.073         |\n| Flan-T5-xxl    | 11B                 | 11.48               | 0.083         | BLOOMz-7B     | 7B                  | 14.46              | 0.104         |\n```", "id": 18}
{"type": "figure", "content": "Fig. 5. A plot of the total emissions (in grams of \\(CO_2eq\\)) for 1,000 inferences for all multi-purpose models.", "doc_id": "luccioni2024", "page": 11, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 5. A plot of the total emissions (in grams of \\(CO_2eq\\)) for 1,000 inferences for all multi-purpose models.", "id": 19}
{"type": "table", "content": "Table 4. Average input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study. The darker the cell, the more carbon was output by the model for the task.\nAverage input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study. The darker the cell, the more carbon was output by the model for the task.", "doc_id": "luccioni2024", "page": 11, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 4. Average input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study. The darker the cell, the more carbon was output by the model for the task.\nAverage input and output length (in number of tokens) for the 8 zero-shot models and 9 tasks examined as part of our study. The darker the cell, the more carbon was output by the model for the task.", "id": 20}
{"type": "figure", "content": "Fig. 6. A plot of the output length (X axis) and carbon emissions (Y axis) for the summarization task. The symbol refers to the type of architecture (BLOOMz vs Flan-T5), symbol size references the relative model size (in terms of the number of parameters), and color the input length.", "doc_id": "luccioni2024", "page": 11, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 6. A plot of the output length (X axis) and carbon emissions (Y axis) for the summarization task. The symbol refers to the type of architecture (BLOOMz vs Flan-T5), symbol size references the relative model size (in terms of the number of parameters), and color the input length.", "id": 21}
{"type": "section", "content": "4.3 Comparing model training and inference costs\n\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs reach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as their initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the ML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz nor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers describing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family of models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation paper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate based on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary information to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption per inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4 × 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers reported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times more than BLOOMz-7B, being roughly 25 times bigger [31] - although this included API deployment of the model, which is not the case for the models in our study.", "doc_id": "luccioni2024", "page": 12, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "4.3 Comparing model training and inference costs\n\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs reach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as their initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the ML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz nor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers describing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family of models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation paper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate based on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary information to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption per inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4 × 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers reported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times more than BLOOMz-7B, being roughly 25 times bigger [31] - although this included API deployment of the model, which is not the case for the models in our study.", "original_types": ["text", "header"], "id": 22}
{"type": "table", "content": "Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.", "doc_id": "luccioni2024", "page": 12, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.", "id": 23}
{"type": "table", "content": "Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.", "doc_id": "luccioni2024", "page": 12, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.", "id": 24}
{"type": "section", "content": "5. DISCUSSION\n\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due to its distributed nature — compared to the relatively time- and location-constrained nature of training — making it difficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to keep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We provide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore the results we present here. We also highlight the main high-level takeaways of our study below:\n\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the most energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image captioning, and image generation.\n\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically, tasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving generating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\n\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a similar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that more computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for tasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as other tasks and datasets.\n\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial numbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of models from the BLOOMz family, and found that the parity between training/finetuning and inference grows with model size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model deployment, this parity can be reached quickly for many popular models.\n\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these same tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question answering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large, and less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference", "doc_id": "luccioni2024", "page": 13, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "5. DISCUSSION\n\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due to its distributed nature — compared to the relatively time- and location-constrained nature of training — making it difficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to keep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We provide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore the results we present here. We also highlight the main high-level takeaways of our study below:\n\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the most energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image captioning, and image generation.\n\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically, tasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving generating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\n\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a similar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that more computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for tasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as other tasks and datasets.\n\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial numbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of models from the BLOOMz family, and found that the parity between training/finetuning and inference grows with model size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model deployment, this parity can be reached quickly for many popular models.\n\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these same tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question answering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large, and less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference", "original_types": ["text", "header"], "id": 25}
{"type": "section", "content": "between multi-purpose models and task-specific models is amplified as the length of output gets longer.\n\nWe find this last point to be the most compelling takeaway of our study, given the current paradigm shift away from smaller models finetuned for a specific task towards models that are meant to carry out a multitude of tasks at once, deployed to respond to a barrage of user queries in real time. This transition has been happening both in ML research since the advent of GPT-3 [5], which illustrated the potential for few- and zero-shot learning with language models, as well as in consumer settings, with LLMs such as GPT-4 and PaLM being deployed in user-facing products such as web search [4, 18], email, and navigation [17], where smaller, task-specific versions of models such as BERT were previously used [3, 16]. While it is hard to quantify the environmental impacts of this transition given the lack of transparency of technology companies regarding both the number of parameters, architecture and carbon emissions of their products, we can make a comparison based on the experiments carried out in the present study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering (bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70g CO2eq per 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for BLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM model, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see the benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see convincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web search and navigation, given these models’ energy requirements.", "doc_id": "luccioni2024", "page": 14, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "between multi-purpose models and task-specific models is amplified as the length of output gets longer.\n\nWe find this last point to be the most compelling takeaway of our study, given the current paradigm shift away from smaller models finetuned for a specific task towards models that are meant to carry out a multitude of tasks at once, deployed to respond to a barrage of user queries in real time. This transition has been happening both in ML research since the advent of GPT-3 [5], which illustrated the potential for few- and zero-shot learning with language models, as well as in consumer settings, with LLMs such as GPT-4 and PaLM being deployed in user-facing products such as web search [4, 18], email, and navigation [17], where smaller, task-specific versions of models such as BERT were previously used [3, 16]. While it is hard to quantify the environmental impacts of this transition given the lack of transparency of technology companies regarding both the number of parameters, architecture and carbon emissions of their products, we can make a comparison based on the experiments carried out in the present study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering (bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70g CO2eq per 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for BLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification with the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of CO2eq per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM model, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see the benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see convincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web search and navigation, given these models’ energy requirements.", "original_types": ["text"], "id": 26}
{"type": "section", "content": "Finally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon emissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between training, finetuning and inference energy requirements carried out in Section 4.3 is, to our knowledge, the first comparison of its kind, and paves the way to a better understanding of how the different stages of an ML model’s lifecycle add up in terms of energy use. These are important data points that can help inform both our fellow AI researchers and practitioners, as well as policy-makers who are working towards estimating and regulating the environmental impacts of AI models and ICT in general. We recognize that our study is not representative of all deployment contexts and constraints – our intent is to establish a set of initial data points and to set the stage for testing and comparing other models. In fact, our study highlights many potential avenues for future research aimed towards a better understanding of the myriad factors that influence the efficiency of inference, including the choice of architecture, the usage of techniques such as distillation, the number of parameters, the choice of hardware and the numerical (i.e. floating point) precision of model parameters. While we encourage continued work analysing open-source models, we note that the growing lack of transparency in model architecture and training details makes this line of work, alongside many branches relating to fairness and accountability in machine learning, increasingly difficult to carry out. Given our findings and the increased deployment of generative, multi-purpose AI models, we hope that both ML researchers and practitioners will practice transparency regarding the nature and impacts of their models, to enable better understanding of their environmental impacts.", "doc_id": "luccioni2024", "page": 14, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Finally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon emissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between training, finetuning and inference energy requirements carried out in Section 4.3 is, to our knowledge, the first comparison of its kind, and paves the way to a better understanding of how the different stages of an ML model’s lifecycle add up in terms of energy use. These are important data points that can help inform both our fellow AI researchers and practitioners, as well as policy-makers who are working towards estimating and regulating the environmental impacts of AI models and ICT in general. We recognize that our study is not representative of all deployment contexts and constraints – our intent is to establish a set of initial data points and to set the stage for testing and comparing other models. In fact, our study highlights many potential avenues for future research aimed towards a better understanding of the myriad factors that influence the efficiency of inference, including the choice of architecture, the usage of techniques such as distillation, the number of parameters, the choice of hardware and the numerical (i.e. floating point) precision of model parameters. While we encourage continued work analysing open-source models, we note that the growing lack of transparency in model architecture and training details makes this line of work, alongside many branches relating to fairness and accountability in machine learning, increasingly difficult to carry out. Given our findings and the increased deployment of generative, multi-purpose AI models, we hope that both ML researchers and practitioners will practice transparency regarding the nature and impacts of their models, to enable better understanding of their environmental impacts.", "original_types": ["text"], "id": 27}
{"type": "section", "content": "ETHICAL CONSIDERATIONS STATEMENT\n\nThe main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon emissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements. In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq. In order to reduce our impacts as much as possible, we did all up-front experimentations on smaller portions of the dataset (to reduce wasted resources).\n\nRESEARCHER POSITIONALITY STATEMENT\n\nThe authors of this paper have backgrounds in theoretical and applied machine learning and work in institutions based in North America. We therefore recognize that our way of planning and running experiments is not necessarily reflective of other institutions from other regions, or the constraints faced by researchers from institutions with more limited access to compute.\n\nADVERSE IMPACTS STATEMENT\n\nWe recognize that our work can be perceived as a critique of ML deployment in general, given the analysis that we provide of its environmental impacts. This could be used as an argument to stop pursuing ML research and development, or as a way of targeting specific companies or organizations. Our intention, however, is to shed additional light on the environmental impacts of ML, in order to help model developers and researchers make more informed choices as a function of their environmental footprint or energy usage.\n\nACKNOWLEDGMENTS\n\nWe thank Will Alpine, Nima Boscarino, Priya Donti, Régis Pierrard, David Rolnick, Roy Schwartz and Rajiv Shah for their useful feedback and suggestions.", "doc_id": "luccioni2024", "page": 15, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "ETHICAL CONSIDERATIONS STATEMENT\n\nThe main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon emissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements. In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq. In order to reduce our impacts as much as possible, we did all up-front experimentations on smaller portions of the dataset (to reduce wasted resources).\n\nRESEARCHER POSITIONALITY STATEMENT\n\nThe authors of this paper have backgrounds in theoretical and applied machine learning and work in institutions based in North America. We therefore recognize that our way of planning and running experiments is not necessarily reflective of other institutions from other regions, or the constraints faced by researchers from institutions with more limited access to compute.\n\nADVERSE IMPACTS STATEMENT\n\nWe recognize that our work can be perceived as a critique of ML deployment in general, given the analysis that we provide of its environmental impacts. This could be used as an argument to stop pursuing ML research and development, or as a way of targeting specific companies or organizations. Our intention, however, is to shed additional light on the environmental impacts of ML, in order to help model developers and researchers make more informed choices as a function of their environmental footprint or energy usage.\n\nACKNOWLEDGMENTS\n\nWe thank Will Alpine, Nima Boscarino, Priya Donti, Régis Pierrard, David Rolnick, Roy Schwartz and Rajiv Shah for their useful feedback and suggestions.", "original_types": ["text", "header"], "id": 28}
{"type": "section", "content": "REFERENCES\n\n[1] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[2] Jeff Barr. 2019. Amazon ec2 update-inf1 instances with AWS inferentia chips for high performance cost-effective inferencing. https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n\n[3] Bing. 2019. Bing delivers its largest improvement in search experience using Azure GPUs. https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/\n\n[4] Bing. 2023. Confirmed: the new Bing runs on OpenAI’s GPT-4. https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4\n\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.\n\n[6] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems. 1–7.\n\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. https://doi.org/10.48550/ARXIV.2210.11416\n\n[9] Rishit Dagli and Ali Mustufa Shaikh. 2021. CPPE-5: Medical Personal Protective Equipment Dataset. arXiv:2112.09569 [cs.CV]\n\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. 2021. RedCaps: web-curated image-text data created by the people, for the people. arXiv:2111.11431 [cs.CV]\n\n[11] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. 2021. Compute and energy consumption trends in deep learning inference. arXiv preprint arXiv:2109.05472 (2021).\n\n[12] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.", "doc_id": "luccioni2024", "page": 16, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "REFERENCES\n\n[1] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[2] Jeff Barr. 2019. Amazon ec2 update-inf1 instances with AWS inferentia chips for high performance cost-effective inferencing. https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n\n[3] Bing. 2019. Bing delivers its largest improvement in search experience using Azure GPUs. https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/\n\n[4] Bing. 2023. Confirmed: the new Bing runs on OpenAI’s GPT-4. https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4\n\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.\n\n[6] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI Inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems. 1–7.\n\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n\n[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. https://doi.org/10.48550/ARXIV.2210.11416\n\n[9] Rishit Dagli and Ali Mustufa Shaikh. 2021. CPPE-5: Medical Personal Protective Equipment Dataset. arXiv:2112.09569 [cs.CV]\n\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. 2021. RedCaps: web-curated image-text data created by the people, for the people. arXiv:2111.11431 [cs.CV]\n\n[11] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. 2021. Compute and energy consumption trends in deep learning inference. arXiv preprint arXiv:2109.05472 (2021).\n\n[12] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.", "original_types": ["text", "header"], "id": 29}
{"type": "section", "content": "[13] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang. 2023. LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. arXiv preprint arXiv:2309.14393 (2023).\n\n[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.5371628\n\n[15] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, Hong Kong, China, 70–79. https://doi.org/10.18653/v1/D19-5409\n\n[16] Google. 2019. Understanding searches better than ever before. https://blog.google/products/search/search-language-understanding-bert/\n\n[17] Google. 2023. Bard can now connect to your Google apps and services. https://blog.google/products/bard/google-bard-new-features-update-sept-2023/\n\n[18] Google. 2023. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/\n\n[19] Walid A Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant Shenoy. 2023. CarbonScaler: Leveraging Cloud Workload Elasticity for Optimizing Carbon-Efficiency. arXiv preprint arXiv:2302.08681 (2023).\n\n[20] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In NeurIPS. 1693–1701. http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend\n\n[21] Ralph Hintemann and Simon Hinterholzer. 2022. Cloud computing drives the growth of the data center industry and its energy consumption. Data centers 2022. ResearchGate (2022).\n\n[22] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\n\n[23] Matt Gardner Johannes Welbl, Nelson F. Liu. 2017. Crowdsourcing Multiple Choice Science Questions. arXiv:1707.06209v1.\n\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision 123 (2017), 32–73. https://doi.org/10.1007/s11263-016-0981-7\n\n[25] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical Report.\n\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).", "doc_id": "luccioni2024", "page": 16, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "[13] Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, and Lei Jiang. 2023. LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. arXiv preprint arXiv:2309.14393 (2023).\n\n[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.5371628\n\n[15] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization. Association for Computational Linguistics, Hong Kong, China, 70–79. https://doi.org/10.18653/v1/D19-5409\n\n[16] Google. 2019. Understanding searches better than ever before. https://blog.google/products/search/search-language-understanding-bert/\n\n[17] Google. 2023. Bard can now connect to your Google apps and services. https://blog.google/products/bard/google-bard-new-features-update-sept-2023/\n\n[18] Google. 2023. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/\n\n[19] Walid A Hanafy, Qianlin Liang, Noman Bashir, David Irwin, and Prashant Shenoy. 2023. CarbonScaler: Leveraging Cloud Workload Elasticity for Optimizing Carbon-Efficiency. arXiv preprint arXiv:2302.08681 (2023).\n\n[20] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In NeurIPS. 1693–1701. http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend\n\n[21] Ralph Hintemann and Simon Hinterholzer. 2022. Cloud computing drives the growth of the data center industry and its energy consumption. Data centers 2022. ResearchGate (2022).\n\n[22] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\n\n[23] Matt Gardner Johannes Welbl, Nelson F. Liu. 2017. Crowdsourcing Multiple Choice Science Questions. arXiv:1707.06209v1.\n\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision 123 (2017), 32–73. https://doi.org/10.1007/s11263-016-0981-7\n\n[25] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical Report.\n\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).", "original_types": ["text"], "id": 30}
{"type": "section", "content": "[27] Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, and Julien Launay. 2022. A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model. In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Association for Computational Linguistics, virtual+Dublin, 84–94. https://doi.org/10.18653/v1/2022.bigscience-1.8\n\n[28] George Leopold. 2019. AWS to Offer NVIDIA’s T4 GPUs for AI Inferencing. www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/\n\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll’ar, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6–12, 2014, Proceedings, Part V 13. Springer, 740–755.\n\n[30] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023)\n\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language model. arXiv preprint arXiv:2211.02001 (2022)\n\n[32] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Portland, Oregon, USA, 142–150. http://www.aclweb.org/anthology/P11-1015\n\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]\n\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 (2022)\n\n[35] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. ArXiv abs/1808.08745 (2018)\n\n[36] Will Oremus. 2023. AI chatbots lose money every time you use them. That is a problem. Washington Post (2023). https://www.washingtonpost.com/technology/2023/06/05/chatgpt-hidden-cost-gpu-compute/\n\n[37] Pedro Javier Ortiz Su’arez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures (Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019), Piotr Bański, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L", "doc_id": "luccioni2024", "page": 17, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "[27] Imad Lakim, Ebtesam Almazrouei, Ibrahim Abualhaol, Merouane Debbah, and Julien Launay. 2022. A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model. In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models. Association for Computational Linguistics, virtual+Dublin, 84–94. https://doi.org/10.18653/v1/2022.bigscience-1.8\n\n[28] George Leopold. 2019. AWS to Offer NVIDIA’s T4 GPUs for AI Inferencing. www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/\n\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll’ar, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6–12, 2014, Proceedings, Part V 13. Springer, 740–755.\n\n[30] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023)\n\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language model. arXiv preprint arXiv:2211.02001 (2022)\n\n[32] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Portland, Oregon, USA, 142–150. http://www.aclweb.org/anthology/P11-1015\n\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]\n\n[34] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786 (2022)\n\n[35] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. ArXiv abs/1808.08745 (2018)\n\n[36] Will Oremus. 2023. AI chatbots lose money every time you use them. That is a problem. Washington Post (2023). https://www.washingtonpost.com/technology/2023/06/05/chatgpt-hidden-cost-gpu-compute/\n\n[37] Pedro Javier Ortiz Su’arez, Benoit Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures (Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019), Piotr Bański, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc Kupietz, Harald L", "original_types": ["text"], "id": 31}
{"type": "section", "content": "[38] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual Name Tagging and Linking for 282 Languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1946–1958. https://doi.org/10.18653/v1/P17-1178\n\n[39] Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL\n\n[40] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. https://doi.org/10.48550/ARXIV.2204.05149\n\n[41] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021)\n\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv e-prints (2019). arXiv:1910.10683\n\n[43] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. arXiv:1806.03822 [cs.CL]\n\n[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv:1606.05250 (2016). arXiv:1606.05250\n\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 3 (2015), 211–252. https://doi.org/10.1007/s11263-015-0816-y\n\n[46] Gustavo Santana. 2023. Stable Diffusion Prompts. https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts\n\n[47] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing\n\n[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, 1631–1642. https://www.aclweb.org/anthology/D13-1170\n\n[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019)", "doc_id": "luccioni2024", "page": 17, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "[38] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual Name Tagging and Linking for 282 Languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1946–1958. https://doi.org/10.18653/v1/P17-1178\n\n[39] Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL\n\n[40] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. https://doi.org/10.48550/ARXIV.2204.05149\n\n[41] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021)\n\n[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv e-prints (2019). arXiv:1910.10683\n\n[43] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. arXiv:1806.03822 [cs.CL]\n\n[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv:1606.05250 (2016). arXiv:1606.05250\n\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 3 (2015), 211–252. https://doi.org/10.1007/s11263-015-0816-y\n\n[46] Gustavo Santana. 2023. Stable Diffusion Prompts. https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts\n\n[47] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing\n\n[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, 1631–1642. https://www.aclweb.org/anthology/D13-1170\n\n[49] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019)", "original_types": ["text"], "id": 32}
{"type": "section", "content": "[50] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. 142–147. https://www.aclweb.org/anthology/W03-0419\n\n[51] US Environmental Protection Agencyy. 2024. Greenhouse Gases Equivalencies Calculator - Calculations and References. https://www.epa.gov/energy/greenhouse-gases-equivalencies-calculator-calculations-and-references", "doc_id": "luccioni2024", "page": 17, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "[50] Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. 142–147. https://www.aclweb.org/anthology/W03-0419\n\n[51] US Environmental Protection Agencyy. 2024. Greenhouse Gases Equivalencies Calculator - Calculations and References. https://www.epa.gov/energy/greenhouse-gases-equivalencies-calculator-calculations-and-references", "original_types": ["text"], "id": 33}
{"type": "section", "content": "[52] Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, Helen Ngo, et al. 2022. Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement. arXiv preprint arXiv:2210.01970 (2022).\n\n[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint arXiv:1905.00537 (2019).\n\n[54] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: A Large-Scale Prompt Gallery Dataset for Text-to-Image Generative Models. arXiv:2210.14896 [cs] (2022). https://arxiv.org/abs/2210.14896\n\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n\n[56] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n\n[57] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. arXiv:2304.05977 [cs.CV]\n\n[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV).", "doc_id": "luccioni2024", "page": 18, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "[52] Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, Helen Ngo, et al. 2022. Evaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement. arXiv preprint arXiv:2210.01970 (2022).\n\n[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint arXiv:1905.00537 (2019).\n\n[54] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: A Large-Scale Prompt Gallery Dataset for Text-to-Image Generative Models. arXiv:2210.14896 [cs] (2022). https://arxiv.org/abs/2210.14896\n\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n\n[56] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n\n[57] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[58] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. arXiv:2304.05977 [cs.CV]\n\n[59] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV).", "original_types": ["text"], "id": 34}
{"type": "section", "content": "FULL LIST OF TASK-SPECIFIC MODELS TESTED", "doc_id": "luccioni2024", "page": 19, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "FULL LIST OF TASK-SPECIFIC MODELS TESTED", "original_types": ["header"], "id": 35}
{"type": "table", "content": "Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.\n```markdown\n| Task | Models | Task | Models |\n|------|--------|------|--------|\n| image classification | microsoft/resnet-50\n| | | microsoft/beit-base-patch16-224\n| | | | google/vit-base-patch16-384\n| | | | facebook/convnextv2-tiny-22k-384\n| | | | microsoft/resnet-18\n| | | | google/mobilenet_v1_0.75_192\n| | | | facebook/convnextv2-tiny-1k-224\n| | | | google/vit-base-patch16-224\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | |", "doc_id": "luccioni2024", "page": 19, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.\n```markdown\n| Task | Models | Task | Models |\n|------|--------|------|--------|\n| image classification | microsoft/resnet-50\n| | | microsoft/beit-base-patch16-224\n| | | | google/vit-base-patch16-384\n| | | | facebook/convnextv2-tiny-22k-384\n| | | | microsoft/resnet-18\n| | | | google/mobilenet_v1_0.75_192\n| | | | facebook/convnextv2-tiny-1k-224\n| | | | google/vit-base-patch16-224\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | | pszemraj/led-large-book-summary\n| | | | google/pegasus-xsum\n| | | | google/pegasus-large\n| | | | google/pegasus-multi_news\n| | | | facebook/bart-large-cnn\n| | | | ainize/bart-base-cnn\n| | | | | runwayml/stable-diffusion-v1-5\n| | | | stabilityai/stable-diffusion-2-1\n| | | | stabilityai/stable-diffusion-xl-base-1.0\n| | | | CompVis/stable-diffusion-v1-4\n| | | | prompthero/openjourney\n| | | | dreamlike-art/dreamlike-photoreal-2.0\n| | | | nota-ai/bk-sdm-tiny\n| | | | segmind/tiny-sd\n| | | | | bert-base-uncased\n| | | | xlm-roberta-base\n| | | | distilbert-base-uncased\n| | | | roberta-base\n| | | | albert-base-v2\n| | | | bert-base-cased\n| | | | microsoft/deberta-base\n| | | | bert-base-multilingual-cased\n| | | | | gpt2\n| | | | bigscience/bloom-560m\n| | | | distilgpt2\n| | | | facebook/opt-6.7b\n| | | | EleutherAI/gpt-neo-125m\n| | | | gpt2-medium\n| | | | facebook/opt-1.3b\n| | | | gpt2-xl\n| | | | | facebook/detr-resnet-50\n| | | | hustvl/yolos-tiny\n| | | | jozhang97/deta-swin-large\n| | | | facebook/detr-resnet-101\n| | | | hustvl/yolos-small\n| | | | SenseTime/deformable-detr\n| | | | polejowska/detr-r50-cd45rb-8ah-6l\n| | | | polejowska/detr-r50-cd45rb-1ah-6l\n| | | | | QCRI/bert-base-multilingual-cased-pos-english\n| | | | dslim/bert-base-NER\n| | | | dslim/bert-large-NER\n| | | | Jean-Baptiste/roberta-large-ner-english\n| | | | oliverguhr/fullstop-punctuation-multilang-large\n| | | | Babelscape/wikineural-multilingual-ner\n| | | | ml6team/keyphrase-extraction-distilbert-inspec\n| | | | obi/deid_roberta_i2b2\n| | | | | distilbert-base-uncased-distilled-squad\n| | | | distilbert-base-cased-distilled-squad\n| | | | deepset/roberta-base-squad2\n| | | | bert-large-uncased-whole-word-masking-finetuned-squad\n| | | | timpal0l/mdeberta-v3-base-squad2\n| | | | deepset/tinyroberta-squad2\n| | | | deepset/electra-base-squad2\n| | | | deepset/bert-large-uncased-whole-word-masking-squad2\n| | | | | nlpconnect/vit-gpt2-image-captioning\n| | | | Salesforce/blip-image-captioning-large\n| | | | Salesforce/blip-image-captioning-base\n| | | | microsoft/git-large-coco\n| | | | Salesforce/blip2-flan-t5-xl\n| | | | Salesforce/blip2-opt-2.7b\n| | | | ydshieh/vit-gpt2-coco-en\n| | | | microsoft/git-base\n| | | | | sshleifer/distilbart-xsum-12-6\n| | | | sshleifer/distilbart-cnn-12-6\n| | | |", "id": 36}
{"type": "section", "content": "B  MODEL EVALUATION", "doc_id": "luccioni2024", "page": 20, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "B  MODEL EVALUATION", "original_types": ["header"], "id": 37}
{"type": "figure", "content": "Fig. 7. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and summarization accuracy (y axis), with dot size indicating the quantity of emissions.", "doc_id": "luccioni2024", "page": 20, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 7. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and summarization accuracy (y axis), with dot size indicating the quantity of emissions.", "id": 38}
{"type": "figure", "content": "Fig. 8. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and question answering accuracy (y axis), with dot size indicating the quantity of emissions.", "doc_id": "luccioni2024", "page": 20, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Fig. 8. A plot of model size, measured in number of parameters (x axis, in logarithmic scale) and question answering accuracy (y axis), with dot size indicating the quantity of emissions.", "id": 39}
{"type": "table", "content": "Table 7. Full performance metrics for the 32 models (24 finetuned, 8 multi-purpose) that we evaluated as part of our study.\nmodel | SST 2 (acc) | IMDB (acc) | Rotten Tomatoes (acc) | SciQ (acc) | SQuAD (F1) | SQuAD v2 (F1, has answer) | SamSUM (ROUGE) | XSum (ROUGE) | CNN (ROUGE)\n\nbloomz-560m | 0.92 | 0.94 | 0.85 | 0.92 | 0.43 | 0.21 | 0.23 | 0.15 | 0.10\nbloomz-1b7 | 0.94 | 0.97 | 0.93 | 0.96 | 0.50 | 0.25 | 0.26 | 0.16 | 0.18\nbloomz-3b | 0.95 | 0.98 | 0.95 | 0.97 | 0.53 | 0.26 | 0.28 | 0.17 | 0.21\nbloomz-7b1 | 0.94 | 0.98 | 0.95 | 0.97 | 0.54 | 0.27 | 0.32 | 0.21 | 0.09\nflan-t5-xxl | 0.96 | 0.97 | 0.92 | 0.72 | 0.98 | 0.49 | 0.30 | 0.37 | 0.23\nflan-t5-xl | 0.96 | 0.97 | 0.93 | 0.66 | 0.97 | 0.49 | 0.49 | 0.38 | 0.24\nflan-t5-large | 0.94 | 0.96 | 0.92 | 0.53 | 0.97 | 0.50 | 0.45 | 0.30 | 0.24\nflan-t5-base | 0.93 | 0.95 | 0.88 | 0.61 | 0.95 | 0.48 | 0.46 | 0.32 | 0.23\ndistilbert-base-uncased -distilled-squad | 0.44 | 0.87 | 0.86\n\ndistilbert-base-cased -distilled-squad | 0.46 | 0.87 | 0.87\n\ndeepset/roberta-base-squad2 | 0.48 | 0.93 | 0.83\n\nbert-large-uncased-whole-word-masking-finetuned-squad | 0.48 | 0.93 | 0.84\n\ntimpal0l/mdeberta-v3-base-squad2 | 0.46 | 0.91 | 0.90\n\ndeepset/tinyroberta-squad2 | 0.45 | 0.98 | 0.91\n\ndeepset/electra-base-squad2 | 0.48 | 0.89 | 0.82\n\ndeepset/bert-large-uncased-whole-word-masking-squad2 | 0.46 | 0.92 | 0.92\n\nsshleifer/distilbart-xsum-12-6 | 0.20 | 0.45 | 0.23\n\nsshleifer/distilbart-cnn-12-6 | 0.29 | 0.21 | 0.44\n\npszemraj/led-large-book-summary | 0.33 | 0.16 | 0.33\n\npegasus-xsum | 0.22 | 0.22 | 0.22\n\npegasus-large | 0.27 | 0.17 | 0.34\n\npegasus-multi_news | 0.12 | 0.16 | 0.29\n\nfacebook/bart-large-cnn | 0.32 | 0.21 | 0.44\n\nainize/bart-base-cnn | 0.27 | 0.16 | 0.26\n\ndistilbert-base-uncased-finetuned-sst-2-english | 0.99 | 0.88 | 0.90\n\nnlptown/bert-base-multilingual-uncased-sentiment | 0.75 | 0.85 | 0.73\n\ntwitter-roberta-base-sentiment-latest | 0.82 | 0.80 | 0.77\n\ncardiffnlp/twitter-xlm-roberta-base-sentiment | 0.79 | 0.71 | 0.74\n\ntlwerra/distilbert-imdb | 0.88 | 0.93 | 0.82\n\ntiebert/sentiment-roberta-large-english | 0.92 | 0.92 | 0.92\n\nfiniteautomata/bertweet-base-sentiment-analysis | 0.82 | 0.72 | 0.77\n\nsbcBI/sentiment_analysis_model | 0.81 | 0.75 | 0.76", "doc_id": "luccioni2024", "page": 21, "url": "https://arxiv.org/pdf/2311.16863", "embedded_text": "Table 7. Full performance metrics for the 32 models (24 finetuned, 8 multi-purpose) that we evaluated as part of our study.\nmodel | SST 2 (acc) | IMDB (acc) | Rotten Tomatoes (acc) | SciQ (acc) | SQuAD (F1) | SQuAD v2 (F1, has answer) | SamSUM (ROUGE) | XSum (ROUGE) | CNN (ROUGE)\n\nbloomz-560m | 0.92 | 0.94 | 0.85 | 0.92 | 0.43 | 0.21 | 0.23 | 0.15 | 0.10\nbloomz-1b7 | 0.94 | 0.97 | 0.93 | 0.96 | 0.50 | 0.25 | 0.26 | 0.16 | 0.18\nbloomz-3b | 0.95 | 0.98 | 0.95 | 0.97 | 0.53 | 0.26 | 0.28 | 0.17 | 0.21\nbloomz-7b1 | 0.94 | 0.98 | 0.95 | 0.97 | 0.54 | 0.27 | 0.32 | 0.21 | 0.09\nflan-t5-xxl | 0.96 | 0.97 | 0.92 | 0.72 | 0.98 | 0.49 | 0.30 | 0.37 | 0.23\nflan-t5-xl | 0.96 | 0.97 | 0.93 | 0.66 | 0.97 | 0.49 | 0.49 | 0.38 | 0.24\nflan-t5-large | 0.94 | 0.96 | 0.92 | 0.53 | 0.97 | 0.50 | 0.45 | 0.30 | 0.24\nflan-t5-base | 0.93 | 0.95 | 0.88 | 0.61 | 0.95 | 0.48 | 0.46 | 0.32 | 0.23\ndistilbert-base-uncased -distilled-squad | 0.44 | 0.87 | 0.86\n\ndistilbert-base-cased -distilled-squad | 0.46 | 0.87 | 0.87\n\ndeepset/roberta-base-squad2 | 0.48 | 0.93 | 0.83\n\nbert-large-uncased-whole-word-masking-finetuned-squad | 0.48 | 0.93 | 0.84\n\ntimpal0l/mdeberta-v3-base-squad2 | 0.46 | 0.91 | 0.90\n\ndeepset/tinyroberta-squad2 | 0.45 | 0.98 | 0.91\n\ndeepset/electra-base-squad2 | 0.48 | 0.89 | 0.82\n\ndeepset/bert-large-uncased-whole-word-masking-squad2 | 0.46 | 0.92 | 0.92\n\nsshleifer/distilbart-xsum-12-6 | 0.20 | 0.45 | 0.23\n\nsshleifer/distilbart-cnn-12-6 | 0.29 | 0.21 | 0.44\n\npszemraj/led-large-book-summary | 0.33 | 0.16 | 0.33\n\npegasus-xsum | 0.22 | 0.22 | 0.22\n\npegasus-large | 0.27 | 0.17 | 0.34\n\npegasus-multi_news | 0.12 | 0.16 | 0.29\n\nfacebook/bart-large-cnn | 0.32 | 0.21 | 0.44\n\nainize/bart-base-cnn | 0.27 | 0.16 | 0.26\n\ndistilbert-base-uncased-finetuned-sst-2-english | 0.99 | 0.88 | 0.90\n\nnlptown/bert-base-multilingual-uncased-sentiment | 0.75 | 0.85 | 0.73\n\ntwitter-roberta-base-sentiment-latest | 0.82 | 0.80 | 0.77\n\ncardiffnlp/twitter-xlm-roberta-base-sentiment | 0.79 | 0.71 | 0.74\n\ntlwerra/distilbert-imdb | 0.88 | 0.93 | 0.82\n\ntiebert/sentiment-roberta-large-english | 0.92 | 0.92 | 0.92\n\nfiniteautomata/bertweet-base-sentiment-analysis | 0.82 | 0.72 | 0.77\n\nsbcBI/sentiment_analysis_model | 0.81 | 0.75 | 0.76", "id": 40}
{"type": "section", "content": "Abstract\n\nThe computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models—T5, Meena, GShard, Switch Transformer, and GPT-3—and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.\n\n1. Introduction", "doc_id": "patterson2021", "page": 1, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Abstract\n\nThe computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models—T5, Meena, GShard, Switch Transformer, and GPT-3—and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.\n\n1. Introduction", "original_types": ["text", "header"], "id": 41}
{"type": "section", "content": "As ML models increase in scale, a general trend is that they become more accurate and more capable. However, larger models translate to greater computing demands and, by extension, greater energy demands. We focus on natural language processing (NLP) because it is important in Google products and because of the recent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch Transformer [Fed21], and GPT-3 [Bro20]. Recent studies attempt to evaluate the environmental impact of this trend in NLP, which is difficult [Str19]. Here we investigate and share the estimates of the energy consumed and CO2e3 of these recent and large NLP models. We also reduce by 88X an earlier estimate of the CO2e for the neural architecture search for Evolved Transformer [So19, Str19] by characterizing the actual search process on the hardware and datacenter on which it was performed (see Appendices C and D). Our investigation into CO2e revealed surprises and misunderstandings about the full Deep Neural Network (DNN) lifecycle, the datacenters and hardware that run them, the variations in energy mix, and the difficulty of assessing CO2e accurately. Note that we are evaluating the CO2e of operating computers and datacenters, but not fabricating and recycling them (see [Gup20] for the latter topic). To make it easier for the ML community to understand the real impact of training and how to reduce it, we endorse prior calls for new publication norms for computationally intensive ML models:", "doc_id": "patterson2021", "page": 1, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "As ML models increase in scale, a general trend is that they become more accurate and more capable. However, larger models translate to greater computing demands and, by extension, greater energy demands. We focus on natural language processing (NLP) because it is important in Google products and because of the recent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch Transformer [Fed21], and GPT-3 [Bro20]. Recent studies attempt to evaluate the environmental impact of this trend in NLP, which is difficult [Str19]. Here we investigate and share the estimates of the energy consumed and CO2e3 of these recent and large NLP models. We also reduce by 88X an earlier estimate of the CO2e for the neural architecture search for Evolved Transformer [So19, Str19] by characterizing the actual search process on the hardware and datacenter on which it was performed (see Appendices C and D). Our investigation into CO2e revealed surprises and misunderstandings about the full Deep Neural Network (DNN) lifecycle, the datacenters and hardware that run them, the variations in energy mix, and the difficulty of assessing CO2e accurately. Note that we are evaluating the CO2e of operating computers and datacenters, but not fabricating and recycling them (see [Gup20] for the latter topic). To make it easier for the ML community to understand the real impact of training and how to reduce it, we endorse prior calls for new publication norms for computationally intensive ML models:", "original_types": ["text"], "id": 42}
{"type": "section", "content": "1. Introduction\n\nWe must assess CO2e correctly, but it is hard to quantify precisely in part because all the required information is rarely reported or publicly available (e.g., datacenter, hardware, energy mix) and in part because it is hard to uncover important details afterwards (see Section 4.1). To make the carbon costs of training transparent, we encourage more researchers to measure energy usage and CO2e—or to get a rough estimate using a tool like ML Emissions Calculator [Lac19] (Section 4.3)—and publish the data.\n\nWe agree with [Str19,Sch20,Hen20] that efficiency should be an evaluation criterion for publishing ML research on computationally intensive models besides accuracy and related measures, since we need to encourage advances across the board as the most sustainable energy is the energy you don’t use.\n\nAnd even if we could bring CO2e to zero in cloud datacenters, reducing training time matters, both because “time is money,” and because cheaper training lets more people participate. Hence, we also second the recommendation of [Str19] for more researchers to publish the number of accelerators and their time to train computationally intensive models to inspire progress in reducing training costs.\n\nWe believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.\n\nThe following sections summarize the findings that led to these recommendations. They also document our CO2e estimates, highlight recent advances that curb the CO2e of ML, and estimate the CO2e from training the five recent large NLP models mentioned above. We end by updating the results of [Str19] on the emissions of the Evolved Transformer neural architecture search and discussing common misperceptions.\n\nWe start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a concrete example by nearly two orders of magnitude.\n\n2. Energy Consumption and Carbon Footprint of an NLP Model\n\nElectricity required to run an ML model is a function of the algorithm, the program that implements it, the number of processors that run the program, the speed and power of those processors, a datecenter’s efficiency in delivering power and cooling the processors, and the energy supply mix (renewable, gas, coal, etc.). A simplified formula for the carbon footprint of an ML model that takes these factors into account is:\n\nFootprint = (electrical energy_train + queries × electrical energy_inference) × CO2e_datacenter/KWh", "doc_id": "patterson2021", "page": 2, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "1. Introduction\n\nWe must assess CO2e correctly, but it is hard to quantify precisely in part because all the required information is rarely reported or publicly available (e.g., datacenter, hardware, energy mix) and in part because it is hard to uncover important details afterwards (see Section 4.1). To make the carbon costs of training transparent, we encourage more researchers to measure energy usage and CO2e—or to get a rough estimate using a tool like ML Emissions Calculator [Lac19] (Section 4.3)—and publish the data.\n\nWe agree with [Str19,Sch20,Hen20] that efficiency should be an evaluation criterion for publishing ML research on computationally intensive models besides accuracy and related measures, since we need to encourage advances across the board as the most sustainable energy is the energy you don’t use.\n\nAnd even if we could bring CO2e to zero in cloud datacenters, reducing training time matters, both because “time is money,” and because cheaper training lets more people participate. Hence, we also second the recommendation of [Str19] for more researchers to publish the number of accelerators and their time to train computationally intensive models to inspire progress in reducing training costs.\n\nWe believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models.\n\nThe following sections summarize the findings that led to these recommendations. They also document our CO2e estimates, highlight recent advances that curb the CO2e of ML, and estimate the CO2e from training the five recent large NLP models mentioned above. We end by updating the results of [Str19] on the emissions of the Evolved Transformer neural architecture search and discussing common misperceptions.\n\nWe start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a concrete example by nearly two orders of magnitude.\n\n2. Energy Consumption and Carbon Footprint of an NLP Model\n\nElectricity required to run an ML model is a function of the algorithm, the program that implements it, the number of processors that run the program, the speed and power of those processors, a datecenter’s efficiency in delivering power and cooling the processors, and the energy supply mix (renewable, gas, coal, etc.). A simplified formula for the carbon footprint of an ML model that takes these factors into account is:\n\nFootprint = (electrical energy_train + queries × electrical energy_inference) × CO2e_datacenter/KWh", "original_types": ["text", "header"], "id": 43}
{"type": "table", "content": "Table 1. See Appendix A for more detail^4. Estimates of CO2e for Transformer and Evolved Transformer for P100 and TPU v2 are based on power measurements.^5 Evolved Transformer (Medium) reached the same accuracy as Transformer (Big) in [So19]. CO2e is shown both before (“gross”) and after (“net”) accounting for 24/7 reduction via real time, local carbon free energy purchases (Appendix B). To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).", "doc_id": "patterson2021", "page": 3, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Table 1. See Appendix A for more detail^4. Estimates of CO2e for Transformer and Evolved Transformer for P100 and TPU v2 are based on power measurements.^5 Evolved Transformer (Medium) reached the same accuracy as Transformer (Big) in [So19]. CO2e is shown both before (“gross”) and after (“net”) accounting for 24/7 reduction via real time, local carbon free energy purchases (Appendix B). To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).", "id": 44}
{"type": "figure", "content": "Figure 1. Improvement in CO2e over Transformer (Big) on P100 GPUs in an average US datacenter versus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter.", "doc_id": "patterson2021", "page": 3, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Figure 1. Improvement in CO2e over Transformer (Big) on P100 GPUs in an average US datacenter versus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter.", "id": 45}
{"type": "table", "content": "Table 2. Small and large units for energy and carbon footprint in this paper, plus airline travel CO2e used for perspective on the relative size of ML emissions compared to other activities (Section 4.8).", "doc_id": "patterson2021", "page": 3, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Table 2. Small and large units for energy and carbon footprint in this paper, plus airline travel CO2e used for perspective on the relative size of ML emissions compared to other activities (Section 4.8).", "id": 46}
{"type": "section", "content": "The peak TeraFLOPS/second is 19 for P100 and 46 for TPU v2.\n\n2.1 Algorithm/program improvement\n\nThe Evolved Transformer (Medium) model discovered by So et al. [So19] using neural architecture search (see Section 4.1) uses 1.6X fewer FLOPS and 1.1X–1.3X less time than Transformer (Big) at slightly higher accuracy (see Table 1 and Appendix A)6. Business Rationale. Training faster saves ML researchers time as well as saves their organizations money and reduces CO2e.\n\n2.2 Processor improvement\n\nGoogle’s custom TPU v2 processor runs Transformer (Big) 4.3X faster than P100 GPUs and Evolved Transformer (Medium) 5.2X faster.7 TPU v2 also uses less power: 1.3X less for Transformer and 1.2X less for Evolved Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively. Business Rationale. The substantial increase in the scope and scale of deep learning over the past decade has created the opportunity to build customized hardware that is tailored to the kinds of computations involved in training and serving DNN models. Instead of using GPUs like many other organizations, over the past seven years Google has designed, built, and deployed four generations of custom Tensor Processing Unit (TPU) hardware for DNNs to accelerate model training and serving [Jou21]. To get a better return on their investment, cloud companies actually aim for improved cost-performance, as opposed to simply performance. Cost here means Total Cost of Ownership (TCO), which includes the annual operating costs such as electricity consumed and amortization of capital expenditures for the computer, cooling, power distribution, and the building. Jouppi et al. show that power consumption is nearly perfectly linearly correlated with TCO8 [Jou21], so performance/TCO gains also help performance/Watt, saving money and reducing CO2e.\n\n2.3 Datacenter improvement", "doc_id": "patterson2021", "page": "3-4", "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "The peak TeraFLOPS/second is 19 for P100 and 46 for TPU v2.\n\n2.1 Algorithm/program improvement\n\nThe Evolved Transformer (Medium) model discovered by So et al. [So19] using neural architecture search (see Section 4.1) uses 1.6X fewer FLOPS and 1.1X–1.3X less time than Transformer (Big) at slightly higher accuracy (see Table 1 and Appendix A)6. Business Rationale. Training faster saves ML researchers time as well as saves their organizations money and reduces CO2e.\n\n2.2 Processor improvement\n\nGoogle’s custom TPU v2 processor runs Transformer (Big) 4.3X faster than P100 GPUs and Evolved Transformer (Medium) 5.2X faster.7 TPU v2 also uses less power: 1.3X less for Transformer and 1.2X less for Evolved Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively. Business Rationale. The substantial increase in the scope and scale of deep learning over the past decade has created the opportunity to build customized hardware that is tailored to the kinds of computations involved in training and serving DNN models. Instead of using GPUs like many other organizations, over the past seven years Google has designed, built, and deployed four generations of custom Tensor Processing Unit (TPU) hardware for DNNs to accelerate model training and serving [Jou21]. To get a better return on their investment, cloud companies actually aim for improved cost-performance, as opposed to simply performance. Cost here means Total Cost of Ownership (TCO), which includes the annual operating costs such as electricity consumed and amortization of capital expenditures for the computer, cooling, power distribution, and the building. Jouppi et al. show that power consumption is nearly perfectly linearly correlated with TCO8 [Jou21], so performance/TCO gains also help performance/Watt, saving money and reducing CO2e.\n\n2.3 Datacenter improvement", "original_types": ["text", "header"], "id": 47}
{"type": "section", "content": "A useful quantitative metric of datacenter efficiency is the energy overhead above and beyond what directly powers the computing equipment inside the datacenters. If the overhead were 50%, the Power Usage Effectiveness (PUE) would be 1.50. The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59. Google publishes its datacenter PUE online every quarter. The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are roughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see [Höl20]), but we’ll limit the quantitative improvement in this paper to the easy-to-measure PUE. More broadly, since cloud datacenters are much more energy efficient, the long-feared explosion of datacenter energy usage has not materialized. A recent paper in Science [Mas20] found that global datacenter energy consumption increased by only 6% compared with 2010, despite computing capacity increasing by 550% over the same time period [Mas21]. Business Rationale. Cloud companies strive for energy efficient datacenters since it saves money and lowers emissions. Perhaps we should add “energy is money” to Ben Franklin’s “time is money” advice?\n\n2.4 Energy mix improvement\n\nThe gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21]. After matching Google’s clean energy purchase per its 24/7 carbon-free energy framework (see Appendix B), the net CO2e drops to 0.080 for the Iowa datacenter where we ran Evolved Transformer, which is 5.4X better. Business Rationale. Transmitting electricity long distances is more expensive and less efficient than sending information as photons over optical fibers [Arm10]. Cloud computing allows companies like Google to have a global portfolio of datacenters, many of which are placed where the grid is cleaner (e.g., Finland) or where companies can purchase clean energy directly (e.g., Iowa). In 2020 Google announced a new objective in its energy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7. For our 24/7 carbon-free energy accounting (see Appendix B), we deduct from the hourly consumption all", "doc_id": "patterson2021", "page": 4, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "A useful quantitative metric of datacenter efficiency is the energy overhead above and beyond what directly powers the computing equipment inside the datacenters. If the overhead were 50%, the Power Usage Effectiveness (PUE) would be 1.50. The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59. Google publishes its datacenter PUE online every quarter. The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are roughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see [Höl20]), but we’ll limit the quantitative improvement in this paper to the easy-to-measure PUE. More broadly, since cloud datacenters are much more energy efficient, the long-feared explosion of datacenter energy usage has not materialized. A recent paper in Science [Mas20] found that global datacenter energy consumption increased by only 6% compared with 2010, despite computing capacity increasing by 550% over the same time period [Mas21]. Business Rationale. Cloud companies strive for energy efficient datacenters since it saves money and lowers emissions. Perhaps we should add “energy is money” to Ben Franklin’s “time is money” advice?\n\n2.4 Energy mix improvement\n\nThe gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21]. After matching Google’s clean energy purchase per its 24/7 carbon-free energy framework (see Appendix B), the net CO2e drops to 0.080 for the Iowa datacenter where we ran Evolved Transformer, which is 5.4X better. Business Rationale. Transmitting electricity long distances is more expensive and less efficient than sending information as photons over optical fibers [Arm10]. Cloud computing allows companies like Google to have a global portfolio of datacenters, many of which are placed where the grid is cleaner (e.g., Finland) or where companies can purchase clean energy directly (e.g., Iowa). In 2020 Google announced a new objective in its energy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7. For our 24/7 carbon-free energy accounting (see Appendix B), we deduct from the hourly consumption all", "original_types": ["text", "header"], "id": 48}
{"type": "section", "content": "2.5 Summary: Formulas for energy consumption and carbon footprint of training\n\nReducing CO2e is not only a moral obligation but ultimately sound business. To decrease the footprint of training, an ML researcher should pick the DNN model, the processor, and the datacenter carefully.9 Cutting energy saves money and CO2e and improving the energy mix reduces CO2e. We refactor the equation above for training into energy consumption and its carbon footprint (tCO2e means metric tons of CO2e):\n\nKWh = Hours to train × Number of Processors × Average Power per Processor × PUE ÷ 1000\n\ntCO2e = KWh × kg CO2e per KWh ÷ 1000\n\nWe believe it is straightforward for ML practitioners to calculate energy consumption. They already know hours to train and number of processors. Google and Facebook publish PUE of their datacenters, so that is easy to look up for those clouds. If cloud providers don’t share PUE, use the US average PUE as in [Str19]. We measured the power of the processors during training, which is ideal, but using the average of the training of several similar models is probably sufficient and much easier.10 Table 3 shows the average power and standard deviation for the processors and DNNs that we measured in this paper.", "doc_id": "patterson2021", "page": 5, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "2.5 Summary: Formulas for energy consumption and carbon footprint of training\n\nReducing CO2e is not only a moral obligation but ultimately sound business. To decrease the footprint of training, an ML researcher should pick the DNN model, the processor, and the datacenter carefully.9 Cutting energy saves money and CO2e and improving the energy mix reduces CO2e. We refactor the equation above for training into energy consumption and its carbon footprint (tCO2e means metric tons of CO2e):\n\nKWh = Hours to train × Number of Processors × Average Power per Processor × PUE ÷ 1000\n\ntCO2e = KWh × kg CO2e per KWh ÷ 1000\n\nWe believe it is straightforward for ML practitioners to calculate energy consumption. They already know hours to train and number of processors. Google and Facebook publish PUE of their datacenters, so that is easy to look up for those clouds. If cloud providers don’t share PUE, use the US average PUE as in [Str19]. We measured the power of the processors during training, which is ideal, but using the average of the training of several similar models is probably sufficient and much easier.10 Table 3 shows the average power and standard deviation for the processors and DNNs that we measured in this paper.", "original_types": ["text", "header"], "id": 49}
{"type": "table", "content": "Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].", "doc_id": "patterson2021", "page": 5, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].", "id": 50}
{"type": "table", "content": "Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].", "doc_id": "patterson2021", "page": 5, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].", "id": 51}
{"type": "section", "content": "The final piece is the CO2e of the datacenter at the time the model was run. Google calculates the average per month, which is close enough, and it is now available for Google employees to look up. Without access to such a dashboard, use the ML Emissions Calculator [Lac19] or Green Algorithms tool [Lan20] that estimate the CO2e mix by region (see Figure 6 below)11. While not absolutely necessary, we hope the ML community will lobby all cloud providers to reveal the actual energy mix, since it can vary within a region. For example, to let customers pick the datacenter based on CO2e, Google Cloud recently released the percentage of carbon-free energy and gross CO2e of its datacenters and committed to publishing updated figures going forward.\n\nWe next show the impact of these three choices on much larger NLP models.", "doc_id": "patterson2021", "page": 5, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "The final piece is the CO2e of the datacenter at the time the model was run. Google calculates the average per month, which is close enough, and it is now available for Google employees to look up. Without access to such a dashboard, use the ML Emissions Calculator [Lac19] or Green Algorithms tool [Lan20] that estimate the CO2e mix by region (see Figure 6 below)11. While not absolutely necessary, we hope the ML community will lobby all cloud providers to reveal the actual energy mix, since it can vary within a region. For example, to let customers pick the datacenter based on CO2e, Google Cloud recently released the percentage of carbon-free energy and gross CO2e of its datacenters and committed to publishing updated figures going forward.\n\nWe next show the impact of these three choices on much larger NLP models.", "original_types": ["text"], "id": 52}
{"type": "table", "content": "Table 4. CO2e for NLP models (see Appendix A)12. V100's TDP is closer to average power due to Turbo mode and DVFS. TPUs don’t offer them, so their TDP is much higher than their average power.\n232 MWh and emissions was 96 tCO2e. As Evolved Transformer saved 48 tCO2e alone for the single use case of developing Meena (see Table 4), the 3.2 net tCO2e cost for its development returned 15:1. GShard is composed of a set of lightweight annotation APIs that provide an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code [Lep20]. It enabled scaling up of a multilingual neural machine translation Transformer model with sparsely gated mixture-of-experts (MoE) [Sha17] using automatic sharding. The GShard-600B model is a particular use of that framework for training a multi-lingual translation model with 600B total parameters. Sparse models can have many model parameters while requiring much less computation than dense models. Training GShard-600B used 24 MWh and produced 4.3 net tCO2e. Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources. We estimated it used 179 MWh and produced 59 net tCO2e.", "doc_id": "patterson2021", "page": 6, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Table 4. CO2e for NLP models (see Appendix A)12. V100's TDP is closer to average power due to Turbo mode and DVFS. TPUs don’t offer them, so their TDP is much higher than their average power.\n232 MWh and emissions was 96 tCO2e. As Evolved Transformer saved 48 tCO2e alone for the single use case of developing Meena (see Table 4), the 3.2 net tCO2e cost for its development returned 15:1. GShard is composed of a set of lightweight annotation APIs that provide an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code [Lep20]. It enabled scaling up of a multilingual neural machine translation Transformer model with sparsely gated mixture-of-experts (MoE) [Sha17] using automatic sharding. The GShard-600B model is a particular use of that framework for training a multi-lingual translation model with 600B total parameters. Sparse models can have many model parameters while requiring much less computation than dense models. Training GShard-600B used 24 MWh and produced 4.3 net tCO2e. Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources. We estimated it used 179 MWh and produced 59 net tCO2e.", "id": 53}
{"type": "section", "content": "GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse language model at the time [Bro20]. It achieves strong performance on many NLP datasets. A winner of the best paper award at NeurIPS 2020, this 8-month-old paper already has ~700 citations and made mainstream media headlines.13 It is now available for commercial use. One potential energy benefit of a large language model like GPT-3 is that they exhibit few-shot generalization, which means that they don’t need to be retrained for every new task like smaller models [Wan20]. Its estimated carbon emissions due to training are 552 tCO2e and its energy consumption is 1287 MWh.14 Table 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.", "doc_id": "patterson2021", "page": 7, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse language model at the time [Bro20]. It achieves strong performance on many NLP datasets. A winner of the best paper award at NeurIPS 2020, this 8-month-old paper already has ~700 citations and made mainstream media headlines.13 It is now available for commercial use. One potential energy benefit of a large language model like GPT-3 is that they exhibit few-shot generalization, which means that they don’t need to be retrained for every new task like smaller models [Wan20]. Its estimated carbon emissions due to training are 552 tCO2e and its energy consumption is 1287 MWh.14 Table 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.", "original_types": ["text"], "id": 54}
{"type": "figure", "content": "Figure 2. Total FLOPS versus number of parameters relative to Transformer (Big) in a log-log graph (Table 1). While all are not doing the same tasks, a reason T5 has relatively lower FLOPS relative to its number of parameters is that it trains until the accuracy is good enough instead of to the best possible accuracy. [Kap20] notes that some architectures have a much lower footprint than others at equivalent accuracy and suggests that significant power might be saved by revisiting accuracy requirements.", "doc_id": "patterson2021", "page": 7, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Figure 2. Total FLOPS versus number of parameters relative to Transformer (Big) in a log-log graph (Table 1). While all are not doing the same tasks, a reason T5 has relatively lower FLOPS relative to its number of parameters is that it trains until the accuracy is good enough instead of to the best possible accuracy. [Kap20] notes that some architectures have a much lower footprint than others at equivalent accuracy and suggests that significant power might be saved by revisiting accuracy requirements.", "id": 55}
{"type": "figure", "content": "Figure 3. Accelerator years of computation, energy consumption, and CO2e for five large NLP DNNs.", "doc_id": "patterson2021", "page": 7, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Figure 3. Accelerator years of computation, energy consumption, and CO2e for five large NLP DNNs.", "id": 56}
{"type": "section", "content": "4. Discussion\n\nIn this section, we address the additional factors relating to carbon emissions due to training NLP models. We start by revisiting the estimate of neural architecture search in [Str19] and end with example benefits of some NLP models.\n\n4.1 Estimating the cost of neural architecture search (NAS)", "doc_id": "patterson2021", "page": 8, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4. Discussion\n\nIn this section, we address the additional factors relating to carbon emissions due to training NLP models. We start by revisiting the estimate of neural architecture search in [Str19] and end with example benefits of some NLP models.\n\n4.1 Estimating the cost of neural architecture search (NAS)", "original_types": ["text", "header"], "id": 57}
{"type": "section", "content": "The Evolved Transformer neural architecture search (NAS) was used as an example of an expensive NLP model [Str19]. Although it is now surpassed by other models in terms of training cost (Table 4), we discuss it here as a concrete example of the complexity of estimating the cost of a ML method retroactively.\n\nAs Table 4 shows, the actual cost of Evolved Transformer NAS is nearly two orders of magnitude smaller than previously estimated [Str19]. Why the discrepancy? The answer is that, in addition to the efficiency of Google datacenters, there was a confusion in estimating the energy cost of NAS. In Evolved Transformer NAS, researchers used a small proxy task to search for the best models to save time and money, and then scaled up the found models to full size. Small proxies may not be obvious, which made it hard to estimate the CO2e correctly in retrospect from the NAS paper [So19]. Due to the misunderstanding of the usage of proxy tasks in NAS, it was assumed the search was done with full size tasks. Because of this assumption, despite considerable effort on their part, Strubell et al.'s energy estimate for NAS ended up 18.7X too high for the average organization (see Appendix C) and 88X off in emissions for energy-efficient organizations like Google (see Appendix D). This example led us to our first recommendation—that more researchers measure energy usage and CO2e for computationally intensive projects, and report them when practical, rather than counting on others to estimate it retrospectively.\n\nAnother confusion in the general public is the misperception that NAS (and therefore, the cost associated with NAS) is conducted once per model training. In practice, however, NAS is generally not performed once per model training, but once per problem domain+architectural search space combination. For example, the Evolved Transformer, found by NAS on translation, can be used for language modeling without a new search [So19, Adi20]. Unfortunately, results in the earlier work by [Str19] characterizing NAS were misattributed to single model training costs in the popular press.\n\nAs an analogy, NAS is like optimizing the energy efficiency and cost of an LED light bulb with extensive simulations on a supercomputer, training a model is akin to building LED light bulbs, and inference is analogous to all the customers using LEDs to light their homes. The analogous confusion would be claiming that the one-time upfront supercomputer simulation cost should be included in the CO2e cost of every light bulb manufactured. In this analogy, the onetime CO2 expenditure of the supercomputer simulations can be more than paid back with the improved energy-efficiency of the mass-produced light bulbs, as was the case for the actual NAS of [So19] (see next paragraph).\n\nIn terms of cost-benefit tradeoff, NAS can also lead to improved energy efficiency in training of downstream applications, and the benefit can dramatically outweigh the cost. Figure 4 shows that the Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation. The use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 tCO2e during the training of the Meena DNN (see Tables 1 and 4). The savings from this single reuse in Meena are ~15X larger than the energy cost of running the search to discover it. The results of the Evolved Transformer neural", "doc_id": "patterson2021", "page": 8, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "The Evolved Transformer neural architecture search (NAS) was used as an example of an expensive NLP model [Str19]. Although it is now surpassed by other models in terms of training cost (Table 4), we discuss it here as a concrete example of the complexity of estimating the cost of a ML method retroactively.\n\nAs Table 4 shows, the actual cost of Evolved Transformer NAS is nearly two orders of magnitude smaller than previously estimated [Str19]. Why the discrepancy? The answer is that, in addition to the efficiency of Google datacenters, there was a confusion in estimating the energy cost of NAS. In Evolved Transformer NAS, researchers used a small proxy task to search for the best models to save time and money, and then scaled up the found models to full size. Small proxies may not be obvious, which made it hard to estimate the CO2e correctly in retrospect from the NAS paper [So19]. Due to the misunderstanding of the usage of proxy tasks in NAS, it was assumed the search was done with full size tasks. Because of this assumption, despite considerable effort on their part, Strubell et al.'s energy estimate for NAS ended up 18.7X too high for the average organization (see Appendix C) and 88X off in emissions for energy-efficient organizations like Google (see Appendix D). This example led us to our first recommendation—that more researchers measure energy usage and CO2e for computationally intensive projects, and report them when practical, rather than counting on others to estimate it retrospectively.\n\nAnother confusion in the general public is the misperception that NAS (and therefore, the cost associated with NAS) is conducted once per model training. In practice, however, NAS is generally not performed once per model training, but once per problem domain+architectural search space combination. For example, the Evolved Transformer, found by NAS on translation, can be used for language modeling without a new search [So19, Adi20]. Unfortunately, results in the earlier work by [Str19] characterizing NAS were misattributed to single model training costs in the popular press.\n\nAs an analogy, NAS is like optimizing the energy efficiency and cost of an LED light bulb with extensive simulations on a supercomputer, training a model is akin to building LED light bulbs, and inference is analogous to all the customers using LEDs to light their homes. The analogous confusion would be claiming that the one-time upfront supercomputer simulation cost should be included in the CO2e cost of every light bulb manufactured. In this analogy, the onetime CO2 expenditure of the supercomputer simulations can be more than paid back with the improved energy-efficiency of the mass-produced light bulbs, as was the case for the actual NAS of [So19] (see next paragraph).\n\nIn terms of cost-benefit tradeoff, NAS can also lead to improved energy efficiency in training of downstream applications, and the benefit can dramatically outweigh the cost. Figure 4 shows that the Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation. The use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 tCO2e during the training of the Meena DNN (see Tables 1 and 4). The savings from this single reuse in Meena are ~15X larger than the energy cost of running the search to discover it. The results of the Evolved Transformer neural", "id": 58}
{"type": "section", "content": "4.2 There are more resources used for training than the only final training run\n\n[Str19] and others point out that it often takes many attempts to get everything set up correctly before the final training run, so the final training run does not reflect the total cost. Since it’s hard to improve what you can’t measure, one issue is how to account for such costs accurately. Fortunately, an internal Google product is underway that will record information about the training process, originally intended to keep track of information like data provenance. The developers now plan to add energy consumption so that Googlers can better understand the full training lifecycle. An example of an open source tool to record such information is experiment-impact-tracker [Hen20]. In addition, the developers of ML Emissions Calculator [Lac19] are currently working on CodeCarbon, whose goal is to measure/approximate carbon consumption automatically. Alas, there will be no way to verify the claims in papers of preliminary training development. A lesson of computer benchmarking is that requiring the release of all information so that others could recreate your results was an effective deterrent to fudging the numbers. If more computationally intensive ML papers included energy consumption and carbon footprint of the final training run with sufficient details that others could check,", "doc_id": "patterson2021", "page": 9, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.2 There are more resources used for training than the only final training run\n\n[Str19] and others point out that it often takes many attempts to get everything set up correctly before the final training run, so the final training run does not reflect the total cost. Since it’s hard to improve what you can’t measure, one issue is how to account for such costs accurately. Fortunately, an internal Google product is underway that will record information about the training process, originally intended to keep track of information like data provenance. The developers now plan to add energy consumption so that Googlers can better understand the full training lifecycle. An example of an open source tool to record such information is experiment-impact-tracker [Hen20]. In addition, the developers of ML Emissions Calculator [Lac19] are currently working on CodeCarbon, whose goal is to measure/approximate carbon consumption automatically. Alas, there will be no way to verify the claims in papers of preliminary training development. A lesson of computer benchmarking is that requiring the release of all information so that others could recreate your results was an effective deterrent to fudging the numbers. If more computationally intensive ML papers included energy consumption and carbon footprint of the final training run with sufficient details that others could check,", "original_types": ["text", "header"], "id": 59}
{"type": "section", "content": "4.3 Measurements are more interesting than extrapolations\n\nAlthough extrapolations of carbon emissions are relatively easy, more attention should be paid to actual experiments that have been conducted rather than to hypothetical case studies. As a problematic example, since large NLP models can take a month to train, developers cannot afford to do the full training task many times. Like [So19] for NAS, they likely use a smaller task to explore the space for a limited training time. One indication comes from the AutoML work in [Li21]. Their exploration computation cost was roughly equal to the final training cost.", "doc_id": "patterson2021", "page": 10, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.3 Measurements are more interesting than extrapolations\n\nAlthough extrapolations of carbon emissions are relatively easy, more attention should be paid to actual experiments that have been conducted rather than to hypothetical case studies. As a problematic example, since large NLP models can take a month to train, developers cannot afford to do the full training task many times. Like [So19] for NAS, they likely use a smaller task to explore the space for a limited training time. One indication comes from the AutoML work in [Li21]. Their exploration computation cost was roughly equal to the final training cost.", "original_types": ["text", "header"], "id": 60}
{"type": "section", "content": "4.4 Standard ML algorithmic techniques can improve energy efficiency\n\nThere are many algorithmic techniques that can improve the energy efficiency of machine learning models. Some techniques can achieve the same accuracy with less overall computation. Others can use a large, already-trained model as a starting point and yield a lighter-weight, more computationally efficient model with almost the same accuracy. These techniques all serve to reduce the computational cost and therefore energy and carbon emissions of models. Some of these techniques include:\n\n• Distillation transfers the knowledge from large models into smaller, more computationally efficient models [Hin15, San20].\n• Pruning, quantization, and efficient coding can improve the energy efficiency of DNNs 3X–7X [Han15].", "doc_id": "patterson2021", "page": 11, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.4 Standard ML algorithmic techniques can improve energy efficiency\n\nThere are many algorithmic techniques that can improve the energy efficiency of machine learning models. Some techniques can achieve the same accuracy with less overall computation. Others can use a large, already-trained model as a starting point and yield a lighter-weight, more computationally efficient model with almost the same accuracy. These techniques all serve to reduce the computational cost and therefore energy and carbon emissions of models. Some of these techniques include:\n\n• Distillation transfers the knowledge from large models into smaller, more computationally efficient models [Hin15, San20].\n• Pruning, quantization, and efficient coding can improve the energy efficiency of DNNs 3X–7X [Han15].", "original_types": ["text", "header", "list"], "id": 61}
{"type": "section", "content": "4.5 It matters which datacenter is used, even within the same organization\n\nWe were amazed by how much it matters where and when a DNN is trained. Moreover, this option is likely the easiest path for ML practitioners to reduce CO2e. For example, after reading early drafts of this paper, some colleagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model. Reviewers of early drafts suggested that datacenter energy use is a zero-sum game. They thought that any tasks run in a green datacenter simply shift other work to dirtier datacenters, so there is no net gain. It’s not true, but that speculation reveals many seemingly plausible but incorrect fallacies:", "doc_id": "patterson2021", "page": 12, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.5 It matters which datacenter is used, even within the same organization\n\nWe were amazed by how much it matters where and when a DNN is trained. Moreover, this option is likely the easiest path for ML practitioners to reduce CO2e. For example, after reading early drafts of this paper, some colleagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model. Reviewers of early drafts suggested that datacenter energy use is a zero-sum game. They thought that any tasks run in a green datacenter simply shift other work to dirtier datacenters, so there is no net gain. It’s not true, but that speculation reveals many seemingly plausible but incorrect fallacies:", "original_types": ["text", "header"], "id": 62}
{"type": "section", "content": "• {'text': 'Fallacy: Datacenters are fully utilized. Applications are deployed to handle worst case demand depending on the time of day and day of the week, so for much of the time resources are idle [Arm10].'}\n• {'text': 'Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy much more land than they need initially at a site so that they can construct more buildings in the future without first traversing the lengthy process of acquiring land [Bar18].'}\n• {'text': 'Fallacy: Renewable energy is fixed and can’t grow. There is often an excess of renewable energy at some times of day (see Appendix B). The amount of solar and wind energy is also a function of the investment as well as weather conditions. Google’s long term renewable energy procurement normally invests in the creation of new renewable energy resources. The greater the use and investment in renewable energy, the more money is available to buy and deploy new solar panels and wind turbines, thereby increasing the renewable energy supply. Thus, it’s not the case that Google’s use of renewable energy means other residents must use dirty energy. Appendix B introduces issues around carbon free energy use and investment.'}\n• {'text': 'Fallacy: Google NLP model training competes with other tasks in the datacenter. Google trains large models on ML supercomputers that even have their own interconnection network, so ML training is distinct from CPU-only tasks [Jou20]. Tasks for CPUs don’t interfere with TPUs, and vice versa.'}\n• {'text': 'Fallacy: Training must run in all datacenters. While user facing inference applications need global distribution in order to provide low-latency access to users all around the world [Jou21], there is no problem to limit ML training computation to a smaller number of (green) datacenters. For example, Google is currently deploying numerous TPU v4s, many of which will be located in windy Oklahoma, whose net CO2e/KWh is even lower than Iowa.'}\n• {'text': 'Fallacy: There is no business reason to reduce carbon emissions. Reducing climate change certainly has long-term economic benefits for everyone. Google has been carbon neutral since 2007 and has procured enough additional renewable energy to match 100% of its datacenter energy usage since 2017, so the impact of the remaining carbon from training at Google is zero even today. Other hyperscalers aim for carbon neutrality by 2025 or 2030, so the whole cloud may become carbon neutral. With its new 24/7 local carbon-free energy goal by 2030, Google is now focused on purchasing carbon-free energy to match its hourly load at the same location as its datacenters with the goal to decarbonize its electricity supply (see Appendix B).'}\n\nThe next question that arose is whether such green datacenters are available to only a few ML practitioners.", "doc_id": "patterson2021", "page": 12, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "• {'text': 'Fallacy: Datacenters are fully utilized. Applications are deployed to handle worst case demand depending on the time of day and day of the week, so for much of the time resources are idle [Arm10].'}\n• {'text': 'Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy much more land than they need initially at a site so that they can construct more buildings in the future without first traversing the lengthy process of acquiring land [Bar18].'}\n• {'text': 'Fallacy: Renewable energy is fixed and can’t grow. There is often an excess of renewable energy at some times of day (see Appendix B). The amount of solar and wind energy is also a function of the investment as well as weather conditions. Google’s long term renewable energy procurement normally invests in the creation of new renewable energy resources. The greater the use and investment in renewable energy, the more money is available to buy and deploy new solar panels and wind turbines, thereby increasing the renewable energy supply. Thus, it’s not the case that Google’s use of renewable energy means other residents must use dirty energy. Appendix B introduces issues around carbon free energy use and investment.'}\n• {'text': 'Fallacy: Google NLP model training competes with other tasks in the datacenter. Google trains large models on ML supercomputers that even have their own interconnection network, so ML training is distinct from CPU-only tasks [Jou20]. Tasks for CPUs don’t interfere with TPUs, and vice versa.'}\n• {'text': 'Fallacy: Training must run in all datacenters. While user facing inference applications need global distribution in order to provide low-latency access to users all around the world [Jou21], there is no problem to limit ML training computation to a smaller number of (green) datacenters. For example, Google is currently deploying numerous TPU v4s, many of which will be located in windy Oklahoma, whose net CO2e/KWh is even lower than Iowa.'}\n• {'text': 'Fallacy: There is no business reason to reduce carbon emissions. Reducing climate change certainly has long-term economic benefits for everyone. Google has been carbon neutral since 2007 and has procured enough additional renewable energy to match 100% of its datacenter energy usage since 2017, so the impact of the remaining carbon from training at Google is zero even today. Other hyperscalers aim for carbon neutrality by 2025 or 2030, so the whole cloud may become carbon neutral. With its new 24/7 local carbon-free energy goal by 2030, Google is now focused on purchasing carbon-free energy to match its hourly load at the same location as its datacenters with the goal to decarbonize its electricity supply (see Appendix B).'}\n\nThe next question that arose is whether such green datacenters are available to only a few ML practitioners.", "original_types": ["text", "list"], "id": 63}
{"type": "section", "content": "4.6 Many have access to energy-optimized datacenters\n\nThe increasing use of cloud computing has decreased the energy intensity19 of datacenters 20% annually since 2010 [Has20]. Access to energy-optimized, low-cost cloud datacenters is not restricted to employees of a few companies; people around the world can rent computers in them using services like Alibaba Cloud, Amazon Web Services, Google Cloud Platform, and Microsoft Azure.20 Moreover, Alibaba, Amazon, and Google offer access to their custom processors for DNNs through their cloud service. The popularity of the public cloud is indicated by its annual growth in business by up to 50% since 2010 [Sch21]. Many believe the cloud’s efficiencies in cost and energy mean that it is the ultimate future of all datacenters [Arm10, Sch21]. The next topic reminds us that reducing cost and energy consumption remains important no matter how green the cloud becomes.\n\n4.7 Reducing the cost of training matters too\n\nThough many have access to these relatively efficient compute resources and cloud companies may dramatically reduce their carbon footprint in the future, it’s still important to reduce the economic cost of training. Saving money obviously matters to everyone, but expensive training of NLP models also makes this research style unattainable for many researchers21,22. This inequity of access to state-of-the-art models is another strong motivator, alongside environmental concerns, to incentivize the development of energy-efficient ML models that work as well as their computationally hungrier counterparts. One issue that was difficult for us during our investigation was to put into perspective the 4 to 552 tCO2e from training of these NLP models, which the next subsection explores.\n\n4.8 How does training a large NLP model compare to other activities?", "doc_id": "patterson2021", "page": 13, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.6 Many have access to energy-optimized datacenters\n\nThe increasing use of cloud computing has decreased the energy intensity19 of datacenters 20% annually since 2010 [Has20]. Access to energy-optimized, low-cost cloud datacenters is not restricted to employees of a few companies; people around the world can rent computers in them using services like Alibaba Cloud, Amazon Web Services, Google Cloud Platform, and Microsoft Azure.20 Moreover, Alibaba, Amazon, and Google offer access to their custom processors for DNNs through their cloud service. The popularity of the public cloud is indicated by its annual growth in business by up to 50% since 2010 [Sch21]. Many believe the cloud’s efficiencies in cost and energy mean that it is the ultimate future of all datacenters [Arm10, Sch21]. The next topic reminds us that reducing cost and energy consumption remains important no matter how green the cloud becomes.\n\n4.7 Reducing the cost of training matters too\n\nThough many have access to these relatively efficient compute resources and cloud companies may dramatically reduce their carbon footprint in the future, it’s still important to reduce the economic cost of training. Saving money obviously matters to everyone, but expensive training of NLP models also makes this research style unattainable for many researchers21,22. This inequity of access to state-of-the-art models is another strong motivator, alongside environmental concerns, to incentivize the development of energy-efficient ML models that work as well as their computationally hungrier counterparts. One issue that was difficult for us during our investigation was to put into perspective the 4 to 552 tCO2e from training of these NLP models, which the next subsection explores.\n\n4.8 How does training a large NLP model compare to other activities?", "original_types": ["text", "header"], "id": 64}
{"type": "section", "content": "4.9 Are the benefits of NLP models worth the energy cost?\n\nA recent example of a societal benefit of NLP is the COVID-19 Research Explorer, which helps scientists and researchers efficiently pore through articles for answers or evidence to COVID-19-related questions. It is powered by BERT, a Transformer-style model trained for the biomedical domain [Hal20]. Its training consumed ~2.8 MWh and produced 0.13 tCO2e, about one-tenth of a SF-NY round trip by one passenger. A more widespread example is the use of BERT in search. English is the most popular language on the web. This use of BERT takes models that learn from improvements in English and applies them to other languages. In particular, BERT significantly improved featured snippets—short text summary at the top of Google research results—in languages like Hindi, Korean, and Portuguese.", "doc_id": "patterson2021", "page": 14, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "4.9 Are the benefits of NLP models worth the energy cost?\n\nA recent example of a societal benefit of NLP is the COVID-19 Research Explorer, which helps scientists and researchers efficiently pore through articles for answers or evidence to COVID-19-related questions. It is powered by BERT, a Transformer-style model trained for the biomedical domain [Hal20]. Its training consumed ~2.8 MWh and produced 0.13 tCO2e, about one-tenth of a SF-NY round trip by one passenger. A more widespread example is the use of BERT in search. English is the most popular language on the web. This use of BERT takes models that learn from improvements in English and applies them to other languages. In particular, BERT significantly improved featured snippets—short text summary at the top of Google research results—in languages like Hindi, Korean, and Portuguese.", "original_types": ["text", "header"], "id": 65}
{"type": "figure", "content": "Figure 7: Reproduction of Figure 6 from [Lep20] with annotations. Translation quality comparison of multilingual Mixture of Expert (MoE) Transformer models trained with GShard showing the increase in BLEU score versus a separate baseline Transformer model trained on each language pair for 100 languages to English. MoE models have large model capacity but are only partially activated for any given token. The source languages are grouped on the x-axis by the resources available for each language in billions of speakers, with languages like French and Spanish on the left (>1B examples) and languages like Sindhi and Yoruba on the right (<1M examples). The BLEU score improvements from larger models and multilingual training are high for all languages but are even higher for low-resource languages—the graph’s right-hand side is higher than the left—so Yoruba translation quality benefits more than Spanish translation quality.", "doc_id": "patterson2021", "page": 14, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Figure 7: Reproduction of Figure 6 from [Lep20] with annotations. Translation quality comparison of multilingual Mixture of Expert (MoE) Transformer models trained with GShard showing the increase in BLEU score versus a separate baseline Transformer model trained on each language pair for 100 languages to English. MoE models have large model capacity but are only partially activated for any given token. The source languages are grouped on the x-axis by the resources available for each language in billions of speakers, with languages like French and Spanish on the left (>1B examples) and languages like Sindhi and Yoruba on the right (<1M examples). The BLEU score improvements from larger models and multilingual training are high for all languages but are even higher for low-resource languages—the graph’s right-hand side is higher than the left—so Yoruba translation quality benefits more than Spanish translation quality.", "id": 66}
{"type": "section", "content": "5. Conclusion\n\nGlobal climate change is a threat to economies, human health, and the environment, and the ML community needs to do its share to limit its carbon emissions.27 We’re thankful that papers like [Lac19, Str19, Sch20, Hen20] helped make the ML community aware of this important issue. Improving the energy efficiency of algorithms, datacenters, hardware, and software has long been a business priority for Google and other Cloud companies. For example, Gshard-600B operates much more efficiently than other large NLP models and ML accelerators are more efficient than off-the-shelf hardware.\n\nAs mentioned in the introduction, we make three suggestions for publications on compute intensive models that could eventually help reduce their CO2e footprint: report energy consumed and CO2e explicitly, ML conferences should reward improvements in efficiency as well as traditional metrics, and include the time and number of processors for training to help everyone understand its cost. We believe power will be included in upcoming MLPerf benchmarks, which is an important step in the right direction.\n\nIf the ML community working on computationally intensive models starts competing on training quality and carbon footprint rather than on accuracy alone, the most efficient datacenters and hardware might see the highest ML demand. If paired with publication incentives to improve emission metrics in addition to accuracy, we can imagine a virtuous cycle that slows the growth of the carbon footprint of ML by accelerating innovations in the efficiency and cost of algorithms, systems, hardware, datacenters, and carbon free energy.", "doc_id": "patterson2021", "page": 15, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "5. Conclusion\n\nGlobal climate change is a threat to economies, human health, and the environment, and the ML community needs to do its share to limit its carbon emissions.27 We’re thankful that papers like [Lac19, Str19, Sch20, Hen20] helped make the ML community aware of this important issue. Improving the energy efficiency of algorithms, datacenters, hardware, and software has long been a business priority for Google and other Cloud companies. For example, Gshard-600B operates much more efficiently than other large NLP models and ML accelerators are more efficient than off-the-shelf hardware.\n\nAs mentioned in the introduction, we make three suggestions for publications on compute intensive models that could eventually help reduce their CO2e footprint: report energy consumed and CO2e explicitly, ML conferences should reward improvements in efficiency as well as traditional metrics, and include the time and number of processors for training to help everyone understand its cost. We believe power will be included in upcoming MLPerf benchmarks, which is an important step in the right direction.\n\nIf the ML community working on computationally intensive models starts competing on training quality and carbon footprint rather than on accuracy alone, the most efficient datacenters and hardware might see the highest ML demand. If paired with publication incentives to improve emission metrics in addition to accuracy, we can imagine a virtuous cycle that slows the growth of the carbon footprint of ML by accelerating innovations in the efficiency and cost of algorithms, systems, hardware, datacenters, and carbon free energy.", "original_types": ["text", "header"], "id": 67}
{"type": "section", "content": "References\n\n[Adi20] Adiwardana, D., Luong, M., R. So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le. Q. Towards a Human-like Open-Domain Chatbot. arXiv preprint arXiv:2001.09977.\n\n[Arm10] Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica, I. and Zaharia, M., 2010. A view of cloud computing. Communications of the ACM, 53(4), pp.50-58.\n\n[Bar19] Barr, J. December 3, 2019. Amazon EC2 Update, aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n\n[Bro20] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D. July 22, 2020. Language models are few-shot learners. NeurIPS 2020. arXiv preprint arXiv:2005.14165.\n\n[Ben21] Bender, E., Gebru, T., McMillan-Major, A. Shmithell, S. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? FAccT 2021. http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf.\n\n[Car21] Carbon Offset Research and Education, 2021, Carbon Offset Guide, https://www.offsetguide.org/\n\n[Cha19] Chang, K.W., Prabhakaran, V. and Ordonez, V., 2019, November. Bias and fairness in natural language processing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts. https://arxiv.org/pdf/1908.09635.pdf.\n\n[Cri21] Criddle, C., February 10, 2021. Bitcoin consumes more electricity than Argentina, www.bbc.com/news/technology-56012952.\n\n[Dig21] Digiconomist, 2021, Bitcoin Energy Consumption Index, https://digiconomist.net/bitcoin-energy-consumption/.\n\n[Dod19] Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019. Show Your Work: Improved Reporting of Experimental Results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). www.aclweb.org/anthology/D19-1224/.\n\n[Dod20] Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, N., 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.\n\n[Evo19] Apache-licensed Evolved Transformer open-source implementation in tensorflow/tensor2tensor GitHub repository. https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py.", "doc_id": "patterson2021", "page": 16, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "References\n\n[Adi20] Adiwardana, D., Luong, M., R. So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le. Q. Towards a Human-like Open-Domain Chatbot. arXiv preprint arXiv:2001.09977.\n\n[Arm10] Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica, I. and Zaharia, M., 2010. A view of cloud computing. Communications of the ACM, 53(4), pp.50-58.\n\n[Bar19] Barr, J. December 3, 2019. Amazon EC2 Update, aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/\n\n[Bro20] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D. July 22, 2020. Language models are few-shot learners. NeurIPS 2020. arXiv preprint arXiv:2005.14165.\n\n[Ben21] Bender, E., Gebru, T., McMillan-Major, A. Shmithell, S. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? FAccT 2021. http://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf.\n\n[Car21] Carbon Offset Research and Education, 2021, Carbon Offset Guide, https://www.offsetguide.org/\n\n[Cha19] Chang, K.W., Prabhakaran, V. and Ordonez, V., 2019, November. Bias and fairness in natural language processing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts. https://arxiv.org/pdf/1908.09635.pdf.\n\n[Cri21] Criddle, C., February 10, 2021. Bitcoin consumes more electricity than Argentina, www.bbc.com/news/technology-56012952.\n\n[Dig21] Digiconomist, 2021, Bitcoin Energy Consumption Index, https://digiconomist.net/bitcoin-energy-consumption/.\n\n[Dod19] Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019. Show Your Work: Improved Reporting of Experimental Results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). www.aclweb.org/anthology/D19-1224/.\n\n[Dod20] Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, N., 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.\n\n[Evo19] Apache-licensed Evolved Transformer open-source implementation in tensorflow/tensor2tensor GitHub repository. https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py.", "original_types": ["text", "header"], "id": 68}
{"type": "section", "content": "[Fed21] Fedus, W., Zoph, B., Shazeer, N., January 11, 2021, Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961.\n\n[Gar19] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E.H. and Beutel, A., 2019, January. Counterfactual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 219-226). https://research.google/pubs/pub47670/.\n\n[Goo16] Google, December 2016, Achieving Our 100% Renewable Energy Purchasing Goal and Going Beyond, https://static.googleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal.pdf.\n\n[Goo20] Google, Environmental Report 2020, https://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf.\n\n[Goo21] Google, February 2021, 24/7 Carbon-Free Energy: Methodologies and Metrics, https://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf.\n\n[Gup20] Gupta, U., Kim, Y.G., Lee, S., Tse, J., Lee, H.H.S., Wei, G.Y., Brooks, D. and Wu, C.J., 2020. Chasing Carbon: The Elusive Environmental Footprint of Computing. arXiv preprint arXiv:2011.02839.\n\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19, https://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html.\n\n[Han15] Han, S., Pool, J., Tran, J. and Dally, W.J., 2015. Learning both weights and connections for efficient neural networks. ICLR 2016. arXiv preprint arXiv:1510.00149.\n\n[Hen20] Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D. and Pineau, J., 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research. https://jmlr.org/papers/v21/20-312.html.\n\n[Her20] Hernandez, D. and Brown, T.B., 2020. Measuring the algorithmic efficiency of neural networks. arXiv preprint arXiv:2005.04305. https://arxiv.org/abs/2005.04305.\n\n[Hin15] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n\n[Höl20] Hölzle, U., Feb 27, 2020. datacenters are more energy efficient than ever. blog.google/outreach-initiatives/sustainability/data-centers-energy-efficient.\n\n[Joh20] Johnson, M., April 22, 2020, A Scalable Approach to Reducing Gender Bias in Google Translate, https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html.", "doc_id": "patterson2021", "page": 16, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "[Fed21] Fedus, W., Zoph, B., Shazeer, N., January 11, 2021, Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/abs/2101.03961.\n\n[Gar19] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E.H. and Beutel, A., 2019, January. Counterfactual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 219-226). https://research.google/pubs/pub47670/.\n\n[Goo16] Google, December 2016, Achieving Our 100% Renewable Energy Purchasing Goal and Going Beyond, https://static.googleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal.pdf.\n\n[Goo20] Google, Environmental Report 2020, https://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf.\n\n[Goo21] Google, February 2021, 24/7 Carbon-Free Energy: Methodologies and Metrics, https://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf.\n\n[Gup20] Gupta, U., Kim, Y.G., Lee, S., Tse, J., Lee, H.H.S., Wei, G.Y., Brooks, D. and Wu, C.J., 2020. Chasing Carbon: The Elusive Environmental Footprint of Computing. arXiv preprint arXiv:2011.02839.\n\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19, https://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html.\n\n[Han15] Han, S., Pool, J., Tran, J. and Dally, W.J., 2015. Learning both weights and connections for efficient neural networks. ICLR 2016. arXiv preprint arXiv:1510.00149.\n\n[Hen20] Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D. and Pineau, J., 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research. https://jmlr.org/papers/v21/20-312.html.\n\n[Her20] Hernandez, D. and Brown, T.B., 2020. Measuring the algorithmic efficiency of neural networks. arXiv preprint arXiv:2005.04305. https://arxiv.org/abs/2005.04305.\n\n[Hin15] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n\n[Höl20] Hölzle, U., Feb 27, 2020. datacenters are more energy efficient than ever. blog.google/outreach-initiatives/sustainability/data-centers-energy-efficient.\n\n[Joh20] Johnson, M., April 22, 2020, A Scalable Approach to Reducing Gender Bias in Google Translate, https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html.", "original_types": ["text"], "id": 69}
{"type": "section", "content": "[Jou21] Jouppi, N., Yoon, D-H, Jablin, T., Kurian, G., Laudon, J., Li, S., Ma, P., Ma, X., Patil, N.,Prasad, S., Young, C., Zhou, Z., and Patterson, D., May 2021. Ten Lessons From Three Generations Shaped Google’s TPUv4i, to appear, the 48th International Symposium on Computer Architecture.\n\n[Kap20] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and Amodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n\n[Kär18] Kärcher B. Formation and radiative forcing of contrail cirrus. Nature communications. 2018 May 8;9(1):1-7. https://www.nature.com/articles/s41467-018-04068-0.\n\n[Kuc18] Kuczmaszki, J. and Johnson, M., 2018. Gender-aware natural language translation. www.tdcommons.org/dpubs_series/1577/.\n\n[Lac19] Lacoste, A., Luccioni, A., Schmidt, V. and Dandres, T., 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700.\n\n[Lan20] Lannelongue, L., Grealey, J. and Inouye, M., 2020. Green algorithms: Quantifying the carbon footprint of computation. arXiv: 2007.07610.\n\n[Leo19] Leopold, G. March 19, 2019, AWS to Offer Nvidia’s T4 GPUs for AI Inferencing, www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/.\n\n[Lep20] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020. GShard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.\n\n[Li21] Li, S., Tan, M., Pang, R., Li, A., Cheng, L., Le, Q. and Jouppi, N.P., 2021. Searching for Fast Model Families on Datacenter Accelerators. arXiv preprint arXiv:2102.05610.\n\n[Liu18] Liu, H., Simonyan, K. and Yang, Y., 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.\n\n[Luc21] Luccioni, A., and Schmidt, V.. March 2021, Private Communication.\n\n[Mas20] Masanet, E., Shehabi, A., Lei, N., Smith, S. and Koomey, J., 2020. Recalibrating global datacenter energy-use estimates. Science, 367(6481), pp.984-986. https://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n\n[Mas21] Masanet, E., March 24, 2021, Data Center Energy Analysis: Past, Present, and Future, lecture at UCSB.\n\n[Mer19] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2019. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635. https://arxiv.org/pdf/1908.09635.pdf.\n\n[Pha18] Pham, H., Guan, M., Zoph, B., Le, Q. and Dean, J., 2018, July. Efficient neural architecture search via parameters sharing. In International Conference on Machine Learning (pp. 4095-4104). PMLR. arXiv preprint arXiv:1802.03268.\n\n[Rad20] Radovanovic, A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows, https://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blow.", "doc_id": "patterson2021", "page": 17, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "[Jou21] Jouppi, N., Yoon, D-H, Jablin, T., Kurian, G., Laudon, J., Li, S., Ma, P., Ma, X., Patil, N.,Prasad, S., Young, C., Zhou, Z., and Patterson, D., May 2021. Ten Lessons From Three Generations Shaped Google’s TPUv4i, to appear, the 48th International Symposium on Computer Architecture.\n\n[Kap20] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and Amodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n\n[Kär18] Kärcher B. Formation and radiative forcing of contrail cirrus. Nature communications. 2018 May 8;9(1):1-7. https://www.nature.com/articles/s41467-018-04068-0.\n\n[Kuc18] Kuczmaszki, J. and Johnson, M., 2018. Gender-aware natural language translation. www.tdcommons.org/dpubs_series/1577/.\n\n[Lac19] Lacoste, A., Luccioni, A., Schmidt, V. and Dandres, T., 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700.\n\n[Lan20] Lannelongue, L., Grealey, J. and Inouye, M., 2020. Green algorithms: Quantifying the carbon footprint of computation. arXiv: 2007.07610.\n\n[Leo19] Leopold, G. March 19, 2019, AWS to Offer Nvidia’s T4 GPUs for AI Inferencing, www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/.\n\n[Lep20] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020. GShard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.\n\n[Li21] Li, S., Tan, M., Pang, R., Li, A., Cheng, L., Le, Q. and Jouppi, N.P., 2021. Searching for Fast Model Families on Datacenter Accelerators. arXiv preprint arXiv:2102.05610.\n\n[Liu18] Liu, H., Simonyan, K. and Yang, Y., 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.\n\n[Luc21] Luccioni, A., and Schmidt, V.. March 2021, Private Communication.\n\n[Mas20] Masanet, E., Shehabi, A., Lei, N., Smith, S. and Koomey, J., 2020. Recalibrating global datacenter energy-use estimates. Science, 367(6481), pp.984-986. https://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n\n[Mas21] Masanet, E., March 24, 2021, Data Center Energy Analysis: Past, Present, and Future, lecture at UCSB.\n\n[Mer19] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2019. A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635. https://arxiv.org/pdf/1908.09635.pdf.\n\n[Pha18] Pham, H., Guan, M., Zoph, B., Le, Q. and Dean, J., 2018, July. Efficient neural architecture search via parameters sharing. In International Conference on Machine Learning (pp. 4095-4104). PMLR. arXiv preprint arXiv:1802.03268.\n\n[Rad20] Radovanovic, A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows, https://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blow.", "original_types": ["text"], "id": 70}
{"type": "section", "content": "[Raf19] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\n\n[Rit20] Ritchie, H., October 22, 2020, Climate change and flying: what share of global CO2 emissions come from aviation? https://ourworldindata.org/co2-emissions-from-aviation.\n\n[Ryo14] Ryor, J.N. and Tawney, L.E.T.H.A., 2014. Utility-Scale Renewable Energy: Understanding Cost Parity. Paris: World Resources Institute. https://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf.\n\n[San20] Sanh, V., Debut, L., Chaumond, J. and Wolf, T., 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n[Sch20] Schwartz, R., Dodge, J., Smith, N.A. and Etzioni, O., 2020. Green AI. Communications of the ACM, 63(12), pp.54-63. https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext.\n\n[Sch21] Schleier-Smith, J., Sreekanti, V., Khandelwal, A., Carreira, J., Yadwadkar, N., Popa, R., Joseph E. Gonzalez,J., Ion Stoica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next Phase of Cloud Computing, Communications of the ACM, 64(5).\n\n[Sha17] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J., 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. ICLR 2017. arXiv preprint arXiv:1701.06538.\n\n[So19] So, D., Le, Q. and Liang, C., 2019, May. The Evolved Transformer. In International Conference on Machine Learning 2019 (pp. 5877-5886). PMLR. arXiv preprint arXiv:1901.11117.\n\n[Str19] Strubell, E., Ganesh, A. and McCallum, A., 2019. Energy and policy considerations for deep learning in NLP. ACL 2019. arXiv preprint arXiv:1906.02243.\n\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.\n\n[Tan19] Tan, M. and Le, Q., 2019, May. EfficientNet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. arXiv preprint arXiv:1905.11946.\n\n[USE21] US Energy Information Administration, 2021, FAQ How much carbon dioxide is produced per kilowatt hour of U.S. electricity generation? https://www.eia.gov/tools/faqs/faq.php?id=74&t=11.\n\n[Vas17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017. Attention is all you need. NeurIPS 2017. arXiv preprint arXiv:1706.03762.\n\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 53(3), pp.1-34.", "doc_id": "patterson2021", "page": 17, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "[Raf19] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\n\n[Rit20] Ritchie, H., October 22, 2020, Climate change and flying: what share of global CO2 emissions come from aviation? https://ourworldindata.org/co2-emissions-from-aviation.\n\n[Ryo14] Ryor, J.N. and Tawney, L.E.T.H.A., 2014. Utility-Scale Renewable Energy: Understanding Cost Parity. Paris: World Resources Institute. https://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf.\n\n[San20] Sanh, V., Debut, L., Chaumond, J. and Wolf, T., 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n[Sch20] Schwartz, R., Dodge, J., Smith, N.A. and Etzioni, O., 2020. Green AI. Communications of the ACM, 63(12), pp.54-63. https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext.\n\n[Sch21] Schleier-Smith, J., Sreekanti, V., Khandelwal, A., Carreira, J., Yadwadkar, N., Popa, R., Joseph E. Gonzalez,J., Ion Stoica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next Phase of Cloud Computing, Communications of the ACM, 64(5).\n\n[Sha17] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J., 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. ICLR 2017. arXiv preprint arXiv:1701.06538.\n\n[So19] So, D., Le, Q. and Liang, C., 2019, May. The Evolved Transformer. In International Conference on Machine Learning 2019 (pp. 5877-5886). PMLR. arXiv preprint arXiv:1901.11117.\n\n[Str19] Strubell, E., Ganesh, A. and McCallum, A., 2019. Energy and policy considerations for deep learning in NLP. ACL 2019. arXiv preprint arXiv:1906.02243.\n\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.\n\n[Tan19] Tan, M. and Le, Q., 2019, May. EfficientNet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR. arXiv preprint arXiv:1905.11946.\n\n[USE21] US Energy Information Administration, 2021, FAQ How much carbon dioxide is produced per kilowatt hour of U.S. electricity generation? https://www.eia.gov/tools/faqs/faq.php?id=74&t=11.\n\n[Vas17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017. Attention is all you need. NeurIPS 2017. arXiv preprint arXiv:1706.03762.\n\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020. Generalizing from a few examples: A survey on few-shot learning. ACM Computing Surveys, 53(3), pp.1-34.", "original_types": ["text"], "id": 71}
{"type": "section", "content": "Appendix A. Details of CO₂ Estimates for Four Large NLP Models in Tables 1 and 4\n\nWe describe below how we derived the values in Tables 1 and 4.", "doc_id": "patterson2021", "page": 18, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Appendix A. Details of CO₂ Estimates for Four Large NLP Models in Tables 1 and 4\n\nWe describe below how we derived the values in Tables 1 and 4.", "original_types": ["text", "header"], "id": 72}
{"type": "section", "content": "Appendix B. Carbon Offset and 24/7 Carbon Free Energy\n\nWhile energy consumption is relatively straightforward, policies to reduce carbon footprint are not. One reason is that they have as much to do about economics and accounting as they do about physics. This short appendix tries to clarify the distinction between conventional carbon offsets, Google’s goal for 2030 of 24/7 Carbon Free Energy (CFE) for its global datacenters and campuses, and what it is doing in 2021 to set the groundwork for 2030. Readers interested in greater depth should take a look at [Ryo14, Goog16, Goo21].\n\nConventional carbon offsets try to create economic incentives to create projects that avoid or remove CO2e. When pursuing the mitigation of carbon emissions from electricity production and consumption, a company can match their MWh of consumption with MWh of clean energy through certificates called RECs (Renewable Energy Certificates). The rules for accounting and compensation, are defined as part of the GHG Protocol, under Scope 2 for electricity. Under the current Scope 2 Guidance, 1MWh of energy used in July in, say, Georgia that produces carbon dioxide can be compensated by purchasing 1MWh of CFE in Montana in November. Typically, the period of accounting is a calendar year. Google achieved carbon neutrality using conventional carbon offsets starting in 2007.29\n\nAs part of the GHG Protocol, the World Resource Institute defines terms and economic mechanisms to ensure consistency of claims about carbon. They defined the following [Car21, Ryo14] (also see Figure 8):\n\n● Additionality: CO2e reductions are additional if they would not have occurred in the absence of a market for offset credits. Additionality is essential for the quality of carbon offset credits—if their associated CO2e reductions are not additional, then purchasing offset credits in lieu of reducing your own emissions will make climate change worse.\n\n● The Grid: The transmission and distribution system that connects generators and end-users.\n\n● Levelized Cost Of Energy (LCOE): The projected total system and operating costs divided by total KWh produced over the lifetime of the project or contract.\n\n● Power Purchase Agreement (PPA): A fixed-price contractual agreement to purchase a power plant’s energy, typically calculated using LCOE.\n\n● Renewable Energy Certificate (REC)30: A market-based instrument that represents the property rights to the environmental, social, and other non-power attributes of renewable electricity generation. The goal is a certificate that ensures the energy purchased is genuinely renewable and not double counted.\n\nGoogle’s target for 2030 is to go beyond the traditional Scope 2 rules to restrict both the location and the accounting period.\n\n● Instead of anywhere in a continent, the CFE purchase should be on the same geographically local grid.\n\n● Instead of the accounting period being one year, the accounting should be within the hour.", "doc_id": "patterson2021", "page": 19, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Appendix B. Carbon Offset and 24/7 Carbon Free Energy\n\nWhile energy consumption is relatively straightforward, policies to reduce carbon footprint are not. One reason is that they have as much to do about economics and accounting as they do about physics. This short appendix tries to clarify the distinction between conventional carbon offsets, Google’s goal for 2030 of 24/7 Carbon Free Energy (CFE) for its global datacenters and campuses, and what it is doing in 2021 to set the groundwork for 2030. Readers interested in greater depth should take a look at [Ryo14, Goog16, Goo21].\n\nConventional carbon offsets try to create economic incentives to create projects that avoid or remove CO2e. When pursuing the mitigation of carbon emissions from electricity production and consumption, a company can match their MWh of consumption with MWh of clean energy through certificates called RECs (Renewable Energy Certificates). The rules for accounting and compensation, are defined as part of the GHG Protocol, under Scope 2 for electricity. Under the current Scope 2 Guidance, 1MWh of energy used in July in, say, Georgia that produces carbon dioxide can be compensated by purchasing 1MWh of CFE in Montana in November. Typically, the period of accounting is a calendar year. Google achieved carbon neutrality using conventional carbon offsets starting in 2007.29\n\nAs part of the GHG Protocol, the World Resource Institute defines terms and economic mechanisms to ensure consistency of claims about carbon. They defined the following [Car21, Ryo14] (also see Figure 8):\n\n● Additionality: CO2e reductions are additional if they would not have occurred in the absence of a market for offset credits. Additionality is essential for the quality of carbon offset credits—if their associated CO2e reductions are not additional, then purchasing offset credits in lieu of reducing your own emissions will make climate change worse.\n\n● The Grid: The transmission and distribution system that connects generators and end-users.\n\n● Levelized Cost Of Energy (LCOE): The projected total system and operating costs divided by total KWh produced over the lifetime of the project or contract.\n\n● Power Purchase Agreement (PPA): A fixed-price contractual agreement to purchase a power plant’s energy, typically calculated using LCOE.\n\n● Renewable Energy Certificate (REC)30: A market-based instrument that represents the property rights to the environmental, social, and other non-power attributes of renewable electricity generation. The goal is a certificate that ensures the energy purchased is genuinely renewable and not double counted.\n\nGoogle’s target for 2030 is to go beyond the traditional Scope 2 rules to restrict both the location and the accounting period.\n\n● Instead of anywhere in a continent, the CFE purchase should be on the same geographically local grid.\n\n● Instead of the accounting period being one year, the accounting should be within the hour.", "original_types": ["text", "header"], "id": 73}
{"type": "section", "content": "To achieve 100% 24/7 local CFE, grids would need to offer both real time accounting of the CFE fraction of the standard grid and the generating companies must offer more flexible options to allow consumers to pick CFE any time of the day, not just when the wind blows or when the sun shines. Ideally, grid operators and generating companies will deliver on that vision, and the standards will evolve to certify and quantify the 24/7 CFE approach. But we are not there yet.\n\nFigure 8 helps explain what Google is doing today. Google signs long-term contracts as PPAs with renewable energy generating companies to try to cover Google’s electricity consumption.31 One benefit of long-term contracts is that they guarantee a reliable income stream for many years and therefore make such projects more easily financeable. To hit its 24/7 target, Google will continue to purchase clean energy from various sources such as energy storage and energy generation to ensure it has a clean energy supply at all 24 hours of the day, 7 days a week.", "doc_id": "patterson2021", "page": 19, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "To achieve 100% 24/7 local CFE, grids would need to offer both real time accounting of the CFE fraction of the standard grid and the generating companies must offer more flexible options to allow consumers to pick CFE any time of the day, not just when the wind blows or when the sun shines. Ideally, grid operators and generating companies will deliver on that vision, and the standards will evolve to certify and quantify the 24/7 CFE approach. But we are not there yet.\n\nFigure 8 helps explain what Google is doing today. Google signs long-term contracts as PPAs with renewable energy generating companies to try to cover Google’s electricity consumption.31 One benefit of long-term contracts is that they guarantee a reliable income stream for many years and therefore make such projects more easily financeable. To hit its 24/7 target, Google will continue to purchase clean energy from various sources such as energy storage and energy generation to ensure it has a clean energy supply at all 24 hours of the day, 7 days a week.", "original_types": ["text"], "id": 74}
{"type": "section", "content": "The percentage of CFE for a datacenter is reported ex-post, after load, production, and grid mix data are settled and made available to Google. With the current 24/7 CFE framework, when Google cannot get 100% CFE from the grid plus its clean energy contracts in a given hour, the shortfall counts against the goal. When the grid and renewable energy contracts overshoot in a given hour, Google doesn’t get any extra credit for it, as the accounting period is reset every hour.32 Since Google can estimate how much CFE is expected in a specific region based on the grid and its multi-year clean energy contract, it incentivizes programs to run in this region.33\n\nTables 1 and 4 show this distinction as gross CO2e (energy from the grid) and the net CO2e (after applying the 24/7 local renewable energy purchase from the long-term contracts). Since you can’t label electrons, there is no guarantee that Google is using exactly the same clean energy that it paid for, but in our view the overall effect is the same.\n\nAlas, Google’s large models in Table 4 were run in the Georgia datacenter, where in the past there was no or little difference between gross and net CO2e. Regions that have generator companies that can supply clean energy 24/7 and offer marketplaces that allow companies to acquire clean energy at any time of day will be more compelling to expand future growth of compute from a carbon impact perspective. A great example is Oklahoma, which allowed Google to average 95.6% net CFE for 2020. This is a case of where the grass actually is greener in Oklahoma than in Georgia. As mentioned above, in 2021 many new TPU v4 accelerators will be deployed in windy Oklahoma.", "doc_id": "patterson2021", "page": 20, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "The percentage of CFE for a datacenter is reported ex-post, after load, production, and grid mix data are settled and made available to Google. With the current 24/7 CFE framework, when Google cannot get 100% CFE from the grid plus its clean energy contracts in a given hour, the shortfall counts against the goal. When the grid and renewable energy contracts overshoot in a given hour, Google doesn’t get any extra credit for it, as the accounting period is reset every hour.32 Since Google can estimate how much CFE is expected in a specific region based on the grid and its multi-year clean energy contract, it incentivizes programs to run in this region.33\n\nTables 1 and 4 show this distinction as gross CO2e (energy from the grid) and the net CO2e (after applying the 24/7 local renewable energy purchase from the long-term contracts). Since you can’t label electrons, there is no guarantee that Google is using exactly the same clean energy that it paid for, but in our view the overall effect is the same.\n\nAlas, Google’s large models in Table 4 were run in the Georgia datacenter, where in the past there was no or little difference between gross and net CO2e. Regions that have generator companies that can supply clean energy 24/7 and offer marketplaces that allow companies to acquire clean energy at any time of day will be more compelling to expand future growth of compute from a carbon impact perspective. A great example is Oklahoma, which allowed Google to average 95.6% net CFE for 2020. This is a case of where the grass actually is greener in Oklahoma than in Georgia. As mentioned above, in 2021 many new TPU v4 accelerators will be deployed in windy Oklahoma.", "original_types": ["text"], "id": 75}
{"type": "figure", "content": "Figure 8. This figure explains how fixed-floating swaps work for Renewable Energy Certificates (RECs). (Reproduced from [Goo16].) Instead of accounting over a full year at a mix of locations as in step 4, 24/7 CFE does the accounting separately for every hour in the year in the same single location.", "doc_id": "patterson2021", "page": 20, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Figure 8. This figure explains how fixed-floating swaps work for Renewable Energy Certificates (RECs). (Reproduced from [Goo16].) Instead of accounting over a full year at a mix of locations as in step 4, 24/7 CFE does the accounting separately for every hour in the year in the same single location.", "id": 76}
{"type": "section", "content": "Appendix C. Details of a CO2e Estimate for NAS in an Average Datacenter\n\n[Str19] estimates the CO2e for the neural architecture search (NAS) to find the more-efficient Evolved Transformer architecture done by [So19] at Google as 626,155 pounds (284 tCO2e). The estimate in [Str19] was done for the hypothetical scenario of running the computation on P100 GPUs in the average U.S. datacenter with the average U.S. grid energy mix. The authors of this note represent a superset of the authors of [So19], and we agree that the information needed for an accurate estimate was scattered in several subsections in the So et al. paper, which makes it difficult to determine the actual CO2e. This experience is one reason we suggest that ML conferences encourage future NLP papers that are computationally expensive to include a calculation of energy consumed and CO2e to make sure all the details are included, as it’s difficult to determine them retrospectively, as we shall see.\n\nNAS costs in [Str19] are derived from the NAS process described in section 5.2 of [So19]: “The search ran for 15K child models, requiring a total of 979M train steps. Over 13K models did not make it past the first hurdle, drastically reducing the resources required to view the 240 thousandth train step for top models, which would have cost 3.6B training steps for the same number of models without hurdles. After the search concluded, we then selected the top 20 models and trained them for the full 300K steps, each on a single TPU V.2 chip.”\n\nThe projection of the So et al. NAS cost by Strubell et al. overestimates the actual Evolved Transformer search cost. Strubell et al. assumed each evaluation in the search is conducted using a large configuration: Transformer (Big) with batch size 32,768. However, So et al. actually used a small proxy configuration (Section 3.3 of [So19]) to reduce compute cost (and CO2e). This proxy version used Transformer (Base) rather than Transformer (Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096 while keeping the number of training steps unchanged, reducing the cost/step by a further 8x.", "doc_id": "patterson2021", "page": 21, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Appendix C. Details of a CO2e Estimate for NAS in an Average Datacenter\n\n[Str19] estimates the CO2e for the neural architecture search (NAS) to find the more-efficient Evolved Transformer architecture done by [So19] at Google as 626,155 pounds (284 tCO2e). The estimate in [Str19] was done for the hypothetical scenario of running the computation on P100 GPUs in the average U.S. datacenter with the average U.S. grid energy mix. The authors of this note represent a superset of the authors of [So19], and we agree that the information needed for an accurate estimate was scattered in several subsections in the So et al. paper, which makes it difficult to determine the actual CO2e. This experience is one reason we suggest that ML conferences encourage future NLP papers that are computationally expensive to include a calculation of energy consumed and CO2e to make sure all the details are included, as it’s difficult to determine them retrospectively, as we shall see.\n\nNAS costs in [Str19] are derived from the NAS process described in section 5.2 of [So19]: “The search ran for 15K child models, requiring a total of 979M train steps. Over 13K models did not make it past the first hurdle, drastically reducing the resources required to view the 240 thousandth train step for top models, which would have cost 3.6B training steps for the same number of models without hurdles. After the search concluded, we then selected the top 20 models and trained them for the full 300K steps, each on a single TPU V.2 chip.”\n\nThe projection of the So et al. NAS cost by Strubell et al. overestimates the actual Evolved Transformer search cost. Strubell et al. assumed each evaluation in the search is conducted using a large configuration: Transformer (Big) with batch size 32,768. However, So et al. actually used a small proxy configuration (Section 3.3 of [So19]) to reduce compute cost (and CO2e). This proxy version used Transformer (Base) rather than Transformer (Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096 while keeping the number of training steps unchanged, reducing the cost/step by a further 8x.", "original_types": ["text", "header"], "id": 77}
{"type": "section", "content": "As a result, the calculations below suggest that CO2e from the misunderstanding about the use of the smaller proxy task were overestimated by a factor of ~18.7: Assume the Carbon Emission Estimation Method in [Str19]: CO2e = num_chips x num_train_steps x hours/train_steps x emission/chip_per_hour num_train_steps = 979,000,000 # From [So19] emission_per_chip_per_hour ~= 0.2855296 pounds CO2e # From [Str19] Table 334. Estimation of Compute Cost in [Str19]: 8 P100s for batch size 32,768 (packed version) from [Vas17] (4096 per GPU): num_chips = 8 The Training speed of Transformer Big on P100 from [Vas17]: hours_per_train_steps = 84 hours / 300,000 = 0.00028 (Section 5.2 in [Vas17]) CO2e = 8 * 979,000,000 * 0.00028 * 0.2855296 = 626,155 lbs (284 t) Estimation of Compute Cost if using GPUs of the Actual Setting Adopted in [So19]: 1 P100 for batch size 32,768 / 8=4096 (Section 4.1 second paragraph in [So19]). num_chips = 1 (Section 4.3 in [So19], note that the actual search used one TPU v2 chip to fit the same batch size as one P100) Training speed of Transformer Base on P100 from [Vas17]: hours_per_train_steps = 12 hours / 100,000 = 0.00012 (Section 5.2 in [Vas17]) CO2e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 33,544 lbs (15.2 t) Appendix D shows a ~5X further reduction in CO2e by adjusting for the hardware and datacenter where the NAS occurred rather than for P100s in a hypothetical US average datacenter.", "doc_id": "patterson2021", "page": 21, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "As a result, the calculations below suggest that CO2e from the misunderstanding about the use of the smaller proxy task were overestimated by a factor of ~18.7: Assume the Carbon Emission Estimation Method in [Str19]: CO2e = num_chips x num_train_steps x hours/train_steps x emission/chip_per_hour num_train_steps = 979,000,000 # From [So19] emission_per_chip_per_hour ~= 0.2855296 pounds CO2e # From [Str19] Table 334. Estimation of Compute Cost in [Str19]: 8 P100s for batch size 32,768 (packed version) from [Vas17] (4096 per GPU): num_chips = 8 The Training speed of Transformer Big on P100 from [Vas17]: hours_per_train_steps = 84 hours / 300,000 = 0.00028 (Section 5.2 in [Vas17]) CO2e = 8 * 979,000,000 * 0.00028 * 0.2855296 = 626,155 lbs (284 t) Estimation of Compute Cost if using GPUs of the Actual Setting Adopted in [So19]: 1 P100 for batch size 32,768 / 8=4096 (Section 4.1 second paragraph in [So19]). num_chips = 1 (Section 4.3 in [So19], note that the actual search used one TPU v2 chip to fit the same batch size as one P100) Training speed of Transformer Base on P100 from [Vas17]: hours_per_train_steps = 12 hours / 100,000 = 0.00012 (Section 5.2 in [Vas17]) CO2e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 33,544 lbs (15.2 t) Appendix D shows a ~5X further reduction in CO2e by adjusting for the hardware and datacenter where the NAS occurred rather than for P100s in a hypothetical US average datacenter.", "original_types": ["text"], "id": 78}
{"type": "section", "content": "Appendix D. Details of a CO2e Estimate for Google’s Actual NAS\n\nTo calculate the emissions of the actual NAS in [So19] at Google, where the search was actually performed, we must adjust by three more factors beyond the assumptions in Appendix C:\n\n1. We use Google Georgia datacenter’s PUE from the period in which the search computation was run (1.10 in Table 4) instead of the US average in 2018 (1.58).\n\n2. Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018. For Google, we use the Georgia datacenter’s average CO2e/KWh for the month when NAS was performed (0.431 CO2e/KWh in Table 4).\n\n3. So et al. used Google TPU v2 accelerators, not NVIDIA P100 GPUs as modeled in [Str19]. TPU v2s are much faster, so the search process takes 32,633 TPU v2 hours instead of 117,780 P100 hours. We measured the power when running the [So19] NAS computation on TPU v2, including the memory, fans, network interfaces, and the CPU host. The average power was 208 Watts. [Str19] estimated the power per P100 as 189 Watts35. The performance/Watt for NAS of TPU v2 improved ( 117,780 / 32,633 ) * ( 189 / 208 ) or 3.3X.\n\nOur estimate of the actual NAS search that So et al. ran at Google after adjusting for the correct datacenter PUE, CO2e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO2e (7096 lbs).36 This actual emissions value is 88X smaller than the incorrect estimate of the carbon emissions of this search found in Strubell et al. If we reran the NAS search today on TPU v2s in Google’s Iowa datacenter with 24/7 local, real time net CO2e reduction instead of Google’s Georgia datacenter, it would drop from 3.2 tCO2e to 0.6 tCO2e (476X smaller). If we reran using newer TPUs, tCO2e would shrink further.\n\nWhen, where, how, and on which hardware training occurs matters in addition to what DNN is trained, which is why it’s best to include energy consumed and CO2e in a publication rather than relying on others to estimate it correctly afterwards.", "doc_id": "patterson2021", "page": 22, "url": "https://arxiv.org/pdf/2104.10350", "embedded_text": "Appendix D. Details of a CO2e Estimate for Google’s Actual NAS\n\nTo calculate the emissions of the actual NAS in [So19] at Google, where the search was actually performed, we must adjust by three more factors beyond the assumptions in Appendix C:\n\n1. We use Google Georgia datacenter’s PUE from the period in which the search computation was run (1.10 in Table 4) instead of the US average in 2018 (1.58).\n\n2. Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018. For Google, we use the Georgia datacenter’s average CO2e/KWh for the month when NAS was performed (0.431 CO2e/KWh in Table 4).\n\n3. So et al. used Google TPU v2 accelerators, not NVIDIA P100 GPUs as modeled in [Str19]. TPU v2s are much faster, so the search process takes 32,633 TPU v2 hours instead of 117,780 P100 hours. We measured the power when running the [So19] NAS computation on TPU v2, including the memory, fans, network interfaces, and the CPU host. The average power was 208 Watts. [Str19] estimated the power per P100 as 189 Watts35. The performance/Watt for NAS of TPU v2 improved ( 117,780 / 32,633 ) * ( 189 / 208 ) or 3.3X.\n\nOur estimate of the actual NAS search that So et al. ran at Google after adjusting for the correct datacenter PUE, CO2e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO2e (7096 lbs).36 This actual emissions value is 88X smaller than the incorrect estimate of the carbon emissions of this search found in Strubell et al. If we reran the NAS search today on TPU v2s in Google’s Iowa datacenter with 24/7 local, real time net CO2e reduction instead of Google’s Georgia datacenter, it would drop from 3.2 tCO2e to 0.6 tCO2e (476X smaller). If we reran using newer TPUs, tCO2e would shrink further.\n\nWhen, where, how, and on which hardware training occurs matters in addition to what DNN is trained, which is why it’s best to include energy consumed and CO2e in a publication rather than relying on others to estimate it correctly afterwards.", "original_types": ["text", "header"], "id": 79}
{"type": "section", "content": "PREFACE\n\nThe One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society. It considers the science, engineering, and deployment of AI-enabled computing systems. As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. The Study Panel reviews AI’s progress in the years following the immediately prior report, envisions the potential advances that lie ahead, and describes the technical and societal challenges and opportunities these advances raise, including in such arenas as ethics, economics, and the design of systems compatible with human cognition. The overarching purpose of the One Hundred Year Study’s periodic expert review is to provide a collected and connected set of reflections about AI and its influences as the field advances. The studies are expected to develop syntheses and assessments that provide expert-informed guidance for directions in AI research, development, and systems design, as well as programs and policies to help ensure that these systems broadly benefit individuals and society.\n\nThe One Hundred Year Study is modeled on an earlier effort informally known as the “AAAI Asilomar Study.” During 2008-2009, the then president of the Association for the Advancement of Artificial Intelligence (AAAI), Eric Horvitz, assembled a group of AI experts from multiple institutions and areas of the field, along with scholars of cognitive science, philosophy, and law. Working in distributed subgroups, the participants addressed near-term AI developments, long-term possibilities, and legal and ethical concerns, and then came together in a three-day meeting at Asilomar to share and discuss their findings. A short written report on the intensive meeting discussions, amplified by the participants’ subsequent discussions with other colleagues, generated widespread interest and debate in the field and beyond.\n\nThe impact of the Asilomar meeting, and important advances in AI that included AI algorithms and technologies starting to enter daily life around the globe, spurred the idea of a long-term recurring study of AI and its influence on people and society. The One Hundred Year Study was subsequently endowed at a university to enable", "doc_id": "stone2022", "page": 1, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "PREFACE\n\nThe One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society. It considers the science, engineering, and deployment of AI-enabled computing systems. As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI. The Study Panel reviews AI’s progress in the years following the immediately prior report, envisions the potential advances that lie ahead, and describes the technical and societal challenges and opportunities these advances raise, including in such arenas as ethics, economics, and the design of systems compatible with human cognition. The overarching purpose of the One Hundred Year Study’s periodic expert review is to provide a collected and connected set of reflections about AI and its influences as the field advances. The studies are expected to develop syntheses and assessments that provide expert-informed guidance for directions in AI research, development, and systems design, as well as programs and policies to help ensure that these systems broadly benefit individuals and society.\n\nThe One Hundred Year Study is modeled on an earlier effort informally known as the “AAAI Asilomar Study.” During 2008-2009, the then president of the Association for the Advancement of Artificial Intelligence (AAAI), Eric Horvitz, assembled a group of AI experts from multiple institutions and areas of the field, along with scholars of cognitive science, philosophy, and law. Working in distributed subgroups, the participants addressed near-term AI developments, long-term possibilities, and legal and ethical concerns, and then came together in a three-day meeting at Asilomar to share and discuss their findings. A short written report on the intensive meeting discussions, amplified by the participants’ subsequent discussions with other colleagues, generated widespread interest and debate in the field and beyond.\n\nThe impact of the Asilomar meeting, and important advances in AI that included AI algorithms and technologies starting to enter daily life around the globe, spurred the idea of a long-term recurring study of AI and its influence on people and society. The One Hundred Year Study was subsequently endowed at a university to enable", "original_types": ["text", "header"], "id": 80}
{"type": "section", "content": "Preface\n\nextended deep thought and cross-disciplinary scholarly investigations that could inspire innovation and provide intelligent advice to government agencies and industry.\n\nExecutive Summary\n\nThis report is the first in the planned series of studies that will continue for at least a hundred years. The Standing Committee defined a Study Panel charge for the inaugural Study Panel in the summer of 2015 and recruited Professor Peter Stone, at the University of Texas at Austin, to chair the panel. The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015. The participants represent diverse specialties and geographic regions, genders, and career stages.\n\nOverview\n\nThe Standing Committee extensively discussed ways to frame the Study Panel charge to consider both recent advances in AI and potential societal impacts on jobs, the environment, transportation, public safety, healthcare, community engagement, and government. The committee considered various ways to focus the study, including surveying subfields and their status, examining a particular technology such as machine learning or natural language processing, and studying particular application areas such as healthcare or transportation. The committee ultimately chose a thematic focus on “AI and Life in 2030” to recognize that AI’s various uses and impacts will not occur independently of one another, or of a multitude of other societal and technological developments. Acknowledging the central role cities have played throughout most of human experience, the focus was narrowed to the large urban areas where most people live. The Standing Committee further narrowed the focus to a typical North American city in recognition of the great variability of urban settings and cultures around the world, and limits on the first Study Panel’s efforts. The Standing Committee expects that the projections, assessments, and proactive guidance stemming from the study will have broader global relevance and is making plans for future studies to expand the scope of the project internationally.\n\nTable of Contents", "doc_id": "stone2022", "page": 2, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Preface\n\nextended deep thought and cross-disciplinary scholarly investigations that could inspire innovation and provide intelligent advice to government agencies and industry.\n\nExecutive Summary\n\nThis report is the first in the planned series of studies that will continue for at least a hundred years. The Standing Committee defined a Study Panel charge for the inaugural Study Panel in the summer of 2015 and recruited Professor Peter Stone, at the University of Texas at Austin, to chair the panel. The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015. The participants represent diverse specialties and geographic regions, genders, and career stages.\n\nOverview\n\nThe Standing Committee extensively discussed ways to frame the Study Panel charge to consider both recent advances in AI and potential societal impacts on jobs, the environment, transportation, public safety, healthcare, community engagement, and government. The committee considered various ways to focus the study, including surveying subfields and their status, examining a particular technology such as machine learning or natural language processing, and studying particular application areas such as healthcare or transportation. The committee ultimately chose a thematic focus on “AI and Life in 2030” to recognize that AI’s various uses and impacts will not occur independently of one another, or of a multitude of other societal and technological developments. Acknowledging the central role cities have played throughout most of human experience, the focus was narrowed to the large urban areas where most people live. The Standing Committee further narrowed the focus to a typical North American city in recognition of the great variability of urban settings and cultures around the world, and limits on the first Study Panel’s efforts. The Standing Committee expects that the projections, assessments, and proactive guidance stemming from the study will have broader global relevance and is making plans for future studies to expand the scope of the project internationally.\n\nTable of Contents", "original_types": ["text", "header"], "id": 81}
{"type": "table", "content": "Table of Contents", "doc_id": "stone2022", "page": 2, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Table of Contents", "id": 82}
{"type": "section", "content": "PREFACE\nEXECUTIVE SUMMARY\nOVERVIEW\nSECTION I: WHAT IS ARTIFICIAL INTELLIGENCE?\nDefining AI\nAI Research Trends\n\nSECTION II: AI BY DOMAIN\nTransportation\nHome/Service Robots\nHealthcare\nEducation\nLow-resource Communities\nPublic Safety and Security\nEmployment and Workplace\nEntertainment\n\nSECTION III: PROSPECTS AND RECOMMENDATIONS FOR AI PUBLIC POLICY\nAI Policy, Now and in the Future\n\nAPPENDIX I: A SHORT HISTORY OF AI\n", "doc_id": "stone2022", "page": 2, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "PREFACE\nEXECUTIVE SUMMARY\nOVERVIEW\nSECTION I: WHAT IS ARTIFICIAL INTELLIGENCE?\nDefining AI\nAI Research Trends\n\nSECTION II: AI BY DOMAIN\nTransportation\nHome/Service Robots\nHealthcare\nEducation\nLow-resource Communities\nPublic Safety and Security\nEmployment and Workplace\nEntertainment\n\nSECTION III: PROSPECTS AND RECOMMENDATIONS FOR AI PUBLIC POLICY\nAI Policy, Now and in the Future\n\nAPPENDIX I: A SHORT HISTORY OF AI\n", "original_types": ["text"], "id": 83}
{"type": "section", "content": "As one consequence of the decision to focus on life in North American cities, military applications were deemed to be outside the scope of this initial report. This is not to minimize the importance of careful monitoring and deliberation about the implications of AI advances for defense and warfare, including potentially destabilizing developments and deployments.\n\nThe report is designed to address four intended audiences. For the general public, it aims to provide an accessible, scientifically and technologically accurate portrayal of the current state of AI and its potential. For industry, the report describes relevant technologies and legal and ethical challenges, and may help guide resource allocation. The report is also directed to local, national, and international governments to help them better plan for AI in governance. Finally, the report can help AI researchers, as well as their institutions and funders, to set priorities and consider the ethical and legal issues raised by AI research and its applications.\n\nGiven the unique nature of the One Hundred Year Study on AI, we expect that future generations of Standing Committees and Study Panels, as well as research scientists, policy experts, leaders in the private and public sectors, and the general public, will reflect on this assessment as they make new assessments of AI’s future. We hope that this first effort in the series stretching out before us will be useful for both its failures and successes in accurately predicting the trajectory and influences of AI.\n\nThe Standing Committee is grateful to the members of the Study Panel for investing their expertise, perspectives, and significant time to the creation of this inaugural report. We especially thank Professor Peter Stone for agreeing to serve as chair of the study and for his wise, skillful, and dedicated leadership of the panel, its discussions, and creation of the report.", "doc_id": "stone2022", "page": 3, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "As one consequence of the decision to focus on life in North American cities, military applications were deemed to be outside the scope of this initial report. This is not to minimize the importance of careful monitoring and deliberation about the implications of AI advances for defense and warfare, including potentially destabilizing developments and deployments.\n\nThe report is designed to address four intended audiences. For the general public, it aims to provide an accessible, scientifically and technologically accurate portrayal of the current state of AI and its potential. For industry, the report describes relevant technologies and legal and ethical challenges, and may help guide resource allocation. The report is also directed to local, national, and international governments to help them better plan for AI in governance. Finally, the report can help AI researchers, as well as their institutions and funders, to set priorities and consider the ethical and legal issues raised by AI research and its applications.\n\nGiven the unique nature of the One Hundred Year Study on AI, we expect that future generations of Standing Committees and Study Panels, as well as research scientists, policy experts, leaders in the private and public sectors, and the general public, will reflect on this assessment as they make new assessments of AI’s future. We hope that this first effort in the series stretching out before us will be useful for both its failures and successes in accurately predicting the trajectory and influences of AI.\n\nThe Standing Committee is grateful to the members of the Study Panel for investing their expertise, perspectives, and significant time to the creation of this inaugural report. We especially thank Professor Peter Stone for agreeing to serve as chair of the study and for his wise, skillful, and dedicated leadership of the panel, its discussions, and creation of the report.", "original_types": ["text"], "id": 84}
{"type": "table", "content": "Standing Committee of the One Hundred Year Study of Artificial Intelligence\nBarbara J. Grosz, Chair\nRuss Altman\nEric Horvitz\nAlan Mackworth\nTom Mitchell\nDeirdre Mulligan\nYoav Shoham", "doc_id": "stone2022", "page": 3, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Standing Committee of the One Hundred Year Study of Artificial Intelligence\nBarbara J. Grosz, Chair\nRuss Altman\nEric Horvitz\nAlan Mackworth\nTom Mitchell\nDeirdre Mulligan\nYoav Shoham", "id": 85}
{"type": "section", "content": "STUDY PANEL\n\nPeter Stone, University of Texas at Austin, Chair\nRodney Brooks, Rethink Robotics\nErik Brynjolfsson, Massachusetts Institute of Technology\nRyan Calo, University of Washington\nOren Etzioni, Allen Institute for AI\nGreg Hager, Johns Hopkins University\nJulia Hirschberg, Columbia University\nShivaram Kalyanakrishnan, Indian Institute of Technology Bombay\nEce Kamar, Microsoft Research\nSarit Kraus, Bar Ilan University\nKevin Leyton-Brown, University of British Columbia\nDavid Parkes, Harvard University\nWilliam Press, University of Texas at Austin\nAnnaLee (Anno) Saxenian, University of California, Berkeley\nJulie Shah, Massachusetts Institute of Technology\nMilind Tambe, University of Southern California\nAstro Teller, X\n\nAcknowledgments: The members of the Study Panel gratefully acknowledge the support of and valuable input from the Standing Committee, especially the chair, Barbara Grosz, who handled with supreme grace the unenviable role of mediating between two large, very passionate committees. We also thank Kerry Tremain for his tireless and insightful input on the written product during the extensive editing and polishing process, which unquestionably strengthened the report considerably.", "doc_id": "stone2022", "page": 3, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "STUDY PANEL\n\nPeter Stone, University of Texas at Austin, Chair\nRodney Brooks, Rethink Robotics\nErik Brynjolfsson, Massachusetts Institute of Technology\nRyan Calo, University of Washington\nOren Etzioni, Allen Institute for AI\nGreg Hager, Johns Hopkins University\nJulia Hirschberg, Columbia University\nShivaram Kalyanakrishnan, Indian Institute of Technology Bombay\nEce Kamar, Microsoft Research\nSarit Kraus, Bar Ilan University\nKevin Leyton-Brown, University of British Columbia\nDavid Parkes, Harvard University\nWilliam Press, University of Texas at Austin\nAnnaLee (Anno) Saxenian, University of California, Berkeley\nJulie Shah, Massachusetts Institute of Technology\nMilind Tambe, University of Southern California\nAstro Teller, X\n\nAcknowledgments: The members of the Study Panel gratefully acknowledge the support of and valuable input from the Standing Committee, especially the chair, Barbara Grosz, who handled with supreme grace the unenviable role of mediating between two large, very passionate committees. We also thank Kerry Tremain for his tireless and insightful input on the written product during the extensive editing and polishing process, which unquestionably strengthened the report considerably.", "original_types": ["text"], "id": 86}
{"type": "section", "content": "EXECUTIVE SUMMARY", "doc_id": "stone2022", "page": 4, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "EXECUTIVE SUMMARY", "original_types": ["header"], "id": 87}
{"type": "section", "content": "Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason, and take action. While the rate of progress in AI has been patchy and unpredictable, there have been significant advances since the field’s inception sixty years ago. Once a mostly academic area of study, twenty-first century AI enables a constellation of mainstream technologies that are having a substantial impact on everyday lives. Computer vision and AI planning, for example, drive the video games that are now a bigger entertainment industry than Hollywood. Deep learning, a form of machine learning based on layered representations of variables referred to as neural networks, has made speech-understanding practical on our phones and in our kitchens, and its algorithms can be applied widely to an array of applications that rely on pattern recognition. Natural Language Processing (NLP) and knowledge representation and reasoning have enabled a machine to beat the Jeopardy champion and are bringing new power to Web searches. While impressive, these technologies are highly tailored to particular tasks. Each application typically requires years of specialized research and careful, unique construction. In similarly targeted applications, substantial increases in the future uses of AI technologies, including more self-driving cars, healthcare diagnostics and targeted treatments, and physical assistance for elder care can be expected. AI and robotics will also be applied across the globe in industries struggling to attract younger workers, such as agriculture, food processing, fulfillment centers, and factories. They will facilitate delivery of online purchases through flying drones, self-driving trucks, or robots that can get up the stairs to the front door. This report is the first in a series to be issued at regular intervals as a part of the One Hundred Year Study on Artificial Intelligence (AI100). Starting from a charge given by the AI100 Standing Committee to consider the likely influences of AI in a typical North American city by the year 2030, the 2015 Study Panel, comprising experts in AI and other relevant areas focused their attention on eight domains they considered most salient: transportation; service robots; healthcare; education; low-resource communities; public safety and security; employment and workplace; and entertainment. In each of these domains, the report both reflects on progress in the past fifteen years and anticipates developments in the coming fifteen years. Though drawing from a common source of research, each domain reflects different AI influences and challenges, such as the difficulty of creating safe and reliable hardware (transportation and service robots), the difficulty of smoothly interacting with human experts (healthcare and education), the challenge of gaining public trust (low-resource communities and public safety and security), the challenge of overcoming fears of marginalizing humans (employment and workplace), and the social and societal risk of diminishing interpersonal interactions (entertainment). The report begins with a reflection on what constitutes Artificial Intelligence, and concludes with recommendations concerning AI-related policy. These recommendations include accruing technical expertise about AI in government and devoting more resources—and removing impediments—to research on the fairness, security, privacy, and societal impacts of AI systems. Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future. Instead, increasingly useful applications of AI, with potentially profound positive impacts on our society and economy are likely to emerge between now and 2030, the period this report considers. At the same time, many of these developments will spur disruptions in", "doc_id": "stone2022", "page": 4, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason, and take action. While the rate of progress in AI has been patchy and unpredictable, there have been significant advances since the field’s inception sixty years ago. Once a mostly academic area of study, twenty-first century AI enables a constellation of mainstream technologies that are having a substantial impact on everyday lives. Computer vision and AI planning, for example, drive the video games that are now a bigger entertainment industry than Hollywood. Deep learning, a form of machine learning based on layered representations of variables referred to as neural networks, has made speech-understanding practical on our phones and in our kitchens, and its algorithms can be applied widely to an array of applications that rely on pattern recognition. Natural Language Processing (NLP) and knowledge representation and reasoning have enabled a machine to beat the Jeopardy champion and are bringing new power to Web searches. While impressive, these technologies are highly tailored to particular tasks. Each application typically requires years of specialized research and careful, unique construction. In similarly targeted applications, substantial increases in the future uses of AI technologies, including more self-driving cars, healthcare diagnostics and targeted treatments, and physical assistance for elder care can be expected. AI and robotics will also be applied across the globe in industries struggling to attract younger workers, such as agriculture, food processing, fulfillment centers, and factories. They will facilitate delivery of online purchases through flying drones, self-driving trucks, or robots that can get up the stairs to the front door. This report is the first in a series to be issued at regular intervals as a part of the One Hundred Year Study on Artificial Intelligence (AI100). Starting from a charge given by the AI100 Standing Committee to consider the likely influences of AI in a typical North American city by the year 2030, the 2015 Study Panel, comprising experts in AI and other relevant areas focused their attention on eight domains they considered most salient: transportation; service robots; healthcare; education; low-resource communities; public safety and security; employment and workplace; and entertainment. In each of these domains, the report both reflects on progress in the past fifteen years and anticipates developments in the coming fifteen years. Though drawing from a common source of research, each domain reflects different AI influences and challenges, such as the difficulty of creating safe and reliable hardware (transportation and service robots), the difficulty of smoothly interacting with human experts (healthcare and education), the challenge of gaining public trust (low-resource communities and public safety and security), the challenge of overcoming fears of marginalizing humans (employment and workplace), and the social and societal risk of diminishing interpersonal interactions (entertainment). The report begins with a reflection on what constitutes Artificial Intelligence, and concludes with recommendations concerning AI-related policy. These recommendations include accruing technical expertise about AI in government and devoting more resources—and removing impediments—to research on the fairness, security, privacy, and societal impacts of AI systems. Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future. Instead, increasingly useful applications of AI, with potentially profound positive impacts on our society and economy are likely to emerge between now and 2030, the period this report considers. At the same time, many of these developments will spur disruptions in", "id": 88}
{"type": "section", "content": "4\n\nhow human labor is augmented or replaced by AI, creating new challenges for the economy and society more broadly. Application design and policy decisions made in the near term are likely to have long-lasting influences on the nature and directions of such developments, making it important for AI researchers, developers, social scientists, and policymakers to balance the imperative to innovate with mechanisms to ensure that AI’s economic and social benefits are broadly shared across society. If society approaches these technologies primarily with fear and suspicion, missteps that slow AI’s development or drive it underground will result, impeding important work on ensuring the safety and reliability of AI technologies. On the other hand, if society approaches AI with a more open mind, the technologies emerging from the field could profoundly transform society for the better in the coming decades.\n\nStudy Panel: Peter Stone, Chair, University of Texas at Austin, Rodney Brooks, Rethink Robotics, Erik Brynjolfsson, Massachusetts Institute of Technology, Ryan Calo, University of Washington, Oren Etzioni, Allen Institute for AI, Greg Hager, Johns Hopkins University, Julia Hirschberg, Columbia University, Shivaram Kalyanakrishnan, Indian Institute of Technology Bombay, Ece Kamar, Microsoft Research, Sarit Kraus, Bar Ilan University. Kevin Leyton-Brown, University of British Columbia, David Parkes, Harvard University, William Press, University of Texas at Austin, AnnaLee (Anno) Saxenian, University of California, Berkeley, Julie Shah, Massachusetts Institute of Technology, Milind Tambe, University of Southern California, Astro Teller, X\n\nStanding Committee of the One Hundred Year Study of Artificial Intelligence: Barbara J. Grosz, Chair, Russ Altman, Eric Horvitz, Alan Mackworth, Tom Mitchell, Deidre Mulligan, Yoav Shoham", "doc_id": "stone2022", "page": "4-5", "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "4\n\nhow human labor is augmented or replaced by AI, creating new challenges for the economy and society more broadly. Application design and policy decisions made in the near term are likely to have long-lasting influences on the nature and directions of such developments, making it important for AI researchers, developers, social scientists, and policymakers to balance the imperative to innovate with mechanisms to ensure that AI’s economic and social benefits are broadly shared across society. If society approaches these technologies primarily with fear and suspicion, missteps that slow AI’s development or drive it underground will result, impeding important work on ensuring the safety and reliability of AI technologies. On the other hand, if society approaches AI with a more open mind, the technologies emerging from the field could profoundly transform society for the better in the coming decades.\n\nStudy Panel: Peter Stone, Chair, University of Texas at Austin, Rodney Brooks, Rethink Robotics, Erik Brynjolfsson, Massachusetts Institute of Technology, Ryan Calo, University of Washington, Oren Etzioni, Allen Institute for AI, Greg Hager, Johns Hopkins University, Julia Hirschberg, Columbia University, Shivaram Kalyanakrishnan, Indian Institute of Technology Bombay, Ece Kamar, Microsoft Research, Sarit Kraus, Bar Ilan University. Kevin Leyton-Brown, University of British Columbia, David Parkes, Harvard University, William Press, University of Texas at Austin, AnnaLee (Anno) Saxenian, University of California, Berkeley, Julie Shah, Massachusetts Institute of Technology, Milind Tambe, University of Southern California, Astro Teller, X\n\nStanding Committee of the One Hundred Year Study of Artificial Intelligence: Barbara J. Grosz, Chair, Russ Altman, Eric Horvitz, Alan Mackworth, Tom Mitchell, Deidre Mulligan, Yoav Shoham", "original_types": ["text"], "id": 89}
{"type": "section", "content": "OVERVIEW", "doc_id": "stone2022", "page": 6, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "OVERVIEW", "original_types": ["header"], "id": 90}
{"type": "section", "content": "The frightening, futurist portrayals of Artificial Intelligence that dominate films and novels, and shape the popular imagination, are fictional. In reality, AI is already changing our daily lives, almost entirely in ways that improve human health, safety, and productivity. Unlike in the movies, there is no race of superhuman robots on the horizon or probably even possible. And while the potential to abuse AI technologies must be acknowledged and addressed, their greater potential is, among other things, to make driving safer, help children learn, and extend and enhance people’s lives. In fact, beneficial AI applications in schools, homes, and hospitals are already growing at an accelerated pace. Major research universities devote departments to AI studies, and technology companies such as Apple, Facebook, Google, IBM, and Microsoft spend heavily to explore AI applications they regard as critical to their futures. Even Hollywood uses AI technologies to bring its dystopian AI fantasies to the screen. Innovations relying on computer-based vision, speech recognition, and Natural Language Processing have driven these changes, as have concurrent scientific and technological advances in related fields. AI is also changing how people interact with technology. Many people have already grown accustomed to touching and talking to their smart phones. People’s future relationships with machines will become ever more nuanced, fluid, and personalized as AI systems learn to adapt to individual personalities and goals. These AI applications will help monitor people’s well-being, alert them to risks ahead, and deliver services when needed or wanted. For example, in a mere fifteen years in a typical North American city—the time frame and scope of this report—AI applications are likely to transform transportation toward self-driving vehicles with on-time pickup and delivery of people and packages. This alone will reconfigure the urban landscape, as traffic jams and parking challenges become obsolete. This study’s focus on a typical North American city is deliberate and meant to highlight specific changes affecting the everyday lives of the millions of people who inhabit them. The Study Panel further narrowed its inquiry to eight domains where AI is already having or is projected to have the greatest impact: transportation, healthcare, education, low-resource communities, public safety and security, employment and workplace, home/service robots, and entertainment. Though drawing from a common source of research, AI technologies have influenced and will continue to influence these domains differently. Each domain faces varied AI-related challenges, including the difficulty of creating safe and reliable hardware for sensing and effecting (transportation and service robots), the difficulty of smoothly interacting with human experts (healthcare and education), the challenge of gaining public trust (low-resource communities and public safety and security), the challenge of overcoming fears of marginalizing humans (employment and workplace) and the risk of diminishing interpersonal interaction (entertainment). Some domains are primarily business sectors, such as transportation and healthcare, while others are more oriented to consumers, such as entertainment and home service robots. Some cut across sectors, such as employment/workplace and low-resource communities. In each domain, even as AI continues to deliver important benefits, it also raises important ethical and social issues, including privacy concerns. Robots and other AI technologies have already begun to displace jobs in some sectors. As a society, we are now at a crucial juncture in determining how to deploy AI-based technologies in ways that promote, not hinder, democratic values such as freedom, equality, and transparency. For individuals, the quality of the lives we lead and how our contributions are valued are likely to shift gradually, but markedly. Over the next several years, AI research, systems development, and social and regulatory frameworks will shape how the benefits of AI are weighed against its costs and risks, and how broadly these benefits are spread.", "doc_id": "stone2022", "page": 6, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "The frightening, futurist portrayals of Artificial Intelligence that dominate films and novels, and shape the popular imagination, are fictional. In reality, AI is already changing our daily lives, almost entirely in ways that improve human health, safety, and productivity. Unlike in the movies, there is no race of superhuman robots on the horizon or probably even possible. And while the potential to abuse AI technologies must be acknowledged and addressed, their greater potential is, among other things, to make driving safer, help children learn, and extend and enhance people’s lives. In fact, beneficial AI applications in schools, homes, and hospitals are already growing at an accelerated pace. Major research universities devote departments to AI studies, and technology companies such as Apple, Facebook, Google, IBM, and Microsoft spend heavily to explore AI applications they regard as critical to their futures. Even Hollywood uses AI technologies to bring its dystopian AI fantasies to the screen. Innovations relying on computer-based vision, speech recognition, and Natural Language Processing have driven these changes, as have concurrent scientific and technological advances in related fields. AI is also changing how people interact with technology. Many people have already grown accustomed to touching and talking to their smart phones. People’s future relationships with machines will become ever more nuanced, fluid, and personalized as AI systems learn to adapt to individual personalities and goals. These AI applications will help monitor people’s well-being, alert them to risks ahead, and deliver services when needed or wanted. For example, in a mere fifteen years in a typical North American city—the time frame and scope of this report—AI applications are likely to transform transportation toward self-driving vehicles with on-time pickup and delivery of people and packages. This alone will reconfigure the urban landscape, as traffic jams and parking challenges become obsolete. This study’s focus on a typical North American city is deliberate and meant to highlight specific changes affecting the everyday lives of the millions of people who inhabit them. The Study Panel further narrowed its inquiry to eight domains where AI is already having or is projected to have the greatest impact: transportation, healthcare, education, low-resource communities, public safety and security, employment and workplace, home/service robots, and entertainment. Though drawing from a common source of research, AI technologies have influenced and will continue to influence these domains differently. Each domain faces varied AI-related challenges, including the difficulty of creating safe and reliable hardware for sensing and effecting (transportation and service robots), the difficulty of smoothly interacting with human experts (healthcare and education), the challenge of gaining public trust (low-resource communities and public safety and security), the challenge of overcoming fears of marginalizing humans (employment and workplace) and the risk of diminishing interpersonal interaction (entertainment). Some domains are primarily business sectors, such as transportation and healthcare, while others are more oriented to consumers, such as entertainment and home service robots. Some cut across sectors, such as employment/workplace and low-resource communities. In each domain, even as AI continues to deliver important benefits, it also raises important ethical and social issues, including privacy concerns. Robots and other AI technologies have already begun to displace jobs in some sectors. As a society, we are now at a crucial juncture in determining how to deploy AI-based technologies in ways that promote, not hinder, democratic values such as freedom, equality, and transparency. For individuals, the quality of the lives we lead and how our contributions are valued are likely to shift gradually, but markedly. Over the next several years, AI research, systems development, and social and regulatory frameworks will shape how the benefits of AI are weighed against its costs and risks, and how broadly these benefits are spread.", "id": 91}
{"type": "section", "content": "An accurate and sophisticated picture of AI—one that competes with its popular portrayal—is hampered at the start by the difficulty of pinning down a precise definition of artificial intelligence. In the approaches the Study Panel considered, none suggest there is currently a “general purpose” AI. While drawing on common research and technologies, AI systems are specialized to accomplish particular tasks, and each application requires years of focused research and a careful, unique construction. As a result, progress is uneven within and among the eight domains.\n\nA prime example is Transportation, where a few key technologies have catalyzed the widespread adoption of AI with astonishing speed. Autonomous transportation will soon be commonplace and, as most people’s first experience with physically embodied AI systems, will strongly influence the public’s perception of AI. As cars become better drivers than people, city-dwellers will own fewer cars, live further from work, and spend time differently, leading to an entirely new urban organization. In the typical North American city in 2030, physically embodied AI applications will not be limited to cars, but are likely to include trucks, flying vehicles, and personal robots. Improvements in safe and reliable hardware will spur innovation over the next fifteen years, as they will with Home/Service Robots, which have already entered people’s houses, primarily in the form of vacuum cleaners. Better chips, low-cost 3D sensors, cloud-based machine learning, and advances in speech understanding will enhance future robots’ services and their interactions with people. Special purpose robots will deliver packages, clean offices, and enhance security. But technical constraints and the high costs of reliable mechanical devices will continue to limit commercial opportunities to narrowly defined applications for the foreseeable future.\n\nIn Healthcare, there has been an immense forward leap in collecting useful data from personal monitoring devices and mobile apps, from electronic health records (EHR) in clinical settings and, to a lesser extent, from surgical robots designed to assist with medical procedures and service robots supporting hospital operations. AI-based applications could improve health outcomes and the quality of life for millions of people in the coming years. Though clinical applications have been slow to move from the computer science lab to the real-world, there are hopeful signs that the pace of innovation will improve. Advances in healthcare can be promoted via the development of incentives and mechanisms for sharing data and for removing overbearing policy, regulatory, and commercial obstacles. For many applications, AI systems will have to work closely with care providers and patients to gain their trust. Advances in how intelligent machines interact naturally with caregivers, patients, and patients’ families are crucial.", "doc_id": "stone2022", "page": 7, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "An accurate and sophisticated picture of AI—one that competes with its popular portrayal—is hampered at the start by the difficulty of pinning down a precise definition of artificial intelligence. In the approaches the Study Panel considered, none suggest there is currently a “general purpose” AI. While drawing on common research and technologies, AI systems are specialized to accomplish particular tasks, and each application requires years of focused research and a careful, unique construction. As a result, progress is uneven within and among the eight domains.\n\nA prime example is Transportation, where a few key technologies have catalyzed the widespread adoption of AI with astonishing speed. Autonomous transportation will soon be commonplace and, as most people’s first experience with physically embodied AI systems, will strongly influence the public’s perception of AI. As cars become better drivers than people, city-dwellers will own fewer cars, live further from work, and spend time differently, leading to an entirely new urban organization. In the typical North American city in 2030, physically embodied AI applications will not be limited to cars, but are likely to include trucks, flying vehicles, and personal robots. Improvements in safe and reliable hardware will spur innovation over the next fifteen years, as they will with Home/Service Robots, which have already entered people’s houses, primarily in the form of vacuum cleaners. Better chips, low-cost 3D sensors, cloud-based machine learning, and advances in speech understanding will enhance future robots’ services and their interactions with people. Special purpose robots will deliver packages, clean offices, and enhance security. But technical constraints and the high costs of reliable mechanical devices will continue to limit commercial opportunities to narrowly defined applications for the foreseeable future.\n\nIn Healthcare, there has been an immense forward leap in collecting useful data from personal monitoring devices and mobile apps, from electronic health records (EHR) in clinical settings and, to a lesser extent, from surgical robots designed to assist with medical procedures and service robots supporting hospital operations. AI-based applications could improve health outcomes and the quality of life for millions of people in the coming years. Though clinical applications have been slow to move from the computer science lab to the real-world, there are hopeful signs that the pace of innovation will improve. Advances in healthcare can be promoted via the development of incentives and mechanisms for sharing data and for removing overbearing policy, regulatory, and commercial obstacles. For many applications, AI systems will have to work closely with care providers and patients to gain their trust. Advances in how intelligent machines interact naturally with caregivers, patients, and patients’ families are crucial.", "original_types": ["text"], "id": 92}
{"type": "section", "content": "Enabling more fluid interactions between people and promising AI technologies also remains a critical challenge in Education, which has seen considerable progress in the same period. Though quality education will always require active engagement by human teachers, AI promises to enhance education at all levels, especially by providing personalization at scale. Interactive machine tutors are now being matched to students for teaching science, math, language, and other disciplines. Natural Language Processing, machine learning, and crowdsourcing have boosted online learning and enabled teachers in higher education to multiply the size of their classrooms while addressing individual students’ learning needs and styles. Over the next fifteen years in a typical North American city, the use of these technologies in the classroom and in the home is likely to expand significantly, provided they can be meaningfully integrated with face-to-face learning.\n\nBeyond education, many opportunities exist for AI methods to assist Low-resource Communities by providing mitigations and solutions to a variety of social problems. Traditionally, funders have underinvested in AI research lacking commercial application. With targeted incentives and funding priorities,", "doc_id": "stone2022", "page": 7, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Enabling more fluid interactions between people and promising AI technologies also remains a critical challenge in Education, which has seen considerable progress in the same period. Though quality education will always require active engagement by human teachers, AI promises to enhance education at all levels, especially by providing personalization at scale. Interactive machine tutors are now being matched to students for teaching science, math, language, and other disciplines. Natural Language Processing, machine learning, and crowdsourcing have boosted online learning and enabled teachers in higher education to multiply the size of their classrooms while addressing individual students’ learning needs and styles. Over the next fifteen years in a typical North American city, the use of these technologies in the classroom and in the home is likely to expand significantly, provided they can be meaningfully integrated with face-to-face learning.\n\nBeyond education, many opportunities exist for AI methods to assist Low-resource Communities by providing mitigations and solutions to a variety of social problems. Traditionally, funders have underinvested in AI research lacking commercial application. With targeted incentives and funding priorities,", "original_types": ["text"], "id": 93}
{"type": "section", "content": "Longer term, AI may be thought of as a radically different mechanism for wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasures.", "doc_id": "stone2022", "page": 8, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Longer term, AI may be thought of as a radically different mechanism for wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasures.", "original_types": ["header"], "id": 94}
{"type": "section", "content": "AI technologies could help address the needs of low-resource communities, and budding efforts are promising. Using data mining and machine learning, for example, AI has been used to create predictive models to help government agencies address issues such as prevention of lead poisoning in at-risk children and distribution of food efficiently. These budding efforts suggest more could be done, particularly if agencies and organizations can engage and build trust with these communities. Gaining public trust is also a challenge for AI use by Public Safety and Security professionals. North American cities and federal agencies have already begun to deploy AI technologies in border administration and law enforcement. By 2030, they will rely heavily upon them, including improved cameras and drones for surveillance, algorithms to detect financial fraud, and predictive policing. The latter raises the specter of innocent people being unjustifiably monitored, and care must be taken to avoid systematizing human bias and to protect civil liberties. Well-deployed AI prediction tools have the potential to provide new kinds of transparency about data and inferences, and may be applied to detect, remove, or reduce human bias, rather than reinforcing it. Social and political decisions are likewise at play in AI’s influences on Employment and Workplace trends, such as the safety nets needed to protect people from structural changes in the economy. AI is poised to replace people in certain kinds of jobs, such as in the driving of taxis and trucks. However, in many realms, AI will likely replace tasks rather than jobs in the near term, and will also create new kinds of jobs. But the new jobs that will emerge are harder to imagine in advance than the existing jobs that will likely be lost. AI will also lower the cost of many goods and services, effectively making everyone better off. Longer term, AI may be thought of as a radically different mechanism for wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasures. It is not too soon for social debate on how the economic fruits of AI technologies should be shared. Entertainment has been transformed by social networks and other platforms for sharing and browsing blogs, videos, and photos, which rely on techniques actively developed in NLP, information retrieval, image processing, crowdsourcing, and machine learning. Some traditional sources of entertainment have also embraced AI to compose music, create stage performances, and even to generate 3D scenes from natural language text. The enthusiasm with which people have already responded to AI-driven entertainment has been surprising. As with many aspects of AI, there is ongoing debate about the extent to which the technology replaces or enhances sociability. AI will increasingly enable entertainment that is more interactive, personalized, and engaging. Research should be directed toward understanding how to leverage these attributes for individuals’ and society’s benefit.", "doc_id": "stone2022", "page": 8, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "AI technologies could help address the needs of low-resource communities, and budding efforts are promising. Using data mining and machine learning, for example, AI has been used to create predictive models to help government agencies address issues such as prevention of lead poisoning in at-risk children and distribution of food efficiently. These budding efforts suggest more could be done, particularly if agencies and organizations can engage and build trust with these communities. Gaining public trust is also a challenge for AI use by Public Safety and Security professionals. North American cities and federal agencies have already begun to deploy AI technologies in border administration and law enforcement. By 2030, they will rely heavily upon them, including improved cameras and drones for surveillance, algorithms to detect financial fraud, and predictive policing. The latter raises the specter of innocent people being unjustifiably monitored, and care must be taken to avoid systematizing human bias and to protect civil liberties. Well-deployed AI prediction tools have the potential to provide new kinds of transparency about data and inferences, and may be applied to detect, remove, or reduce human bias, rather than reinforcing it. Social and political decisions are likewise at play in AI’s influences on Employment and Workplace trends, such as the safety nets needed to protect people from structural changes in the economy. AI is poised to replace people in certain kinds of jobs, such as in the driving of taxis and trucks. However, in many realms, AI will likely replace tasks rather than jobs in the near term, and will also create new kinds of jobs. But the new jobs that will emerge are harder to imagine in advance than the existing jobs that will likely be lost. AI will also lower the cost of many goods and services, effectively making everyone better off. Longer term, AI may be thought of as a radically different mechanism for wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasures. It is not too soon for social debate on how the economic fruits of AI technologies should be shared. Entertainment has been transformed by social networks and other platforms for sharing and browsing blogs, videos, and photos, which rely on techniques actively developed in NLP, information retrieval, image processing, crowdsourcing, and machine learning. Some traditional sources of entertainment have also embraced AI to compose music, create stage performances, and even to generate 3D scenes from natural language text. The enthusiasm with which people have already responded to AI-driven entertainment has been surprising. As with many aspects of AI, there is ongoing debate about the extent to which the technology replaces or enhances sociability. AI will increasingly enable entertainment that is more interactive, personalized, and engaging. Research should be directed toward understanding how to leverage these attributes for individuals’ and society’s benefit.", "id": 95}
{"type": "section", "content": "What’s next for AI research?\n\nmarkets for data-driven products, and the economic incentives to find new products and markets, have also stimulated research advances. Now, as it becomes a central force in society, the field of AI is shifting toward building intelligent systems that can collaborate effectively with people, and that are more generally human-aware, including creative ways to develop interactive and scalable ways for people to teach robots. These trends drive the currently “hot” areas of AI research into both fundamental methods and application areas:\n\nLarge-scale machine learning concerns the design of learning algorithms, as well as scaling existing algorithms, to work with extremely large data sets.\n\nDeep learning, a class of learning procedures, has facilitated object recognition in images, video labeling, and activity recognition, and is making significant inroads into other areas of perception, such as audio, speech, and natural language processing.\n\nReinforcement learning is a framework that shifts the focus of machine learning from pattern recognition to experience-driven sequential decision-making. It promises to carry AI applications forward toward taking actions in the real world. While largely confined to academia over the past several decades, it is now seeing some practical, real-world successes.\n\nRobotics is currently concerned with how to train a robot to interact with the world around it in generalizable and predictable ways, how to facilitate manipulation of objects in interactive environments, and how to interact with people. Advances in robotics will rely on commensurate advances to improve the reliability and generality of computer vision and other forms of machine perception.\n\nComputer vision is currently the most prominent form of machine perception. It has been the sub-area of AI most transformed by the rise of deep learning. For the first time, computers are able to perform some vision tasks better than people. Much current research is focused on automatic image and video captioning.\n\nNatural Language Processing, often coupled with automatic speech recognition, is quickly becoming a commodity for widely spoken languages with large data sets. Research is now shifting to develop refined and capable systems that are able to interact with people through dialog, not just react to stylized requests. Great strides have also been made in machine translation among different languages, with more real-time person-to-person exchanges on the near horizon.\n\nCollaborative systems research investigates models and algorithms to help develop autonomous systems that can work collaboratively with other systems and with humans.\n\nCrowdsourcing and human computation research investigates methods to augment computer systems by making automated calls to human expertise to solve problems that computers alone cannot solve well.", "doc_id": "stone2022", "page": "8-9", "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "What’s next for AI research?\n\nmarkets for data-driven products, and the economic incentives to find new products and markets, have also stimulated research advances. Now, as it becomes a central force in society, the field of AI is shifting toward building intelligent systems that can collaborate effectively with people, and that are more generally human-aware, including creative ways to develop interactive and scalable ways for people to teach robots. These trends drive the currently “hot” areas of AI research into both fundamental methods and application areas:\n\nLarge-scale machine learning concerns the design of learning algorithms, as well as scaling existing algorithms, to work with extremely large data sets.\n\nDeep learning, a class of learning procedures, has facilitated object recognition in images, video labeling, and activity recognition, and is making significant inroads into other areas of perception, such as audio, speech, and natural language processing.\n\nReinforcement learning is a framework that shifts the focus of machine learning from pattern recognition to experience-driven sequential decision-making. It promises to carry AI applications forward toward taking actions in the real world. While largely confined to academia over the past several decades, it is now seeing some practical, real-world successes.\n\nRobotics is currently concerned with how to train a robot to interact with the world around it in generalizable and predictable ways, how to facilitate manipulation of objects in interactive environments, and how to interact with people. Advances in robotics will rely on commensurate advances to improve the reliability and generality of computer vision and other forms of machine perception.\n\nComputer vision is currently the most prominent form of machine perception. It has been the sub-area of AI most transformed by the rise of deep learning. For the first time, computers are able to perform some vision tasks better than people. Much current research is focused on automatic image and video captioning.\n\nNatural Language Processing, often coupled with automatic speech recognition, is quickly becoming a commodity for widely spoken languages with large data sets. Research is now shifting to develop refined and capable systems that are able to interact with people through dialog, not just react to stylized requests. Great strides have also been made in machine translation among different languages, with more real-time person-to-person exchanges on the near horizon.\n\nCollaborative systems research investigates models and algorithms to help develop autonomous systems that can work collaboratively with other systems and with humans.\n\nCrowdsourcing and human computation research investigates methods to augment computer systems by making automated calls to human expertise to solve problems that computers alone cannot solve well.", "original_types": ["text", "header"], "id": 96}
{"type": "section", "content": "Algorithmic game theory and computational social choice draw attention to the economic and social computing dimensions of AI, such as how systems can handle potentially misaligned incentives, including self-interested human participants or firms and the automated AI-based agents representing them.\n\nInternet of Things (IoT) research is devoted to the idea that a wide array of devices, including appliances, vehicles, buildings, and cameras, can be interconnected to collect and share their abundant sensory information to use for intelligent purposes.\n\nNeuromorphic computing is a set of technologies that seek to mimic biological neural networks to improve the hardware efficiency and robustness of computing systems, often replacing an older emphasis on separate modules for input/output, instruction-processing, and memory.", "doc_id": "stone2022", "page": 9, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Algorithmic game theory and computational social choice draw attention to the economic and social computing dimensions of AI, such as how systems can handle potentially misaligned incentives, including self-interested human participants or firms and the automated AI-based agents representing them.\n\nInternet of Things (IoT) research is devoted to the idea that a wide array of devices, including appliances, vehicles, buildings, and cameras, can be interconnected to collect and share their abundant sensory information to use for intelligent purposes.\n\nNeuromorphic computing is a set of technologies that seek to mimic biological neural networks to improve the hardware efficiency and robustness of computing systems, often replacing an older emphasis on separate modules for input/output, instruction-processing, and memory.", "original_types": ["text"], "id": 97}
{"type": "section", "content": "AI policy, now and in the future\n\nThe measure of success for AI applications is the value they create for human lives. In that light, they should be designed to enable people to understand AI systems successfully, participate in their use, and build their trust. Public policies should help ease society’s adaptation to AI applications, extend their benefits, and mitigate their inevitable errors and failures. Debate about how AI is deployed, including concerns about how privacy is protected and AI’s benefits fairly shared, should be encouraged. Given the speed with which AI technologies are being realized, and concomitant concerns about their implications, the Study Panel recommends that all layers of government acquire technical expertise in AI. Further, research on the fairness, security, privacy, and societal implications of AI systems should be encouraged by removing impediments and increasing private and public spending to support it.", "doc_id": "stone2022", "page": 10, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "AI policy, now and in the future\n\nThe measure of success for AI applications is the value they create for human lives. In that light, they should be designed to enable people to understand AI systems successfully, participate in their use, and build their trust. Public policies should help ease society’s adaptation to AI applications, extend their benefits, and mitigate their inevitable errors and failures. Debate about how AI is deployed, including concerns about how privacy is protected and AI’s benefits fairly shared, should be encouraged. Given the speed with which AI technologies are being realized, and concomitant concerns about their implications, the Study Panel recommends that all layers of government acquire technical expertise in AI. Further, research on the fairness, security, privacy, and societal implications of AI systems should be encouraged by removing impediments and increasing private and public spending to support it.", "original_types": ["text", "header"], "id": 98}
{"type": "section", "content": "the abilities and efficiency of people who have access to them. Policies should be evaluated as to whether they foster democratic values and equitable sharing of AI’s benefits, or concentrate power and benefits in the hands of a fortunate few.\n\nAs this report documents, significant AI-related advances have already had an impact on North American cities over the past fifteen years, and even more substantial developments will occur over the next fifteen. Recent advances are largely due to the growth and analysis of large data sets enabled by the internet, advances in sensory technologies and, more recently, applications of “deep learning.” In the coming years, as the public encounters new AI applications in domains such as transportation and healthcare, they must be introduced in ways that build trust and understanding, and respect human and civil rights. While encouraging innovation, policies and processes should address ethical, privacy, and security implications, and should work to ensure that the benefits of AI technologies will be spread broadly and fairly. Doing so will be critical if Artificial Intelligence research and its applications are to exert a positive influence on North American urban life in 2030 and beyond.", "doc_id": "stone2022", "page": 11, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "the abilities and efficiency of people who have access to them. Policies should be evaluated as to whether they foster democratic values and equitable sharing of AI’s benefits, or concentrate power and benefits in the hands of a fortunate few.\n\nAs this report documents, significant AI-related advances have already had an impact on North American cities over the past fifteen years, and even more substantial developments will occur over the next fifteen. Recent advances are largely due to the growth and analysis of large data sets enabled by the internet, advances in sensory technologies and, more recently, applications of “deep learning.” In the coming years, as the public encounters new AI applications in domains such as transportation and healthcare, they must be introduced in ways that build trust and understanding, and respect human and civil rights. While encouraging innovation, policies and processes should address ethical, privacy, and security implications, and should work to ensure that the benefits of AI technologies will be spread broadly and fairly. Doing so will be critical if Artificial Intelligence research and its applications are to exert a positive influence on North American urban life in 2030 and beyond.", "original_types": ["text"], "id": 99}
{"type": "section", "content": "Section I: What is Artificial Intelligence?\n\nThis section describes how researchers and practitioners define “Artificial Intelligence,” and the areas of AI research and application that are currently thriving. It proffers definitions of what AI is and is not, and describes some of the currently “hot” areas of AI Research. This section lays the groundwork for Section II, which elaborates on AI’s impacts and future in eight domains and Section III, which describes issues related to AI design and public policy and makes recommendations for encouraging AI innovation while protecting democratic values.\n\nDefining AI", "doc_id": "stone2022", "page": 12, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Section I: What is Artificial Intelligence?\n\nThis section describes how researchers and practitioners define “Artificial Intelligence,” and the areas of AI research and application that are currently thriving. It proffers definitions of what AI is and is not, and describes some of the currently “hot” areas of AI Research. This section lays the groundwork for Section II, which elaborates on AI’s impacts and future in eight domains and Section III, which describes issues related to AI design and public policy and makes recommendations for encouraging AI innovation while protecting democratic values.\n\nDefining AI", "original_types": ["text", "header"], "id": 100}
{"type": "section", "content": "Curiously, the lack of a precise, universally accepted definition of AI probably has helped the field to grow, blossom, and advance at an ever-accelerating pace. Practitioners, researchers, and developers of AI are instead guided by a rough sense of direction and an imperative to “get on with it.” Still, a definition remains important and Nils J. Nilsson has provided a useful one: “Artificial intelligence is that activity devoted to making machines intelligent, and intelligence is that quality that enables an entity to function appropriately and with foresight in its environment.”3 From this perspective, characterizing AI depends on the credit one is willing to give synthesized software and hardware for functioning “appropriately” and with “foresight.” A simple electronic calculator performs calculations much faster than the human brain, and almost never makes a mistake.4 Is a calculator intelligent? Like Nilsson, the Study Panel takes a broad view that intelligence lies on a multi-dimensional spectrum. According to this view, the difference between an arithmetic calculator and a human brain is not one of kind, but of scale, speed, degree of autonomy, and generality. The same factors can be used to evaluate every other instance of intelligence—speech recognition software, animal brains, cruise-control systems in cars, Go-playing programs, thermostats—and to place them at some appropriate location in the spectrum. Although our broad interpretation places the calculator within the intelligence spectrum, such simple devices bear little resemblance to today’s AI. The frontier of AI has moved far ahead and functions of the calculator are only one among the millions that today’s smartphones can perform. AI developers now work on improving, generalizing, and scaling up the intelligence currently found on smartphones. In fact, the field of AI is a continual endeavor to push forward the frontier of machine intelligence. Ironically, AI suffers the perennial fate of losing claim to its acquisitions, which eventually and inevitably get pulled inside the frontier, a repeating pattern known as the “AI effect” or the “odd paradox”—AI brings a new technology into the common fold, people become accustomed to this technology, it stops being considered AI, and newer technology emerges.5 The same pattern will continue in the future. AI does not “deliver” a life-changing product as a bolt from the blue. Rather, AI technologies continue to get better in a continual, incremental way.", "doc_id": "stone2022", "page": 12, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Curiously, the lack of a precise, universally accepted definition of AI probably has helped the field to grow, blossom, and advance at an ever-accelerating pace. Practitioners, researchers, and developers of AI are instead guided by a rough sense of direction and an imperative to “get on with it.” Still, a definition remains important and Nils J. Nilsson has provided a useful one: “Artificial intelligence is that activity devoted to making machines intelligent, and intelligence is that quality that enables an entity to function appropriately and with foresight in its environment.”3 From this perspective, characterizing AI depends on the credit one is willing to give synthesized software and hardware for functioning “appropriately” and with “foresight.” A simple electronic calculator performs calculations much faster than the human brain, and almost never makes a mistake.4 Is a calculator intelligent? Like Nilsson, the Study Panel takes a broad view that intelligence lies on a multi-dimensional spectrum. According to this view, the difference between an arithmetic calculator and a human brain is not one of kind, but of scale, speed, degree of autonomy, and generality. The same factors can be used to evaluate every other instance of intelligence—speech recognition software, animal brains, cruise-control systems in cars, Go-playing programs, thermostats—and to place them at some appropriate location in the spectrum. Although our broad interpretation places the calculator within the intelligence spectrum, such simple devices bear little resemblance to today’s AI. The frontier of AI has moved far ahead and functions of the calculator are only one among the millions that today’s smartphones can perform. AI developers now work on improving, generalizing, and scaling up the intelligence currently found on smartphones. In fact, the field of AI is a continual endeavor to push forward the frontier of machine intelligence. Ironically, AI suffers the perennial fate of losing claim to its acquisitions, which eventually and inevitably get pulled inside the frontier, a repeating pattern known as the “AI effect” or the “odd paradox”—AI brings a new technology into the common fold, people become accustomed to this technology, it stops being considered AI, and newer technology emerges.5 The same pattern will continue in the future. AI does not “deliver” a life-changing product as a bolt from the blue. Rather, AI technologies continue to get better in a continual, incremental way.", "original_types": ["text"], "id": 101}
{"type": "section", "content": "The human measure\n\nNotably, the characterization of intelligence as a spectrum grants no special status to the human brain. But to date human intelligence has no match in the biological and artificial worlds for sheer versatility, with the abilities “to reason, achieve goals, understand and generate language, perceive and respond to sensory inputs, prove mathematical theorems, play challenging games, synthesize and summarize information, create art and music, and even write histories.”6\n\nThis makes human intelligence a natural choice for benchmarking the progress of AI. It may even be proposed, as a rule of thumb, that any activity computers are able to perform and people once performed should be counted as an instance of intelligence. But matching any human ability is only a sufficient condition, not a necessary one. There are already many systems that exceed human intelligence, at least in speed, such as scheduling the daily arrivals and departures of thousands of flights in an airport.\n\nAI’s long quest—and eventual success—to beat human players at the game of chess offered a high-profile instance for comparing human to machine intelligence. Chess has fascinated people for centuries. When the possibility of building computers became imminent, Alan Turing, who many consider the father of computer science, “mentioned the idea of computers showing intelligence with chess as a paradigm.”7 Without access to powerful computers, “Turing played a game in which he simulated the computer, taking about half an hour per move.”\n\nBut it was only after a long line of improvements in the sixties and seventies—contributed by groups at Carnegie Mellon, Stanford, MIT, The Institute for Theoretical and Experimental Physics at Moscow, and Northwestern University—that chess-playing programs started gaining proficiency. The final push came through a long-running project at IBM, which culminated with the Deep Blue program beating Garry Kasparov, then the world chess champion, by a score of 3.5-2.5 in 1997. Curiously, no sooner had AI caught up with its elusive target than Deep Blue was portrayed as a collection of “brute force methods” that wasn’t “real intelligence.”8 In fact, IBM’s subsequent publication about Deep Blue, which gives extensive details about its search and evaluation procedures, doesn’t mention the word “intelligent” even once!9 Was Deep Blue intelligent or not? Once again, the frontier had moved.\n\nAn operational definition\n\nAI can also be defined by what AI researchers do. This report views AI primarily as a branch of computer science that studies the properties of intelligence by synthesizing intelligence.10 Though the advent of AI has depended on the rapid progress of hardware computing resources, the focus here on software reflects a trend in the AI community. More recently, though, progress in building hardware tailored for neural-network-based computing11 has created a", "doc_id": "stone2022", "page": 13, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "The human measure\n\nNotably, the characterization of intelligence as a spectrum grants no special status to the human brain. But to date human intelligence has no match in the biological and artificial worlds for sheer versatility, with the abilities “to reason, achieve goals, understand and generate language, perceive and respond to sensory inputs, prove mathematical theorems, play challenging games, synthesize and summarize information, create art and music, and even write histories.”6\n\nThis makes human intelligence a natural choice for benchmarking the progress of AI. It may even be proposed, as a rule of thumb, that any activity computers are able to perform and people once performed should be counted as an instance of intelligence. But matching any human ability is only a sufficient condition, not a necessary one. There are already many systems that exceed human intelligence, at least in speed, such as scheduling the daily arrivals and departures of thousands of flights in an airport.\n\nAI’s long quest—and eventual success—to beat human players at the game of chess offered a high-profile instance for comparing human to machine intelligence. Chess has fascinated people for centuries. When the possibility of building computers became imminent, Alan Turing, who many consider the father of computer science, “mentioned the idea of computers showing intelligence with chess as a paradigm.”7 Without access to powerful computers, “Turing played a game in which he simulated the computer, taking about half an hour per move.”\n\nBut it was only after a long line of improvements in the sixties and seventies—contributed by groups at Carnegie Mellon, Stanford, MIT, The Institute for Theoretical and Experimental Physics at Moscow, and Northwestern University—that chess-playing programs started gaining proficiency. The final push came through a long-running project at IBM, which culminated with the Deep Blue program beating Garry Kasparov, then the world chess champion, by a score of 3.5-2.5 in 1997. Curiously, no sooner had AI caught up with its elusive target than Deep Blue was portrayed as a collection of “brute force methods” that wasn’t “real intelligence.”8 In fact, IBM’s subsequent publication about Deep Blue, which gives extensive details about its search and evaluation procedures, doesn’t mention the word “intelligent” even once!9 Was Deep Blue intelligent or not? Once again, the frontier had moved.\n\nAn operational definition\n\nAI can also be defined by what AI researchers do. This report views AI primarily as a branch of computer science that studies the properties of intelligence by synthesizing intelligence.10 Though the advent of AI has depended on the rapid progress of hardware computing resources, the focus here on software reflects a trend in the AI community. More recently, though, progress in building hardware tailored for neural-network-based computing11 has created a", "original_types": ["text", "header"], "id": 102}
{"type": "section", "content": "Human intelligence has no match in the biological and artificial worlds for sheer versatility, with the abilities “to reason, achieve goals, understand and generate language... create art and music, and even write histories.”\n\nUntil the turn of the millennium, AI’s appeal lay largely in its promise to deliver, but in the last fifteen years, much of that promise has been redeemed.15 AI technologies already pervade our lives. As they becomes a central force in society, the field is shifting from simply building systems that are intelligent to building intelligent systems that are human-aware and trustworthy.\n\nSeveral factors have fueled the AI revolution. Foremost among them is the maturing of machine learning, supported in part by cloud computing resources and wide-spread, web-based data gathering. Machine learning has been propelled dramatically forward by “deep learning,” a form of adaptive artificial neural networks trained using a method called backpropagation.16 This leap in the performance of information processing algorithms has been accompanied by significant progress in hardware technology for basic operations such as sensing, perception, and object recognition. New platforms and markets for data-driven products, and the economic incentives to find new products and markets, have also contributed to the advent of AI-driven technology.\n\nAll these trends drive the “hot” areas of research described below. This compilation is meant simply to reflect the areas that, by one metric or another, currently receive greater attention than others. They are not necessarily more important or valuable than other ones. Indeed, some of the currently “hot” areas were less popular in past years, and it is likely that other areas will similarly re-emerge in the future.\n\nAI RESEARCH TRENDS\n\nLarge-scale machine learning Many of the basic problems in machine learning (such as supervised and unsupervised learning) are well-understood. A major focus of current efforts is to scale existing algorithms to work with extremely large data sets. For example, whereas traditional methods could afford to make several passes over the data set, modern ones are designed to make only a single pass; in some cases, only sublinear methods (those that only look at a fraction of the data) can be admitted.\n\nDeep learning The ability to successfully train convolutional neural networks has most benefited the field of computer vision, with applications such as object recognition, video", "doc_id": "stone2022", "page": 14, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Human intelligence has no match in the biological and artificial worlds for sheer versatility, with the abilities “to reason, achieve goals, understand and generate language... create art and music, and even write histories.”\n\nUntil the turn of the millennium, AI’s appeal lay largely in its promise to deliver, but in the last fifteen years, much of that promise has been redeemed.15 AI technologies already pervade our lives. As they becomes a central force in society, the field is shifting from simply building systems that are intelligent to building intelligent systems that are human-aware and trustworthy.\n\nSeveral factors have fueled the AI revolution. Foremost among them is the maturing of machine learning, supported in part by cloud computing resources and wide-spread, web-based data gathering. Machine learning has been propelled dramatically forward by “deep learning,” a form of adaptive artificial neural networks trained using a method called backpropagation.16 This leap in the performance of information processing algorithms has been accompanied by significant progress in hardware technology for basic operations such as sensing, perception, and object recognition. New platforms and markets for data-driven products, and the economic incentives to find new products and markets, have also contributed to the advent of AI-driven technology.\n\nAll these trends drive the “hot” areas of research described below. This compilation is meant simply to reflect the areas that, by one metric or another, currently receive greater attention than others. They are not necessarily more important or valuable than other ones. Indeed, some of the currently “hot” areas were less popular in past years, and it is likely that other areas will similarly re-emerge in the future.\n\nAI RESEARCH TRENDS\n\nLarge-scale machine learning Many of the basic problems in machine learning (such as supervised and unsupervised learning) are well-understood. A major focus of current efforts is to scale existing algorithms to work with extremely large data sets. For example, whereas traditional methods could afford to make several passes over the data set, modern ones are designed to make only a single pass; in some cases, only sublinear methods (those that only look at a fraction of the data) can be admitted.\n\nDeep learning The ability to successfully train convolutional neural networks has most benefited the field of computer vision, with applications such as object recognition, video", "original_types": ["text", "header"], "id": 103}
{"type": "section", "content": "Reinforcement learning\n\nWhereas traditional machine learning has mostly focused on pattern mining, reinforcement learning shifts the focus to decision making, and is a technology that will help AI to advance more deeply into the realm of learning about and executing actions in the real world. It has existed for several decades as a framework for experience-driven sequential decision-making, but the methods have not found great success in practice, mainly owing to issues of representation and scaling. However, the advent of deep learning has provided reinforcement learning with a “shot in the arm.” The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning. AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning.\n\nRobotics\n\nRobotic navigation, at least in static environments, is largely solved. Current efforts consider how to train a robot to interact with the world around it in generalizable and predictable ways. A natural requirement that arises in interactive environments is manipulation, another topic of current interest. The deep learning revolution is only beginning to influence robotics, in large part because it is far more difficult to acquire the large labeled data sets that have driven other learning-based areas of AI. Reinforcement learning (see above), which obviates the requirement of labeled data, may help bridge this gap but requires systems to be able to safely explore a policy space without committing errors that harm the system itself or others. Advances in reliable machine perception, including computer vision, force, and tactile perception, much of which will be driven by machine learning, will continue to be key enablers to advancing the capabilities of robotics.\n\nComputer vision\n\nComputer vision is currently the most prominent form of machine perception. It has been the sub-area of AI most transformed by the rise of deep learning. Until just a few years ago, support vector machines were the method of choice for most visual classification tasks. But the confluence of large-scale computing, especially on GPUs, the availability of large datasets, especially via the internet, and refinements of neural network algorithms has led to dramatic improvements in performance on benchmark tasks (e.g., classification on ImageNet17). For the first time, computers are able to perform some (narrowly defined) visual classification tasks better than people. Much current research is focused on automatic image and video captioning.\n\nNatural Language Processing", "doc_id": "stone2022", "page": 15, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Reinforcement learning\n\nWhereas traditional machine learning has mostly focused on pattern mining, reinforcement learning shifts the focus to decision making, and is a technology that will help AI to advance more deeply into the realm of learning about and executing actions in the real world. It has existed for several decades as a framework for experience-driven sequential decision-making, but the methods have not found great success in practice, mainly owing to issues of representation and scaling. However, the advent of deep learning has provided reinforcement learning with a “shot in the arm.” The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning. AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning.\n\nRobotics\n\nRobotic navigation, at least in static environments, is largely solved. Current efforts consider how to train a robot to interact with the world around it in generalizable and predictable ways. A natural requirement that arises in interactive environments is manipulation, another topic of current interest. The deep learning revolution is only beginning to influence robotics, in large part because it is far more difficult to acquire the large labeled data sets that have driven other learning-based areas of AI. Reinforcement learning (see above), which obviates the requirement of labeled data, may help bridge this gap but requires systems to be able to safely explore a policy space without committing errors that harm the system itself or others. Advances in reliable machine perception, including computer vision, force, and tactile perception, much of which will be driven by machine learning, will continue to be key enablers to advancing the capabilities of robotics.\n\nComputer vision\n\nComputer vision is currently the most prominent form of machine perception. It has been the sub-area of AI most transformed by the rise of deep learning. Until just a few years ago, support vector machines were the method of choice for most visual classification tasks. But the confluence of large-scale computing, especially on GPUs, the availability of large datasets, especially via the internet, and refinements of neural network algorithms has led to dramatic improvements in performance on benchmark tasks (e.g., classification on ImageNet17). For the first time, computers are able to perform some (narrowly defined) visual classification tasks better than people. Much current research is focused on automatic image and video captioning.\n\nNatural Language Processing", "original_types": ["text", "header"], "id": 104}
{"type": "section", "content": "Often coupled with automatic speech recognition, Natural Language Processing is another very active area of machine perception. It is quickly becoming a commodity for mainstream languages with large data sets. Google announced that 20% of current mobile queries are done by voice,18 and recent demonstrations have proven the possibility of real-time translation. Research is now shifting towards developing refined and capable systems that are able to interact with people through dialog, not just react to stylized requests.", "doc_id": "stone2022", "page": 15, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Often coupled with automatic speech recognition, Natural Language Processing is another very active area of machine perception. It is quickly becoming a commodity for mainstream languages with large data sets. Google announced that 20% of current mobile queries are done by voice,18 and recent demonstrations have proven the possibility of real-time translation. Research is now shifting towards developing refined and capable systems that are able to interact with people through dialog, not just react to stylized requests.", "original_types": ["text"], "id": 105}
{"type": "section", "content": "Collaborative systems\n\nResearch on collaborative systems investigates models and algorithms to help develop autonomous systems that can work collaboratively with other systems and with humans. This research relies on developing formal models of collaboration, and studies the capabilities needed for systems to become effective partners. There is growing interest in applications that can utilize the complementary strengths of humans and machines—for humans to help AI systems to overcome their limitations, and for agents to augment human abilities and activities.\n\nNatural Language Processing is a very active area of machine perception. Research is now shifting towards developing systems that are able to interact with people through dialog, not just react to stylized requests.\n\nCrowdsourcing and human computation\n\nSince human abilities are superior to automated methods for accomplishing many tasks, research on crowdsourcing and human computation investigates methods to augment computer systems by utilizing human intelligence to solve problems that computers alone cannot solve well. Introduced only about fifteen years ago, this research now has an established presence in AI. The best-known example of crowdsourcing is Wikipedia, a knowledge repository that is maintained and updated by netizens and that far exceeds traditionally-compiled information sources, such as encyclopedias and dictionaries, in scale and depth. Crowdsourcing focuses on devising innovative ways to harness human intelligence. Citizen science platforms energize volunteers to solve scientific problems, while paid crowdsourcing platforms such as Amazon Mechanical Turk provide automated access to human intelligence on demand. Work in this area has facilitated advances in other subfields of AI, including computer vision and NLP, by enabling large amounts of labeled training data and/or human interaction data to be collected in a short amount of time. Current research efforts explore ideal divisions of tasks between humans and machines based on their differing capabilities and costs.\n\nAlgorithmic game theory and computational social choice", "doc_id": "stone2022", "page": 16, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Collaborative systems\n\nResearch on collaborative systems investigates models and algorithms to help develop autonomous systems that can work collaboratively with other systems and with humans. This research relies on developing formal models of collaboration, and studies the capabilities needed for systems to become effective partners. There is growing interest in applications that can utilize the complementary strengths of humans and machines—for humans to help AI systems to overcome their limitations, and for agents to augment human abilities and activities.\n\nNatural Language Processing is a very active area of machine perception. Research is now shifting towards developing systems that are able to interact with people through dialog, not just react to stylized requests.\n\nCrowdsourcing and human computation\n\nSince human abilities are superior to automated methods for accomplishing many tasks, research on crowdsourcing and human computation investigates methods to augment computer systems by utilizing human intelligence to solve problems that computers alone cannot solve well. Introduced only about fifteen years ago, this research now has an established presence in AI. The best-known example of crowdsourcing is Wikipedia, a knowledge repository that is maintained and updated by netizens and that far exceeds traditionally-compiled information sources, such as encyclopedias and dictionaries, in scale and depth. Crowdsourcing focuses on devising innovative ways to harness human intelligence. Citizen science platforms energize volunteers to solve scientific problems, while paid crowdsourcing platforms such as Amazon Mechanical Turk provide automated access to human intelligence on demand. Work in this area has facilitated advances in other subfields of AI, including computer vision and NLP, by enabling large amounts of labeled training data and/or human interaction data to be collected in a short amount of time. Current research efforts explore ideal divisions of tasks between humans and machines based on their differing capabilities and costs.\n\nAlgorithmic game theory and computational social choice", "original_types": ["text", "header"], "id": 106}
{"type": "section", "content": "New attention is being drawn to the economic and social computing dimensions of AI, including incentive structures. Distributed AI and multi-agent systems have been studied since the early 1980s, gained prominence starting in the late 1990s, and were accelerated by the internet. A natural requirement is that systems handle potentially misaligned incentives, including self-interested human participants or firms, as well as automated AI-based agents representing them. Topics receiving attention include computational mechanism design (an economic theory of incentive design, seeking incentive-compatible systems where inputs are truthfully reported), computational social choice (a theory for how to aggregate rank orders on alternatives), incentive aligned information elicitation (prediction markets, scoring rules, peer prediction) and algorithmic game theory (the equilibria of markets, network games, and parlor games such as Poker—a game where significant advances have been made in recent years through abstraction techniques and no-regret learning).\n\nInternet of Things (IoT)\n\nA growing body of research is devoted to the idea that a wide array of devices can be interconnected to collect and share their sensory information. Such devices can include appliances, vehicles, buildings, cameras, and other things. While it’s a matter of technology and wireless networking to connect the devices, AI can process and use the resulting huge amounts of data for intelligent and useful purposes. Currently, these devices use a bewildering array of incompatible communication protocols. AI could help tame this Tower of Babel.\n\nNeuromorphic Computing\n\nTraditional computers implement the von Neumann model of computing, which separates the modules for input/output, instruction-processing, and memory. With the success of deep neural networks on a wide array of tasks, manufacturers are", "doc_id": "stone2022", "page": 16, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "New attention is being drawn to the economic and social computing dimensions of AI, including incentive structures. Distributed AI and multi-agent systems have been studied since the early 1980s, gained prominence starting in the late 1990s, and were accelerated by the internet. A natural requirement is that systems handle potentially misaligned incentives, including self-interested human participants or firms, as well as automated AI-based agents representing them. Topics receiving attention include computational mechanism design (an economic theory of incentive design, seeking incentive-compatible systems where inputs are truthfully reported), computational social choice (a theory for how to aggregate rank orders on alternatives), incentive aligned information elicitation (prediction markets, scoring rules, peer prediction) and algorithmic game theory (the equilibria of markets, network games, and parlor games such as Poker—a game where significant advances have been made in recent years through abstraction techniques and no-regret learning).\n\nInternet of Things (IoT)\n\nA growing body of research is devoted to the idea that a wide array of devices can be interconnected to collect and share their sensory information. Such devices can include appliances, vehicles, buildings, cameras, and other things. While it’s a matter of technology and wireless networking to connect the devices, AI can process and use the resulting huge amounts of data for intelligent and useful purposes. Currently, these devices use a bewildering array of incompatible communication protocols. AI could help tame this Tower of Babel.\n\nNeuromorphic Computing\n\nTraditional computers implement the von Neumann model of computing, which separates the modules for input/output, instruction-processing, and memory. With the success of deep neural networks on a wide array of tasks, manufacturers are", "original_types": ["text", "header"], "id": 107}
{"type": "section", "content": "Overall trends and the future of AI research\n\nThe resounding success of the data-driven paradigm has displaced the traditional paradigms of AI. Procedures such as theorem proving and logic-based knowledge representation and reasoning are receiving reduced attention, in part because of the ongoing challenge of connecting with real-world groundings. Planning, which was a mainstay of AI research in the seventies and eighties, has also received less attention of late due in part to its strong reliance on modeling assumptions that are hard to satisfy in realistic applications. Model-based approaches—such as physics-based approaches to vision and traditional control and mapping in robotics—have by and large given way to data-driven approaches that close the loop with sensing the results of actions in the task at hand. Bayesian reasoning and graphical models, which were very popular even quite recently, also appear to be going out of favor, having been drowned by the deluge of data and the remarkable success of deep learning. Over the next fifteen years, the Study Panel expects an increasing focus on developing systems that are human-aware, meaning that they specifically model, and are specifically designed for, the characteristics of the people with whom they are meant to interact. There is a lot of interest in trying to find new, creative ways to develop interactive and scalable ways to teach robots. Also, IoT-type systems—devices and the cloud—are becoming increasingly popular, as is thinking about social and economic dimensions of AI. In the coming years, new perception/object recognition capabilities and robotic platforms that are human-safe will grow, as will data-driven products and their markets. The Study Panel also expects a reemergence of some of the traditional forms of AI as practitioners come to realize the inevitable limitations of purely end-to-end deep learning approaches. We encourage young researchers not to reinvent the wheel, but rather to maintain an awareness of the significant progress in many areas of AI during the first fifty years of the field, and in related fields such as control theory, cognitive science, and psychology.", "doc_id": "stone2022", "page": 17, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Overall trends and the future of AI research\n\nThe resounding success of the data-driven paradigm has displaced the traditional paradigms of AI. Procedures such as theorem proving and logic-based knowledge representation and reasoning are receiving reduced attention, in part because of the ongoing challenge of connecting with real-world groundings. Planning, which was a mainstay of AI research in the seventies and eighties, has also received less attention of late due in part to its strong reliance on modeling assumptions that are hard to satisfy in realistic applications. Model-based approaches—such as physics-based approaches to vision and traditional control and mapping in robotics—have by and large given way to data-driven approaches that close the loop with sensing the results of actions in the task at hand. Bayesian reasoning and graphical models, which were very popular even quite recently, also appear to be going out of favor, having been drowned by the deluge of data and the remarkable success of deep learning. Over the next fifteen years, the Study Panel expects an increasing focus on developing systems that are human-aware, meaning that they specifically model, and are specifically designed for, the characteristics of the people with whom they are meant to interact. There is a lot of interest in trying to find new, creative ways to develop interactive and scalable ways to teach robots. Also, IoT-type systems—devices and the cloud—are becoming increasingly popular, as is thinking about social and economic dimensions of AI. In the coming years, new perception/object recognition capabilities and robotic platforms that are human-safe will grow, as will data-driven products and their markets. The Study Panel also expects a reemergence of some of the traditional forms of AI as practitioners come to realize the inevitable limitations of purely end-to-end deep learning approaches. We encourage young researchers not to reinvent the wheel, but rather to maintain an awareness of the significant progress in many areas of AI during the first fifty years of the field, and in related fields such as control theory, cognitive science, and psychology.", "original_types": ["text", "header"], "id": 108}
{"type": "section", "content": "Section II: AI BY DOMAIN\n\nThough different instances of AI research and practice share common technologies, such as machine learning, they also vary considerably in different sectors of the economy and society. We call these sectors “domains,” and in this section describe the different states of AI research and implementation, as well as impacts and distinct challenges, in eight of them: transportation; home/service robotics; healthcare; education; low-resource communities; public safety and security; employment and workplace; and entertainment. Based on these analyses, we also predict trends in a typical North American city over the next fifteen years. Contrary to AI’s typical depiction in popular culture, we seek to offer a balanced overview of the ways in which AI is already beginning to transform everyday life, and how those transformations are likely to grow by the year 2030.", "doc_id": "stone2022", "page": 18, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Section II: AI BY DOMAIN\n\nThough different instances of AI research and practice share common technologies, such as machine learning, they also vary considerably in different sectors of the economy and society. We call these sectors “domains,” and in this section describe the different states of AI research and implementation, as well as impacts and distinct challenges, in eight of them: transportation; home/service robotics; healthcare; education; low-resource communities; public safety and security; employment and workplace; and entertainment. Based on these analyses, we also predict trends in a typical North American city over the next fifteen years. Contrary to AI’s typical depiction in popular culture, we seek to offer a balanced overview of the ways in which AI is already beginning to transform everyday life, and how those transformations are likely to grow by the year 2030.", "original_types": ["text", "header"], "id": 109}
{"type": "section", "content": "Automated capabilities in commercial cars\n\nThey already had a number of functionalities that combined real-time sensing with perception and decision-making such as Anti-lock Braking Systems (ABS), airbag control, Traction Control Systems (TCS), and Electronic Stability Control (ESC).22 Automated capabilities have been introduced into commercial cars gradually since 2003 as summarized in the following table.", "doc_id": "stone2022", "page": 19, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Automated capabilities in commercial cars\n\nThey already had a number of functionalities that combined real-time sensing with perception and decision-making such as Anti-lock Braking Systems (ABS), airbag control, Traction Control Systems (TCS), and Electronic Stability Control (ESC).22 Automated capabilities have been introduced into commercial cars gradually since 2003 as summarized in the following table.", "original_types": ["text", "header"], "id": 110}
{"type": "table", "content": "Automated Functionality in Commercial Cars\n| Context | Automated Functionality | Release Date |\n| --- | --- | --- |\n| Parking | Intelligent Parking Assist System | Since 200323 |\n| Parking | Summon | Since 201624 |\n| Arterial & Highway | Lane departure system | Since 2004 in North America25 |\n| Arterial & Highway | Adaptive cruise control | Since 2005 in North America26 |\n| Highway | Blind spot monitoring | 200727 |\n| Highway | Lane changing | 201528 |\n", "doc_id": "stone2022", "page": 19, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Automated Functionality in Commercial Cars\n| Context | Automated Functionality | Release Date |\n| --- | --- | --- |\n| Parking | Intelligent Parking Assist System | Since 200323 |\n| Parking | Summon | Since 201624 |\n| Arterial & Highway | Lane departure system | Since 2004 in North America25 |\n| Arterial & Highway | Adaptive cruise control | Since 2005 in North America26 |\n| Highway | Blind spot monitoring | 200727 |\n| Highway | Lane changing | 201528 |\n", "id": 111}
{"type": "section", "content": "These functionalities assist drivers or completely take over well-defined activities for increased safety and comfort. Current cars can park themselves, perform adaptive cruise control on highways, steer themselves during stop-and-go traffic, and alert drivers about objects in blind spots during lane changes. Vision and radar technology were leveraged to develop pre-collision systems that let cars autonomously brake when risk of a collision is detected. Deep learning also has been applied to improve automobiles’ capacity to detect objects in the environment and recognize sound.29\n\nSelf-driving vehicles\n\nSince the 1930s, science fiction writers dreamed of a future with self-driving cars, and building them has been a challenge for the AI community since the 1960s. By the 2000s, the dream of autonomous vehicles became a reality in the sea and sky, and even on Mars, but self-driving cars existed only as research prototypes in labs. Driving in a city was considered to be a problem too complex for automation due to factors like pedestrians, heavy traffic, and the many unexpected events that can happen outside of the car’s control. Although the technological components required to\n\n22 Carl Liersch, “Vehicle Technology Timeline: From Automated to Driverless,” Robert Bosch (Australia) Pty. Ltd., 2014, accessed August 1, 2016, http://dpti.sa.gov.au/__data/assets/pdf_file/0009/246807/Carl_Liersch_Presentation.pdf.\n23 “Intelligent Parking Assist System,” Wikipedia, last modified July 26, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Intelligent_Parking_Assist_System.\n24 The Tesla Motors Team, “Summon Your Tesla from Your Phone,” Tesla, January 10, 2016, accessed August 1, 2016, https://www.teslamotors.com/blog/summon-your-tesla-your-phone.\n25 Lane departure warning system,” Wikipedia, last modified July 24, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Lane_departure_warning_system.\n26 “Autonomous cruise control system,” Wikipedia, last modified July 30, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Autonomous_cruise_control_system.\n27 “Blind spot monitor,” Wikipedia, last modified April 20, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Blind_spot_monitor.\n28 Dana Hull, “Tesla Starts Rolling Out Autopilot Features,” Bloomberg Technology, October 14, 2015, accessed August 1, 2016, http://www.bloomberg.com/news/articles/2015-10-14/tesla-software-upgrade-adds-automated-lane-changing-to-model-s.\n29 Aaron Tilley, “New Qualcomm Chip Brings Deep Learning To Cars,” Forbes, January 5, 2016, accessed August 1, 2016, http://www.forbes.com/sites/aarontilley/2016/01/05/along-with-nvidia-new-qualcomm-chip-brings-deep-learning-to-cars/#4cb4e9235357.", "doc_id": "stone2022", "page": 19, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "These functionalities assist drivers or completely take over well-defined activities for increased safety and comfort. Current cars can park themselves, perform adaptive cruise control on highways, steer themselves during stop-and-go traffic, and alert drivers about objects in blind spots during lane changes. Vision and radar technology were leveraged to develop pre-collision systems that let cars autonomously brake when risk of a collision is detected. Deep learning also has been applied to improve automobiles’ capacity to detect objects in the environment and recognize sound.29\n\nSelf-driving vehicles\n\nSince the 1930s, science fiction writers dreamed of a future with self-driving cars, and building them has been a challenge for the AI community since the 1960s. By the 2000s, the dream of autonomous vehicles became a reality in the sea and sky, and even on Mars, but self-driving cars existed only as research prototypes in labs. Driving in a city was considered to be a problem too complex for automation due to factors like pedestrians, heavy traffic, and the many unexpected events that can happen outside of the car’s control. Although the technological components required to\n\n22 Carl Liersch, “Vehicle Technology Timeline: From Automated to Driverless,” Robert Bosch (Australia) Pty. Ltd., 2014, accessed August 1, 2016, http://dpti.sa.gov.au/__data/assets/pdf_file/0009/246807/Carl_Liersch_Presentation.pdf.\n23 “Intelligent Parking Assist System,” Wikipedia, last modified July 26, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Intelligent_Parking_Assist_System.\n24 The Tesla Motors Team, “Summon Your Tesla from Your Phone,” Tesla, January 10, 2016, accessed August 1, 2016, https://www.teslamotors.com/blog/summon-your-tesla-your-phone.\n25 Lane departure warning system,” Wikipedia, last modified July 24, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Lane_departure_warning_system.\n26 “Autonomous cruise control system,” Wikipedia, last modified July 30, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Autonomous_cruise_control_system.\n27 “Blind spot monitor,” Wikipedia, last modified April 20, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Blind_spot_monitor.\n28 Dana Hull, “Tesla Starts Rolling Out Autopilot Features,” Bloomberg Technology, October 14, 2015, accessed August 1, 2016, http://www.bloomberg.com/news/articles/2015-10-14/tesla-software-upgrade-adds-automated-lane-changing-to-model-s.\n29 Aaron Tilley, “New Qualcomm Chip Brings Deep Learning To Cars,” Forbes, January 5, 2016, accessed August 1, 2016, http://www.forbes.com/sites/aarontilley/2016/01/05/along-with-nvidia-new-qualcomm-chip-brings-deep-learning-to-cars/#4cb4e9235357.", "original_types": ["text", "header"], "id": 112}
{"type": "section", "content": "We will see self-driving and remotely controlled delivery vehicles, flying vehicles, and trucks. Peer-to-peer transportation services such as ridesharing are also likely to utilize self-driving vehicles.\n\nmake such autonomous driving possible were available in 2000—and indeed some autonomous car prototypes existed30,31,32—few predicted that mainstream companies would be developing and deploying autonomous cars by 2015. During the first Defense Advanced Research Projects Agency (DARPA) “grand challenge” on autonomous driving in 2004, research teams failed to complete the challenge in a limited desert setting.\n\nBut in eight short years, from 2004-2012, speedy and surprising progress occurred in both academia and industry. Advances in sensing technology and machine learning for perception tasks has sped progress and, as a result, Google’s autonomous vehicles and Tesla’s semi-autonomous cars are driving on city streets today. Google’s self-driving cars, which have logged more than 1,500,000 miles (300,000 miles without an accident),33 are completely autonomous—no human input needed. Tesla has widely released self-driving capability to existing cars with a software update.34 Their cars are semi-autonomous, with human drivers expected to stay engaged and take over if they detect a potential problem. It is not yet clear whether this semi-autonomous approach is sustainable, since as people become more confident in the cars’ capabilities, they are likely to pay less attention to the road, and become less reliable when they are most needed. The first traffic fatality involving an autonomous car, which occurred in June of 2016, brought this question into sharper focus.35\n\nIn the near future, sensing algorithms will achieve super-human performance for capabilities required for driving. Automated perception, including vision, is already near or at human-level performance for well-defined tasks such as recognition and tracking. Advances in perception will be followed by algorithmic improvements in higher level reasoning capabilities such as planning. A recent report predicts self-driving cars to be widely adopted by 2020.36 And the adoption of self-driving capabilities won’t be limited to personal transportation. We will see self-driving and remotely controlled delivery vehicles, flying vehicles, and trucks. Peer-to-peer transportation services such as ridesharing are also likely to utilize self-driving vehicles. Beyond self-driving cars, advances in robotics will facilitate the creation and adoption of other types of autonomous vehicles, including robots and drones.", "doc_id": "stone2022", "page": 20, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "We will see self-driving and remotely controlled delivery vehicles, flying vehicles, and trucks. Peer-to-peer transportation services such as ridesharing are also likely to utilize self-driving vehicles.\n\nmake such autonomous driving possible were available in 2000—and indeed some autonomous car prototypes existed30,31,32—few predicted that mainstream companies would be developing and deploying autonomous cars by 2015. During the first Defense Advanced Research Projects Agency (DARPA) “grand challenge” on autonomous driving in 2004, research teams failed to complete the challenge in a limited desert setting.\n\nBut in eight short years, from 2004-2012, speedy and surprising progress occurred in both academia and industry. Advances in sensing technology and machine learning for perception tasks has sped progress and, as a result, Google’s autonomous vehicles and Tesla’s semi-autonomous cars are driving on city streets today. Google’s self-driving cars, which have logged more than 1,500,000 miles (300,000 miles without an accident),33 are completely autonomous—no human input needed. Tesla has widely released self-driving capability to existing cars with a software update.34 Their cars are semi-autonomous, with human drivers expected to stay engaged and take over if they detect a potential problem. It is not yet clear whether this semi-autonomous approach is sustainable, since as people become more confident in the cars’ capabilities, they are likely to pay less attention to the road, and become less reliable when they are most needed. The first traffic fatality involving an autonomous car, which occurred in June of 2016, brought this question into sharper focus.35\n\nIn the near future, sensing algorithms will achieve super-human performance for capabilities required for driving. Automated perception, including vision, is already near or at human-level performance for well-defined tasks such as recognition and tracking. Advances in perception will be followed by algorithmic improvements in higher level reasoning capabilities such as planning. A recent report predicts self-driving cars to be widely adopted by 2020.36 And the adoption of self-driving capabilities won’t be limited to personal transportation. We will see self-driving and remotely controlled delivery vehicles, flying vehicles, and trucks. Peer-to-peer transportation services such as ridesharing are also likely to utilize self-driving vehicles. Beyond self-driving cars, advances in robotics will facilitate the creation and adoption of other types of autonomous vehicles, including robots and drones.", "original_types": ["text", "header"], "id": 113}
{"type": "section", "content": "It is not yet clear how much better self-driving cars need to become to encourage broad acceptance. The collaboration required in semi-self-driving cars and its implications for the cognitive load of human drivers is not well understood. But if future self-driving cars are adopted with the predicted speed, and they exceed human-level performance in driving, other significant societal changes will follow. Self-driving cars will eliminate one of the biggest causes of accidental death and injury in United States, and lengthen people’s life expectancy. On average, a", "doc_id": "stone2022", "page": 20, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "It is not yet clear how much better self-driving cars need to become to encourage broad acceptance. The collaboration required in semi-self-driving cars and its implications for the cognitive load of human drivers is not well understood. But if future self-driving cars are adopted with the predicted speed, and they exceed human-level performance in driving, other significant societal changes will follow. Self-driving cars will eliminate one of the biggest causes of accidental death and injury in United States, and lengthen people’s life expectancy. On average, a", "original_types": ["text"], "id": 114}
{"type": "section", "content": "A commuter in the United States spends twenty-five minutes driving each way.37 With self-driving car technology, people will have more time to work or entertain themselves during their commutes. And the increased comfort and decreased cognitive load with self-driving cars and shared transportation may affect where people choose to live. The reduced need for parking may affect the way cities and public spaces are designed. Self-driving cars may also serve to increase the freedom and mobility of different subgroups of the population, including youth, elderly and disabled.\n\nSelf-driving cars and peer-to-peer transportation services may eliminate the need to own a vehicle. The effect on total car use is hard to predict. Trips of empty vehicles and people’s increased willingness to travel may lead to more total miles driven. Alternatively, shared autonomous vehicles—people using cars as a service rather than owning their own—may reduce total miles, especially if combined with well-constructed incentives, such as tolls or discounts, to spread out travel demand, share trips, and reduce congestion. The availability of shared transportation may displace the need for public transportation—or public transportation may change form towards personal rapid transit, already available in four cities,38 which uses small capacity vehicles to transport people on demand and point-to-point between many stations.39\n\nAs autonomous vehicles become more widespread, questions will arise over their security, including how to ensure that technologies are safe and properly tested under different road conditions prior to their release. Autonomous vehicles and the connected transportation infrastructure will create a new venue for hackers to exploit vulnerabilities to attack. Ethical questions are also involved in programming cars to act in situations in which human injury or death is inevitable, especially when there are split-second choices to be made about whom to put at risk. The legal systems in most states in the US do not have rules covering self-driving cars. As of 2016, four states in the US (Nevada, Florida, California, and Michigan), Ontario in Canada, the United Kingdom, France, and Switzerland have passed rules for the testing of self-driving cars on public roads. Even these laws do not address issues about responsibility and assignment of blame for an accident for self-driving and semi-self-driving cars.40\n\nTransportation planning\n\nBy 2005, cities had started investing in the transportation infrastructure to develop sensing capabilities for vehicle and pedestrian traffic.41 The sensors currently used include inductive loops, video cameras, remote traffic microwave sensors, radars, and GPS.42 For example, in 2013 New York started using a combination of microwave sensors, a network of cameras, and pass readers to detect vehicle traffic in the city.43", "doc_id": "stone2022", "page": 21, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "A commuter in the United States spends twenty-five minutes driving each way.37 With self-driving car technology, people will have more time to work or entertain themselves during their commutes. And the increased comfort and decreased cognitive load with self-driving cars and shared transportation may affect where people choose to live. The reduced need for parking may affect the way cities and public spaces are designed. Self-driving cars may also serve to increase the freedom and mobility of different subgroups of the population, including youth, elderly and disabled.\n\nSelf-driving cars and peer-to-peer transportation services may eliminate the need to own a vehicle. The effect on total car use is hard to predict. Trips of empty vehicles and people’s increased willingness to travel may lead to more total miles driven. Alternatively, shared autonomous vehicles—people using cars as a service rather than owning their own—may reduce total miles, especially if combined with well-constructed incentives, such as tolls or discounts, to spread out travel demand, share trips, and reduce congestion. The availability of shared transportation may displace the need for public transportation—or public transportation may change form towards personal rapid transit, already available in four cities,38 which uses small capacity vehicles to transport people on demand and point-to-point between many stations.39\n\nAs autonomous vehicles become more widespread, questions will arise over their security, including how to ensure that technologies are safe and properly tested under different road conditions prior to their release. Autonomous vehicles and the connected transportation infrastructure will create a new venue for hackers to exploit vulnerabilities to attack. Ethical questions are also involved in programming cars to act in situations in which human injury or death is inevitable, especially when there are split-second choices to be made about whom to put at risk. The legal systems in most states in the US do not have rules covering self-driving cars. As of 2016, four states in the US (Nevada, Florida, California, and Michigan), Ontario in Canada, the United Kingdom, France, and Switzerland have passed rules for the testing of self-driving cars on public roads. Even these laws do not address issues about responsibility and assignment of blame for an accident for self-driving and semi-self-driving cars.40\n\nTransportation planning\n\nBy 2005, cities had started investing in the transportation infrastructure to develop sensing capabilities for vehicle and pedestrian traffic.41 The sensors currently used include inductive loops, video cameras, remote traffic microwave sensors, radars, and GPS.42 For example, in 2013 New York started using a combination of microwave sensors, a network of cameras, and pass readers to detect vehicle traffic in the city.43", "original_types": ["text", "header"], "id": 115}
{"type": "section", "content": "Ethical questions arise when programming cars to act in situations in which human injury or death is inevitable, especially when there are split-second choices to be made about whom to put at risk.\n\nCities use AI methods to optimize services in several ways, such as bus and subway schedules, and tracking traffic conditions to dynamically adjust speed limits or apply smart pricing on highways, bridges, and HOV lanes.44 45 46 Using sensors and cameras in the road network, they can also optimize traffic light timing for improving traffic flow and to help with automated enforcement.47 48 These dynamic strategies are aimed at better utilizing the limited resources in the transportation network, and are made possible by the availability of data and the widespread connectivity of individuals.", "doc_id": "stone2022", "page": 22, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Ethical questions arise when programming cars to act in situations in which human injury or death is inevitable, especially when there are split-second choices to be made about whom to put at risk.\n\nCities use AI methods to optimize services in several ways, such as bus and subway schedules, and tracking traffic conditions to dynamically adjust speed limits or apply smart pricing on highways, bridges, and HOV lanes.44 45 46 Using sensors and cameras in the road network, they can also optimize traffic light timing for improving traffic flow and to help with automated enforcement.47 48 These dynamic strategies are aimed at better utilizing the limited resources in the transportation network, and are made possible by the availability of data and the widespread connectivity of individuals.", "original_types": ["text", "header"], "id": 116}
{"type": "section", "content": "AI is likely to have an increasing impact on city infrastructure. Accurate predictive models of individuals’ movements, their preferences, and their goals are likely to emerge with the greater availability of data. The ethical issues regarding such an emergence are discussed in Section III of this report.\n\nThe United States Department of Transportation released a call for proposals in 2016 asking medium-size cities to imagine smart city infrastructure for transportation.55 This initiative plans to award forty million dollars to a city to demonstrate how technology and data can be used to reimagine the movement of people as well as goods.\n\nOne vision is a network of connected vehicles that can reach a high level of safety in driving with car-to-car communication.56 If this vision becomes reality, we expect advances in multi-agent coordination, collaboration, and planning will have a significant impact on future cars and play a role in making the transportation system more reliable and efficient. Robots are also likely to take part in transportation by carrying individuals and packages (c.f., Segway robot). For transportation of goods, interest in drones has increased, and Amazon is currently testing a delivery system using them,57 although questions remain about the appropriate safety rules and regulations.\n\nThe increased sensing capabilities, adoption of drones, and the connected transportation infrastructure will also raise concerns about the privacy of individuals and the safety of private data. In coming years, these and related transportation issues will need to be addressed either by preemptive action on the part of industry or within the legal framework. As noted in the Section III policy discussion, how well this is done will affect the pace and scope of AI-related advances in the transportation sector.\n\nOn-demand transportation\n\nOn-demand transportation services such as Uber and Lyft have emerged as another pivotal application of sensing, connectivity, and AI,58 with algorithms for matching drivers to passengers by location and suitability (reputation modeling).59 60 Through dynamic pricing, these services ration access by willingness-to-pay, with dynamic pricing also encouraging an increase in the supply of drivers, and have become a popular method for transportation in cities. With their rapid advance have come multiple policy and legal issues, such as competition with existing taxi services and concerns about lack of regulation and safety. On-demand transportation services seem likely to be a major force towards self-driving cars.\n\nCarpooling and ridesharing have long been seen as a promising approach to decrease traffic congestion and better utilize personal transportation resources. Services such as Zimride and Nuride bring together people sharing similar routes for a joint trip. But this approach to carpooling has failed to gain traction on a large scale.", "doc_id": "stone2022", "page": 23, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "AI is likely to have an increasing impact on city infrastructure. Accurate predictive models of individuals’ movements, their preferences, and their goals are likely to emerge with the greater availability of data. The ethical issues regarding such an emergence are discussed in Section III of this report.\n\nThe United States Department of Transportation released a call for proposals in 2016 asking medium-size cities to imagine smart city infrastructure for transportation.55 This initiative plans to award forty million dollars to a city to demonstrate how technology and data can be used to reimagine the movement of people as well as goods.\n\nOne vision is a network of connected vehicles that can reach a high level of safety in driving with car-to-car communication.56 If this vision becomes reality, we expect advances in multi-agent coordination, collaboration, and planning will have a significant impact on future cars and play a role in making the transportation system more reliable and efficient. Robots are also likely to take part in transportation by carrying individuals and packages (c.f., Segway robot). For transportation of goods, interest in drones has increased, and Amazon is currently testing a delivery system using them,57 although questions remain about the appropriate safety rules and regulations.\n\nThe increased sensing capabilities, adoption of drones, and the connected transportation infrastructure will also raise concerns about the privacy of individuals and the safety of private data. In coming years, these and related transportation issues will need to be addressed either by preemptive action on the part of industry or within the legal framework. As noted in the Section III policy discussion, how well this is done will affect the pace and scope of AI-related advances in the transportation sector.\n\nOn-demand transportation\n\nOn-demand transportation services such as Uber and Lyft have emerged as another pivotal application of sensing, connectivity, and AI,58 with algorithms for matching drivers to passengers by location and suitability (reputation modeling).59 60 Through dynamic pricing, these services ration access by willingness-to-pay, with dynamic pricing also encouraging an increase in the supply of drivers, and have become a popular method for transportation in cities. With their rapid advance have come multiple policy and legal issues, such as competition with existing taxi services and concerns about lack of regulation and safety. On-demand transportation services seem likely to be a major force towards self-driving cars.\n\nCarpooling and ridesharing have long been seen as a promising approach to decrease traffic congestion and better utilize personal transportation resources. Services such as Zimride and Nuride bring together people sharing similar routes for a joint trip. But this approach to carpooling has failed to gain traction on a large scale.", "original_types": ["text", "header"], "id": 117}
{"type": "section", "content": "Interacting with people\n\nFor decades, people have imagined wildly different, futuristic-looking transportation vehicles. Although future cars will be smarter and drones will be available widely, it is unlikely that by 2030 we will have widely adopted transportation vehicles that look and function differently than the ones we have today. Our Study Panel doesn’t expect drones that can fly, swim, and drive, or flying quadcopters to become a common means of transportation in this time horizon (although prototypes exist today).\n\nWe do expect humans to become partners to self-driving cars and drones in their training, execution, and evaluation. This partnering will happen both when humans are co-located with machines and also virtually. We predict advances in algorithms to facilitate machine learning from human input. We also expect models and algorithms for modeling of human attention, and to support communication and coordination between humans and machine. This is an integral part of the development of future vehicles.\n\nOver the next fifteen years, coincident advances in mechanical and AI technologies promise to increase the safe and reliable use and utility of home robots in a typical North American city.\n\nRobots have entered people’s homes in the past fifteen years. Disappointingly slow growth in the diversity of applications has occurred simultaneously with increasingly sophisticated AI deployed on existing applications. AI advances are often inspired by mechanical innovations, which in turn prompt new AI techniques to be introduced.\n\nOver the next fifteen years, coincident advances in mechanical and AI technologies promise to increase the safe and reliable use and utility of home robots in a typical North American city. Special purpose robots will deliver packages, clean offices, and enhance security, but technical constraints and the high costs of reliable mechanical devices will continue to limit commercial opportunities to narrowly defined applications for the foreseeable future. As with self-driving cars and other new transportation machines, the difficulty of creating reliable, market-ready hardware is not to be underestimated.\n\nHOME/SERVICE ROBOTS\n\nVacuum cleaners\n\nIn 2001, after many years of development, the Electrolux Trilobite, a vacuum cleaning robot, became the first commercial home robot. It had a simple control system to do obstacle avoidance, and some navigation. A year later, iRobot introduced Roomba, which was a tenth the price of the Trilobite and, with only 512 bytes of RAM, ran a behavior based controller. The most intelligent thing it did was to avoid falling down stairs. Since then, sixteen million Roombas have been deployed all over the world and several other competing brands now exist.", "doc_id": "stone2022", "page": 24, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Interacting with people\n\nFor decades, people have imagined wildly different, futuristic-looking transportation vehicles. Although future cars will be smarter and drones will be available widely, it is unlikely that by 2030 we will have widely adopted transportation vehicles that look and function differently than the ones we have today. Our Study Panel doesn’t expect drones that can fly, swim, and drive, or flying quadcopters to become a common means of transportation in this time horizon (although prototypes exist today).\n\nWe do expect humans to become partners to self-driving cars and drones in their training, execution, and evaluation. This partnering will happen both when humans are co-located with machines and also virtually. We predict advances in algorithms to facilitate machine learning from human input. We also expect models and algorithms for modeling of human attention, and to support communication and coordination between humans and machine. This is an integral part of the development of future vehicles.\n\nOver the next fifteen years, coincident advances in mechanical and AI technologies promise to increase the safe and reliable use and utility of home robots in a typical North American city.\n\nRobots have entered people’s homes in the past fifteen years. Disappointingly slow growth in the diversity of applications has occurred simultaneously with increasingly sophisticated AI deployed on existing applications. AI advances are often inspired by mechanical innovations, which in turn prompt new AI techniques to be introduced.\n\nOver the next fifteen years, coincident advances in mechanical and AI technologies promise to increase the safe and reliable use and utility of home robots in a typical North American city. Special purpose robots will deliver packages, clean offices, and enhance security, but technical constraints and the high costs of reliable mechanical devices will continue to limit commercial opportunities to narrowly defined applications for the foreseeable future. As with self-driving cars and other new transportation machines, the difficulty of creating reliable, market-ready hardware is not to be underestimated.\n\nHOME/SERVICE ROBOTS\n\nVacuum cleaners\n\nIn 2001, after many years of development, the Electrolux Trilobite, a vacuum cleaning robot, became the first commercial home robot. It had a simple control system to do obstacle avoidance, and some navigation. A year later, iRobot introduced Roomba, which was a tenth the price of the Trilobite and, with only 512 bytes of RAM, ran a behavior based controller. The most intelligent thing it did was to avoid falling down stairs. Since then, sixteen million Roombas have been deployed all over the world and several other competing brands now exist.", "original_types": ["text", "header"], "id": 118}
{"type": "section", "content": "As the processing power and RAM capacity of low cost embedded processors improved from its dismal state in the year 2000, the AI capabilities of these robots also improved dramatically. Simple navigation, self-charging, and actions for dealing with full dust bins were added, followed by ability to deal with electrical cords and rug tassels, enabled by a combination of mechanical improvements and sensor based perception. More recently, the addition of full VSLAM (Visual Simultaneous Location and Mapping)— an AI technology that had been around for twenty years—has enabled the robots to build a complete 3D world model of a house as they clean, and become more efficient in their cleaning coverage.\n\nEarly expectations that many new applications would be found for home robots have not materialized. Robot vacuum cleaners are restricted to localized flat areas, while real homes have lots of single steps, and often staircases; there has been very little research on robot mobility inside real homes. Hardware platforms remain challenging to build, and there are few applications that people want enough to buy. Perceptual algorithms", "doc_id": "stone2022", "page": 24, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "As the processing power and RAM capacity of low cost embedded processors improved from its dismal state in the year 2000, the AI capabilities of these robots also improved dramatically. Simple navigation, self-charging, and actions for dealing with full dust bins were added, followed by ability to deal with electrical cords and rug tassels, enabled by a combination of mechanical improvements and sensor based perception. More recently, the addition of full VSLAM (Visual Simultaneous Location and Mapping)— an AI technology that had been around for twenty years—has enabled the robots to build a complete 3D world model of a house as they clean, and become more efficient in their cleaning coverage.\n\nEarly expectations that many new applications would be found for home robots have not materialized. Robot vacuum cleaners are restricted to localized flat areas, while real homes have lots of single steps, and often staircases; there has been very little research on robot mobility inside real homes. Hardware platforms remain challenging to build, and there are few applications that people want enough to buy. Perceptual algorithms", "original_types": ["text"], "id": 119}
{"type": "section", "content": "Home robots 2030\n\nDespite the slow growth to date of robots in the home, there are signs that this will change in the next fifteen years. Corporations such as Amazon Robotics and Uber are developing large economies of scale using various aggregation technologies. Also:\n\nSystem in Module (SiM), with a lot of System on Chip (SoC) subsystems, are now being pushed out the door by phone-chip makers (Qualcomm’s Snapdragon, Samsung’s Artik, etc.). These are better than supercomputers of less than ten years ago with eight or more sixty-four-bit cores, and specialized silicon for cryptography, camera drivers, additional DSPs, and hard silicon for certain perceptual algorithms. This means that low cost devices will be able to support much more onboard AI than we have been able to consider over the last fifteen years.\n\nCloud (“someone else’s computer”) is going to enable more rapid release of new software on home robots, and more sharing of data sets gathered in many different homes, which will in turn feed cloud-based machine learning, and then power improvements to already deployed robots.\n\nThe great advances in speech understanding and image labeling enabled by deep learning will enhance robots’ interactions with people in their homes.\n\nLow cost 3D sensors, driven by gaming platforms, have fueled work on 3D perception algorithms by thousands of researchers worldwide, which will speed the development and adoption of home and service robots.\n\nIn the past three years, low cost and safe robot arms have been introduced to hundreds of research labs around the world, sparking a new class of research on manipulation that will eventually be applicable in the home, perhaps around 2025. More than half a dozen startups around the world are developing AI-based robots for the home, for now concentrating mainly on social interaction. New ethics and privacy issues may surface as a result.\n\nHEALTHCARE\n\nFor AI technologies, healthcare has long been viewed as a promising domain. AI-based applications could improve health outcomes and quality of life for millions of people in the coming years—but only if they gain the trust of doctors, nurses, and patients, and if policy, regulatory, and commercial obstacles are removed. Prime applications include clinical decision support, patient monitoring and coaching, automated devices to assist in surgery or patient care, and management of healthcare systems. Recent successes, such as mining social media to infer possible health risks, machine learning to predict patients at risk, and robotics to support surgery, have expanded a sense of possibility for AI in healthcare. Improvements in methods for interacting with medical professionals and patients will be a critical challenge.", "doc_id": "stone2022", "page": 25, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Home robots 2030\n\nDespite the slow growth to date of robots in the home, there are signs that this will change in the next fifteen years. Corporations such as Amazon Robotics and Uber are developing large economies of scale using various aggregation technologies. Also:\n\nSystem in Module (SiM), with a lot of System on Chip (SoC) subsystems, are now being pushed out the door by phone-chip makers (Qualcomm’s Snapdragon, Samsung’s Artik, etc.). These are better than supercomputers of less than ten years ago with eight or more sixty-four-bit cores, and specialized silicon for cryptography, camera drivers, additional DSPs, and hard silicon for certain perceptual algorithms. This means that low cost devices will be able to support much more onboard AI than we have been able to consider over the last fifteen years.\n\nCloud (“someone else’s computer”) is going to enable more rapid release of new software on home robots, and more sharing of data sets gathered in many different homes, which will in turn feed cloud-based machine learning, and then power improvements to already deployed robots.\n\nThe great advances in speech understanding and image labeling enabled by deep learning will enhance robots’ interactions with people in their homes.\n\nLow cost 3D sensors, driven by gaming platforms, have fueled work on 3D perception algorithms by thousands of researchers worldwide, which will speed the development and adoption of home and service robots.\n\nIn the past three years, low cost and safe robot arms have been introduced to hundreds of research labs around the world, sparking a new class of research on manipulation that will eventually be applicable in the home, perhaps around 2025. More than half a dozen startups around the world are developing AI-based robots for the home, for now concentrating mainly on social interaction. New ethics and privacy issues may surface as a result.\n\nHEALTHCARE\n\nFor AI technologies, healthcare has long been viewed as a promising domain. AI-based applications could improve health outcomes and quality of life for millions of people in the coming years—but only if they gain the trust of doctors, nurses, and patients, and if policy, regulatory, and commercial obstacles are removed. Prime applications include clinical decision support, patient monitoring and coaching, automated devices to assist in surgery or patient care, and management of healthcare systems. Recent successes, such as mining social media to infer possible health risks, machine learning to predict patients at risk, and robotics to support surgery, have expanded a sense of possibility for AI in healthcare. Improvements in methods for interacting with medical professionals and patients will be a critical challenge.", "original_types": ["text", "header"], "id": 120}
{"type": "section", "content": "As in other domains, data is a key enabler. There has been an immense forward leap in collecting useful data from personal monitoring devices and mobile apps, from electronic health records (EHR) in clinical settings and, to a lesser extent, from robots designed to assist with medical procedures and hospital operations. But using this data to enable more finely-grained diagnostics and treatments for both individual patients and patient populations has proved difficult. Research and deployment have been slowed by outdated regulations and incentive structures. Poor human-computer interaction methods and the inherent difficulties and risks of implementing technologies in such a large and complex system have slowed realization of AI’s", "doc_id": "stone2022", "page": 25, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "As in other domains, data is a key enabler. There has been an immense forward leap in collecting useful data from personal monitoring devices and mobile apps, from electronic health records (EHR) in clinical settings and, to a lesser extent, from robots designed to assist with medical procedures and hospital operations. But using this data to enable more finely-grained diagnostics and treatments for both individual patients and patient populations has proved difficult. Research and deployment have been slowed by outdated regulations and incentive structures. Poor human-computer interaction methods and the inherent difficulties and risks of implementing technologies in such a large and complex system have slowed realization of AI’s", "original_types": ["text"], "id": 121}
{"type": "section", "content": "AI-based applications could improve health outcomes and quality of life for millions of people in the coming years—but only if they gain the trust of doctors, nurses, and patients.\n\nFor decades, the vision of an AI-powered clinician’s assistant has been a near cliché. Although there have been successful pilots of AI-related technology in healthcare,62 the current healthcare delivery system unfortunately remains structurally ill-suited to absorb and deploy rapid advances. Incentives provided by the Affordable Care Act have accelerated the penetration of electronic health records (EHRs) into clinical practice, but implementation has been poor, eroding clinicians’ confidence in their usefulness. A small group of companies control the EHR market, and user interfaces are widely considered substandard, including annoying pop-ups that physicians routinely dismiss. The promise of new analytics using data from EHRs, including AI, remains largely unrealized due to these and other regulatory and structural barriers. Looking ahead to the next fifteen years, AI advances, if coupled with sufficient data and well-targeted systems, promise to change the cognitive tasks assigned to human clinicians. Physicians now routinely solicit verbal descriptions of symptoms from presenting patients and, in their heads, correlate patterns against the clinical presentation of known diseases. With automated assistance, the physician could instead supervise this process, applying her or his experience and intuition to guide the input process and to evaluate the output of the machine intelligence. The literal “hands-on” experience of the physician will remain critical. A significant challenge is to optimally integrate the human dimensions of care with automated reasoning processes. To achieve future advances, clinicians must be involved and engaged at the outset to ensure that systems are well-engineered and trusted. Already, a new generation of more tech savvy physicians routinely utilize specialized apps on mobile devices. At the same time, workloads on primary care clinicians have increased to the point that they are grateful for help from any quarter. Thus, the opportunity to exploit new learning methods, to create structured patterns of inference by mining the scientific literature automatically, and to create true cognitive assistants by supporting free-form dialogue, has never been greater. Provided these advances are not stymied by regulatory, legal, and social barriers, immense improvements to the value of healthcare are within our grasp.\n\nHealthcare analytics", "doc_id": "stone2022", "page": 26, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "AI-based applications could improve health outcomes and quality of life for millions of people in the coming years—but only if they gain the trust of doctors, nurses, and patients.\n\nFor decades, the vision of an AI-powered clinician’s assistant has been a near cliché. Although there have been successful pilots of AI-related technology in healthcare,62 the current healthcare delivery system unfortunately remains structurally ill-suited to absorb and deploy rapid advances. Incentives provided by the Affordable Care Act have accelerated the penetration of electronic health records (EHRs) into clinical practice, but implementation has been poor, eroding clinicians’ confidence in their usefulness. A small group of companies control the EHR market, and user interfaces are widely considered substandard, including annoying pop-ups that physicians routinely dismiss. The promise of new analytics using data from EHRs, including AI, remains largely unrealized due to these and other regulatory and structural barriers. Looking ahead to the next fifteen years, AI advances, if coupled with sufficient data and well-targeted systems, promise to change the cognitive tasks assigned to human clinicians. Physicians now routinely solicit verbal descriptions of symptoms from presenting patients and, in their heads, correlate patterns against the clinical presentation of known diseases. With automated assistance, the physician could instead supervise this process, applying her or his experience and intuition to guide the input process and to evaluate the output of the machine intelligence. The literal “hands-on” experience of the physician will remain critical. A significant challenge is to optimally integrate the human dimensions of care with automated reasoning processes. To achieve future advances, clinicians must be involved and engaged at the outset to ensure that systems are well-engineered and trusted. Already, a new generation of more tech savvy physicians routinely utilize specialized apps on mobile devices. At the same time, workloads on primary care clinicians have increased to the point that they are grateful for help from any quarter. Thus, the opportunity to exploit new learning methods, to create structured patterns of inference by mining the scientific literature automatically, and to create true cognitive assistants by supporting free-form dialogue, has never been greater. Provided these advances are not stymied by regulatory, legal, and social barriers, immense improvements to the value of healthcare are within our grasp.\n\nHealthcare analytics", "original_types": ["text", "header"], "id": 122}
{"type": "section", "content": "At the population level, AI’s ability to mine outcomes from millions of patient clinical records promises to enable finer-grained, more personalized diagnosis and treatment. Automated discovery of genotype-phenotype connections will also become possible as full, once-in-a-lifetime genome sequencing becomes routine for each patient. A related (and perhaps earlier) capability will be to find “patients like mine” as a way to inform treatment decisions based on analysis of a similar cohort. Traditional and non-traditional healthcare data, augmented by social platforms, may lead to the emergence of self-defined subpopulations, each managed by a surrounding ecosystem of healthcare providers augmented with automated recommendation and monitoring systems. These developments have the potential to radically transform healthcare", "doc_id": "stone2022", "page": 26, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "At the population level, AI’s ability to mine outcomes from millions of patient clinical records promises to enable finer-grained, more personalized diagnosis and treatment. Automated discovery of genotype-phenotype connections will also become possible as full, once-in-a-lifetime genome sequencing becomes routine for each patient. A related (and perhaps earlier) capability will be to find “patients like mine” as a way to inform treatment decisions based on analysis of a similar cohort. Traditional and non-traditional healthcare data, augmented by social platforms, may lead to the emergence of self-defined subpopulations, each managed by a surrounding ecosystem of healthcare providers augmented with automated recommendation and monitoring systems. These developments have the potential to radically transform healthcare", "original_types": ["text"], "id": 123}
{"type": "section", "content": "delivery as medical procedures and lifetime clinical records for hundreds of millions of individuals become available. Similarly, the automated capture of personal environmental data from wearable devices will expand personalized medicine. These activities are becoming more commercially viable as vendors discover ways to engage large populations (e.g. ShareCare)63 and then to create population-scale data that can be mined to produce individualized analytics and recommendations.\n\nUnfortunately, the FDA has been slow to approve innovative diagnostic software, and there are many remaining barriers to rapid innovation. HIPAA (Health Insurance Portability and Accountability Act) requirements for protecting patient privacy create legal barriers to the flow of patient data to applications that could utilize AI technologies. Unanticipated negative effects of approved drugs could show up routinely, sooner, and more rigorously than they do today, but mobile apps that analyze drug interactions may be blocked from pulling the necessary information from patient records. More generally, AI research and innovation in healthcare are hampered by the lack of widely accepted methods and standards for privacy protection. The FDA has been slow to approve innovative software, in part due to an unclear understanding of the cost/benefit tradeoffs of these systems. If regulators (principally the FDA) recognize that effective post-marketing reporting is a dependable hedge against some safety risks, faster initial approval of new treatments and interventions may become possible.\n\nAutomated image interpretation has also been a promising subject of study for decades. Progress on interpreting large archives of weakly-labeled images, such as large photo archives scraped from the web, has been explosive. At first blush, it is surprising that there has not been a similar revolution in interpretation of medical images. Most medical imaging modalities (CT, MR, ultrasound) are inherently digital, the images are all archived, and there are large, established companies with internal R&D (e.g. Siemens, Philips, GE) devoted to imaging.\n\nBut several barriers have limited progress to date. Most hospital image archives have only gone digital over the past decade. More importantly, the problem in medicine is not to recognize what is in the image (is this a liver or a kidney?), but rather to make a fine-grained judgement about it (does the slightly darker smudge in the liver suggest a potentially cancerous tumor?). Strict regulations govern these high-stakes judgements. Even with state-of-the-art technologies, a radiologist will still likely have to look at the images, so the value proposition is not yet compelling. Also, healthcare regulations preclude easy federation of data across institutions. Thus, only very large organizations of integrated care, such as Kaiser Permanente, are able to attack these problems.", "doc_id": "stone2022", "page": 27, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "delivery as medical procedures and lifetime clinical records for hundreds of millions of individuals become available. Similarly, the automated capture of personal environmental data from wearable devices will expand personalized medicine. These activities are becoming more commercially viable as vendors discover ways to engage large populations (e.g. ShareCare)63 and then to create population-scale data that can be mined to produce individualized analytics and recommendations.\n\nUnfortunately, the FDA has been slow to approve innovative diagnostic software, and there are many remaining barriers to rapid innovation. HIPAA (Health Insurance Portability and Accountability Act) requirements for protecting patient privacy create legal barriers to the flow of patient data to applications that could utilize AI technologies. Unanticipated negative effects of approved drugs could show up routinely, sooner, and more rigorously than they do today, but mobile apps that analyze drug interactions may be blocked from pulling the necessary information from patient records. More generally, AI research and innovation in healthcare are hampered by the lack of widely accepted methods and standards for privacy protection. The FDA has been slow to approve innovative software, in part due to an unclear understanding of the cost/benefit tradeoffs of these systems. If regulators (principally the FDA) recognize that effective post-marketing reporting is a dependable hedge against some safety risks, faster initial approval of new treatments and interventions may become possible.\n\nAutomated image interpretation has also been a promising subject of study for decades. Progress on interpreting large archives of weakly-labeled images, such as large photo archives scraped from the web, has been explosive. At first blush, it is surprising that there has not been a similar revolution in interpretation of medical images. Most medical imaging modalities (CT, MR, ultrasound) are inherently digital, the images are all archived, and there are large, established companies with internal R&D (e.g. Siemens, Philips, GE) devoted to imaging.\n\nBut several barriers have limited progress to date. Most hospital image archives have only gone digital over the past decade. More importantly, the problem in medicine is not to recognize what is in the image (is this a liver or a kidney?), but rather to make a fine-grained judgement about it (does the slightly darker smudge in the liver suggest a potentially cancerous tumor?). Strict regulations govern these high-stakes judgements. Even with state-of-the-art technologies, a radiologist will still likely have to look at the images, so the value proposition is not yet compelling. Also, healthcare regulations preclude easy federation of data across institutions. Thus, only very large organizations of integrated care, such as Kaiser Permanente, are able to attack these problems.", "original_types": ["text"], "id": 124}
{"type": "section", "content": "Still, automated/augmented image interpretation has started to gain momentum. The next fifteen years will probably not bring fully automated radiology, but initial forays into image “triage” or second level checking will likely improve the speed and cost-effectiveness of medical imaging. When coupled with electronic patient record systems, large-scale machine learning techniques could be applied to medical image data. For example, multiple major healthcare systems have archives of millions of patient scans, each of which has an associated radiological report, and most have an associated patient record. Already, papers are appearing in the literature showing that deep neural networks can be trained to produce basic radiological findings, with high reliability, by training from this data.64", "doc_id": "stone2022", "page": 27, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Still, automated/augmented image interpretation has started to gain momentum. The next fifteen years will probably not bring fully automated radiology, but initial forays into image “triage” or second level checking will likely improve the speed and cost-effectiveness of medical imaging. When coupled with electronic patient record systems, large-scale machine learning techniques could be applied to medical image data. For example, multiple major healthcare systems have archives of millions of patient scans, each of which has an associated radiological report, and most have an associated patient record. Already, papers are appearing in the literature showing that deep neural networks can be trained to produce basic radiological findings, with high reliability, by training from this data.64", "original_types": ["text"], "id": 125}
{"type": "section", "content": "Healthcare robotics\n\nFifteen years ago, healthcare robotics was largely science fiction. One company called Robodoc,65 a spin-out from IBM, developed robotic systems for orthopedic surgeries, such as hip and knee replacements. The technology worked, but the company struggled commercially, and was ultimately shut down and acquired for its technology.66 More recently, though, the research and practical use of surgical robotics has exploded.\n\nIn 2000 Intuitive Surgical67 introduced the da Vinci system, a novel technology initially marketed to support minimally invasive heart bypass surgery, and then gained substantial market traction for treatment of prostate cancer and merged with its only major competition, Computer Motion, in 2003. The da Vinci, now in its fourth generation, provides 3D visualization (as opposed to 2D monocular laparoscopy) and wristed instruments in an ergonomic platform. It is considered the standard of care in multiple laparoscopic procedures, and used in nearly three quarters of a million procedures a year,68 providing not only a physical platform, but also a new data platform for studying the process of surgery.\n\nThe da Vinci anticipates a day when much greater insight into how medical professionals carry out the process of providing interventional medical care will be possible. The presence of the da Vinci in day-to-day operation has also opened the doors to new types of innovation—from new instrumentation to image fusion to novel biomarkers—creating its own innovation ecosystem. The success of the platform has inspired potential competitors in robotic surgery, most notably the Alphabet spin-off Verb, in collaboration with J&J/Ethicon.69 There are likely to be many more, each exploring a unique niche or space and building out an ecosystem of sensing, data analytics, augmentation, and automation.\n\nIntelligent automation in hospital operations has been less successful. The story is not unlike surgical robotics. Twenty years ago, one company, HelpMate, created a robot for hospital deliveries,70 such as meals and medical records, but ultimately went bankrupt. More recently, Aethon71 introduced TUG Robots for basic deliveries, but few hospitals have invested in this technology to date. However, robotics in other service industries such as hotels and warehouses, including Amazon Robotics (formerly Kiva), are demonstrating that these technologies are practical and cost effective in at least some large-scale settings, and may ultimately spur additional innovation in health care.\n\nLooking ahead, many tasks that appear in healthcare will be amenable to augmentation, but will not be fully automated. For example, robots may be able to deliver goods to the right room in a hospital, but then require a person to pick them up and place them in their final location. Walking a patient down the corridor may", "doc_id": "stone2022", "page": 28, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Healthcare robotics\n\nFifteen years ago, healthcare robotics was largely science fiction. One company called Robodoc,65 a spin-out from IBM, developed robotic systems for orthopedic surgeries, such as hip and knee replacements. The technology worked, but the company struggled commercially, and was ultimately shut down and acquired for its technology.66 More recently, though, the research and practical use of surgical robotics has exploded.\n\nIn 2000 Intuitive Surgical67 introduced the da Vinci system, a novel technology initially marketed to support minimally invasive heart bypass surgery, and then gained substantial market traction for treatment of prostate cancer and merged with its only major competition, Computer Motion, in 2003. The da Vinci, now in its fourth generation, provides 3D visualization (as opposed to 2D monocular laparoscopy) and wristed instruments in an ergonomic platform. It is considered the standard of care in multiple laparoscopic procedures, and used in nearly three quarters of a million procedures a year,68 providing not only a physical platform, but also a new data platform for studying the process of surgery.\n\nThe da Vinci anticipates a day when much greater insight into how medical professionals carry out the process of providing interventional medical care will be possible. The presence of the da Vinci in day-to-day operation has also opened the doors to new types of innovation—from new instrumentation to image fusion to novel biomarkers—creating its own innovation ecosystem. The success of the platform has inspired potential competitors in robotic surgery, most notably the Alphabet spin-off Verb, in collaboration with J&J/Ethicon.69 There are likely to be many more, each exploring a unique niche or space and building out an ecosystem of sensing, data analytics, augmentation, and automation.\n\nIntelligent automation in hospital operations has been less successful. The story is not unlike surgical robotics. Twenty years ago, one company, HelpMate, created a robot for hospital deliveries,70 such as meals and medical records, but ultimately went bankrupt. More recently, Aethon71 introduced TUG Robots for basic deliveries, but few hospitals have invested in this technology to date. However, robotics in other service industries such as hotels and warehouses, including Amazon Robotics (formerly Kiva), are demonstrating that these technologies are practical and cost effective in at least some large-scale settings, and may ultimately spur additional innovation in health care.\n\nLooking ahead, many tasks that appear in healthcare will be amenable to augmentation, but will not be fully automated. For example, robots may be able to deliver goods to the right room in a hospital, but then require a person to pick them up and place them in their final location. Walking a patient down the corridor may", "original_types": ["text", "header"], "id": 126}
{"type": "section", "content": "be relatively simple once a patient is standing in a walker (though will certainly not be trivial for patients recovering from surgery and/or elderly patients, especially in corridors crowded with equipment and other people). Driving a needle to place a suture is relatively straightforward once the needle is correctly placed.72 This implies that many future systems will involve intimate interaction between people and machines and require technologies that facilitate collaboration between them.\n\nThe growth of automation will enable new insights into healthcare processes. Historically, robotics has not been a strongly data-driven or data-oriented science. This is changing as (semi)automation infiltrates healthcare. As the new surgical, delivery, and patient care platforms come online, the beginnings of quantification and predictive analytics are being built on top of data coming from these platforms.73 This data will be used to assess quality of performance, identify deficiencies, errors, or potential optimizations, and will be used as feedback to improve performance. In short, these platforms will facilitate making the connection between what is done, and the outcome achieved, making true “closed-loop” medicine a real possibility.\n\nMobile health\n\nTo date, evidence-driven analytics on healthcare have relied on traditional healthcare data—mainly the electronic medical records discussed above. In the clinical setting, there are hopeful trends towards bringing new data to bear. For example, Tele-Language enables a human clinician to conduct language therapy sessions with multiple patients simultaneously with the aid of an AI agent trained by the clinician. And Lifegraph, which extracts behavioral patterns and creates alerts from data passively collected from a patient’s smartphone, has been adopted by psychiatrists in Israel to detect early signs of distressful behavior in patients.\n\nLooking ahead, driven by the mobile computing revolution, the astonishing growth of “biometrics in the wild”—and the explosion of platforms and applications that use them—is a hopeful and unanticipated trend. Thousands of mobile apps now offer information, introduce behavior modification, or identify groups of “people like me.” This, combined with the emerging trend of more specialized motion tracking devices, such as Fitbit, and the emerging (inter)connectedness between the home environment and health-monitoring devices, has created a vibrant new sector of innovation.\n\nBy combining social and healthcare data, some healthcare apps can perform data mining, learning, and prediction from captured data, though their predictions are relatively rudimentary. The convergence of data and functionality across applications will likely spur new and even obvious products, such as an exercise app that not only proposes a schedule for exercise but also suggests the best time to do it, and provides coaching to stick to that schedule.", "doc_id": "stone2022", "page": 29, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "be relatively simple once a patient is standing in a walker (though will certainly not be trivial for patients recovering from surgery and/or elderly patients, especially in corridors crowded with equipment and other people). Driving a needle to place a suture is relatively straightforward once the needle is correctly placed.72 This implies that many future systems will involve intimate interaction between people and machines and require technologies that facilitate collaboration between them.\n\nThe growth of automation will enable new insights into healthcare processes. Historically, robotics has not been a strongly data-driven or data-oriented science. This is changing as (semi)automation infiltrates healthcare. As the new surgical, delivery, and patient care platforms come online, the beginnings of quantification and predictive analytics are being built on top of data coming from these platforms.73 This data will be used to assess quality of performance, identify deficiencies, errors, or potential optimizations, and will be used as feedback to improve performance. In short, these platforms will facilitate making the connection between what is done, and the outcome achieved, making true “closed-loop” medicine a real possibility.\n\nMobile health\n\nTo date, evidence-driven analytics on healthcare have relied on traditional healthcare data—mainly the electronic medical records discussed above. In the clinical setting, there are hopeful trends towards bringing new data to bear. For example, Tele-Language enables a human clinician to conduct language therapy sessions with multiple patients simultaneously with the aid of an AI agent trained by the clinician. And Lifegraph, which extracts behavioral patterns and creates alerts from data passively collected from a patient’s smartphone, has been adopted by psychiatrists in Israel to detect early signs of distressful behavior in patients.\n\nLooking ahead, driven by the mobile computing revolution, the astonishing growth of “biometrics in the wild”—and the explosion of platforms and applications that use them—is a hopeful and unanticipated trend. Thousands of mobile apps now offer information, introduce behavior modification, or identify groups of “people like me.” This, combined with the emerging trend of more specialized motion tracking devices, such as Fitbit, and the emerging (inter)connectedness between the home environment and health-monitoring devices, has created a vibrant new sector of innovation.\n\nBy combining social and healthcare data, some healthcare apps can perform data mining, learning, and prediction from captured data, though their predictions are relatively rudimentary. The convergence of data and functionality across applications will likely spur new and even obvious products, such as an exercise app that not only proposes a schedule for exercise but also suggests the best time to do it, and provides coaching to stick to that schedule.", "original_types": ["text", "header"], "id": 127}
{"type": "section", "content": "Elder care\n\nOver the next fifteen years the number of elderly in the United States will grow by over 50%.74 The National Bureau of Labor Statistics projects that home health aides will grow 38% over the next ten years. Despite the broad opportunities in this domain—basic social support, interaction and communication devices, home health monitoring, a variety of simple in-home physical aids such as walkers, and light meal preparation—little has happened over the past fifteen years. But the coming generational shift will accompany a change in technology acceptance among the elderly. Currently, someone who is seventy was born in 1946 and may have first experienced some form of personalized IT in middle age or later, while a fifty-year-old today is far more technology-friendly and savvy. As a result, there will be a growing interest and market for already available and maturing technologies to support physical, emotional, social, and mental health. Here are a few likely examples by category:\n\nLife quality and independence\n\nAutomated transportation will support continued independence and expanded social horizons.\nSharing of information will help families remain engaged with one another at a distance, and predictive analytics may be used to “nudge” family groups toward positive behaviors, such as reminders to “call home.”\nSmart devices in the home will help with daily living activities when needed, such as cooking and, if robot manipulation capabilities improve sufficiently, dressing and toileting.\n\nHealth and wellness\n\nMobile applications that monitor movement and activities, coupled with social platforms, will be able to make recommendations to maintain mental and physical health.\nIn-home health monitoring and health information access will be able to detect changes in mood or behavior and alert caregivers.\nPersonalized health management will help mitigate the complexities associated with multiple co-morbid conditions and/or treatment interactions.\n\nTreatments and devices", "doc_id": "stone2022", "page": 30, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Elder care\n\nOver the next fifteen years the number of elderly in the United States will grow by over 50%.74 The National Bureau of Labor Statistics projects that home health aides will grow 38% over the next ten years. Despite the broad opportunities in this domain—basic social support, interaction and communication devices, home health monitoring, a variety of simple in-home physical aids such as walkers, and light meal preparation—little has happened over the past fifteen years. But the coming generational shift will accompany a change in technology acceptance among the elderly. Currently, someone who is seventy was born in 1946 and may have first experienced some form of personalized IT in middle age or later, while a fifty-year-old today is far more technology-friendly and savvy. As a result, there will be a growing interest and market for already available and maturing technologies to support physical, emotional, social, and mental health. Here are a few likely examples by category:\n\nLife quality and independence\n\nAutomated transportation will support continued independence and expanded social horizons.\nSharing of information will help families remain engaged with one another at a distance, and predictive analytics may be used to “nudge” family groups toward positive behaviors, such as reminders to “call home.”\nSmart devices in the home will help with daily living activities when needed, such as cooking and, if robot manipulation capabilities improve sufficiently, dressing and toileting.\n\nHealth and wellness\n\nMobile applications that monitor movement and activities, coupled with social platforms, will be able to make recommendations to maintain mental and physical health.\nIn-home health monitoring and health information access will be able to detect changes in mood or behavior and alert caregivers.\nPersonalized health management will help mitigate the complexities associated with multiple co-morbid conditions and/or treatment interactions.\n\nTreatments and devices", "original_types": ["text", "header"], "id": 128}
{"type": "section", "content": "Better hearing aids and visual assistive devices will mitigate the effects of hearing and vision loss, improving safety and social connection.\nPersonalized rehabilitation and in-home therapy will reduce the need for hospital or care facility stays.\nPhysical assistive devices (intelligent walkers, wheel chairs, and exoskeletons) will extend the range of activities of an infirm individual.\nThe Study Panel expects an explosion of low-cost sensing technologies that can provide substantial capabilities to the elderly in their homes. In principle, social agents with a physical presence and simple physical capabilities (e.g. a mobile robot with basic communication capabilities) could provide a platform for new innovations. However, doing so will require integration across multiple areas of AI—Natural Language Processing, reasoning, learning, perception, and robotics—to create a system that is useful and usable by the elderly.\nThese innovations will also introduce questions regarding privacy within various circles, including friends, family, and care-givers, and create new challenges to accommodate an evermore active and engaged population far past retirement.", "doc_id": "stone2022", "page": 30, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Better hearing aids and visual assistive devices will mitigate the effects of hearing and vision loss, improving safety and social connection.\nPersonalized rehabilitation and in-home therapy will reduce the need for hospital or care facility stays.\nPhysical assistive devices (intelligent walkers, wheel chairs, and exoskeletons) will extend the range of activities of an infirm individual.\nThe Study Panel expects an explosion of low-cost sensing technologies that can provide substantial capabilities to the elderly in their homes. In principle, social agents with a physical presence and simple physical capabilities (e.g. a mobile robot with basic communication capabilities) could provide a platform for new innovations. However, doing so will require integration across multiple areas of AI—Natural Language Processing, reasoning, learning, perception, and robotics—to create a system that is useful and usable by the elderly.\nThese innovations will also introduce questions regarding privacy within various circles, including friends, family, and care-givers, and create new challenges to accommodate an evermore active and engaged population far past retirement.", "original_types": ["text"], "id": 129}
{"type": "section", "content": "EDUCATION\n\nThe past fifteen years have seen considerable AI advances in education. Applications are in wide use by educators and learners today, with some variation between K-12 and university settings. Though quality education will always require active engagement by human teachers, AI promises to enhance education at all levels, especially by providing personalization at scale. Similar to healthcare, resolving how to best integrate human interaction and face-to-face learning with promising AI technologies remains a key challenge.\n\nRobots have long been popular educational devices, starting with the early Lego Mindstorms kits developed with the MIT Media Lab in the 1980s. Intelligent Tutoring Systems (ITS) for science, math, language, and other disciplines match students with interactive machine tutors. Natural Language Processing, especially when combined with machine learning and crowdsourcing, has boosted online learning and enabled teachers to multiply the size of their classrooms while simultaneously addressing individual students’ learning needs and styles. The data sets from large online learning systems have fueled rapid growth in learning analytics.\n\nStill, schools and universities have been slow in adopting AI technologies primarily due to lack of funds and lack of solid evidence that they help students achieve learning objectives. Over the next fifteen years in a typical North American city, the use of intelligent tutors and other AI technologies to assist teachers in the classroom and in the home is likely to expand significantly, as will learning based on virtual reality applications. But computer-based learning systems are not likely to fully replace human teaching in schools.\n\nTeaching robots\n\nToday, more sophisticated and versatile kits for use in K-12 schools are available from a number of companies that create robots with new sensing technologies programmable in a variety of languages. Ozobot is a robot that teaches children to code and reason deductively while configuring it to dance or play based on color-coded patterns.75 Cubelets help teach children logical thinking through assembling robot blocks to think, act, or sense, depending upon the function of the different blocks.76 Wonder Workshop’s Dash and Dot span a range of programming capabilities. Children eight years old and older can create simple actions using a visual programming language, Blockly, or build iOS and Android applications using C or Java.77 PLEO rb is a robot pet that helps children learn biology by teaching the robot to react to different aspects of the environment.78 However, while fun and engaging for some, in order for such kits to become widespread, there will need to be compelling evidence that they improve students’ academic performance.\n\nIntelligent Tutoring Systems (ITS) and online learning", "doc_id": "stone2022", "page": 31, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "EDUCATION\n\nThe past fifteen years have seen considerable AI advances in education. Applications are in wide use by educators and learners today, with some variation between K-12 and university settings. Though quality education will always require active engagement by human teachers, AI promises to enhance education at all levels, especially by providing personalization at scale. Similar to healthcare, resolving how to best integrate human interaction and face-to-face learning with promising AI technologies remains a key challenge.\n\nRobots have long been popular educational devices, starting with the early Lego Mindstorms kits developed with the MIT Media Lab in the 1980s. Intelligent Tutoring Systems (ITS) for science, math, language, and other disciplines match students with interactive machine tutors. Natural Language Processing, especially when combined with machine learning and crowdsourcing, has boosted online learning and enabled teachers to multiply the size of their classrooms while simultaneously addressing individual students’ learning needs and styles. The data sets from large online learning systems have fueled rapid growth in learning analytics.\n\nStill, schools and universities have been slow in adopting AI technologies primarily due to lack of funds and lack of solid evidence that they help students achieve learning objectives. Over the next fifteen years in a typical North American city, the use of intelligent tutors and other AI technologies to assist teachers in the classroom and in the home is likely to expand significantly, as will learning based on virtual reality applications. But computer-based learning systems are not likely to fully replace human teaching in schools.\n\nTeaching robots\n\nToday, more sophisticated and versatile kits for use in K-12 schools are available from a number of companies that create robots with new sensing technologies programmable in a variety of languages. Ozobot is a robot that teaches children to code and reason deductively while configuring it to dance or play based on color-coded patterns.75 Cubelets help teach children logical thinking through assembling robot blocks to think, act, or sense, depending upon the function of the different blocks.76 Wonder Workshop’s Dash and Dot span a range of programming capabilities. Children eight years old and older can create simple actions using a visual programming language, Blockly, or build iOS and Android applications using C or Java.77 PLEO rb is a robot pet that helps children learn biology by teaching the robot to react to different aspects of the environment.78 However, while fun and engaging for some, in order for such kits to become widespread, there will need to be compelling evidence that they improve students’ academic performance.\n\nIntelligent Tutoring Systems (ITS) and online learning", "original_types": ["text", "header"], "id": 130}
{"type": "section", "content": "ITS have been developed from research laboratory projects such as Why-2 Atlas, which supported human-machine dialogue to solve physics problems early in the era.79 The rapid migration of ITS from laboratory experimental stages to real use is", "doc_id": "stone2022", "page": 31, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "ITS have been developed from research laboratory projects such as Why-2 Atlas, which supported human-machine dialogue to solve physics problems early in the era.79 The rapid migration of ITS from laboratory experimental stages to real use is", "original_types": ["text"], "id": 131}
{"type": "section", "content": "It can be argued that AI is the secret sauce that has enabled instructors, particularly in higher education, to multiply the size of their classrooms by a few orders of magnitude—class sizes of a few tens of thousands are not uncommon.\n\nDownloadable software and online systems such as Carnegie Speech or Duolingo provide foreign language training using Automatic Speech Recognition (ASR) and NLP techniques to recognize language errors and help users correct them.80 Tutoring systems such as the Carnegie Cognitive Tutor81 have been used in US high schools to help students learn mathematics. Other ITS have been developed for training in geography, circuits, medical diagnosis, computer literacy and programming, genetics, and chemistry. Cognitive tutors use software to mimic the role of a good human tutor by, for example, providing hints when a student gets stuck on a math problem. Based on the hint requested and the answer provided, the tutor offers context specific feedback.\n\nApplications are growing in higher education. An ITS called SHERLOCK82 is beginning to be used to teach Air Force technicians to diagnose electrical systems problems in aircraft. And the University of Southern California’s Information Sciences Institute has developed more advanced avatar-based training modules to train military personnel being sent to international posts in appropriate behavior when dealing with people from different cultural backgrounds. New algorithms for personalized tutoring, such as Bayesian Knowledge Tracing, enable individualized mastery learning and problem sequencing.83 Most surprising has been the explosion of the Massive Open Online Courses (MOOCs) and other models of online education at all levels—including the use of tools like Wikipedia and Khan Academy as well as sophisticated learning management systems that build in synchronous as well as asynchronous education and adaptive learning tools. Since the late 1990s, companies such as the Educational Testing Service and Pearson have been developing automatic NLP assessment tools to co-grade essays in standardized testing.84 Many of the MOOCs which have become so popular, including those created by EdX, Coursera, and Udacity, are making use of NLP, machine learning, and crowdsourcing techniques for grading short-answer and essay questions as well as programming assignments.85 Online education systems that support graduate-level professional education and lifelong learning are also expanding rapidly. These systems have great promise because the need for face-to-face interaction is less important for working professionals and career changers. While not the leaders in AI-supported systems and applications, they will become early adopters as the technologies are tested and validated.", "doc_id": "stone2022", "page": 32, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "It can be argued that AI is the secret sauce that has enabled instructors, particularly in higher education, to multiply the size of their classrooms by a few orders of magnitude—class sizes of a few tens of thousands are not uncommon.\n\nDownloadable software and online systems such as Carnegie Speech or Duolingo provide foreign language training using Automatic Speech Recognition (ASR) and NLP techniques to recognize language errors and help users correct them.80 Tutoring systems such as the Carnegie Cognitive Tutor81 have been used in US high schools to help students learn mathematics. Other ITS have been developed for training in geography, circuits, medical diagnosis, computer literacy and programming, genetics, and chemistry. Cognitive tutors use software to mimic the role of a good human tutor by, for example, providing hints when a student gets stuck on a math problem. Based on the hint requested and the answer provided, the tutor offers context specific feedback.\n\nApplications are growing in higher education. An ITS called SHERLOCK82 is beginning to be used to teach Air Force technicians to diagnose electrical systems problems in aircraft. And the University of Southern California’s Information Sciences Institute has developed more advanced avatar-based training modules to train military personnel being sent to international posts in appropriate behavior when dealing with people from different cultural backgrounds. New algorithms for personalized tutoring, such as Bayesian Knowledge Tracing, enable individualized mastery learning and problem sequencing.83 Most surprising has been the explosion of the Massive Open Online Courses (MOOCs) and other models of online education at all levels—including the use of tools like Wikipedia and Khan Academy as well as sophisticated learning management systems that build in synchronous as well as asynchronous education and adaptive learning tools. Since the late 1990s, companies such as the Educational Testing Service and Pearson have been developing automatic NLP assessment tools to co-grade essays in standardized testing.84 Many of the MOOCs which have become so popular, including those created by EdX, Coursera, and Udacity, are making use of NLP, machine learning, and crowdsourcing techniques for grading short-answer and essay questions as well as programming assignments.85 Online education systems that support graduate-level professional education and lifelong learning are also expanding rapidly. These systems have great promise because the need for face-to-face interaction is less important for working professionals and career changers. While not the leaders in AI-supported systems and applications, they will become early adopters as the technologies are tested and validated.", "original_types": ["text", "header"], "id": 132}
{"type": "section", "content": "It can be argued that AI is the secret sauce that has enabled instructors, particularly in higher education, to multiply the size of their classrooms by a few orders of magnitude—class sizes of a few tens of thousands are not uncommon. In order to continually test large classes of students, automated generation of the questions is", "doc_id": "stone2022", "page": 32, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "It can be argued that AI is the secret sauce that has enabled instructors, particularly in higher education, to multiply the size of their classrooms by a few orders of magnitude—class sizes of a few tens of thousands are not uncommon. In order to continually test large classes of students, automated generation of the questions is", "original_types": ["text"], "id": 133}
{"type": "section", "content": "Learning analytics\n\nData sets being collected from massive scale online learning systems, ranging from MOOCs to Khan Academy, as well as smaller scale online programs, have fueled the rapid growth of the field of learning analytics. Online courses are not only good for widespread delivery, but are natural vehicles for data collection and experimental instrumentation that will contribute to scientific findings and improving the quality of learning at scale. Organizations such as the Society for Learning Analytics Research (SOLAR), and the rise of conferences including the Learning Analytics and Knowledge Conference89 and the Learning at Scale Conference (L@S)90 reflect this trend. This community applies deep learning, natural language processing, and other AI techniques to analysis of student engagement, behavior, and outcomes. Current projects seek to model common student misconceptions, predict which students are at risk of failure, and provide real-time student feedback that is tightly integrated with learning outcomes. Recent work has also been devoted to understanding the cognitive processes involved in comprehension, writing, knowledge acquisition, and memory, and to applying that understanding to educational practice by developing and testing educational technologies.\n\nChallenges and opportunities\n\nOne might have expected more and more sophisticated use of AI technologies in schools, colleges, and universities by now. Much of its absence can be explained by the lack of financial resources of these institutions as well as the lack of data establishing the technologies’ effectiveness. These problems are being addressed, albeit slowly, by private foundations and by numerous programs to train primarily secondary school teachers in summer programs. As in other areas of AI, excessive hype and promises about the capabilities of MOOCs have meant that expectations frequently exceed the reality. The experiences of certain institutions, such as San Jose State University’s experiment with Udacity,91 have led to more sober assessment of the potential of the new educational technologies.", "doc_id": "stone2022", "page": 33, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Learning analytics\n\nData sets being collected from massive scale online learning systems, ranging from MOOCs to Khan Academy, as well as smaller scale online programs, have fueled the rapid growth of the field of learning analytics. Online courses are not only good for widespread delivery, but are natural vehicles for data collection and experimental instrumentation that will contribute to scientific findings and improving the quality of learning at scale. Organizations such as the Society for Learning Analytics Research (SOLAR), and the rise of conferences including the Learning Analytics and Knowledge Conference89 and the Learning at Scale Conference (L@S)90 reflect this trend. This community applies deep learning, natural language processing, and other AI techniques to analysis of student engagement, behavior, and outcomes. Current projects seek to model common student misconceptions, predict which students are at risk of failure, and provide real-time student feedback that is tightly integrated with learning outcomes. Recent work has also been devoted to understanding the cognitive processes involved in comprehension, writing, knowledge acquisition, and memory, and to applying that understanding to educational practice by developing and testing educational technologies.\n\nChallenges and opportunities\n\nOne might have expected more and more sophisticated use of AI technologies in schools, colleges, and universities by now. Much of its absence can be explained by the lack of financial resources of these institutions as well as the lack of data establishing the technologies’ effectiveness. These problems are being addressed, albeit slowly, by private foundations and by numerous programs to train primarily secondary school teachers in summer programs. As in other areas of AI, excessive hype and promises about the capabilities of MOOCs have meant that expectations frequently exceed the reality. The experiences of certain institutions, such as San Jose State University’s experiment with Udacity,91 have led to more sober assessment of the potential of the new educational technologies.", "original_types": ["text", "header"], "id": 134}
{"type": "section", "content": "While formal education will not disappear, the Study Panel believes that MOOCs and other forms of online education will become part of learning at all levels, from K-12 through university, in a blended classroom experience.\n\nIn the next fifteen years, it is likely that human teachers will be assisted by AI technologies with better human interaction, both in the classroom and in the home. The Study Panel expects that more general and more sophisticated virtual reality scenarios in which students can immerse themselves in subjects from all disciplines will be developed. Some steps in this direction are being taken now by increasing collaborations between AI researchers and researchers in the humanities and social sciences, exemplified by Stanford’s Galileo Correspondence Project92 and Columbia’s Making and Knowing Project.93 These interdisciplinary efforts create interactive experiences with historical documents and the use of Virtual Reality (VR) to explore interactive archeological sites.94 VR techniques are already being used in the natural sciences such as biology, anatomy, geology and astronomy to allow students to interact with environments and objects that are difficult to engage with in the real world. The recreation of past worlds and fictional worlds will become just as popular for studies of arts and other sciences.\n\nAI techniques will increasingly blur the line between formal, classroom education and self-paced, individual learning. Adaptive learning systems, for example, are going to become a core part of the teaching process in higher education because of the pressures to contain cost while serving a larger number of students and moving students through school more quickly. While formal education will not disappear, the Study Panel believes that MOOCs and other forms of online education will become part of learning at all levels, from K-12 through university, in a blended classroom experience. This development will facilitate more customizable approaches to learning, in which students can learn at their own pace using educational techniques that work best for them. Online education systems will learn as the students learn, supporting rapid advances in our understanding of the learning process. Learning analytics, in turn, will accelerate the development of tools for personalized education.", "doc_id": "stone2022", "page": 34, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "While formal education will not disappear, the Study Panel believes that MOOCs and other forms of online education will become part of learning at all levels, from K-12 through university, in a blended classroom experience.\n\nIn the next fifteen years, it is likely that human teachers will be assisted by AI technologies with better human interaction, both in the classroom and in the home. The Study Panel expects that more general and more sophisticated virtual reality scenarios in which students can immerse themselves in subjects from all disciplines will be developed. Some steps in this direction are being taken now by increasing collaborations between AI researchers and researchers in the humanities and social sciences, exemplified by Stanford’s Galileo Correspondence Project92 and Columbia’s Making and Knowing Project.93 These interdisciplinary efforts create interactive experiences with historical documents and the use of Virtual Reality (VR) to explore interactive archeological sites.94 VR techniques are already being used in the natural sciences such as biology, anatomy, geology and astronomy to allow students to interact with environments and objects that are difficult to engage with in the real world. The recreation of past worlds and fictional worlds will become just as popular for studies of arts and other sciences.\n\nAI techniques will increasingly blur the line between formal, classroom education and self-paced, individual learning. Adaptive learning systems, for example, are going to become a core part of the teaching process in higher education because of the pressures to contain cost while serving a larger number of students and moving students through school more quickly. While formal education will not disappear, the Study Panel believes that MOOCs and other forms of online education will become part of learning at all levels, from K-12 through university, in a blended classroom experience. This development will facilitate more customizable approaches to learning, in which students can learn at their own pace using educational techniques that work best for them. Online education systems will learn as the students learn, supporting rapid advances in our understanding of the learning process. Learning analytics, in turn, will accelerate the development of tools for personalized education.", "original_types": ["text", "header"], "id": 135}
{"type": "section", "content": "The current transition from hard copy books to digital and audio media and texts is likely to become prevalent in education as well. Digital reading devices will also become much ‘smarter’, providing students with easy access to additional information about subject matter as they study. Machine Translation (MT) technology will also make it easier to translate educational material into different languages with a fair degree of accuracy, just as it currently translates technical manuals. Textbook translation services that currently depend only upon human translators will increasingly incorporate automatic methods to improve the speed and affordability of their services for school systems.\n\nOnline learning systems will also expand the opportunity for adults and working professionals to enhance their knowledge and skills (or to retool and learn a new field) in a world where these fields are evolving rapidly. This will include the expansion of fully online professional degrees as well as professional certifications based on online coursework.\n\nBroader societal consequences\n\nIn countries where education is difficult for the broad population to obtain, online resources may have a positive effect if the population has the tools to access them. The development of online educational resources should make it easier for foundations that support international educational programs to provide quality", "doc_id": "stone2022", "page": 34, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "The current transition from hard copy books to digital and audio media and texts is likely to become prevalent in education as well. Digital reading devices will also become much ‘smarter’, providing students with easy access to additional information about subject matter as they study. Machine Translation (MT) technology will also make it easier to translate educational material into different languages with a fair degree of accuracy, just as it currently translates technical manuals. Textbook translation services that currently depend only upon human translators will increasingly incorporate automatic methods to improve the speed and affordability of their services for school systems.\n\nOnline learning systems will also expand the opportunity for adults and working professionals to enhance their knowledge and skills (or to retool and learn a new field) in a world where these fields are evolving rapidly. This will include the expansion of fully online professional degrees as well as professional certifications based on online coursework.\n\nBroader societal consequences\n\nIn countries where education is difficult for the broad population to obtain, online resources may have a positive effect if the population has the tools to access them. The development of online educational resources should make it easier for foundations that support international educational programs to provide quality", "original_types": ["text", "header"], "id": 136}
{"type": "section", "content": "LOW-RESOURCE COMMUNITIES\n\nMany opportunities exist for AI to improve conditions for people in low-resource communities in a typical North American city—and, indeed, in some cases it already has. Understanding these direct AI contributions may also inform potential contributions in the poorest parts of the developing world. There has not been a significant focus on these populations in AI gatherings, and, traditionally, AI funders have underinvested in research lacking commercial application. With targeted incentives and funding priorities, AI technologies could help address the needs of low-resource communities. Budding efforts are promising. Counteracting fears that AI may contribute to joblessness and other societal problems, AI may provide mitigations and solutions, particularly if implemented in ways that build trust in them by the affected communities.\n\nMachine learning, data mining approaches\n\nUnder the banner of “data science for social good,” AI has been used to create predictive models to help government agencies more effectively use their limited budgets to address problems such as lead poisoning,97 a major public health concern that has been in the news due to ongoing events in Flint, Michigan. Children may be tested for elevated lead levels, but that unfortunately means the problem is only detected after they have already been poisoned. Many efforts are underway to use predictive models to assist government agencies in prioritizing children at risk, including those who may not yet have been exposed.98 Similarly, the Illinois Department of Human Services (IDHS) uses predictive models to identify pregnant women at risk for adverse birth outcomes in order to maximize the impact of prenatal care. The City of Cincinnati uses them to proactively identify and deploy inspectors to properties at risk of code violations.\n\nScheduling, planning\n\nTask assignment scheduling and planning techniques have been applied by many different groups to distribute food before it spoils from those who may have excess, such as restaurants, to food banks, community centers and individuals.99", "doc_id": "stone2022", "page": 35, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "LOW-RESOURCE COMMUNITIES\n\nMany opportunities exist for AI to improve conditions for people in low-resource communities in a typical North American city—and, indeed, in some cases it already has. Understanding these direct AI contributions may also inform potential contributions in the poorest parts of the developing world. There has not been a significant focus on these populations in AI gatherings, and, traditionally, AI funders have underinvested in research lacking commercial application. With targeted incentives and funding priorities, AI technologies could help address the needs of low-resource communities. Budding efforts are promising. Counteracting fears that AI may contribute to joblessness and other societal problems, AI may provide mitigations and solutions, particularly if implemented in ways that build trust in them by the affected communities.\n\nMachine learning, data mining approaches\n\nUnder the banner of “data science for social good,” AI has been used to create predictive models to help government agencies more effectively use their limited budgets to address problems such as lead poisoning,97 a major public health concern that has been in the news due to ongoing events in Flint, Michigan. Children may be tested for elevated lead levels, but that unfortunately means the problem is only detected after they have already been poisoned. Many efforts are underway to use predictive models to assist government agencies in prioritizing children at risk, including those who may not yet have been exposed.98 Similarly, the Illinois Department of Human Services (IDHS) uses predictive models to identify pregnant women at risk for adverse birth outcomes in order to maximize the impact of prenatal care. The City of Cincinnati uses them to proactively identify and deploy inspectors to properties at risk of code violations.\n\nScheduling, planning\n\nTask assignment scheduling and planning techniques have been applied by many different groups to distribute food before it spoils from those who may have excess, such as restaurants, to food banks, community centers and individuals.99", "original_types": ["text", "header"], "id": 137}
{"type": "section", "content": "Reasoning with social networks and influence maximization\n\nSocial networks can be harnessed to create earlier, less-costly interventions involving large populations. For example, AI might be able to assist in spreading health-related information. In Los Angeles, there are more than 5,000 homeless youth (ages thirteen-twenty-four). Individual interventions are difficult and expensive, and the youths’ mistrust of authority dictates that key messages are best spread through peer leaders. AI programs might be able to leverage homeless youth social networks to strategically select peer leaders to spread health-related information, such as how to avoid spread of HIV. The dynamic, uncertain nature of these networks does pose challenges for AI research.100 Care must also be taken to prevent AI systems from reproducing discriminatory behavior, such as machine learning that identifies people through illegal racial indicators, or through highly-correlated surrogate factors, such as zip codes. But if deployed with great care, greater reliance on AI may well result in a reduction in discrimination overall, since AI programs are inherently more easily audited than humans.", "doc_id": "stone2022", "page": 36, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Reasoning with social networks and influence maximization\n\nSocial networks can be harnessed to create earlier, less-costly interventions involving large populations. For example, AI might be able to assist in spreading health-related information. In Los Angeles, there are more than 5,000 homeless youth (ages thirteen-twenty-four). Individual interventions are difficult and expensive, and the youths’ mistrust of authority dictates that key messages are best spread through peer leaders. AI programs might be able to leverage homeless youth social networks to strategically select peer leaders to spread health-related information, such as how to avoid spread of HIV. The dynamic, uncertain nature of these networks does pose challenges for AI research.100 Care must also be taken to prevent AI systems from reproducing discriminatory behavior, such as machine learning that identifies people through illegal racial indicators, or through highly-correlated surrogate factors, such as zip codes. But if deployed with great care, greater reliance on AI may well result in a reduction in discrimination overall, since AI programs are inherently more easily audited than humans.", "original_types": ["text", "header"], "id": 138}
{"type": "section", "content": "evidence of police malpractice. These improvements could lead to even more widespread surveillance. Some cities have already added drones for surveillance purposes, and police use of drones to maintain security of ports, airports, coastal areas, waterways, industrial facilities is likely to increase, raising concerns about privacy, safety, and other issues.\n\nThe New York Police Department’s CompStat was the first tool pointing toward predictive policing,104 and many police departments now use it.105 Machine learning significantly enhances the ability to predict where and when crimes are more likely to happen and who may commit them. As dramatized in the movie Minority Report, predictive policing tools raise the specter of innocent people being unjustifiably targeted. But well-deployed AI prediction tools have the potential to actually remove or reduce human bias, rather than reinforcing it, and research and resources should be directed toward ensuring this effect.\n\nAI techniques can be used to develop intelligent simulations for training law-enforcement personnel to collaborate. While international criminal organizations and terrorists from different countries are colluding, police forces from different countries still face difficulty in joining forces to fight them. Training international groups of law enforcement personnel to work as teams is very challenging. The European Union, through the Horizon 2020 program, currently supports such attempts in projects such as LawTrain.106 The next step will be to move from simulation to actual investigations by providing tools that support such collaborations.\n\nTools do exist for scanning Twitter and other feeds to look for certain types of events and how they may impact security. For example, AI can help in social network analysis to prevent those at risk from being radicalized by ISIS or other violent groups. Law enforcement agencies are increasingly interested in trying to detect plans for disruptive events from social media, and also to monitor activity at large gatherings of people to analyze security. There is significant work on crowd simulations to determine how crowds can be controlled. At the same time, legitimate concerns have been raised about the potential for law enforcement agencies to overreach and use such tools to violate people’s privacy.", "doc_id": "stone2022", "page": 37, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "evidence of police malpractice. These improvements could lead to even more widespread surveillance. Some cities have already added drones for surveillance purposes, and police use of drones to maintain security of ports, airports, coastal areas, waterways, industrial facilities is likely to increase, raising concerns about privacy, safety, and other issues.\n\nThe New York Police Department’s CompStat was the first tool pointing toward predictive policing,104 and many police departments now use it.105 Machine learning significantly enhances the ability to predict where and when crimes are more likely to happen and who may commit them. As dramatized in the movie Minority Report, predictive policing tools raise the specter of innocent people being unjustifiably targeted. But well-deployed AI prediction tools have the potential to actually remove or reduce human bias, rather than reinforcing it, and research and resources should be directed toward ensuring this effect.\n\nAI techniques can be used to develop intelligent simulations for training law-enforcement personnel to collaborate. While international criminal organizations and terrorists from different countries are colluding, police forces from different countries still face difficulty in joining forces to fight them. Training international groups of law enforcement personnel to work as teams is very challenging. The European Union, through the Horizon 2020 program, currently supports such attempts in projects such as LawTrain.106 The next step will be to move from simulation to actual investigations by providing tools that support such collaborations.\n\nTools do exist for scanning Twitter and other feeds to look for certain types of events and how they may impact security. For example, AI can help in social network analysis to prevent those at risk from being radicalized by ISIS or other violent groups. Law enforcement agencies are increasingly interested in trying to detect plans for disruptive events from social media, and also to monitor activity at large gatherings of people to analyze security. There is significant work on crowd simulations to determine how crowds can be controlled. At the same time, legitimate concerns have been raised about the potential for law enforcement agencies to overreach and use such tools to violate people’s privacy.", "original_types": ["text"], "id": 139}
{"type": "section", "content": "The US Transportation Security Administration (TSA), Coast Guard, and the many other security agencies that currently rely on AI will likely increase their reliance to enable significant efficiency and efficacy improvements.107 AI techniques—vision, speech analysis, and gait analysis—can aid interviewers, interrogators, and security guards in detecting possible deception and criminal behavior. For example, the TSA currently has an ambitious project to redo airport security nationwide.108 Called DARMS, the system is designed to improve efficiency and efficacy of airport security by relying on personal information to tailor security based on a person’s risk categorization and the flights being taken. The future vision for this project is a tunnel that checks people’s security while they walk through it. Once again, developers of this technology should be careful to avoid building in bias (e.g. about a person’s risk level category) through use of datasets that reflect prior bias.109", "doc_id": "stone2022", "page": 37, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "The US Transportation Security Administration (TSA), Coast Guard, and the many other security agencies that currently rely on AI will likely increase their reliance to enable significant efficiency and efficacy improvements.107 AI techniques—vision, speech analysis, and gait analysis—can aid interviewers, interrogators, and security guards in detecting possible deception and criminal behavior. For example, the TSA currently has an ambitious project to redo airport security nationwide.108 Called DARMS, the system is designed to improve efficiency and efficacy of airport security by relying on personal information to tailor security based on a person’s risk categorization and the flights being taken. The future vision for this project is a tunnel that checks people’s security while they walk through it. Once again, developers of this technology should be careful to avoid building in bias (e.g. about a person’s risk level category) through use of datasets that reflect prior bias.109", "original_types": ["text"], "id": 140}
{"type": "section", "content": "EMPLOYMENT AND WORKPLACE\n\nWhile AI technologies are likely to have a profound future impact on employment and workplace trends in a typical North American city, it is difficult to accurately assess current impacts, positive or negative. In the past fifteen years, employment has shifted due to a major recession and increasing globalization, particularly with China’s introduction to the world economy, as well as enormous changes in non-AI digital technology. Since the 1990s, the US has experienced continued growth in productivity and GDP, but median income has stagnated and the employment to population ratio has fallen.", "doc_id": "stone2022", "page": 38, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "EMPLOYMENT AND WORKPLACE\n\nWhile AI technologies are likely to have a profound future impact on employment and workplace trends in a typical North American city, it is difficult to accurately assess current impacts, positive or negative. In the past fifteen years, employment has shifted due to a major recession and increasing globalization, particularly with China’s introduction to the world economy, as well as enormous changes in non-AI digital technology. Since the 1990s, the US has experienced continued growth in productivity and GDP, but median income has stagnated and the employment to population ratio has fallen.", "original_types": ["text", "header"], "id": 141}
{"type": "section", "content": "markets, which often have the effect of lowering barriers to entry and increasing participation—from app stores to AirBnB to taskrabbit. A vibrant research community within AI studies further ways of creating new markets and making existing ones operate more efficiently.\n\nWhile work has intrinsic value, most people work to be able to purchase goods and services they value. Because AI systems perform work that previously required human labor, they have the effect of lowering the cost of many goods and services, effectively making everyone richer. But as exemplified in current political debates, job loss is more salient to people—especially those directly affected—than diffuse economic gains, and AI unfortunately is often framed as a threat to jobs rather than a boon to living standards.\n\nThere is even fear in some quarters that advances in AI will be so rapid as to replace all human jobs—including those that are largely cognitive or involve judgment—within a single generation. This sudden scenario is highly unlikely, but AI will gradually invade almost all employment sectors, requiring a shift away from human labor that computers are able to take over.\n\nThe economic effects of AI on cognitive human jobs will be analogous to the effects of automation and robotics on humans in manufacturing jobs. Many middle-aged workers have lost well-paying factory jobs and the socio-economic status in family and society that traditionally went with such jobs. An even larger fraction of the total workforce may, in the long run, lose well-paying “cognitive” jobs. As labor becomes a less important factor in production as compared to owning intellectual capital, a majority of citizens may find the value of their labor insufficient to pay for a socially acceptable standard of living. These changes will require a political, rather than a purely economic, response concerning what kind of social safety nets should be in place to protect people from large, structural shifts in the economy. Absent mitigating policies, the beneficiaries of these shifts may be a small group at the upper stratum of the society.\n\nIn the short run, education, re-training, and inventing new goods and services may mitigate these effects. Longer term, the current social safety net may need to evolve into better social services for everyone, such as healthcare and education, or a guaranteed basic income. Indeed, countries such as Switzerland and Finland have actively considered such measures. AI may be thought of as a radically different mechanism of wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasure.\n\nIt is not too soon for social debate on how the economic fruits of AI-technologies should be shared. As children in traditional societies support their aging parents, perhaps our artificially intelligent “children” should support us, the “parents” of their intelligence.", "doc_id": "stone2022", "page": 39, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "markets, which often have the effect of lowering barriers to entry and increasing participation—from app stores to AirBnB to taskrabbit. A vibrant research community within AI studies further ways of creating new markets and making existing ones operate more efficiently.\n\nWhile work has intrinsic value, most people work to be able to purchase goods and services they value. Because AI systems perform work that previously required human labor, they have the effect of lowering the cost of many goods and services, effectively making everyone richer. But as exemplified in current political debates, job loss is more salient to people—especially those directly affected—than diffuse economic gains, and AI unfortunately is often framed as a threat to jobs rather than a boon to living standards.\n\nThere is even fear in some quarters that advances in AI will be so rapid as to replace all human jobs—including those that are largely cognitive or involve judgment—within a single generation. This sudden scenario is highly unlikely, but AI will gradually invade almost all employment sectors, requiring a shift away from human labor that computers are able to take over.\n\nThe economic effects of AI on cognitive human jobs will be analogous to the effects of automation and robotics on humans in manufacturing jobs. Many middle-aged workers have lost well-paying factory jobs and the socio-economic status in family and society that traditionally went with such jobs. An even larger fraction of the total workforce may, in the long run, lose well-paying “cognitive” jobs. As labor becomes a less important factor in production as compared to owning intellectual capital, a majority of citizens may find the value of their labor insufficient to pay for a socially acceptable standard of living. These changes will require a political, rather than a purely economic, response concerning what kind of social safety nets should be in place to protect people from large, structural shifts in the economy. Absent mitigating policies, the beneficiaries of these shifts may be a small group at the upper stratum of the society.\n\nIn the short run, education, re-training, and inventing new goods and services may mitigate these effects. Longer term, the current social safety net may need to evolve into better social services for everyone, such as healthcare and education, or a guaranteed basic income. Indeed, countries such as Switzerland and Finland have actively considered such measures. AI may be thought of as a radically different mechanism of wealth creation in which everyone should be entitled to a portion of the world’s AI-produced treasure.\n\nIt is not too soon for social debate on how the economic fruits of AI-technologies should be shared. As children in traditional societies support their aging parents, perhaps our artificially intelligent “children” should support us, the “parents” of their intelligence.", "original_types": ["text"], "id": 142}
{"type": "section", "content": "ENTERTAINMENT", "doc_id": "stone2022", "page": 40, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "ENTERTAINMENT", "original_types": ["header"], "id": 143}
{"type": "section", "content": "With the explosive growth of the internet over the past fifteen years, few can imagine their daily lives without it. Powered by AI, the internet has established user-generated content as a viable source of information and entertainment. Social networks such as Facebook are now pervasive, and they function as personalized channels of social interaction and entertainment—sometimes to the detriment of interpersonal interaction. Apps such as WhatsApp and Snapchat enable smart-phone users to remain constantly “in touch” with peers and share sources of entertainment and information. In on-line communities such as Second Life and role-playing games such as World of Warcraft, people imagine an alternative existence in a virtual world.115 Specialized devices, such as Amazon’s Kindle have also redefined the essentials of long-cherished pastimes. Books can now be browsed and procured with a few swipes of the finger, stored by the thousands in a pocket-sized device, and read in much the same way as a handheld paperback. Trusted platforms now exist for sharing and browsing blogs, videos, photos, and topical discussions, in addition to a variety of other user-generated information. To operate at the scale of the internet, these platforms must rely on techniques that are being actively developed in natural language processing, information retrieval, image processing, crowdsourcing, and machine learning. Algorithms such as collaborative filtering have been developed, for example, to recommend relevant movies, songs, or articles based on the user’s demographic details and browsing history.116 Traditional sources of entertainment have also embraced AI to keep pace with the times. As exemplified in the book and movie Moneyball, professional sport is now subjected to intensive quantitative analysis.117 Beyond aggregate performance statistics, on-field signals can be monitored using sophisticated sensors and cameras. Software has been created for composing music118 and recognizing soundtracks.119 Techniques from computer vision and NLP have been used in creating stage performances.120 Even the lay user can exercise his or her creativity on platforms such as WordsEye, which automatically generates 3D scenes from natural language text.121 AI has also come to the aid of historical research in the arts, and is used extensively in stylometry and, more recently, in the analysis of paintings.122 The enthusiasm with which humans have responded to AI-driven entertainment has been surprising and led to concerns that it reduces interpersonal interaction among human beings. Few predicted that people would spend hours on end interacting with a display. Children often appear to be genuinely happier playing at home on their devices rather than outside with their friends. AI will increasingly enable entertainment that is more interactive, personalized, and engaging. Research should be directed toward understanding how to leverage these attributes for individuals’ and society’s benefit.", "doc_id": "stone2022", "page": 40, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "With the explosive growth of the internet over the past fifteen years, few can imagine their daily lives without it. Powered by AI, the internet has established user-generated content as a viable source of information and entertainment. Social networks such as Facebook are now pervasive, and they function as personalized channels of social interaction and entertainment—sometimes to the detriment of interpersonal interaction. Apps such as WhatsApp and Snapchat enable smart-phone users to remain constantly “in touch” with peers and share sources of entertainment and information. In on-line communities such as Second Life and role-playing games such as World of Warcraft, people imagine an alternative existence in a virtual world.115 Specialized devices, such as Amazon’s Kindle have also redefined the essentials of long-cherished pastimes. Books can now be browsed and procured with a few swipes of the finger, stored by the thousands in a pocket-sized device, and read in much the same way as a handheld paperback. Trusted platforms now exist for sharing and browsing blogs, videos, photos, and topical discussions, in addition to a variety of other user-generated information. To operate at the scale of the internet, these platforms must rely on techniques that are being actively developed in natural language processing, information retrieval, image processing, crowdsourcing, and machine learning. Algorithms such as collaborative filtering have been developed, for example, to recommend relevant movies, songs, or articles based on the user’s demographic details and browsing history.116 Traditional sources of entertainment have also embraced AI to keep pace with the times. As exemplified in the book and movie Moneyball, professional sport is now subjected to intensive quantitative analysis.117 Beyond aggregate performance statistics, on-field signals can be monitored using sophisticated sensors and cameras. Software has been created for composing music118 and recognizing soundtracks.119 Techniques from computer vision and NLP have been used in creating stage performances.120 Even the lay user can exercise his or her creativity on platforms such as WordsEye, which automatically generates 3D scenes from natural language text.121 AI has also come to the aid of historical research in the arts, and is used extensively in stylometry and, more recently, in the analysis of paintings.122 The enthusiasm with which humans have responded to AI-driven entertainment has been surprising and led to concerns that it reduces interpersonal interaction among human beings. Few predicted that people would spend hours on end interacting with a display. Children often appear to be genuinely happier playing at home on their devices rather than outside with their friends. AI will increasingly enable entertainment that is more interactive, personalized, and engaging. Research should be directed toward understanding how to leverage these attributes for individuals’ and society’s benefit.", "id": 144}
{"type": "section", "content": "Imagining the Future\n\nThe success of any form of entertainment is ultimately determined by the individuals and social groups that are its subjects. The modes of entertainment that people find appealing are diverse and change over time. It is therefore hard to predict the forms entertainment will take in the next fifteen years precisely. Nevertheless, current trends suggest at least a few features that the future entertainment landscape is likely to contain.\n\nTo date, the information revolution has mostly unfolded in software. However, with the growing availability of cheaper sensors and devices, greater innovation in the hardware used in entertainment systems is expected. Virtual reality and haptics could enter our living rooms—personalized companion robots are already being developed.123 With the accompanying improvements in Automatic Speech Recognition, the Study Panel expects that interaction with robots and other entertainment systems will become dialogue-based, perhaps constrained at the start, but progressively more human-like. Equally, the interacting systems are predicted to develop new characteristics such as emotion, empathy, and adaptation to environmental rhythms such as time of day.124 Today, an amateur with a video camera and readily-available software tools can make a relatively good movie. In the future, more sophisticated tools and apps will become available to make it even easier to produce high-quality content, for example, to compose music or to choreograph dance using an avatar. The creation and dissemination of entertainment will benefit from the progress of technologies such as ASR, dubbing, and Machine Translation, which will enable content to be customized to different audiences inexpensively. This democratization and proliferation of AI-created media makes it difficult to predict how humans’ taste for entertainment, which are already fluid, will evolve.\n\nWith content increasingly delivered digitally, and large amounts of data being logged about consumers’ preferences and usage characteristics, media powerhouses will be able to micro-analyze and micro-serve content to increasingly specialized segments of the population—down to the individual.125 Conceivably the stage is set for the emergence of media conglomerates acting as “Big Brothers” who are able to control the ideas and online experiences to which specific individuals are exposed. It remains to be seen whether broader society will develop measures to prevent their emergence. This topic, along with others pertaining to AI-related policy, is treated in more detail in the next section.", "doc_id": "stone2022", "page": 41, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Imagining the Future\n\nThe success of any form of entertainment is ultimately determined by the individuals and social groups that are its subjects. The modes of entertainment that people find appealing are diverse and change over time. It is therefore hard to predict the forms entertainment will take in the next fifteen years precisely. Nevertheless, current trends suggest at least a few features that the future entertainment landscape is likely to contain.\n\nTo date, the information revolution has mostly unfolded in software. However, with the growing availability of cheaper sensors and devices, greater innovation in the hardware used in entertainment systems is expected. Virtual reality and haptics could enter our living rooms—personalized companion robots are already being developed.123 With the accompanying improvements in Automatic Speech Recognition, the Study Panel expects that interaction with robots and other entertainment systems will become dialogue-based, perhaps constrained at the start, but progressively more human-like. Equally, the interacting systems are predicted to develop new characteristics such as emotion, empathy, and adaptation to environmental rhythms such as time of day.124 Today, an amateur with a video camera and readily-available software tools can make a relatively good movie. In the future, more sophisticated tools and apps will become available to make it even easier to produce high-quality content, for example, to compose music or to choreograph dance using an avatar. The creation and dissemination of entertainment will benefit from the progress of technologies such as ASR, dubbing, and Machine Translation, which will enable content to be customized to different audiences inexpensively. This democratization and proliferation of AI-created media makes it difficult to predict how humans’ taste for entertainment, which are already fluid, will evolve.\n\nWith content increasingly delivered digitally, and large amounts of data being logged about consumers’ preferences and usage characteristics, media powerhouses will be able to micro-analyze and micro-serve content to increasingly specialized segments of the population—down to the individual.125 Conceivably the stage is set for the emergence of media conglomerates acting as “Big Brothers” who are able to control the ideas and online experiences to which specific individuals are exposed. It remains to be seen whether broader society will develop measures to prevent their emergence. This topic, along with others pertaining to AI-related policy, is treated in more detail in the next section.", "original_types": ["text", "header"], "id": 145}
{"type": "section", "content": "The measure of success for AI applications is the value they create for human lives. Going forward, the ease with which people use and adapt to AI applications will likewise largely determine their success.\n\nThe goal of AI applications must be to create value for society. Our policy recommendations flow from this goal, and, while this report is focused on a typical North American city in 2030, the recommendations are broadly applicable to other places over time. Strategies that enhance our ability to interpret AI systems and participate in their use may help build trust and prevent drastic failures. Care must be taken to augment and enhance human capabilities and interaction, and to avoid discrimination against segments of society. Research to encourage this direction and inform public policy debates should be emphasized. Given the current sector-specific regulation of US industries, new or retooled laws and policies will be needed to address the widespread impacts AI is likely to bring. Rather than “more” or “stricter” regulation, policies should be designed to encourage helpful innovation, generate and transfer expertise, and foster broad corporate and civic responsibility for addressing critical societal issues raised by these technologies. In the long term, AI will enable new wealth creation that will require social debate on how the economic fruits of AI technologies should be shared.", "doc_id": "stone2022", "page": 42, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "The measure of success for AI applications is the value they create for human lives. Going forward, the ease with which people use and adapt to AI applications will likewise largely determine their success.\n\nThe goal of AI applications must be to create value for society. Our policy recommendations flow from this goal, and, while this report is focused on a typical North American city in 2030, the recommendations are broadly applicable to other places over time. Strategies that enhance our ability to interpret AI systems and participate in their use may help build trust and prevent drastic failures. Care must be taken to augment and enhance human capabilities and interaction, and to avoid discrimination against segments of society. Research to encourage this direction and inform public policy debates should be emphasized. Given the current sector-specific regulation of US industries, new or retooled laws and policies will be needed to address the widespread impacts AI is likely to bring. Rather than “more” or “stricter” regulation, policies should be designed to encourage helpful innovation, generate and transfer expertise, and foster broad corporate and civic responsibility for addressing critical societal issues raised by these technologies. In the long term, AI will enable new wealth creation that will require social debate on how the economic fruits of AI technologies should be shared.", "original_types": ["text", "header"], "id": 146}
{"type": "section", "content": "Likewise, AI could widen existing inequalities of opportunity if access to AI technologies—along with the high-powered computation and large-scale data that fuel many of them—is unfairly distributed across society. These technologies will improve the abilities and efficiency of people who have access to them. A person with access to accurate Machine Translation technology will be better able to use learning resources available in different languages. Similarly, if speech translation technology is only available in English, people who do not speak English will be at a disadvantage.\n\nFurther, AI applications and the data they rely upon may reflect the biases of their designers and users, who specify the data sources. This threatens to deepen existing social biases, and concentrate AI’s benefits unequally among different subgroups of society. For example, some speech recognition technologies do not work well for women and people with accents. As AI is increasingly used in critical applications, these biases may surface issues of fairness to diverse groups in society. On the other hand, compared to the well-documented biases in human decision-making, AI-based decision-making tools have the potential to significantly reduce the bias in critical decisions such as who is lent money or sent to jail.\n\nPrivacy concerns about AI-enabled surveillance are also widespread, particularly in cities with pervasive instrumentation. Sousveillance, the recording of an activity by a participant, usually with portable personal devices, has increased as well. Since views about bias and privacy are based on personal and societal ethical and value judgments, the debates over how to address these concerns will likely grow and resist quick resolution. Similarly, since AI is generating significant wealth, debates will grow regarding how the economic fruits of AI technologies should be shared—especially as AI expertise and the underlying data sets that fuel applications are concentrated in a small number of large corporations.\n\nTo help address these concerns about the individual and societal implications of rapidly evolving AI technologies, the Study Panel offers three general policy recommendations:", "doc_id": "stone2022", "page": 43, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Likewise, AI could widen existing inequalities of opportunity if access to AI technologies—along with the high-powered computation and large-scale data that fuel many of them—is unfairly distributed across society. These technologies will improve the abilities and efficiency of people who have access to them. A person with access to accurate Machine Translation technology will be better able to use learning resources available in different languages. Similarly, if speech translation technology is only available in English, people who do not speak English will be at a disadvantage.\n\nFurther, AI applications and the data they rely upon may reflect the biases of their designers and users, who specify the data sources. This threatens to deepen existing social biases, and concentrate AI’s benefits unequally among different subgroups of society. For example, some speech recognition technologies do not work well for women and people with accents. As AI is increasingly used in critical applications, these biases may surface issues of fairness to diverse groups in society. On the other hand, compared to the well-documented biases in human decision-making, AI-based decision-making tools have the potential to significantly reduce the bias in critical decisions such as who is lent money or sent to jail.\n\nPrivacy concerns about AI-enabled surveillance are also widespread, particularly in cities with pervasive instrumentation. Sousveillance, the recording of an activity by a participant, usually with portable personal devices, has increased as well. Since views about bias and privacy are based on personal and societal ethical and value judgments, the debates over how to address these concerns will likely grow and resist quick resolution. Similarly, since AI is generating significant wealth, debates will grow regarding how the economic fruits of AI technologies should be shared—especially as AI expertise and the underlying data sets that fuel applications are concentrated in a small number of large corporations.\n\nTo help address these concerns about the individual and societal implications of rapidly evolving AI technologies, the Study Panel offers three general policy recommendations:", "original_types": ["text"], "id": 147}
{"type": "section", "content": "As a society, we are underinvesting resources in research on the societal implications of AI technologies. Private and public dollars should be directed toward interdisciplinary teams capable of analyzing AI from multiple angles.\n\nteams capable of analyzing AI from multiple angles. Research questions range from basic research into intelligence to methods to assess and affect the safety, privacy, fairness, and other impacts of AI.", "doc_id": "stone2022", "page": 44, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "As a society, we are underinvesting resources in research on the societal implications of AI technologies. Private and public dollars should be directed toward interdisciplinary teams capable of analyzing AI from multiple angles.\n\nteams capable of analyzing AI from multiple angles. Research questions range from basic research into intelligence to methods to assess and affect the safety, privacy, fairness, and other impacts of AI.", "original_types": ["text", "header"], "id": 148}
{"type": "section", "content": "companies such as Google, Facebook, and Amazon have actively lobbied to avoid being designated as critical to the economy, arguing that this would open the door to regulation that would inevitably compromise their rapid product development cycles and ability to innovate.\n\n130 Nonetheless, as the companies creating, operating, and maintaining critical infrastructure use AI, interest will grow in regulating that software.\n\nSome existing regulatory regimes for software safety (for example, the FDA’s regulation of high consequence medical software) require specific software engineering practices at the developer level. However, modern software systems are often assembled from library components which may be supplied by multiple vendors, and are relatively application-independent. It doesn’t seem feasible or desirable to subject all such developers to the standards required for the most critical, rare applications. Nor does it seem advisable to allow unregulated use of such components in safety critical applications. Tradeoffs between promoting innovation and regulating for safety are difficult ones, both conceptually and in practice. At a minimum, regulatory entities will require greater expertise going forward in order to understand the implications of standards and measures put in place by researchers, government, and industry.\n\nPolicy and legal considerations\n\nWhile a comprehensive examination of the ways artificial intelligence (AI) interacts with the law is beyond the scope of this inaugural report, this much seems clear: as a transformative technology, AI has the potential to challenge any number of legal assumptions in the short, medium, and long term. Precisely how law and policy will adapt to advances in AI—and how AI will adapt to values reflected in law and policy—depends on a variety of social, cultural, economic, and other factors, and is likely to vary by jurisdiction.", "doc_id": "stone2022", "page": 45, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "companies such as Google, Facebook, and Amazon have actively lobbied to avoid being designated as critical to the economy, arguing that this would open the door to regulation that would inevitably compromise their rapid product development cycles and ability to innovate.\n\n130 Nonetheless, as the companies creating, operating, and maintaining critical infrastructure use AI, interest will grow in regulating that software.\n\nSome existing regulatory regimes for software safety (for example, the FDA’s regulation of high consequence medical software) require specific software engineering practices at the developer level. However, modern software systems are often assembled from library components which may be supplied by multiple vendors, and are relatively application-independent. It doesn’t seem feasible or desirable to subject all such developers to the standards required for the most critical, rare applications. Nor does it seem advisable to allow unregulated use of such components in safety critical applications. Tradeoffs between promoting innovation and regulating for safety are difficult ones, both conceptually and in practice. At a minimum, regulatory entities will require greater expertise going forward in order to understand the implications of standards and measures put in place by researchers, government, and industry.\n\nPolicy and legal considerations\n\nWhile a comprehensive examination of the ways artificial intelligence (AI) interacts with the law is beyond the scope of this inaugural report, this much seems clear: as a transformative technology, AI has the potential to challenge any number of legal assumptions in the short, medium, and long term. Precisely how law and policy will adapt to advances in AI—and how AI will adapt to values reflected in law and policy—depends on a variety of social, cultural, economic, and other factors, and is likely to vary by jurisdiction.", "original_types": ["text", "header"], "id": 149}
{"type": "section", "content": "American law represents a mixture of common law, federal, state, and local statutes and ordinances, and—perhaps of greatest relevance to AI—regulations. Depending on its instantiation, AI could implicate each of these sources of law. For example, Nevada passed a law broadly permitting autonomous vehicles and instructed the Nevada Department of Motor Vehicles to craft requirements. Meanwhile, the National Highway Transportation Safety Administration has determined that a self-driving car system, rather than the vehicle occupants, can be considered the “driver” of a vehicle. Some car designs sidestep this issue by staying in autonomous mode only when hands are on the wheel (at least every so often), so that the human driver has ultimate control and responsibility. Still, Tesla’s adoption of this strategy did not prevent the first traffic fatality involving an autonomous car, which occurred in June of 2016. Such incidents are sure to influence public attitudes towards autonomous driving. And as most people’s first experience with embodied agents, autonomous transportation will strongly influence the public’s perception of AI.\n\nDriverless cars are, of course, but one example of the many instantiations of AI in services, products, and other contexts. The legal effect of introducing AI into the provision of tax advice, automated trading on the stock market, or generating medical diagnoses will also vary in accordance to the regulators that govern these contexts and the rules that apply within them. Many other examples of AI applications fall within current non-technology-specific policy, including predictive\n\n130 Eric Engleman, “Google Exception in Obama’s Cyber Order Questioned as Unwise Gap,” Bloomberg Technology, March 4, 2013, accessed August 1, 2016, http://www.bloomberg.com/news/articles/2013-03-05/google-exception-in-obama-s-cyber-order-questioned-as-unwise-gap.\n\n131 Ryan Calo, “The Case for a Federal Robotics Commission,” Brookings Report, September 15, 2014, accessed August 1, 2016, http://www.brookings.edu/research/reports2/2014/09/case-for-federal-robotics-commission.", "doc_id": "stone2022", "page": 45, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "American law represents a mixture of common law, federal, state, and local statutes and ordinances, and—perhaps of greatest relevance to AI—regulations. Depending on its instantiation, AI could implicate each of these sources of law. For example, Nevada passed a law broadly permitting autonomous vehicles and instructed the Nevada Department of Motor Vehicles to craft requirements. Meanwhile, the National Highway Transportation Safety Administration has determined that a self-driving car system, rather than the vehicle occupants, can be considered the “driver” of a vehicle. Some car designs sidestep this issue by staying in autonomous mode only when hands are on the wheel (at least every so often), so that the human driver has ultimate control and responsibility. Still, Tesla’s adoption of this strategy did not prevent the first traffic fatality involving an autonomous car, which occurred in June of 2016. Such incidents are sure to influence public attitudes towards autonomous driving. And as most people’s first experience with embodied agents, autonomous transportation will strongly influence the public’s perception of AI.\n\nDriverless cars are, of course, but one example of the many instantiations of AI in services, products, and other contexts. The legal effect of introducing AI into the provision of tax advice, automated trading on the stock market, or generating medical diagnoses will also vary in accordance to the regulators that govern these contexts and the rules that apply within them. Many other examples of AI applications fall within current non-technology-specific policy, including predictive\n\n130 Eric Engleman, “Google Exception in Obama’s Cyber Order Questioned as Unwise Gap,” Bloomberg Technology, March 4, 2013, accessed August 1, 2016, http://www.bloomberg.com/news/articles/2013-03-05/google-exception-in-obama-s-cyber-order-questioned-as-unwise-gap.\n\n131 Ryan Calo, “The Case for a Federal Robotics Commission,” Brookings Report, September 15, 2014, accessed August 1, 2016, http://www.brookings.edu/research/reports2/2014/09/case-for-federal-robotics-commission.", "original_types": ["text"], "id": 150}
{"type": "section", "content": "As AI applications engage in behavior that, were it done by a human, would constitute a crime, courts and other legal actors will have to puzzle through whom to hold accountable and on what theory.\n\nGiven the present structure of American administrative law, it seems unlikely that AI will be treated comprehensively in the near term. Nevertheless, it is possible to enumerate broad categories of legal and policy issues that AI tends to raise in various contexts.", "doc_id": "stone2022", "page": 46, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "As AI applications engage in behavior that, were it done by a human, would constitute a crime, courts and other legal actors will have to puzzle through whom to hold accountable and on what theory.\n\nGiven the present structure of American administrative law, it seems unlikely that AI will be treated comprehensively in the near term. Nevertheless, it is possible to enumerate broad categories of legal and policy issues that AI tends to raise in various contexts.", "original_types": ["text", "header"], "id": 151}
{"type": "section", "content": "Liability (criminal)\n\nIf tort law expects harms to be foreseeable, criminal law goes further to expect that harms be intended. US law in particular attaches great importance to the concept of mens rea—the intending mind. As AI applications engage in behavior that, were it done by a human, would constitute a crime, courts and other legal actors will have to puzzle through whom to hold accountable and on what theory.\n\nAgency\n\nThe above issues raise the question of whether and under what circumstances an AI system could operate as the agent of a person or corporation. Already regulatory bodies in the United States, Canada, and elsewhere are setting the conditions under which software can enter into a binding contract.135 The more AI conducts legally salient activities, the greater the challenge to principles of agency under the law.\n\nCertification\n\nThe very notion of “artificial intelligence” suggests a substitution for human skill and ingenuity. And in many contexts, ranging from driving to performing surgery or practicing law, a human must attain some certification or license before performing a given task. Accordingly, law and policy will have to—and already does—grapple with how to determine competency in an AI system. For example, imagine a robotics company creates a surgical platform capable of autonomously removing an appendix. Or imagine a law firm writes an application capable of rendering legal advice. Today, it is unclear from a legal perspective who in this picture would have to pass the medical boards or legal bar, let alone where they would be required to do so.136\n\nLabor\n\nAs AI substitutes for human roles, some jobs will be eliminated and new jobs will be created. The net effect on jobs is ambiguous, but labor markets are unlikely to benefit everyone evenly. The demand for some types of skills or abilities will likely drop significantly, negatively affecting the employment levels and wages of people with those skills.137 While the ultimate effects on income levels and distribution are not inevitable, they depend substantially on government policies, on the way companies choose to organize work, and on decisions by individuals to invest in learning new skills and seeking new types of work and income opportunities. People who find their employment altered or terminated as a consequence of advances of AI may seek recourse in the legislature and courts. This may be why Littler Mendelson LLP—perhaps the largest employment law firm in the world—has an entire practice group to address robotics and artificial intelligence.\n\nTaxation\n\nFederal, state, and local revenue sources may be affected. Accomplishing a task using AI instead of a person can be faster and more accurate—and avoid employment taxes. As a result, AI applications could increasingly shift investment from payroll and income to capital expenditure. Depending on a state budget’s reliance on payroll and income tax, such a shift could be destabilizing. AI may also display different", "doc_id": "stone2022", "page": 47, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Liability (criminal)\n\nIf tort law expects harms to be foreseeable, criminal law goes further to expect that harms be intended. US law in particular attaches great importance to the concept of mens rea—the intending mind. As AI applications engage in behavior that, were it done by a human, would constitute a crime, courts and other legal actors will have to puzzle through whom to hold accountable and on what theory.\n\nAgency\n\nThe above issues raise the question of whether and under what circumstances an AI system could operate as the agent of a person or corporation. Already regulatory bodies in the United States, Canada, and elsewhere are setting the conditions under which software can enter into a binding contract.135 The more AI conducts legally salient activities, the greater the challenge to principles of agency under the law.\n\nCertification\n\nThe very notion of “artificial intelligence” suggests a substitution for human skill and ingenuity. And in many contexts, ranging from driving to performing surgery or practicing law, a human must attain some certification or license before performing a given task. Accordingly, law and policy will have to—and already does—grapple with how to determine competency in an AI system. For example, imagine a robotics company creates a surgical platform capable of autonomously removing an appendix. Or imagine a law firm writes an application capable of rendering legal advice. Today, it is unclear from a legal perspective who in this picture would have to pass the medical boards or legal bar, let alone where they would be required to do so.136\n\nLabor\n\nAs AI substitutes for human roles, some jobs will be eliminated and new jobs will be created. The net effect on jobs is ambiguous, but labor markets are unlikely to benefit everyone evenly. The demand for some types of skills or abilities will likely drop significantly, negatively affecting the employment levels and wages of people with those skills.137 While the ultimate effects on income levels and distribution are not inevitable, they depend substantially on government policies, on the way companies choose to organize work, and on decisions by individuals to invest in learning new skills and seeking new types of work and income opportunities. People who find their employment altered or terminated as a consequence of advances of AI may seek recourse in the legislature and courts. This may be why Littler Mendelson LLP—perhaps the largest employment law firm in the world—has an entire practice group to address robotics and artificial intelligence.\n\nTaxation\n\nFederal, state, and local revenue sources may be affected. Accomplishing a task using AI instead of a person can be faster and more accurate—and avoid employment taxes. As a result, AI applications could increasingly shift investment from payroll and income to capital expenditure. Depending on a state budget’s reliance on payroll and income tax, such a shift could be destabilizing. AI may also display different", "original_types": ["text", "header"], "id": 152}
{"type": "section", "content": "Politics\n\nAI technologies are already being used by political actors in gerrymandering and targeted “robocalls” designed to suppress votes, and on social media platforms in the form of “bots.”138 They can enable coordinated protest as well as the ability to predict protests, and promote greater transparency in politics by more accurately pinpointing who said what, when. Thus, administrative and regulatory laws regarding AI can be designed to promote greater democratic participation or, if ill-conceived, to reduce it. This list is not exhaustive and focuses largely on domestic policy in the United States, leaving out many areas of law that AI is likely to touch. One lesson that might be drawn concerns the growing disconnect between the context-specific way in which AI is governed today and a wider consideration of themes shared by AI technologies across industries or sectors of society. It could be tempting to create new institutional configurations capable of amassing expertise and setting AI standards across multiple contexts. The Study Panel’s consensus is that attempts to regulate “AI” in general would be misguided, since there is no clear definition of AI (it isn’t any one thing), and the risks and considerations are very different in different domains. Instead, policymakers should recognize that to varying degrees and over time, various industries will need distinct, appropriate, regulations that touch on software built using AI or incorporating AI in some way. The government will need the expertise to scrutinize standards and technology developed by the private and public sector, and to craft regulations where necessary.", "doc_id": "stone2022", "page": 48, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Politics\n\nAI technologies are already being used by political actors in gerrymandering and targeted “robocalls” designed to suppress votes, and on social media platforms in the form of “bots.”138 They can enable coordinated protest as well as the ability to predict protests, and promote greater transparency in politics by more accurately pinpointing who said what, when. Thus, administrative and regulatory laws regarding AI can be designed to promote greater democratic participation or, if ill-conceived, to reduce it. This list is not exhaustive and focuses largely on domestic policy in the United States, leaving out many areas of law that AI is likely to touch. One lesson that might be drawn concerns the growing disconnect between the context-specific way in which AI is governed today and a wider consideration of themes shared by AI technologies across industries or sectors of society. It could be tempting to create new institutional configurations capable of amassing expertise and setting AI standards across multiple contexts. The Study Panel’s consensus is that attempts to regulate “AI” in general would be misguided, since there is no clear definition of AI (it isn’t any one thing), and the risks and considerations are very different in different domains. Instead, policymakers should recognize that to varying degrees and over time, various industries will need distinct, appropriate, regulations that touch on software built using AI or incorporating AI in some way. The government will need the expertise to scrutinize standards and technology developed by the private and public sector, and to craft regulations where necessary.", "original_types": ["text", "header"], "id": 153}
{"type": "section", "content": "privacy as their responsibility. Broad legal mandates encouraged companies to\n\ndevelop a professional staff and processes to enforce privacy controls, engage with\n\noutside stakeholders, and to adapt their practices to technology advances. Requiring\n\ngreater transparency enabled civil society groups and media to become credible\n\nenforcers both in court and in the court of public opinion, making privacy more\n\nsalient to corporate boards and leading them to further invest in privacy protection.\n\nIn AI, too, regulators can strengthen a virtuous cycle of activity involving internal\n\nand external accountability, transparency, and professionalization, rather than narrow\n\ncompliance. As AI is integrated into cities, it will continue to challenge existing\n\nprotections for values such as privacy and accountability. Like other technologies, AI\n\nhas the potential to be used for good or nefarious purposes. This report has tried to\n\nhighlight the potential for both. A vigorous and informed debate about how to best\n\nsteer AI in ways that enrich our lives and our society, while encouraging creativity\n\nin the field, is an urgent and vital need. Policies should be evaluated as to whether\n\nthey democratically foster the development and equitable sharing of AI’s benefits,\n\nor concentrate power and benefits in the hands of a fortunate few. And since future\n\nAI technologies and their effects cannot be foreseen with perfect clarity, policies will\n\nneed to be continually re-evaluated in the context of observed societal challenges and\n\nevidence from fielded systems.\n\nAs this report documents, significant AI-related advances have already had\n\nan impact on North American cities over the past fifteen years, and even more\n\nsubstantial developments will occur over the next fifteen. Recent advances are largely\n\ndue to the growth and analysis of large data sets enabled by the Internet, advances\n\nin sensory technologies and, more recently, applications of “deep learning.” In the\n\ncoming years, as the public encounters new AI applications in domains such as\n\ntransportation and healthcare, they must be introduced in ways that build trust and\n\nunderstanding, and respect human and civil rights. While encouraging innovation,\n\npolicies and processes should address ethical, privacy, and security implications, and\n\nshould work to ensure that the benefits of AI technologies will be spread broadly and\n\nfairly. Doing so will be critical if Artificial Intelligence research and its applications\n\nare to exert a positive influence on North American urban life in 2030 and beyond.", "doc_id": "stone2022", "page": 49, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "privacy as their responsibility. Broad legal mandates encouraged companies to\n\ndevelop a professional staff and processes to enforce privacy controls, engage with\n\noutside stakeholders, and to adapt their practices to technology advances. Requiring\n\ngreater transparency enabled civil society groups and media to become credible\n\nenforcers both in court and in the court of public opinion, making privacy more\n\nsalient to corporate boards and leading them to further invest in privacy protection.\n\nIn AI, too, regulators can strengthen a virtuous cycle of activity involving internal\n\nand external accountability, transparency, and professionalization, rather than narrow\n\ncompliance. As AI is integrated into cities, it will continue to challenge existing\n\nprotections for values such as privacy and accountability. Like other technologies, AI\n\nhas the potential to be used for good or nefarious purposes. This report has tried to\n\nhighlight the potential for both. A vigorous and informed debate about how to best\n\nsteer AI in ways that enrich our lives and our society, while encouraging creativity\n\nin the field, is an urgent and vital need. Policies should be evaluated as to whether\n\nthey democratically foster the development and equitable sharing of AI’s benefits,\n\nor concentrate power and benefits in the hands of a fortunate few. And since future\n\nAI technologies and their effects cannot be foreseen with perfect clarity, policies will\n\nneed to be continually re-evaluated in the context of observed societal challenges and\n\nevidence from fielded systems.\n\nAs this report documents, significant AI-related advances have already had\n\nan impact on North American cities over the past fifteen years, and even more\n\nsubstantial developments will occur over the next fifteen. Recent advances are largely\n\ndue to the growth and analysis of large data sets enabled by the Internet, advances\n\nin sensory technologies and, more recently, applications of “deep learning.” In the\n\ncoming years, as the public encounters new AI applications in domains such as\n\ntransportation and healthcare, they must be introduced in ways that build trust and\n\nunderstanding, and respect human and civil rights. While encouraging innovation,\n\npolicies and processes should address ethical, privacy, and security implications, and\n\nshould work to ensure that the benefits of AI technologies will be spread broadly and\n\nfairly. Doing so will be critical if Artificial Intelligence research and its applications\n\nare to exert a positive influence on North American urban life in 2030 and beyond.", "original_types": ["text"], "id": 154}
{"type": "section", "content": "APPENDIX I: A SHORT HISTORY OF AI\n\nThis Appendix is based primarily on Nilsson’s book140 and written from the prevalent current perspective, which focuses on data intensive methods and big data. However important, this focus has not yet shown itself to be the solution to all problems. A complete and fully balanced history of the field is beyond the scope of this document.\n\nThe field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. The goal was to investigate ways in which machines could be made to simulate aspects of intelligence—the essential idea that has continued to drive the field forward. McCarthy is credited with the first use of the term “artificial intelligence” in the proposal he co-authored for the workshop with Marvin Minsky, Nathaniel Rochester, and Claude Shannon.141 Many of the people who attended soon led significant projects under the banner of AI, including Arthur Samuel, Oliver Selfridge, Ray Solomonoff, Allen Newell, and Herbert Simon.\n\nAlthough the Dartmouth workshop created a unified identity for the field and a dedicated research community, many of the technical ideas that have come to characterize AI existed much earlier. In the eighteenth century, Thomas Bayes provided a framework for reasoning about the probability of events.142 In the nineteenth century, George Boole showed that logical reasoning—dating back to Aristotle—could be performed systematically in the same manner as solving a system of equations.143 By the turn of the twentieth century, progress in the experimental sciences had led to the emergence of the field of statistics,144 which enables inferences to be drawn rigorously from data. The idea of physically engineering a machine to execute sequences of instructions, which had captured the imagination of pioneers such as Charles Babbage, had matured by the 1950s, and resulted in the construction of the first electronic computers.145 Primitive robots, which could sense and act autonomously, had also been built by that time.146\n\nThe most influential ideas underpinning computer science came from Alan Turing, who proposed a formal model of computing. Turing’s classic essay, Computing Machinery and Intelligence,147 imagines the possibility of computers created for simulating intelligence and explores many of the ingredients now associated with AI, including how intelligence might be tested, and how machines might automatically learn. Though these ideas inspired AI, Turing did not have access to the computing resources needed to translate his ideas into action.\n\nSeveral focal areas in the quest for AI emerged between the 1950s and the 1970s.148\n\n140 Nilsson, The Quest for Artificial Intelligence.", "doc_id": "stone2022", "page": 50, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "APPENDIX I: A SHORT HISTORY OF AI\n\nThis Appendix is based primarily on Nilsson’s book140 and written from the prevalent current perspective, which focuses on data intensive methods and big data. However important, this focus has not yet shown itself to be the solution to all problems. A complete and fully balanced history of the field is beyond the scope of this document.\n\nThe field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. The goal was to investigate ways in which machines could be made to simulate aspects of intelligence—the essential idea that has continued to drive the field forward. McCarthy is credited with the first use of the term “artificial intelligence” in the proposal he co-authored for the workshop with Marvin Minsky, Nathaniel Rochester, and Claude Shannon.141 Many of the people who attended soon led significant projects under the banner of AI, including Arthur Samuel, Oliver Selfridge, Ray Solomonoff, Allen Newell, and Herbert Simon.\n\nAlthough the Dartmouth workshop created a unified identity for the field and a dedicated research community, many of the technical ideas that have come to characterize AI existed much earlier. In the eighteenth century, Thomas Bayes provided a framework for reasoning about the probability of events.142 In the nineteenth century, George Boole showed that logical reasoning—dating back to Aristotle—could be performed systematically in the same manner as solving a system of equations.143 By the turn of the twentieth century, progress in the experimental sciences had led to the emergence of the field of statistics,144 which enables inferences to be drawn rigorously from data. The idea of physically engineering a machine to execute sequences of instructions, which had captured the imagination of pioneers such as Charles Babbage, had matured by the 1950s, and resulted in the construction of the first electronic computers.145 Primitive robots, which could sense and act autonomously, had also been built by that time.146\n\nThe most influential ideas underpinning computer science came from Alan Turing, who proposed a formal model of computing. Turing’s classic essay, Computing Machinery and Intelligence,147 imagines the possibility of computers created for simulating intelligence and explores many of the ingredients now associated with AI, including how intelligence might be tested, and how machines might automatically learn. Though these ideas inspired AI, Turing did not have access to the computing resources needed to translate his ideas into action.\n\nSeveral focal areas in the quest for AI emerged between the 1950s and the 1970s.148\n\n140 Nilsson, The Quest for Artificial Intelligence.", "original_types": ["text", "header"], "id": 155}
{"type": "section", "content": "141 J. McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon, “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence,” August 31, 1955, accessed August 1, 2016, http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html.\n\n142 Thomas Bayes, “An Essay towards Solving a Problem in the Doctrine of Chances,” Philosophical Transactions of the Royal Society of London 53 (January 1, 1763): 370–418, accessed August 1, 2016, http://rstl.royalsocietypublishing.org/search?fulltext=an+essay+towards+solving&submit=yes &andorexactfulltext=and&x=0&y=0.\n\n143 George Boole, An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities, (Macmillan, 1854, reprinted with corrections, Dover Publications, New York, NY, 1958, and reissued by Cambridge University Press, 2009), accessed August 1, 2016, http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511693090.\n\n144 “History of statistics,” Wikipedia, Last modified June 3, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/History_of_statistics.\n\n145 Joel N. Shurkin, Engines of the Mind: The Evolution of the Computer from Mainframes to Microprocessors (New York: W. W. Norton & Company, 1996).\n\n146 William Grey Walter, “An Electromechanical Animal,” Dialectica 4 (1950): 42–49.\n\n147 A. M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950): 433–460.\n\n148 Marvin Minsky, “Steps toward Artificial Intelligence,” MIT Media Laboratory, October 24, 1960, accessed August 1, 2016, http://web.media.mit.edu/~minsky/papers/steps.html.", "doc_id": "stone2022", "page": 50, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "141 J. McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon, “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence,” August 31, 1955, accessed August 1, 2016, http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html.\n\n142 Thomas Bayes, “An Essay towards Solving a Problem in the Doctrine of Chances,” Philosophical Transactions of the Royal Society of London 53 (January 1, 1763): 370–418, accessed August 1, 2016, http://rstl.royalsocietypublishing.org/search?fulltext=an+essay+towards+solving&submit=yes &andorexactfulltext=and&x=0&y=0.\n\n143 George Boole, An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities, (Macmillan, 1854, reprinted with corrections, Dover Publications, New York, NY, 1958, and reissued by Cambridge University Press, 2009), accessed August 1, 2016, http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511693090.\n\n144 “History of statistics,” Wikipedia, Last modified June 3, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/History_of_statistics.\n\n145 Joel N. Shurkin, Engines of the Mind: The Evolution of the Computer from Mainframes to Microprocessors (New York: W. W. Norton & Company, 1996).\n\n146 William Grey Walter, “An Electromechanical Animal,” Dialectica 4 (1950): 42–49.\n\n147 A. M. Turing, “Computing Machinery and Intelligence,” Mind 59, no. 236 (1950): 433–460.\n\n148 Marvin Minsky, “Steps toward Artificial Intelligence,” MIT Media Laboratory, October 24, 1960, accessed August 1, 2016, http://web.media.mit.edu/~minsky/papers/steps.html.", "original_types": ["text"], "id": 156}
{"type": "section", "content": "Newell and Simon pioneered the foray into **heuristic search**, an efficient procedure for finding solutions in large, combinatorial spaces. In particular, they applied this idea to construct proofs of mathematical theorems, first through their Logic Theorist program, and then through the General Problem Solver.149 In the area of **computer vision**, early work in character recognition by Selfridge and colleagues150 laid the basis for more complex applications such as face recognition.151 By the late sixties, work had also begun on **natural language processing**.152 “Shakey”, a wheeled robot built at SRI International, launched the field of **mobile robotics**. Samuel’s Checkers-playing program, which improved itself through self-play, was one of the first working instances of a **machine learning** system.153 Rosenblatt’s *Perceptron*,154 a computational model based on biological neurons, became the basis for the field of **artificial neural networks**. Feigenbaum and others advocated155 the case for building **expert systems**—knowledge repositories tailored for specialized domains such as chemistry and medical diagnosis.156 Early conceptual progress assumed the existence of a symbolic system that could be reasoned about and built upon. But by the 1980s, despite this promising headway made into different aspects of artificial intelligence, the field still could boast no significant *practical* successes. This gap between theory and practice arose in part from an insufficient emphasis within the AI community on *grounding* systems physically, with direct access to environmental signals and data. There was also an overemphasis on Boolean (True/False) logic, overlooking the need to quantify uncertainty. The field was forced to take cognizance of these shortcomings in the mid-1980s, since interest in AI began to drop, and funding dried up. Nilsson calls this period the “AI winter.” A much needed resurgence in the nineties built upon the idea that “Good Old-Fashioned AI”157 was inadequate as an end-to-end approach to building intelligent systems. Rather, intelligent systems needed to be built from the ground up, at all times *solving* the task at hand, albeit with different degrees of proficiency.158 Technological progress had also made the task of building systems driven by real-world data more feasible. Cheaper and more reliable hardware for sensing and actuation made robots easier to build. Further, the Internet’s capacity for gathering large amounts of data, and the availability of computing power and storage to process that data, enabled statistical techniques that, by design, derive solutions from data. These developments have allowed AI to emerge in the past two decades as a profound influence on our daily lives, as detailed in Section II.", "doc_id": "stone2022", "page": 51, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Newell and Simon pioneered the foray into **heuristic search**, an efficient procedure for finding solutions in large, combinatorial spaces. In particular, they applied this idea to construct proofs of mathematical theorems, first through their Logic Theorist program, and then through the General Problem Solver.149 In the area of **computer vision**, early work in character recognition by Selfridge and colleagues150 laid the basis for more complex applications such as face recognition.151 By the late sixties, work had also begun on **natural language processing**.152 “Shakey”, a wheeled robot built at SRI International, launched the field of **mobile robotics**. Samuel’s Checkers-playing program, which improved itself through self-play, was one of the first working instances of a **machine learning** system.153 Rosenblatt’s *Perceptron*,154 a computational model based on biological neurons, became the basis for the field of **artificial neural networks**. Feigenbaum and others advocated155 the case for building **expert systems**—knowledge repositories tailored for specialized domains such as chemistry and medical diagnosis.156 Early conceptual progress assumed the existence of a symbolic system that could be reasoned about and built upon. But by the 1980s, despite this promising headway made into different aspects of artificial intelligence, the field still could boast no significant *practical* successes. This gap between theory and practice arose in part from an insufficient emphasis within the AI community on *grounding* systems physically, with direct access to environmental signals and data. There was also an overemphasis on Boolean (True/False) logic, overlooking the need to quantify uncertainty. The field was forced to take cognizance of these shortcomings in the mid-1980s, since interest in AI began to drop, and funding dried up. Nilsson calls this period the “AI winter.” A much needed resurgence in the nineties built upon the idea that “Good Old-Fashioned AI”157 was inadequate as an end-to-end approach to building intelligent systems. Rather, intelligent systems needed to be built from the ground up, at all times *solving* the task at hand, albeit with different degrees of proficiency.158 Technological progress had also made the task of building systems driven by real-world data more feasible. Cheaper and more reliable hardware for sensing and actuation made robots easier to build. Further, the Internet’s capacity for gathering large amounts of data, and the availability of computing power and storage to process that data, enabled statistical techniques that, by design, derive solutions from data. These developments have allowed AI to emerge in the past two decades as a profound influence on our daily lives, as detailed in Section II.", "original_types": ["text"], "id": 157}
{"type": "section", "content": "149 Allen Newell, John C Shaw, and Herbert A Simon, “Report on a general problem-solving program,” *Proceedings of the International Conference on Information Processing*, UNESCO, Paris 15-20 June 1959 (Unesco/Oldenbourg/Butterworths, 1960), 256–264.\n150 O. G. Selfridge, “Pandemonium: A paradigm for learning,” *Proceedings of the Symposium on Mechanization of Thought Processes* (London: H. M. Stationary Office, 1959): 511–531.\n151 Woodrow W. Bledsoe and Helen Chan, “A Man-Machine Facial Recognition System: Some Preliminary Results,” *Technical Report PRI 19A* (Palo Alto, California: Panoramic Research, Inc., 1965).\n152 D. Raj Reddy, “Speech Recognition by Machine: A Review,” *Proceedings of the IEEE 64*, no.4 (April 1976), 501–531.\n153 Arthur Samuel, “Some Studies in Machine Learning Using the Game of Checkers,” *IBM Journal of Research and Development 3*, no. 3 (1959): 210—229.\n154 Frank Rosenblatt, “The Perceptron—A Perceiving and Recognizing Automaton,” *Report 85-460-1*, (Buffalo, New York: Cornell Aeronautical Laboratory, 1957).\n155 “Shakey the robot,” *Wikipedia*, last modified July 11, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Shakey_the_robot.\n156 Edward A. Feigenbaum and Bruce G. Buchanan, “DENDRAL and Meta-DENDRAL: Roots of Knowledge Systems and Expert System Applications,” *Artificial Intelligence 59*, no. 1-2 (1993), 233–240.\n157 John Haugeland, Artificial Intelligence: The Very Idea, (Cambridge, Massachusetts: *MIT Press*, 1985).\n158 Rodney A. Brooks, “Elephants Don’t Play Chess,” *Robotics and Autonomous Systems 6*, no. 1-2 (June 1990): 3–15.", "doc_id": "stone2022", "page": 51, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "149 Allen Newell, John C Shaw, and Herbert A Simon, “Report on a general problem-solving program,” *Proceedings of the International Conference on Information Processing*, UNESCO, Paris 15-20 June 1959 (Unesco/Oldenbourg/Butterworths, 1960), 256–264.\n150 O. G. Selfridge, “Pandemonium: A paradigm for learning,” *Proceedings of the Symposium on Mechanization of Thought Processes* (London: H. M. Stationary Office, 1959): 511–531.\n151 Woodrow W. Bledsoe and Helen Chan, “A Man-Machine Facial Recognition System: Some Preliminary Results,” *Technical Report PRI 19A* (Palo Alto, California: Panoramic Research, Inc., 1965).\n152 D. Raj Reddy, “Speech Recognition by Machine: A Review,” *Proceedings of the IEEE 64*, no.4 (April 1976), 501–531.\n153 Arthur Samuel, “Some Studies in Machine Learning Using the Game of Checkers,” *IBM Journal of Research and Development 3*, no. 3 (1959): 210—229.\n154 Frank Rosenblatt, “The Perceptron—A Perceiving and Recognizing Automaton,” *Report 85-460-1*, (Buffalo, New York: Cornell Aeronautical Laboratory, 1957).\n155 “Shakey the robot,” *Wikipedia*, last modified July 11, 2016, accessed August 1, 2016, https://en.wikipedia.org/wiki/Shakey_the_robot.\n156 Edward A. Feigenbaum and Bruce G. Buchanan, “DENDRAL and Meta-DENDRAL: Roots of Knowledge Systems and Expert System Applications,” *Artificial Intelligence 59*, no. 1-2 (1993), 233–240.\n157 John Haugeland, Artificial Intelligence: The Very Idea, (Cambridge, Massachusetts: *MIT Press*, 1985).\n158 Rodney A. Brooks, “Elephants Don’t Play Chess,” *Robotics and Autonomous Systems 6*, no. 1-2 (June 1990): 3–15.", "original_types": ["text"], "id": 158}
{"type": "section", "content": "In summary, following is a list of some of the traditional sub-areas of AI. As described in Section II, some of them are currently “hotter” than others for various reasons. But that is neither to minimize the historical importance of the others, nor to say that they may not re-emerge as hot areas in the future.\n\n• Search and Planning deal with reasoning about goal-directed behavior. Search plays a key role, for example, in chess-playing programs such as Deep Blue, in deciding which move (behavior) will ultimately lead to a win (goal).\n\n• The area of Knowledge Representation and Reasoning involves processing information (typically when in large amounts) into a structured form that can be queried more reliably and efficiently. IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.159\n\n• Machine Learning is a paradigm that enables systems to automatically improve their performance at a task by observing relevant data. Indeed, machine learning has been the key contributor to the AI surge in the past few decades, ranging from search and product recommendation engines, to systems for speech recognition, fraud detection, image understanding, and countless other tasks that once relied on human skill and judgment. The automation of these tasks has enabled the scaling up of services such as e-commerce.\n\n• As more and more intelligent systems get built, a natural question to consider is how such systems will interact with each other. The field of Multi-Agent Systems considers this question, which is becoming increasingly important in on-line marketplaces and transportation systems.\n\n• From its early days, AI has taken up the design and construction of systems that are embodied in the real world. The area of Robotics investigates fundamental aspects of sensing and acting—and especially their integration—that enable a robot to behave effectively. Since robots and other computer systems share the living world with human beings, the specialized subject of Human Robot Interaction has also become prominent in recent decades.\n\n• Machine perception has always played a central role in AI, partly in developing robotics, but also as a completely independent area of study. The most commonly studied perception modalities are Computer Vision and Natural Language Processing, each of which is attended to by large and vibrant communities.\n\n• Several other focus areas within AI today are consequences of the growth of the Internet. Social Network Analysis investigates the effect of neighborhood relations in influencing the behavior of individuals and communities. Crowdsourcing is yet another innovative problem-solving technique, which relies on harnessing human intelligence (typically from thousands of humans) to solve hard computational problems.", "doc_id": "stone2022", "page": 52, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "In summary, following is a list of some of the traditional sub-areas of AI. As described in Section II, some of them are currently “hotter” than others for various reasons. But that is neither to minimize the historical importance of the others, nor to say that they may not re-emerge as hot areas in the future.\n\n• Search and Planning deal with reasoning about goal-directed behavior. Search plays a key role, for example, in chess-playing programs such as Deep Blue, in deciding which move (behavior) will ultimately lead to a win (goal).\n\n• The area of Knowledge Representation and Reasoning involves processing information (typically when in large amounts) into a structured form that can be queried more reliably and efficiently. IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.159\n\n• Machine Learning is a paradigm that enables systems to automatically improve their performance at a task by observing relevant data. Indeed, machine learning has been the key contributor to the AI surge in the past few decades, ranging from search and product recommendation engines, to systems for speech recognition, fraud detection, image understanding, and countless other tasks that once relied on human skill and judgment. The automation of these tasks has enabled the scaling up of services such as e-commerce.\n\n• As more and more intelligent systems get built, a natural question to consider is how such systems will interact with each other. The field of Multi-Agent Systems considers this question, which is becoming increasingly important in on-line marketplaces and transportation systems.\n\n• From its early days, AI has taken up the design and construction of systems that are embodied in the real world. The area of Robotics investigates fundamental aspects of sensing and acting—and especially their integration—that enable a robot to behave effectively. Since robots and other computer systems share the living world with human beings, the specialized subject of Human Robot Interaction has also become prominent in recent decades.\n\n• Machine perception has always played a central role in AI, partly in developing robotics, but also as a completely independent area of study. The most commonly studied perception modalities are Computer Vision and Natural Language Processing, each of which is attended to by large and vibrant communities.\n\n• Several other focus areas within AI today are consequences of the growth of the Internet. Social Network Analysis investigates the effect of neighborhood relations in influencing the behavior of individuals and communities. Crowdsourcing is yet another innovative problem-solving technique, which relies on harnessing human intelligence (typically from thousands of humans) to solve hard computational problems.", "original_types": ["text"], "id": 159}
{"type": "section", "content": "Although the separation of AI into sub-fields has enabled deep technical progress along several different fronts, synthesizing intelligence at any reasonable scale invariably requires many different ideas to be integrated. For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.", "doc_id": "stone2022", "page": 52, "url": "https://arxiv.org/pdf/2211.06318", "embedded_text": "Although the separation of AI into sub-fields has enabled deep technical progress along several different fronts, synthesizing intelligence at any reasonable scale invariably requires many different ideas to be integrated. For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.", "original_types": ["text"], "id": 160}
{"type": "section", "content": "Abstract\n\nLLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud-based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload characteristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.", "doc_id": "kim2025", "page": 1, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Abstract\n\nLLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud-based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload characteristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.", "original_types": ["text", "header"], "id": 161}
{"type": "section", "content": "II. BACKGROUND AND MOTIVATION\n\nA. LLM Architecture and Inference\n\nLarge-scale language models (LLMs), such as OpenAI’s GPT [2] and Meta’s LLaMA [3], are built on the Transformer [1] architecture. These models consist of a multi-layer structure incorporating Self-Attention mechanisms and Feed-Forward Networks, enabling their broad applicability across various natural language processing (NLP) tasks.\n\nThe LLM inference process is divided into two stages: Prefill and Decode. In the Prefill stage, the input prompt is processed in parallel to generate the initial output tokens. During this process, Query, Key, and Value vectors are computed for each token in the input prompt, capturing contextual information through token-wise interactions. Simultaneously, the computed Key and Value tensors are stored in the GPU memory as a Key-Value Cache (KV Cache) to alleviate computational overhead in subsequent operations.\n\nThe KV Cache is essential for preventing redundant computations in Self-Attention, thereby enhancing inference speed and resource efficiency. For instance, if the Prefill stage computes and stores the Key and Value tensors for the input \n\nIn the Decode stage, new tokens are sequentially generated in an Auto-Regressive manner based on previously generated output tokens. Here, the stored KV Cache is reused to reduce the computational burden of repeated Self-Attention operations and improve processing speed. However, the size of the KV Cache increases significantly with the input length and model size.\n\nFor example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion. This memory constraint may degrade overall system throughput and reduce resource utilization efficiency [1, 21].\n\nB. Memory Optimization for LLM Inference via KV Cache Offloading\n\nDuring LLM inference, the increasing size of the KV Cache can lead to GPU memory exhaustion, resulting in an Out-of-Memory (OoM) issue. To address this, KV Cache Offloading techniques have been proposed [6, 7, 8, 9]. These techniques operate by offloading KV Cache data that exceeds GPU memory capacity to CPU memory or disk and retrieving it back to the GPU when needed for computation. This approach effectively alleviates the GPU memory pressure, enabling the processing of long sequences and large batch sizes. Additionally, it allows efficient inference on lower-end GPUs without requiring additional high-performance GPUs, thus reducing deployment costs.", "doc_id": "kim2025", "page": 2, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "II. BACKGROUND AND MOTIVATION\n\nA. LLM Architecture and Inference\n\nLarge-scale language models (LLMs), such as OpenAI’s GPT [2] and Meta’s LLaMA [3], are built on the Transformer [1] architecture. These models consist of a multi-layer structure incorporating Self-Attention mechanisms and Feed-Forward Networks, enabling their broad applicability across various natural language processing (NLP) tasks.\n\nThe LLM inference process is divided into two stages: Prefill and Decode. In the Prefill stage, the input prompt is processed in parallel to generate the initial output tokens. During this process, Query, Key, and Value vectors are computed for each token in the input prompt, capturing contextual information through token-wise interactions. Simultaneously, the computed Key and Value tensors are stored in the GPU memory as a Key-Value Cache (KV Cache) to alleviate computational overhead in subsequent operations.\n\nThe KV Cache is essential for preventing redundant computations in Self-Attention, thereby enhancing inference speed and resource efficiency. For instance, if the Prefill stage computes and stores the Key and Value tensors for the input \n\nIn the Decode stage, new tokens are sequentially generated in an Auto-Regressive manner based on previously generated output tokens. Here, the stored KV Cache is reused to reduce the computational burden of repeated Self-Attention operations and improve processing speed. However, the size of the KV Cache increases significantly with the input length and model size.\n\nFor example, as shown in Figure 1, in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion. This memory constraint may degrade overall system throughput and reduce resource utilization efficiency [1, 21].\n\nB. Memory Optimization for LLM Inference via KV Cache Offloading\n\nDuring LLM inference, the increasing size of the KV Cache can lead to GPU memory exhaustion, resulting in an Out-of-Memory (OoM) issue. To address this, KV Cache Offloading techniques have been proposed [6, 7, 8, 9]. These techniques operate by offloading KV Cache data that exceeds GPU memory capacity to CPU memory or disk and retrieving it back to the GPU when needed for computation. This approach effectively alleviates the GPU memory pressure, enabling the processing of long sequences and large batch sizes. Additionally, it allows efficient inference on lower-end GPUs without requiring additional high-performance GPUs, thus reducing deployment costs.", "original_types": ["text", "header"], "id": 162}
{"type": "section", "content": "However, latency introduced by data transfer between the GPU and external storage (e.g., CPU memory or disk) is a major limitation of KV Cache Offloading. If the transfer frequency of KV Cache data is high, the increased latency can lead to bandwidth bottlenecks, ultimately degrading inference performance. Therefore, for effective deployment of KV Cache Offloading, it is essential to optimize the process by considering LLM inference characteristics (e.g., sequence length, batch size) and user-defined Service Level Objectives (SLOs), such as maximum allowable response time.\n\nC. Challenges of LLM Inference and KV Cache Offloading in the Cloud\n\nCloud service providers (CSPs) such as Amazon AWS offer a variety of GPU virtual machine (VM) instances. As shown in Table I, the price of these instances varies significantly, ranging from $0.379 (g4ad.xlarge) to $40.96 (p4de.24xlarge), depending on the type of GPU, the memory capacity, and the bandwidth of the network [22].\n\nMoreover, when applying KV Cache Offloading to LLM inference, the trade-off between inference performance and actual cost introduces a complex dilemma. To maximize cost-efficiency, users must carefully optimize their choice of VM and offloading strategy based on: (i) Model size, (ii) Sequence length, and (iii) Service Level Objectives (SLOs), such as maximum response time.\n\nHowever, a systematic framework for making these decisions is currently lacking. As a result, users must experiment with multiple VM options and offloading policies manually to operations and improve processing speed. However, the size of the KV Cache increases significantly with the input length and model size.", "doc_id": "kim2025", "page": 2, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "However, latency introduced by data transfer between the GPU and external storage (e.g., CPU memory or disk) is a major limitation of KV Cache Offloading. If the transfer frequency of KV Cache data is high, the increased latency can lead to bandwidth bottlenecks, ultimately degrading inference performance. Therefore, for effective deployment of KV Cache Offloading, it is essential to optimize the process by considering LLM inference characteristics (e.g., sequence length, batch size) and user-defined Service Level Objectives (SLOs), such as maximum allowable response time.\n\nC. Challenges of LLM Inference and KV Cache Offloading in the Cloud\n\nCloud service providers (CSPs) such as Amazon AWS offer a variety of GPU virtual machine (VM) instances. As shown in Table I, the price of these instances varies significantly, ranging from $0.379 (g4ad.xlarge) to $40.96 (p4de.24xlarge), depending on the type of GPU, the memory capacity, and the bandwidth of the network [22].\n\nMoreover, when applying KV Cache Offloading to LLM inference, the trade-off between inference performance and actual cost introduces a complex dilemma. To maximize cost-efficiency, users must carefully optimize their choice of VM and offloading strategy based on: (i) Model size, (ii) Sequence length, and (iii) Service Level Objectives (SLOs), such as maximum response time.\n\nHowever, a systematic framework for making these decisions is currently lacking. As a result, users must experiment with multiple VM options and offloading policies manually to operations and improve processing speed. However, the size of the KV Cache increases significantly with the input length and model size.", "original_types": ["text", "header"], "id": 163}
{"type": "section", "content": "Various Types of instances provided by AWS.\n\nThis information was available on February 4, 2025 in N.Virginia region.", "doc_id": "kim2025", "page": 3, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Various Types of instances provided by AWS.\n\nThis information was available on February 4, 2025 in N.Virginia region.", "original_types": ["text", "header"], "id": 164}
{"type": "table", "content": "Table 1\nVarious Types of instances provided by AWS.\nThis information was available on February 4, 2025 in N.Virginia region.", "doc_id": "kim2025", "page": 3, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Table 1\nVarious Types of instances provided by AWS.\nThis information was available on February 4, 2025 in N.Virginia region.", "id": 165}
{"type": "figure", "content": "Figure 1", "doc_id": "kim2025", "page": 3, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Figure 1", "id": 166}
{"type": "section", "content": "III. Problem Definition\n\nA. Definition of Service Level Objective (SLO) Metrics\n\nIn cloud environments, large language model (LLM) inference involves a complex trade-off between memory constraints, cost, and service quality. Depending on the type of inference task, users may have different Service Level Objectives (SLOs).\n\nIn this paper, we define two types of inference tasks: Online Inference and Offline Inference.\n\nOnline Inference (e.g., chatbots, voice assistants) prioritizes low response latency (e.g., within 100ms) over query throughput, as real-time responsiveness is crucial. Thus, response time is used as the primary SLO metric.\n\nOffline Inference (e.g., batch processing of large datasets) prioritizes high query throughput over response latency, making throughput the primary SLO metric.\n\nTo encompass both of these metrics under a unified framework, we define Tokens Per Second (TPS) as the SLO metric. TPS represents the number of tokens processed per second, including both input tokens (L_in) and output tokens (L_out).\n\nLLM inference is typically performed in batches, where a batch consists of multiple queries (BS). Given that the total processing time for a batch is denoted as T_E2E, TPS is defined as follows:\n\nTPS = BS × (L_in + L_out) / T_E2E\n\nB. Definition of Cost Efficiency\n\nIn this study, our primary objective is to minimize user costs while ensuring that inference tasks meet their designated SLOs. To achieve this, we define a cost efficiency metric based on the previously introduced Tokens Per Second (TPS) metric.\n\nLet TPS_SLO denote the target TPS required by the user to meet the SLO, and let TPS_actual represent the actual throughput achieved during inference. Considering that the effective processing rate cannot exceed the user-defined SLO threshold, the effective TPS is defined as: TPS_effective = min(TPS_actual, TPS_SLO)", "doc_id": "kim2025", "page": 4, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "III. Problem Definition\n\nA. Definition of Service Level Objective (SLO) Metrics\n\nIn cloud environments, large language model (LLM) inference involves a complex trade-off between memory constraints, cost, and service quality. Depending on the type of inference task, users may have different Service Level Objectives (SLOs).\n\nIn this paper, we define two types of inference tasks: Online Inference and Offline Inference.\n\nOnline Inference (e.g., chatbots, voice assistants) prioritizes low response latency (e.g., within 100ms) over query throughput, as real-time responsiveness is crucial. Thus, response time is used as the primary SLO metric.\n\nOffline Inference (e.g., batch processing of large datasets) prioritizes high query throughput over response latency, making throughput the primary SLO metric.\n\nTo encompass both of these metrics under a unified framework, we define Tokens Per Second (TPS) as the SLO metric. TPS represents the number of tokens processed per second, including both input tokens (L_in) and output tokens (L_out).\n\nLLM inference is typically performed in batches, where a batch consists of multiple queries (BS). Given that the total processing time for a batch is denoted as T_E2E, TPS is defined as follows:\n\nTPS = BS × (L_in + L_out) / T_E2E\n\nB. Definition of Cost Efficiency\n\nIn this study, our primary objective is to minimize user costs while ensuring that inference tasks meet their designated SLOs. To achieve this, we define a cost efficiency metric based on the previously introduced Tokens Per Second (TPS) metric.\n\nLet TPS_SLO denote the target TPS required by the user to meet the SLO, and let TPS_actual represent the actual throughput achieved during inference. Considering that the effective processing rate cannot exceed the user-defined SLO threshold, the effective TPS is defined as: TPS_effective = min(TPS_actual, TPS_SLO)", "original_types": ["text", "header", "equation", "list"], "id": 167}
{"type": "figure", "content": "Fig. 2. Comparison of cost efficiency per GPU instance based on SLO constraints and batch size, based on experimental results using the OPT-2.7B model on an AWS g4dn.xlarge instance with an input length of 512 tokens and an output length of 128 tokens.", "doc_id": "kim2025", "page": 5, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Fig. 2. Comparison of cost efficiency per GPU instance based on SLO constraints and batch size, based on experimental results using the OPT-2.7B model on an AWS g4dn.xlarge instance with an input length of 512 tokens and an output length of 128 tokens.", "id": 168}
{"type": "table", "content": "TABLE II NOTATION AND FORMULAS FOR MODEL AND MEMORY COMPUTATION", "doc_id": "kim2025", "page": 5, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "TABLE II NOTATION AND FORMULAS FOR MODEL AND MEMORY COMPUTATION", "id": 169}
{"type": "section", "content": "This stage plays a crucial role in transforming user requirements into quantitative parameters, establishing the foundation for resource suitability assessment and performance prediction. Ultimately, it is essential for selecting the most cost-efficient GPU instance that meets both performance objectives and budget constraints.\n\nAt this stage, the system evaluates the memory requirements for inference based on the collected user parameters and assesses the feasibility of KV Cache Offloading to identify the most suitable GPU instance candidates. First, the system calculates the total memory requirement Mem_total for the given Transformer-based LLM model and its input-output parameters. This is defined as the sum of the following three components: Mem_total = Mem_model + Mem_activation + Mem_KVcache. Additionally, the base memory requirement is defined as: Mem_base = Mem_model + Mem_activation. These stages follow three key criteria to evaluate GPU instance suitability and Algorithm 1.", "doc_id": "kim2025", "page": 5, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "This stage plays a crucial role in transforming user requirements into quantitative parameters, establishing the foundation for resource suitability assessment and performance prediction. Ultimately, it is essential for selecting the most cost-efficient GPU instance that meets both performance objectives and budget constraints.\n\nAt this stage, the system evaluates the memory requirements for inference based on the collected user parameters and assesses the feasibility of KV Cache Offloading to identify the most suitable GPU instance candidates. First, the system calculates the total memory requirement Mem_total for the given Transformer-based LLM model and its input-output parameters. This is defined as the sum of the following three components: Mem_total = Mem_model + Mem_activation + Mem_KVcache. Additionally, the base memory requirement is defined as: Mem_base = Mem_model + Mem_activation. These stages follow three key criteria to evaluate GPU instance suitability and Algorithm 1.", "original_types": ["text"], "id": 170}
{"type": "section", "content": "Algorithm 1: Resource Suitability Evaluation and Instance Selection (Price Priority)\n\nInput: Memory Requirements:\nMem_model — Model weight memory requirement\nMem_activation — Activation memory requirement\nMem_KVcache — Total KV Cache memory requirement\nMem_KVcache, per_layer — KV Cache memory per layer\nFor each GPU instance i:\nGPU_memory — Total GPU memory\nGPU_price — GPU price\nUser-defined maximum price: P_max\nOutput: GPU candidates that satisfy both price and resource conditions\n\n1 Candidates ← ∅ // Initialize candidate set\n2 Mem_total ← Mem_model + Mem_activation + Mem_KVcache ;\n3 Mem_base ← Mem_model + Mem_activation ;\n4 for each GPU instance i do\n5    if GPU_price ≤ P_max then\n6        // Apply Price Filter\n7            Mem_iavail ← GPU_memory − Mem_base // Calculate Available Memory\n8            if GPU_memory ≥ Mem_total then\n9                // Offloading Not Required\n10                   C_off ← 0 ;\n11                   Add (i, C_off) to Candidate Set ;\n12            else\n13                if GPU_memory < Mem_model OR Mem_KVcache, per_layer > Mem_iavail\n14                    // Offloading Not Possible\n15                        Mark GPU i as Unsuitable // Exclude from candidates\n16                else\n17                    // KV Cache Offloading Required\n18                        C_off ← 1 − Mem_iavail ;\n19                        if Mem_KVcache, per_layer ≤ Mem_iavail then\n20                            // Layer-Level Constraint Check\n21                                Add (i, C_off) to Candidate Set ;\n22                        else\n23                            Mark GPU i as Unsuitable // Exclude from candidates\n24                    end\n25                end\n26            end\n27        Sort Candidate Set by GPU_price in ascending order ;\n28        return Candidate Set Candidates ;\n\nThus, the total task processing time T_task is expressed as follows:", "doc_id": "kim2025", "page": 6, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Algorithm 1: Resource Suitability Evaluation and Instance Selection (Price Priority)\n\nInput: Memory Requirements:\nMem_model — Model weight memory requirement\nMem_activation — Activation memory requirement\nMem_KVcache — Total KV Cache memory requirement\nMem_KVcache, per_layer — KV Cache memory per layer\nFor each GPU instance i:\nGPU_memory — Total GPU memory\nGPU_price — GPU price\nUser-defined maximum price: P_max\nOutput: GPU candidates that satisfy both price and resource conditions\n\n1 Candidates ← ∅ // Initialize candidate set\n2 Mem_total ← Mem_model + Mem_activation + Mem_KVcache ;\n3 Mem_base ← Mem_model + Mem_activation ;\n4 for each GPU instance i do\n5    if GPU_price ≤ P_max then\n6        // Apply Price Filter\n7            Mem_iavail ← GPU_memory − Mem_base // Calculate Available Memory\n8            if GPU_memory ≥ Mem_total then\n9                // Offloading Not Required\n10                   C_off ← 0 ;\n11                   Add (i, C_off) to Candidate Set ;\n12            else\n13                if GPU_memory < Mem_model OR Mem_KVcache, per_layer > Mem_iavail\n14                    // Offloading Not Possible\n15                        Mark GPU i as Unsuitable // Exclude from candidates\n16                else\n17                    // KV Cache Offloading Required\n18                        C_off ← 1 − Mem_iavail ;\n19                        if Mem_KVcache, per_layer ≤ Mem_iavail then\n20                            // Layer-Level Constraint Check\n21                                Add (i, C_off) to Candidate Set ;\n22                        else\n23                            Mark GPU i as Unsuitable // Exclude from candidates\n24                    end\n25                end\n26            end\n27        Sort Candidate Set by GPU_price in ascending order ;\n28        return Candidate Set Candidates ;\n\nThus, the total task processing time T_task is expressed as follows:", "original_types": ["text", "header"], "id": 171}
{"type": "figure", "content": "Fig. 3. Comparison of FLOPS provided by the GPU manufacturer (NVIDIA) and the actual FLOPS utilized when calculating Prefill time on AWS GPU VMs. The results present TFLOPS measurements for three different GPU VMs using the OPT-2.7B model with an input size of 512 tokens and an output size of 128 tokens as batch size grows.", "doc_id": "kim2025", "page": 7, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Fig. 3. Comparison of FLOPS provided by the GPU manufacturer (NVIDIA) and the actual FLOPS utilized when calculating Prefill time on AWS GPU VMs. The results present TFLOPS measurements for three different GPU VMs using the OPT-2.7B model with an input size of 512 tokens and an output size of 128 tokens as batch size grows.", "id": 172}
{"type": "section", "content": "This modeling approach accounts for both cases where offloading is necessary and unnecessary, effectively considering GPU memory constraints and computational performance. By incorporating both computation latency and KV Cache offloading overhead, this approach enables a quantitative analysis of the trade-off between computation and memory access time in both Prefill and Decode stages.\n\nUsing this modeling framework, Tokens Per Second (TPS) can be estimated, allowing for the selection of the most optimal GPU instance for a given inference task. While this theoretical modeling provides a solid foundation, it is important to note that GPU manufacturers' theoretical FLOPS values do not always accurately reflect real-world LLM inference workloads. The limitations of this approach, along with the Compute Time Calibration Function (CTCF) designed to correct these discrepancies, are discussed in Section IV-F.\n\nStep 4: Final Instance Selection Based on SLO\n\nBased on the TPS (Tokens Per Second) values computed for each GPU instance in the previous stage, this step selects the most cost-efficient instance while ensuring that the user's Service Level Objective (SLO) is met. The selection process follows these steps:\n\nFirst, instances that fail to satisfy the user-defined SLO constraint (TPS ≥ TPS_SLO) are eliminated from consideration. Next, the cost efficiency metric (Equation 3) is calculated for each remaining instance. Finally, the instance with the highest cost efficiency is selected. In the event of a tie, the instance with the higher TPS is prioritized.\n\nThe final selection result is presented to the user along with comprehensive details, including instance type, expected TPS, cost, and KV Cache offloading configuration. Additionally, the system provides alternative options and a performance-cost trade-off analysis, enabling users to make an informed decision that is optimized for their specific LLM inference workload.\n\nF. Compute Time Calibration Function (CTCF)\n\nThe theoretical FLOPS values provided by GPU manufacturers do not accurately reflect real-world performance in LLM inference workloads. Figure 3 illustrates the discrepancy between the FLOPS values advertised by the manufacturer and those actually utilized in computation across three different GPU instances. This discrepancy arises from factors such as memory bottlenecks, reduced GPU utilization, and variations in computation patterns, which manifest differently in the Prefill and Decode stages of LLM inference. As a result, selecting a GPU instance solely based on theoretical FLOPS can lead to significant performance mismatches, causing users to incur unnecessary costs. To address this issue, it is essential to introduce a calibration method that aligns theoretical FLOPS values with actual computational performance.", "doc_id": "kim2025", "page": 7, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "This modeling approach accounts for both cases where offloading is necessary and unnecessary, effectively considering GPU memory constraints and computational performance. By incorporating both computation latency and KV Cache offloading overhead, this approach enables a quantitative analysis of the trade-off between computation and memory access time in both Prefill and Decode stages.\n\nUsing this modeling framework, Tokens Per Second (TPS) can be estimated, allowing for the selection of the most optimal GPU instance for a given inference task. While this theoretical modeling provides a solid foundation, it is important to note that GPU manufacturers' theoretical FLOPS values do not always accurately reflect real-world LLM inference workloads. The limitations of this approach, along with the Compute Time Calibration Function (CTCF) designed to correct these discrepancies, are discussed in Section IV-F.\n\nStep 4: Final Instance Selection Based on SLO\n\nBased on the TPS (Tokens Per Second) values computed for each GPU instance in the previous stage, this step selects the most cost-efficient instance while ensuring that the user's Service Level Objective (SLO) is met. The selection process follows these steps:\n\nFirst, instances that fail to satisfy the user-defined SLO constraint (TPS ≥ TPS_SLO) are eliminated from consideration. Next, the cost efficiency metric (Equation 3) is calculated for each remaining instance. Finally, the instance with the highest cost efficiency is selected. In the event of a tie, the instance with the higher TPS is prioritized.\n\nThe final selection result is presented to the user along with comprehensive details, including instance type, expected TPS, cost, and KV Cache offloading configuration. Additionally, the system provides alternative options and a performance-cost trade-off analysis, enabling users to make an informed decision that is optimized for their specific LLM inference workload.\n\nF. Compute Time Calibration Function (CTCF)\n\nThe theoretical FLOPS values provided by GPU manufacturers do not accurately reflect real-world performance in LLM inference workloads. Figure 3 illustrates the discrepancy between the FLOPS values advertised by the manufacturer and those actually utilized in computation across three different GPU instances. This discrepancy arises from factors such as memory bottlenecks, reduced GPU utilization, and variations in computation patterns, which manifest differently in the Prefill and Decode stages of LLM inference. As a result, selecting a GPU instance solely based on theoretical FLOPS can lead to significant performance mismatches, causing users to incur unnecessary costs. To address this issue, it is essential to introduce a calibration method that aligns theoretical FLOPS values with actual computational performance.", "original_types": ["text", "header"], "id": 173}
{"type": "section", "content": "CTCF Modeling: This study conducted preliminary experiments across various batch sizes to analyze the relationship between LLM inference time and batch size. The results consistently showed a linear increase in inference time for both the Prefill and Decode stages. This linear trend was observed across different GPU architectures, including T4, A10G, L4, and L40s, leading to the introduction of a regression-based CTCF model.\n\nCTCF is a linear transformation function that adjusts theoretical computation time to match actual execution time. It is defined as follows:\n\nCTCF(T_compute) = α · T_compute + β\n\nwhere α is a scaling factor that corrects overestimation or underestimation of theoretical computation time, and β is a fixed offset that compensates for systematic delays caused by GPU execution bottlenecks, memory access latency, and other hardware constraints. These parameters are optimized using the least squares method and are determined through pre-profiling experiments.\n\nThrough extensive pre-profiling, α and β values were computed for all AWS GPU instances across various batch sizes and stored as reference data. As shown in Table III, applying these per-instance α and β values significantly reduces the prediction error, bringing the adjusted execution time very close to the actual measurement. Based on this, InferSave profiles α and β values for all available AWS GPU instances, enabling precise FLOPS-based execution time predictions and recommending the optimal instance for users.", "doc_id": "kim2025", "page": 7, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "CTCF Modeling: This study conducted preliminary experiments across various batch sizes to analyze the relationship between LLM inference time and batch size. The results consistently showed a linear increase in inference time for both the Prefill and Decode stages. This linear trend was observed across different GPU architectures, including T4, A10G, L4, and L40s, leading to the introduction of a regression-based CTCF model.\n\nCTCF is a linear transformation function that adjusts theoretical computation time to match actual execution time. It is defined as follows:\n\nCTCF(T_compute) = α · T_compute + β\n\nwhere α is a scaling factor that corrects overestimation or underestimation of theoretical computation time, and β is a fixed offset that compensates for systematic delays caused by GPU execution bottlenecks, memory access latency, and other hardware constraints. These parameters are optimized using the least squares method and are determined through pre-profiling experiments.\n\nThrough extensive pre-profiling, α and β values were computed for all AWS GPU instances across various batch sizes and stored as reference data. As shown in Table III, applying these per-instance α and β values significantly reduces the prediction error, bringing the adjusted execution time very close to the actual measurement. Based on this, InferSave profiles α and β values for all available AWS GPU instances, enabling precise FLOPS-based execution time predictions and recommending the optimal instance for users.", "original_types": ["text"], "id": 174}
{"type": "table", "content": "Table III Values of α, β to calculate adjusted T_Prefill (Model: OPT 2.7B)\nMarkdown representation of the table", "doc_id": "kim2025", "page": 7, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Table III Values of α, β to calculate adjusted T_Prefill (Model: OPT 2.7B)\nMarkdown representation of the table", "id": 175}
{"type": "section", "content": "The CTCF-based correction method effectively compensates for the inherent limitations of theoretical FLOPS values provided by GPU manufacturers, leading to more accurate LLM inference performance predictions.\n\nV. Implementation\n\nWe developed InferSave using Python (3.10.14). For performance modeling and KV cache offloading optimization, we utilized NumPy (1.24.3) for efficient numerical computations", "doc_id": "kim2025", "page": 7, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "The CTCF-based correction method effectively compensates for the inherent limitations of theoretical FLOPS values provided by GPU manufacturers, leading to more accurate LLM inference performance predictions.\n\nV. Implementation\n\nWe developed InferSave using Python (3.10.14). For performance modeling and KV cache offloading optimization, we utilized NumPy (1.24.3) for efficient numerical computations", "original_types": ["text", "header"], "id": 176}
{"type": "section", "content": "VI. Evaluation\n\nA. Experimental setup\n\nFor our evaluation, we conducted two contrasting inference tasks representative of online and offline inference scenarios to comprehensively assess the impact of offloading strategies on cost and performance across various cloud-based GPU instances. The objective of the evaluation is to quantitatively analyze the effects of offloading and the impacts it has on cost and performance efficiency, as well as to pick the optimal instance given a SLO as input. Online inferencing focuses on finding the most price-effective inference while meeting the strict SLO requirement, while offline inferencing relaxes the SLO requirement, allowing for strategies such as offloading and used lower priced instances. All experiments were performed 3 times for each instance to maintain result integrity, and the average of each result were used for analysis. Workload Definition: For a holistic evaluation of InferSave's ability to select the optimal instance in a variety of scenarios, we perform two contrasting inference workloads. • Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests. • Offline Inference workload: To model a batch processing task, an input size of 1024 tokens and an output size of 128 tokens was used. This takes into account tasks such as document summarization and data wrangling. To simulate a batch processing task, the workload evaluates the performance of completing 1000 requests. AWS Cloud Experiment Setup: To maintain uniform experimental conditions and reduce potential disruptions caused by fluctuating cloud workloads, all experiments were carried out on AWS in the us-east-1 (Northern Virginia) region between 9:00 AM and 10:00 PM KST, spanning the period from December 2024 to March 2025. To avoid performance variations due to regional resource contention, testing was evenly distributed across availability zones us-east-1a through us-east-1f. For the GPU-VMs, we utilized g4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G), g6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s) A detailed specification of the instances are specified in Table IV.", "doc_id": "kim2025", "page": 8, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "VI. Evaluation\n\nA. Experimental setup\n\nFor our evaluation, we conducted two contrasting inference tasks representative of online and offline inference scenarios to comprehensively assess the impact of offloading strategies on cost and performance across various cloud-based GPU instances. The objective of the evaluation is to quantitatively analyze the effects of offloading and the impacts it has on cost and performance efficiency, as well as to pick the optimal instance given a SLO as input. Online inferencing focuses on finding the most price-effective inference while meeting the strict SLO requirement, while offline inferencing relaxes the SLO requirement, allowing for strategies such as offloading and used lower priced instances. All experiments were performed 3 times for each instance to maintain result integrity, and the average of each result were used for analysis. Workload Definition: For a holistic evaluation of InferSave's ability to select the optimal instance in a variety of scenarios, we perform two contrasting inference workloads. • Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests. • Offline Inference workload: To model a batch processing task, an input size of 1024 tokens and an output size of 128 tokens was used. This takes into account tasks such as document summarization and data wrangling. To simulate a batch processing task, the workload evaluates the performance of completing 1000 requests. AWS Cloud Experiment Setup: To maintain uniform experimental conditions and reduce potential disruptions caused by fluctuating cloud workloads, all experiments were carried out on AWS in the us-east-1 (Northern Virginia) region between 9:00 AM and 10:00 PM KST, spanning the period from December 2024 to March 2025. To avoid performance variations due to regional resource contention, testing was evenly distributed across availability zones us-east-1a through us-east-1f. For the GPU-VMs, we utilized g4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G), g6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s) A detailed specification of the instances are specified in Table IV.", "original_types": ["text", "header"], "id": 177}
{"type": "table", "content": "Table IV Specifications of VM instances, including 4 GPU-VMs based on AWS specifications.\nMarkdown representation of the table", "doc_id": "kim2025", "page": 8, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Table IV Specifications of VM instances, including 4 GPU-VMs based on AWS specifications.\nMarkdown representation of the table", "id": 178}
{"type": "section", "content": "To validate the effectiveness of InferSave, major transformer-based LLM models such as OPT-1.3B, OPT-2.7B, OPT-6.7B were used for testing in an in-house benchmark suite. To find the optimal performance configuration, tests were conducted by varying the batch size from 1 to 64 under different conditions for single GPU processing. Policy To Select Instance: As stated in Section II-D, there are no clear state of the art methodologies for GPU instance selection for inferencing. Therefore, in our evaluation, we compared the following two baseline approaches with InferSave. • Most powerful instance(Max-Performance): This policy simply chooses the GPU instance that offers the most performance, and aims to lower latency and raise throughput as much as possible. However, this methodology does not take into consideration price, and therefore running costs can be raised needlessly. • Simple performance prediction(InferSave (without KV Cache offloading)): This policy uses theoretical performance metrics (FLOPS, memory bandwidth) to predict performance and select an instance. However, it does not take into consideration the effects of KV Cache offloading, and may not be able to find the most optimal instance.", "doc_id": "kim2025", "page": 8, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "To validate the effectiveness of InferSave, major transformer-based LLM models such as OPT-1.3B, OPT-2.7B, OPT-6.7B were used for testing in an in-house benchmark suite. To find the optimal performance configuration, tests were conducted by varying the batch size from 1 to 64 under different conditions for single GPU processing. Policy To Select Instance: As stated in Section II-D, there are no clear state of the art methodologies for GPU instance selection for inferencing. Therefore, in our evaluation, we compared the following two baseline approaches with InferSave. • Most powerful instance(Max-Performance): This policy simply chooses the GPU instance that offers the most performance, and aims to lower latency and raise throughput as much as possible. However, this methodology does not take into consideration price, and therefore running costs can be raised needlessly. • Simple performance prediction(InferSave (without KV Cache offloading)): This policy uses theoretical performance metrics (FLOPS, memory bandwidth) to predict performance and select an instance. However, it does not take into consideration the effects of KV Cache offloading, and may not be able to find the most optimal instance.", "original_types": ["text"], "id": 179}
{"type": "figure", "content": "Fig. 4. CTCF accuracy analysis. The results illustrate the predicted time (blue), actual time (red), and CTCF-adjusted values (green) for Prefill and Decode times as batch size increases on two different GPU VMs. Additionally, the Error Rate between the CTCF-adjusted time and actual time is presented.", "doc_id": "kim2025", "page": 9, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Fig. 4. CTCF accuracy analysis. The results illustrate the predicted time (blue), actual time (red), and CTCF-adjusted values (green) for Prefill and Decode times as batch size increases on two different GPU VMs. Additionally, the Error Rate between the CTCF-adjusted time and actual time is presented.", "id": 180}
{"type": "figure", "content": "Fig. 5. Comparison of average TPS and cost for different InferSave configurations and the baseline configuration under varying SLO constraints for online inference workloads (Left: Average TPS, Right: Cost).", "doc_id": "kim2025", "page": 9, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Fig. 5. Comparison of average TPS and cost for different InferSave configurations and the baseline configuration under varying SLO constraints for online inference workloads (Left: Average TPS, Right: Cost).", "id": 181}
{"type": "table", "content": "Table VI Comparison of Instance Selection Results by SLO Constraints (100 TPS and 200 TPS)\nThe table compares the instance selection results by SLO constraints for 100 TPS and 200 TPS. It includes columns for SLO, Evaluated Policies, Selected Instances, C_off(%), TPS(avg.), and Total Price($).", "doc_id": "kim2025", "page": 9, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Table VI Comparison of Instance Selection Results by SLO Constraints (100 TPS and 200 TPS)\nThe table compares the instance selection results by SLO constraints for 100 TPS and 200 TPS. It includes columns for SLO, Evaluated Policies, Selected Instances, C_off(%), TPS(avg.), and Total Price($).", "id": 182}
{"type": "figure", "content": "Fig. 6. Comparison of average TPS and cost for different InferSave configurations and the baseline configuration under varying SLO constraints for offline inference workloads (Left: Average TPS, Right: Cost).", "doc_id": "kim2025", "page": 10, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "Fig. 6. Comparison of average TPS and cost for different InferSave configurations and the baseline configuration under varying SLO constraints for offline inference workloads (Left: Average TPS, Right: Cost).", "id": 183}
{"type": "section", "content": "made by each policy. As this workload uses a large input token size, all instances excluding g6e.xlarge make use of KV Cache offloading. Without considering offloading, only one instance can be considered a top choice, and therefore, InferSave without offloading chose the same instance as the Max-Performance policy.", "doc_id": "kim2025", "page": 10, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "made by each policy. As this workload uses a large input token size, all instances excluding g6e.xlarge make use of KV Cache offloading. Without considering offloading, only one instance can be considered a top choice, and therefore, InferSave without offloading chose the same instance as the Max-Performance policy.", "original_types": ["text"], "id": 184}
{"type": "section", "content": "F. Azhar, et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n\nAnthropic, “Message batches api,” 2024. https://www.anthropic.com/news/message-batches-api, Accessed: December 30, 2024.\n\nOpenAI, “Batch processing and rate limits,” 2024. https://platform.openai.com/docs/guides/batch#rate-limits, Accessed: December 30, 2024.\n\nY. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in International Conference on Machine Learning, pp. 31094–31116, PMLR, 2023.\n\nY. Xiong, H. Wu, C. Shao, Z. Wang, R. Zhang, Y. Guo, J. Zhao, K. Zhang, and Z. Pan, “Layerkv: Optimizing large language model serving with layer-wise kv cache management,” 2024.\n\nX. Pan, E. Li, Q. Li, S. Liang, Y. Shan, K. Zhou, Y. Luo, X. Wang, and J. Zhang, “Instinfer: In-storage attention offloading for cost-effective long-context llm inference,” 2024.\n\nR. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, and Y. He, “Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale,” in SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–15, 2022.\n\nG. Cloud, “Compare aws and azure services to google cloud.” https://cloud.google.com/docs/get-started/aws-azure-gcp-service-comparison?hl=ko, 2024. Accessed: 2024-12-26.\n\nA. Harlap, A. Tumanov, A. Chung, G. R. Ganger, and P. B. Gibbons, “Proteus: Agile ml elasticity through tiered reliability in dynamic resource markets,” in 12nd European Conference on Computer Systems, EuroSys '17, p. 589–604, 2017.\n\nY. Kim, K. Kim, Y. Cho, J. Kim, A. Khan, K.-D. Kang, B.-S. An, M.-H. Cha, H.-Y. Kim, and Y. Kim, “DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud,” in IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 2024.\n\nG. Fragiadakis, V. Liagkou, E. Filiopoulou, D. Fragkakis, C. Michalakelis, and M. Nikolaidou, “Cloud services cost comparison: a clustering analysis framework,” Computing, vol. 105, pp. 1–28, 03 2023.\n\nA. Andrzejak, D. Kondo, and S. Yi, “Decision model for cloud computing under sla constraints,” in Proceedings of the IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS '10, pp. 257–266, IEEE, 2010.\n\nP. Kokkinos, T. A. Varvarigou, A. Kretsis, P. Soumplis, and E. A. Varvarigos, “Cost and utilization optimization of amazon ec2 instances,” in Proceedings of the 2013 IEEE Sixth International Conference on Cloud Computing, pp. 518–525, IEEE, 2013.\n\nT. Griggs, X. Liu, J. Yu, D. Kim, W.-L. Chiang, A. Cheung, and I. Stoica, “Mélange: Cost efficient large language model serving by exploiting gpu heterogeneity,” 2024.", "doc_id": "kim2025", "page": 11, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "F. Azhar, et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n\nAnthropic, “Message batches api,” 2024. https://www.anthropic.com/news/message-batches-api, Accessed: December 30, 2024.\n\nOpenAI, “Batch processing and rate limits,” 2024. https://platform.openai.com/docs/guides/batch#rate-limits, Accessed: December 30, 2024.\n\nY. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in International Conference on Machine Learning, pp. 31094–31116, PMLR, 2023.\n\nY. Xiong, H. Wu, C. Shao, Z. Wang, R. Zhang, Y. Guo, J. Zhao, K. Zhang, and Z. Pan, “Layerkv: Optimizing large language model serving with layer-wise kv cache management,” 2024.\n\nX. Pan, E. Li, Q. Li, S. Liang, Y. Shan, K. Zhou, Y. Luo, X. Wang, and J. Zhang, “Instinfer: In-storage attention offloading for cost-effective long-context llm inference,” 2024.\n\nR. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, and Y. He, “Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale,” in SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–15, 2022.\n\nG. Cloud, “Compare aws and azure services to google cloud.” https://cloud.google.com/docs/get-started/aws-azure-gcp-service-comparison?hl=ko, 2024. Accessed: 2024-12-26.\n\nA. Harlap, A. Tumanov, A. Chung, G. R. Ganger, and P. B. Gibbons, “Proteus: Agile ml elasticity through tiered reliability in dynamic resource markets,” in 12nd European Conference on Computer Systems, EuroSys '17, p. 589–604, 2017.\n\nY. Kim, K. Kim, Y. Cho, J. Kim, A. Khan, K.-D. Kang, B.-S. An, M.-H. Cha, H.-Y. Kim, and Y. Kim, “DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud,” in IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 2024.\n\nG. Fragiadakis, V. Liagkou, E. Filiopoulou, D. Fragkakis, C. Michalakelis, and M. Nikolaidou, “Cloud services cost comparison: a clustering analysis framework,” Computing, vol. 105, pp. 1–28, 03 2023.\n\nA. Andrzejak, D. Kondo, and S. Yi, “Decision model for cloud computing under sla constraints,” in Proceedings of the IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems, MASCOTS '10, pp. 257–266, IEEE, 2010.\n\nP. Kokkinos, T. A. Varvarigou, A. Kretsis, P. Soumplis, and E. A. Varvarigos, “Cost and utilization optimization of amazon ec2 instances,” in Proceedings of the 2013 IEEE Sixth International Conference on Cloud Computing, pp. 518–525, IEEE, 2013.\n\nT. Griggs, X. Liu, J. Yu, D. Kim, W.-L. Chiang, A. Cheung, and I. Stoica, “Mélange: Cost efficient large language model serving by exploiting gpu heterogeneity,” 2024.", "original_types": ["text"], "id": 185}
{"type": "section", "content": "C. Nie, R. Fonseca, and Z. Liu, “Aladdin: Joint placement and scaling for slo-aware llm serving,” arXiv preprint arXiv:2405.06856, 2024.\n\nY. Jiang, F. Fu, X. Yao, T. Wang, B. Cui, A. Klimovic, and E. Yoneki, “Thunderserve: High-performance and cost-efficient llm serving in cloud environments,” arXiv preprint arXiv:2502.09334, 2025.\n\nP. Patel, E. Choukse, C. Zhang, A. Shah, Í. Goiri, S. Maleki, and R. Bianchini, “Splitwise: Efficient generative llm inference using phase splitting,” in 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pp. 118–132, IEEE, 2024.\n\nZ. Wang, S. Li, Y. Zhou, X. Li, R. Gu, N. Cam-Tu, C. Tian, and S. Zhong, “Revisiting slo and goodput metrics in llm serving,” arXiv preprint arXiv:2410.14257, 2024.\n\nR. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” in Proceedings of Machine Learning and Systems 5 (MLSys 2023), 2023.\n\nAWS, “Aws amazon ec2 instance types-cloud computing instances.”", "doc_id": "kim2025", "page": 11, "url": "https://arxiv.org/pdf/2504.11816", "embedded_text": "C. Nie, R. Fonseca, and Z. Liu, “Aladdin: Joint placement and scaling for slo-aware llm serving,” arXiv preprint arXiv:2405.06856, 2024.\n\nY. Jiang, F. Fu, X. Yao, T. Wang, B. Cui, A. Klimovic, and E. Yoneki, “Thunderserve: High-performance and cost-efficient llm serving in cloud environments,” arXiv preprint arXiv:2502.09334, 2025.\n\nP. Patel, E. Choukse, C. Zhang, A. Shah, Í. Goiri, S. Maleki, and R. Bianchini, “Splitwise: Efficient generative llm inference using phase splitting,” in 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pp. 118–132, IEEE, 2024.\n\nZ. Wang, S. Li, Y. Zhou, X. Li, R. Gu, N. Cam-Tu, C. Tian, and S. Zhong, “Revisiting slo and goodput metrics in llm serving,” arXiv preprint arXiv:2410.14257, 2024.\n\nR. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” in Proceedings of Machine Learning and Systems 5 (MLSys 2023), 2023.\n\nAWS, “Aws amazon ec2 instance types-cloud computing instances.”", "original_types": ["text"], "id": 186}
{"type": "section", "content": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization\n\nJae-Won Chung, Jeff J. Ma, Ruofan Wu, Jiachen Liu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury\n\nUniversity of Michigan\nThe ML.ENERGY Initiative\n\nAbstract\n\nAs the adoption of Generative AI in real-world services grow explosively, energy has emerged as a critical bottleneck resource. However, energy remains a metric that is often overlooked, under-explored, or poorly understood in the context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services. In this paper, we explain four key design principles for benchmarking ML energy we have acquired over time, and then describe how they are implemented in the ML.ENERGY Benchmark. We then highlight results from the early 2025 iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks, case studies of how ML design choices impact energy consumption, and how automated optimization recommendations can lead to significant (sometimes more than 40%) energy savings without changing what is being computed by the model. The ML.ENERGY Benchmark is open-source and can be easily extended to various customized models and application scenarios.\n\n1. Introduction", "doc_id": "chung2025", "page": 1, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization\n\nJae-Won Chung, Jeff J. Ma, Ruofan Wu, Jiachen Liu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury\n\nUniversity of Michigan\nThe ML.ENERGY Initiative\n\nAbstract\n\nAs the adoption of Generative AI in real-world services grow explosively, energy has emerged as a critical bottleneck resource. However, energy remains a metric that is often overlooked, under-explored, or poorly understood in the context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services. In this paper, we explain four key design principles for benchmarking ML energy we have acquired over time, and then describe how they are implemented in the ML.ENERGY Benchmark. We then highlight results from the early 2025 iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks, case studies of how ML design choices impact energy consumption, and how automated optimization recommendations can lead to significant (sometimes more than 40%) energy savings without changing what is being computed by the model. The ML.ENERGY Benchmark is open-source and can be easily extended to various customized models and application scenarios.\n\n1. Introduction", "original_types": ["text", "header"], "id": 187}
{"type": "section", "content": "Generative AI models have rapidly transitioned from research prototypes to real-world services such as ChatGPT [56], Character AI [6], Sora [57], and Midjourney [50]. However, exponential growth rarely continues without facing scaling bottlenecks; currently for generative AI, one of the most crucial bottlenecks is the energy bottleneck [4, 15–17, 38, 48, 49, 51]. That is, even with fleets of latest GPUs and exploding demand for ML compute, getting access to the energy necessary to power these systems is becoming increasingly costly, slow, and sometimes impossible. This particularly impacts serving real-world services as ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60]. Left unaddressed, the energy bottleneck will not only hinder AI research and development progress [31], but also lead to energy being squeezed out of existing electricity grids and impacting availability and price [4].\nHowever, despite its growing importance, energy remains a secondary consideration compared to traditional optimization objectives like time and accuracy. How much energy does a model consume during inference? What is the right way for energy measurement and accounting during execution, let alone optimization? To bridge this gap, we launched the ML.ENERGY Leaderboard,1 the first inference energy leaderboard for modern generative AI models to the best of our knowledge. We have been gradually expanding the Leaderboard in multiple dimensions to now include (1) 40 different 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks.", "doc_id": "chung2025", "page": 1, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Generative AI models have rapidly transitioned from research prototypes to real-world services such as ChatGPT [56], Character AI [6], Sora [57], and Midjourney [50]. However, exponential growth rarely continues without facing scaling bottlenecks; currently for generative AI, one of the most crucial bottlenecks is the energy bottleneck [4, 15–17, 38, 48, 49, 51]. That is, even with fleets of latest GPUs and exploding demand for ML compute, getting access to the energy necessary to power these systems is becoming increasingly costly, slow, and sometimes impossible. This particularly impacts serving real-world services as ML inference reportedly accounts for 80–90% of the total compute demand [12, 32, 58, 60]. Left unaddressed, the energy bottleneck will not only hinder AI research and development progress [31], but also lead to energy being squeezed out of existing electricity grids and impacting availability and price [4].\nHowever, despite its growing importance, energy remains a secondary consideration compared to traditional optimization objectives like time and accuracy. How much energy does a model consume during inference? What is the right way for energy measurement and accounting during execution, let alone optimization? To bridge this gap, we launched the ML.ENERGY Leaderboard,1 the first inference energy leaderboard for modern generative AI models to the best of our knowledge. We have been gradually expanding the Leaderboard in multiple dimensions to now include (1) 40 different 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Track on Datasets and Benchmarks.", "original_types": ["text"], "id": 188}
{"type": "section", "content": "Figure 1: Overview of the benchmarking and optimization flow of the ML.ENERGY Benchmark.\n\nThe figure illustrates the benchmarking and optimization flow of the ML.ENERGY Benchmark. It shows the process from model and dataset selection, through benchmarking, to optimization and obtaining energy-optimal configurations.", "doc_id": "chung2025", "page": 2, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 1: Overview of the benchmarking and optimization flow of the ML.ENERGY Benchmark.\n\nThe figure illustrates the benchmarking and optimization flow of the ML.ENERGY Benchmark. It shows the process from model and dataset selection, through benchmarking, to optimization and obtaining energy-optimal configurations.", "original_types": ["text", "header"], "id": 189}
{"type": "section", "content": "Our approach.\n\nWe focus on software-based GPU energy measurement for the following reasons:\n\n• GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].\n• Compared to other hardware components, GPU models are more standardized across different systems [13], making measurements useful across systems that use the same GPU.\n• GPUs allow accurate software-based energy measurement [1, 2, 11, 81], allowing measurement tools to be portable across systems without requiring physical hardware access or modification.\n\n2.2 Representing Real-World Deployments\n\nGoal. Benchmarking results often inform real-world deployment optimizations, are used to plan future power capacity and energy usage, affect the design of new hardware and software systems, and serve as base numbers for long term projections that affect policymaking. Therefore, it is crucial that our measurements represent those from real-world deployments as closely as possible.\n\nOur approach.\n\nTo obtain realistic measurements, we adhere to the following principles:\n\n• We adopt production-grade software and hardware (e.g., vLLM [39] on NVIDIA H100 GPUs) and run them with generation request workloads that are representative of real-world use cases.\n• During our measurement, we directly run or closely mimic the state of a serving system during long term deployment. This allows us to capture the steady state energy consumption of the service while using a fixed-size benchmarking dataset.\n\n2.3 Energy Measurement at the Right Granularity\n\nGoal. Energy can be measured at different computation granularities. For instance, for LLM text generation, energy can be reported for the end-to-end benchmarking run, for each generated response, or for each token generated. Our goal is to measure and report energy consumption at a granularity that is neither too coarse (as it only provides limited insight into the runtime behavior of the service) nor too fine (as it may miss important higher-level insights relevant to the service).\n\nOur approach.\n\nAlso aligned with our goal of representing real-world deployments (Section 2.2), our approach is to mainly report energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video). This is because any work less than the full response (e.g., per token) is not considered a complete request, and may ignore model- and task-specific characteristics. For instance, for LLM text generation, different models exhibit different verbosity (i.e., given the same prompt, different models respond with varying number of tokens), and different tasks have vastly different output token length distributions (e.g., chat vs. code generation), all of which we want to capture in our measurements.\n\n2.4 Actionable Measurement Results", "doc_id": "chung2025", "page": 3, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Our approach.\n\nWe focus on software-based GPU energy measurement for the following reasons:\n\n• GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].\n• Compared to other hardware components, GPU models are more standardized across different systems [13], making measurements useful across systems that use the same GPU.\n• GPUs allow accurate software-based energy measurement [1, 2, 11, 81], allowing measurement tools to be portable across systems without requiring physical hardware access or modification.\n\n2.2 Representing Real-World Deployments\n\nGoal. Benchmarking results often inform real-world deployment optimizations, are used to plan future power capacity and energy usage, affect the design of new hardware and software systems, and serve as base numbers for long term projections that affect policymaking. Therefore, it is crucial that our measurements represent those from real-world deployments as closely as possible.\n\nOur approach.\n\nTo obtain realistic measurements, we adhere to the following principles:\n\n• We adopt production-grade software and hardware (e.g., vLLM [39] on NVIDIA H100 GPUs) and run them with generation request workloads that are representative of real-world use cases.\n• During our measurement, we directly run or closely mimic the state of a serving system during long term deployment. This allows us to capture the steady state energy consumption of the service while using a fixed-size benchmarking dataset.\n\n2.3 Energy Measurement at the Right Granularity\n\nGoal. Energy can be measured at different computation granularities. For instance, for LLM text generation, energy can be reported for the end-to-end benchmarking run, for each generated response, or for each token generated. Our goal is to measure and report energy consumption at a granularity that is neither too coarse (as it only provides limited insight into the runtime behavior of the service) nor too fine (as it may miss important higher-level insights relevant to the service).\n\nOur approach.\n\nAlso aligned with our goal of representing real-world deployments (Section 2.2), our approach is to mainly report energy consumption at the granularity of a single, whole generation response to a request (e.g., entire chat response, image, video). This is because any work less than the full response (e.g., per token) is not considered a complete request, and may ignore model- and task-specific characteristics. For instance, for LLM text generation, different models exhibit different verbosity (i.e., given the same prompt, different models respond with varying number of tokens), and different tasks have vastly different output token length distributions (e.g., chat vs. code generation), all of which we want to capture in our measurements.\n\n2.4 Actionable Measurement Results", "original_types": ["text", "header", "list"], "id": 190}
{"type": "section", "content": "Goal. While energy measurements are useful in themselves, they are even more useful when they lead to actionable insights and recommendations. For instance, how much is the potential energy savings of your model without sacrificing accuracy or latency? If your service intends to guarantee a specific generation latency deadline (e.g., 50 ms), what is the energy-optimal configuration, and how much is the potential energy savings?\n\nOur approach.\n\nThe ML.ENERGY Benchmark allows users to provide computation latency constraints specific to their application scenario (e.g., LLM average Time Per Output Token), and will automatically recommend (1) the energy-optimal configuration that meets the latency constraints, and (2) the expected amount of energy savings. Due to the generalizability of our measurements (Section 2.1), these recommendations inform the optimization of a wide range of systems.", "doc_id": "chung2025", "page": 3, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Goal. While energy measurements are useful in themselves, they are even more useful when they lead to actionable insights and recommendations. For instance, how much is the potential energy savings of your model without sacrificing accuracy or latency? If your service intends to guarantee a specific generation latency deadline (e.g., 50 ms), what is the energy-optimal configuration, and how much is the potential energy savings?\n\nOur approach.\n\nThe ML.ENERGY Benchmark allows users to provide computation latency constraints specific to their application scenario (e.g., LLM average Time Per Output Token), and will automatically recommend (1) the energy-optimal configuration that meets the latency constraints, and (2) the expected amount of energy savings. Due to the generalizability of our measurements (Section 2.1), these recommendations inform the optimization of a wide range of systems.", "original_types": ["text", "header"], "id": 191}
{"type": "section", "content": "The ML.ENERGY Benchmark\n\nThe ML.ENERGY Benchmark is a comprehensive tool for measuring and optimizing the inference energy consumption of generative AI models, built upon our core design principles (Section 2). Here, we describe the overall flow of the ML.ENERGY Benchmark (Section 3.1), which includes service-aware energy measurement and accounting (Section 3.2) and automated optimization recommendations (Section 3.3). Finally, we describe extension points of the ML.ENERGY Benchmark that allows users to easily benchmark their customized application scenarios (Section 3.4).\n\nBenchmark Flow\n\nFigure 1 provides an overview of the usage flow of the ML.ENERGY Benchmark. ① First, the generative model to benchmark and the request dataset (set of inputs) to use are selected, alongside with the set of configurations to sweep (e.g., GPU model, parallelism configuration, maximum batch size). ② Then the ML.ENERGY Benchmark runs configurations independently on designated hardware, and measures the time and energy consumption of each configuration using Zeus [2], a library that provides programmatic energy measurement (Section 3.2). ③ After benchmarking is complete, users can specify a latency target based on their application requirements. ④ Given that, the ML.ENERGY Benchmark constructs the time–energy Pareto frontier, and recommends the energy-optimal configuration while satisfying the latency target (Section 3.3).\n\nEnergy Measurement and Service-Aware Energy Accounting\n\nOur goal is to provide per-request energy measurements (Section 2.3) that are representative of real-world deployments (Section 2.2). However, a realistic serving system batches together the generation of multiple requests (e.g., iteration-level batching [82] for LLM text generation), making the energy consumption of a single request dependent on all other requests being processed at the same time. Therefore, we implement measurement and energy accounting methods that capture the batching behavior of different types of models.", "doc_id": "chung2025", "page": 4, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "The ML.ENERGY Benchmark\n\nThe ML.ENERGY Benchmark is a comprehensive tool for measuring and optimizing the inference energy consumption of generative AI models, built upon our core design principles (Section 2). Here, we describe the overall flow of the ML.ENERGY Benchmark (Section 3.1), which includes service-aware energy measurement and accounting (Section 3.2) and automated optimization recommendations (Section 3.3). Finally, we describe extension points of the ML.ENERGY Benchmark that allows users to easily benchmark their customized application scenarios (Section 3.4).\n\nBenchmark Flow\n\nFigure 1 provides an overview of the usage flow of the ML.ENERGY Benchmark. ① First, the generative model to benchmark and the request dataset (set of inputs) to use are selected, alongside with the set of configurations to sweep (e.g., GPU model, parallelism configuration, maximum batch size). ② Then the ML.ENERGY Benchmark runs configurations independently on designated hardware, and measures the time and energy consumption of each configuration using Zeus [2], a library that provides programmatic energy measurement (Section 3.2). ③ After benchmarking is complete, users can specify a latency target based on their application requirements. ④ Given that, the ML.ENERGY Benchmark constructs the time–energy Pareto frontier, and recommends the energy-optimal configuration while satisfying the latency target (Section 3.3).\n\nEnergy Measurement and Service-Aware Energy Accounting\n\nOur goal is to provide per-request energy measurements (Section 2.3) that are representative of real-world deployments (Section 2.2). However, a realistic serving system batches together the generation of multiple requests (e.g., iteration-level batching [82] for LLM text generation), making the energy consumption of a single request dependent on all other requests being processed at the same time. Therefore, we implement measurement and energy accounting methods that capture the batching behavior of different types of models.", "original_types": ["text", "header"], "id": 192}
{"type": "section", "content": "3.3 Automated Optimization Recommendation\n\nOur goal is to provide actionable insights beyond just energy measurements (Section 2.4) by recommending energy-optimal configurations for a given model and task. Central to the optimization recommendation is the construction of the Pareto frontier of energy vs. time, which is a collection of configurations where there are no other configurations that lead to both lower energy and lower time. Then, the energy-optimal configuration is selected based on user-specified latency constraints.\n\nLatency constraints inherently depend on the user’s or application’s needs. For example, for image generation with Diffusion models, computation results are useful only when the full image is generated, so latency constraints would be specified in terms of the time to generate the whole image. On the other hand, for LLM text generation for chat, output tokens are streamed to users (either in written text or synthesized speech) as they are generated. As such, for user-facing conversational AI services, as long as the average time per output token is at least as fast as the users’ reading or listening speed, user experience will not be affected [44]. However, for LLM text generation for coding, where code is likely only useful when it is fully generated, latency constraints would be specified in terms of the time to generate the whole snippet, similar to the case of image generation. Given the latency constraints, the time-energy Pareto frontier is used to suggest the minimum-energy configuration that satisfies the latency constraint.", "doc_id": "chung2025", "page": 5, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "3.3 Automated Optimization Recommendation\n\nOur goal is to provide actionable insights beyond just energy measurements (Section 2.4) by recommending energy-optimal configurations for a given model and task. Central to the optimization recommendation is the construction of the Pareto frontier of energy vs. time, which is a collection of configurations where there are no other configurations that lead to both lower energy and lower time. Then, the energy-optimal configuration is selected based on user-specified latency constraints.\n\nLatency constraints inherently depend on the user’s or application’s needs. For example, for image generation with Diffusion models, computation results are useful only when the full image is generated, so latency constraints would be specified in terms of the time to generate the whole image. On the other hand, for LLM text generation for chat, output tokens are streamed to users (either in written text or synthesized speech) as they are generated. As such, for user-facing conversational AI services, as long as the average time per output token is at least as fast as the users’ reading or listening speed, user experience will not be affected [44]. However, for LLM text generation for coding, where code is likely only useful when it is fully generated, latency constraints would be specified in terms of the time to generate the whole snippet, similar to the case of image generation. Given the latency constraints, the time-energy Pareto frontier is used to suggest the minimum-energy configuration that satisfies the latency constraint.", "original_types": ["text", "header"], "id": 193}
{"type": "section", "content": "Hardware.\n\nAs long as the runtime used by the ML.ENERGY Benchmark (e.g., vLLM) is capable of running on the target hardware and Zeus [2] can measure energy consumption on the target hardware (e.g., NVIDIA/AMD GPUs, Intel/AMD CPUs, Apple Silicon, NVIDIA Jetson platforms), the ML.ENERGY Benchmark can run on the target hardware as is.\n\nMetrics.\n\nEnergy is a fundamental physical quantity that can be used to derive other useful metrics, though these derived metrics are not automatically computed by default as they require context-specific information. Below, we describe how these metrics might be computed based on the benchmark’s outputs.\n\n• {'type': 'text', 'content': 'Average power draw (Watts): Average power draw over the steady state can be calculated by dividing total energy consumption during the steady state by the duration of the steady state.'}\n• {'type': 'text', 'content': 'Throughput per Watt: Work throughput, e.g., request or token generation throughput, divided by average power draw can describe how much service capacity can be extracted from the system given a power budget, which is a critical quantity for datacenter power planning [38].'}\n• {'type': 'text', 'content': 'Monetary cost ($): The electricity cost of compute can be calculated by integrating over time the multiplication of energy consumption and the electricity price in the region and time instance. If there is a specific region and time frame the service is expected to run, choosing that electricity price can simulate the operational electricity cost of deployment. Electricity prices can be obtained from sources like OpenEI.3 Calculating the electricity cost from energy is supported by Zeus [2], the measurement library of choice for the benchmark.'}\n• {'type': 'text', 'content': 'Operational carbon emissions (gCO2e): This quantity estimates the greenhouse gas emissions associated with the electricity consumed. It can be calculated by multiplying energy consumption by the carbon intensity (gCO2e/kWh) of the particular region and time frame in which the benchmark was run. Carbon intensity data can be obtained from sources like ElectricityMaps.4 This is also supported by Zeus [2], the energy measurement library employed by the benchmark.'}", "doc_id": "chung2025", "page": 6, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Hardware.\n\nAs long as the runtime used by the ML.ENERGY Benchmark (e.g., vLLM) is capable of running on the target hardware and Zeus [2] can measure energy consumption on the target hardware (e.g., NVIDIA/AMD GPUs, Intel/AMD CPUs, Apple Silicon, NVIDIA Jetson platforms), the ML.ENERGY Benchmark can run on the target hardware as is.\n\nMetrics.\n\nEnergy is a fundamental physical quantity that can be used to derive other useful metrics, though these derived metrics are not automatically computed by default as they require context-specific information. Below, we describe how these metrics might be computed based on the benchmark’s outputs.\n\n• {'type': 'text', 'content': 'Average power draw (Watts): Average power draw over the steady state can be calculated by dividing total energy consumption during the steady state by the duration of the steady state.'}\n• {'type': 'text', 'content': 'Throughput per Watt: Work throughput, e.g., request or token generation throughput, divided by average power draw can describe how much service capacity can be extracted from the system given a power budget, which is a critical quantity for datacenter power planning [38].'}\n• {'type': 'text', 'content': 'Monetary cost ($): The electricity cost of compute can be calculated by integrating over time the multiplication of energy consumption and the electricity price in the region and time instance. If there is a specific region and time frame the service is expected to run, choosing that electricity price can simulate the operational electricity cost of deployment. Electricity prices can be obtained from sources like OpenEI.3 Calculating the electricity cost from energy is supported by Zeus [2], the measurement library of choice for the benchmark.'}\n• {'type': 'text', 'content': 'Operational carbon emissions (gCO2e): This quantity estimates the greenhouse gas emissions associated with the electricity consumed. It can be calculated by multiplying energy consumption by the carbon intensity (gCO2e/kWh) of the particular region and time frame in which the benchmark was run. Carbon intensity data can be obtained from sources like ElectricityMaps.4 This is also supported by Zeus [2], the energy measurement library employed by the benchmark.'}", "original_types": ["text", "header", "list"], "id": 194}
{"type": "figure", "content": "(a) NVIDIA A100", "doc_id": "chung2025", "page": 7, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "(a) NVIDIA A100", "id": 195}
{"type": "figure", "content": "(b) NVIDIA H100", "doc_id": "chung2025", "page": 7, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "(b) NVIDIA H100", "id": 196}
{"type": "table", "content": "Table 1: Energy per generation of reasoning models on GPQA [64] and NVIDIA H100 GPUs. TP is the tensor parallelism degree, which is also equal to the number of GPUs used.\nDeepSeek distilled Qwen 3 8B [23, 80] Phi 4 reasoning plus 15B [3] Qwen 3 32B [80] Qwen 3 235B-A22B thinking [80]", "doc_id": "chung2025", "page": 7, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Table 1: Energy per generation of reasoning models on GPQA [64] and NVIDIA H100 GPUs. TP is the tensor parallelism degree, which is also equal to the number of GPUs used.\nDeepSeek distilled Qwen 3 8B [23, 80] Phi 4 reasoning plus 15B [3] Qwen 3 32B [80] Qwen 3 235B-A22B thinking [80]", "id": 197}
{"type": "figure", "content": "Figure 4: Phi-3 Mini and Small [26] benchmarked with the chat task on one NVIDIA A100 GPU.", "doc_id": "chung2025", "page": 8, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 4: Phi-3 Mini and Small [26] benchmarked with the chat task on one NVIDIA A100 GPU.", "id": 198}
{"type": "figure", "content": "Figure 5: Power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models.", "doc_id": "chung2025", "page": 8, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 5: Power consumption of Llama 3.1 70B and Stable Diffusion 3 Medium models.", "id": 199}
{"type": "section", "content": "output tokens equals the number of forward passes through the model, longer responses leads to a proportional increase in energy consumption. As humans are known to prefer longer responses [85], this potentially introduces a trade-off between energy consumption and user satisfaction.\n\nThis is even more pronounced for reasoning models, which produce significantly more output tokens. Table 1 shows energy measurements for reasoning models on the GPQA dataset. Reasoning models produce one to two orders of magnitude more output tokens per request compared to standard chat models, significantly increasing energy consumption per generation. Additionally, due to their long output lengths, servers cannot run as large a batch size, preventing them from amortizing energy across more requests. This leads to higher energy per token as well, further increasing energy consumption. As long horizon reasoning and task decomposition become more common in real-world LLM-based applications, we expect this trend to continue.\n\nMemory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows. This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3× more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.\n\nCompute-intensity of operations and power draw. Figure 5 shows the power consumption of Llama 3.1 70B [73] and Stable Diffusion 3 Medium [25] on A100 and H100 GPUs. It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw. Appendix C dives deeper into power consumption with measurements for all models and GPU power breakdowns over time.", "doc_id": "chung2025", "page": 8, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "output tokens equals the number of forward passes through the model, longer responses leads to a proportional increase in energy consumption. As humans are known to prefer longer responses [85], this potentially introduces a trade-off between energy consumption and user satisfaction.\n\nThis is even more pronounced for reasoning models, which produce significantly more output tokens. Table 1 shows energy measurements for reasoning models on the GPQA dataset. Reasoning models produce one to two orders of magnitude more output tokens per request compared to standard chat models, significantly increasing energy consumption per generation. Additionally, due to their long output lengths, servers cannot run as large a batch size, preventing them from amortizing energy across more requests. This leads to higher energy per token as well, further increasing energy consumption. As long horizon reasoning and task decomposition become more common in real-world LLM-based applications, we expect this trend to continue.\n\nMemory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows. This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3× more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.\n\nCompute-intensity of operations and power draw. Figure 5 shows the power consumption of Llama 3.1 70B [73] and Stable Diffusion 3 Medium [25] on A100 and H100 GPUs. It can be seen that the LLM’s power consumption is much lower than what the GPUs can draw at maximum, whereas the Diffusion model’s power consumption is close to the maximum. This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw. Appendix C dives deeper into power consumption with measurements for all models and GPU power breakdowns over time.", "original_types": ["text"], "id": 200}
{"type": "figure", "content": "Figure 6: Energy consumption of SDXL [61] and SDXL Turbo [7] on one NVIDIA A100 GPU.", "doc_id": "chung2025", "page": 9, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 6: Energy consumption of SDXL [61] and SDXL Turbo [7] on one NVIDIA A100 GPU.", "id": 201}
{"type": "figure", "content": "Figure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark.", "doc_id": "chung2025", "page": 9, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 7: Time–energy Pareto frontiers constructed by the ML.ENERGY Benchmark.", "id": 202}
{"type": "section", "content": "5 Related Work\n\nML energy measurement. The Hugging Face LLM-Perf leaderboard [33] is specific to LLMs and reports the per-token energy consumption of LLM text generation, which fails to capture the verbosity and task-specific output token length distribution difference of LLMs (Section 2.3). MLPerf Power [75] provides measurements for ML training and inference, but crucially, requires direct access to the system under test to physically install the power analyzer, which significantly limits who can run the benchmarks (Section 2.1). Furthermore, it benchmarks at most a few model architectures for each task (sometimes only one), failing to provide insights on how ML design choices impact energy consumption. The Hugging Face AI Energy Score leaderboard [27] provides measurement data for broader AI tasks. However, it fixes the inference batch size to 1 for all models, failing to reflect how services are deployed in the real world and thus their energy consumption (Section 2.2). Google disclosed the median energy consumption of their AI service [24]. It provides a comprehensive scope of measurement, even including the energy consumption of idle machines provisioned for stable service operation. However, measurements and reports are based on internal Google systems, workloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the generalizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the first inference energy benchmark for modern generative AI models, and empowers users to not only measure but also optimize the energy consumption of their models. See Appendix D for more details.\n\nML energy optimization. The ML.ENERGY Benchmark provides automated energy optimization recommendations based on energy measurements (Section 3.3). There are several other efforts that also provided automated energy optimizations – while preserving mathematical equivalence and/or model quality – for ML training and inference. Zeus [81], EnvPipe [20], and Perseus [21] optimizes the energy consumption of ML training by adjusting GPU-level and training job-level configurations, either statically after profiling or dynamically during training. μ-Serve [63] and DynamoLLM [68] are also similar, but optimize energy consumption for ML inference clusters. Optimization recommendations by the ML.ENERGY Benchmark are complementary to the techniques proposed by these works. Further, our results support the need for automated cross-layer energy optimizations that span all model, software, and hardware layers [22], as opposed to efforts siloed within a single layer.\n\n6 Conclusion", "doc_id": "chung2025", "page": 10, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "5 Related Work\n\nML energy measurement. The Hugging Face LLM-Perf leaderboard [33] is specific to LLMs and reports the per-token energy consumption of LLM text generation, which fails to capture the verbosity and task-specific output token length distribution difference of LLMs (Section 2.3). MLPerf Power [75] provides measurements for ML training and inference, but crucially, requires direct access to the system under test to physically install the power analyzer, which significantly limits who can run the benchmarks (Section 2.1). Furthermore, it benchmarks at most a few model architectures for each task (sometimes only one), failing to provide insights on how ML design choices impact energy consumption. The Hugging Face AI Energy Score leaderboard [27] provides measurement data for broader AI tasks. However, it fixes the inference batch size to 1 for all models, failing to reflect how services are deployed in the real world and thus their energy consumption (Section 2.2). Google disclosed the median energy consumption of their AI service [24]. It provides a comprehensive scope of measurement, even including the energy consumption of idle machines provisioned for stable service operation. However, measurements and reports are based on internal Google systems, workloads, hardware (TPUs), and model (Gemini) that are not publicly available, limiting the generalizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the first inference energy benchmark for modern generative AI models, and empowers users to not only measure but also optimize the energy consumption of their models. See Appendix D for more details.\n\nML energy optimization. The ML.ENERGY Benchmark provides automated energy optimization recommendations based on energy measurements (Section 3.3). There are several other efforts that also provided automated energy optimizations – while preserving mathematical equivalence and/or model quality – for ML training and inference. Zeus [81], EnvPipe [20], and Perseus [21] optimizes the energy consumption of ML training by adjusting GPU-level and training job-level configurations, either statically after profiling or dynamically during training. μ-Serve [63] and DynamoLLM [68] are also similar, but optimize energy consumption for ML inference clusters. Optimization recommendations by the ML.ENERGY Benchmark are complementary to the techniques proposed by these works. Further, our results support the need for automated cross-layer energy optimizations that span all model, software, and hardware layers [22], as opposed to efforts siloed within a single layer.\n\n6 Conclusion", "original_types": ["text", "header"], "id": 203}
{"type": "section", "content": "In this work, we described the ML.ENERGY Benchmark, a comprehensive energy benchmark for generative AI models that not only provides realistic energy measurements, but also automatically suggests energy-optimal configurations based on user- and app-specific performance constraints. Measurement results show that energy consumption is a metric that is impacted by design choices across the whole AI stack, including application, model, software, and hardware, demonstrating the importance of automated cross-layer energy optimizations instead of siloed optimizations within a single layer. We are confident that the ML.ENERGY Benchmark will democratize the art of measuring, understanding, and optimizing ML energy consumption for the community.\n\nAcknowledgments and Disclosure of Funding\n\nWe would like to thank Yunseok Jang and SymbioticLab members for helpful comments and suggestions on the paper. This work and its authors were in part supported by NSF grants CNS-2104243, CNS-2106184, and CNS-2450085, grants from VMware, the Mozilla Foundation, Cisco, Ford, and GitHub, and gifts from Salesforce and Google. Jae-Won Chung is additionally supported by the Kwanjeong Educational Foundation.", "doc_id": "chung2025", "page": 10, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "In this work, we described the ML.ENERGY Benchmark, a comprehensive energy benchmark for generative AI models that not only provides realistic energy measurements, but also automatically suggests energy-optimal configurations based on user- and app-specific performance constraints. Measurement results show that energy consumption is a metric that is impacted by design choices across the whole AI stack, including application, model, software, and hardware, demonstrating the importance of automated cross-layer energy optimizations instead of siloed optimizations within a single layer. We are confident that the ML.ENERGY Benchmark will democratize the art of measuring, understanding, and optimizing ML energy consumption for the community.\n\nAcknowledgments and Disclosure of Funding\n\nWe would like to thank Yunseok Jang and SymbioticLab members for helpful comments and suggestions on the paper. This work and its authors were in part supported by NSF grants CNS-2104243, CNS-2106184, and CNS-2450085, grants from VMware, the Mozilla Foundation, Cisco, Ford, and GitHub, and gifts from Salesforce and Google. Jae-Won Chung is additionally supported by the Kwanjeong Educational Foundation.", "original_types": ["text", "header"], "id": 204}
{"type": "section", "content": "References\n\n[1] NVIDIA Management Library (NVML). https://developer.nvidia.com/nvidia-management-library-nvml.\n\n[2] Zeus: Deep learning energy measurement and optimization. https://github.com/ml-energy/zeus.\n\n[3] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025.\n\n[4] International Energy Agency. Electricity 2025, 2025.\n\n[5] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-vani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve. In OSDI, 2024.\n\n[6] Character AI. Character ai. https://character.ai, 2023.\n\n[7] Stability AI. Introducing SDXL turbo: A real-time text-to-image generation model, 2023.\n\n[8] AI@Meta. Llama 3 model card. 2024.\n\n[9] AI@Meta. Llama 4 model card. 2024.\n\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023.\n\n[11] Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly, Ayatelrahman Elsayed, Abdel-Hameed Badawy, Gopinath Chennupati, Stephan Eidenbenz, and Nandakishore Santhi. Verified instruction-level energy consumption measurement for NVIDIA GPUs. Proceedings of the 17th ACM International Conference on Computing Frontiers, 2020.\n\n[12] Jeff Bar. Amazon EC2 update – inf1 instances with AWS inferentia chips for high performance cost-effective inferencing, 2019.\n\n[13] Nathan Beniach and Air Street Capital. State of AI report compute index. https://www.stateof.ai/compute.\n\n[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\n\n[15] CBRE. Global data center trends 2023. https://www.cbre.com/insights/reports/global-data-center-trends-2023, 2023.\n\n[16] CBRE. Global data center trends 2024. https://www.cbre.com/insights/reports/global-data-center-trends-2024, 2024.\n\n[17] CBRE. Global data center trends 2025. https://www.cbre.com/insights/reports/global-data-center-trends-2025, 2025.", "doc_id": "chung2025", "page": 11, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "References\n\n[1] NVIDIA Management Library (NVML). https://developer.nvidia.com/nvidia-management-library-nvml.\n\n[2] Zeus: Deep learning energy measurement and optimization. https://github.com/ml-energy/zeus.\n\n[3] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025.\n\n[4] International Energy Agency. Electricity 2025, 2025.\n\n[5] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-vani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve. In OSDI, 2024.\n\n[6] Character AI. Character ai. https://character.ai, 2023.\n\n[7] Stability AI. Introducing SDXL turbo: A real-time text-to-image generation model, 2023.\n\n[8] AI@Meta. Llama 3 model card. 2024.\n\n[9] AI@Meta. Llama 4 model card. 2024.\n\n[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023.\n\n[11] Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly, Ayatelrahman Elsayed, Abdel-Hameed Badawy, Gopinath Chennupati, Stephan Eidenbenz, and Nandakishore Santhi. Verified instruction-level energy consumption measurement for NVIDIA GPUs. Proceedings of the 17th ACM International Conference on Computing Frontiers, 2020.\n\n[12] Jeff Bar. Amazon EC2 update – inf1 instances with AWS inferentia chips for high performance cost-effective inferencing, 2019.\n\n[13] Nathan Beniach and Air Street Capital. State of AI report compute index. https://www.stateof.ai/compute.\n\n[14] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\n\n[15] CBRE. Global data center trends 2023. https://www.cbre.com/insights/reports/global-data-center-trends-2023, 2023.\n\n[16] CBRE. Global data center trends 2024. https://www.cbre.com/insights/reports/global-data-center-trends-2024, 2024.\n\n[17] CBRE. Global data center trends 2025. https://www.cbre.com/insights/reports/global-data-center-trends-2025, 2025.", "original_types": ["text", "header"], "id": 205}
{"type": "section", "content": "[18] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems Datasets and Benchmarks, 2024.\n\n[19] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\n\n[20] Sangjin Choi, Inhoe Koo, Jeongseob Ahn, Myeongjae Jeon, and Youngjin Kwon. EnvPipe: Performance-preserving DNN training framework for saving energy. Proceedings of the 2023 USENIX Annual Technical Conference, 2023.", "doc_id": "chung2025", "page": 11, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[18] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems Datasets and Benchmarks, 2024.\n\n[19] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\n\n[20] Sangjin Choi, Inhoe Koo, Jeongseob Ahn, Myeongjae Jeon, and Youngjin Kwon. EnvPipe: Performance-preserving DNN training framework for saving energy. Proceedings of the 2023 USENIX Annual Technical Conference, 2023.", "original_types": ["text"], "id": 206}
{"type": "section", "content": "[21] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. Reducing energy bloat in large model training. Proceedings of the 30th ACM Symposium on Operating Systems Principles, 2024.\n\n[22] Jae-Won Chung, Nishil Talati, and Mosharaf Chowdhury. Toward cross-layer energy optimizations in AI systems. DOE ASCR Energy-Efficient Computing for Science Workshop, 2024.\n\n[23] DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n\n[24] Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah Goodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes, and James Manyika. Measuring the environmental impact of delivering AI at google scale. arXiv preprint arXiv:2508.15734, 2025.\n\n[25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024.\n\n[26] Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.\n\n[27] Hugging Face. Ai energy score. https://huggingface.github.io/AIEnergyScore, 2025.\n\n[28] Meta GenAI. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023.\n\n[29] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024.\n\n[30] Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, and Patrick Von Platen. Progressive knowledge distillation of stable diffusion xl using layer level loss. arXiv preprint arXiv:2401.02677, 2024.\n\n[31] The White House. Fact sheet: President donald j. trump establishes the national energy dominance council, 2025.\n\n[32] HPCwire. AWS to offer NVIDIA’s T4 GPUs for AI inferencing, 2019.\n\n[33] Régis Pierrard Ilyas Moutawwakil. LLM-Perf leaderboard. https://huggingface.co/spaces/optimum/llm-perf-leaderboard, 2023.\n\n[34] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.", "doc_id": "chung2025", "page": 12, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[21] Jae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. Reducing energy bloat in large model training. Proceedings of the 30th ACM Symposium on Operating Systems Principles, 2024.\n\n[22] Jae-Won Chung, Nishil Talati, and Mosharaf Chowdhury. Toward cross-layer energy optimizations in AI systems. DOE ASCR Energy-Efficient Computing for Science Workshop, 2024.\n\n[23] DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n\n[24] Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah Goodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes, and James Manyika. Measuring the environmental impact of delivering AI at google scale. arXiv preprint arXiv:2508.15734, 2025.\n\n[25] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024.\n\n[26] Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.\n\n[27] Hugging Face. Ai energy score. https://huggingface.github.io/AIEnergyScore, 2025.\n\n[28] Meta GenAI. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023.\n\n[29] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024.\n\n[30] Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, and Patrick Von Platen. Progressive knowledge distillation of stable diffusion xl using layer level loss. arXiv preprint arXiv:2401.02677, 2024.\n\n[31] The White House. Fact sheet: President donald j. trump establishes the national energy dominance council, 2025.\n\n[32] HPCwire. AWS to offer NVIDIA’s T4 GPUs for AI inferencing, 2019.\n\n[33] Régis Pierrard Ilyas Moutawwakil. LLM-Perf leaderboard. https://huggingface.co/spaces/optimum/llm-perf-leaderboard, 2023.\n\n[34] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.", "original_types": ["text"], "id": 207}
{"type": "section", "content": "[35] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n\n[36] Heehoon Kim, Junyeol Ryu, and Jaejin Lee. TCCL: Discovering better communication paths for PCIe GPU clusters. Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 2024.\n\n[37] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference. Architecture and System Support for Transformer Models, 2023.\n\n[38] Helen Kou. Power for AI: Easier said than built. https://about.bnef.com/insights/commodities/power-for-ai-easier-said-than-built/, 2025.", "doc_id": "chung2025", "page": 12, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[35] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n\n[36] Heehoon Kim, Junyeol Ryu, and Jaejin Lee. TCCL: Discovering better communication paths for PCIe GPU clusters. Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 2024.\n\n[37] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference. Architecture and System Support for Transformer Models, 2023.\n\n[38] Helen Kou. Power for AI: Easier said than built. https://about.bnef.com/insights/commodities/power-for-ai-easier-said-than-built/, 2025.", "original_types": ["text"], "id": 208}
{"type": "section", "content": "[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. Proceedings of the 29th Symposium on Operating Systems Principles, 2023.\n\n[40] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint, 2019.\n\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024.\n\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024.\n\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 2023.\n\n[44] Jiachen Liu, Jae-Won Chung, Zhiyu Wu, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes: Defining and enhancing quality-of-experience in LLM-based text streaming services. arXiv preprint arXiv:2404.16283, 2024.\n\n[45] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In NeurIPS, 2023.\n\n[46] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noumane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n\n[47] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 2024.\n\n[48] McKinsey & Company. Investing in the rising data center economy. https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy, 2023.\n\n[49] McKinsey & Company. How data centers and the energy sector can sate AI’s hunger for power. https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power, 2024.\n\n[50] Midjourney. Midjourney. https://midjourney.com, 2022.\n\n[51] Sebastian Moss. Meta’s mark zuckerberg says energy constraints are holding back AI data center buildout, 2024.\n\n[52] NVIDIA. NVIDIA DGX A100 datasheet. https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020.\n\n[53] NVIDIA. NVIDIA DGX H200 datasheet. https://resources.nvidia.com/en-us-dgx-systems/dgx-h200-datasheet, 2024.\n\n[54] NVIDIA. NVIDIA DGX B200 datasheet. https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet, 2025.\n\n[55] OpenAI. What are tokens and how to count them? https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them.\n\n[56] OpenAI. ChatGPT. https://chatgpt.com, 2022.\n\n[57] OpenAI. Sora. https://openai.com/index/sora, 2024.", "doc_id": "chung2025", "page": 13, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[39] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. Proceedings of the 29th Symposium on Operating Systems Principles, 2023.\n\n[40] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint, 2019.\n\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024.\n\n[42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024.\n\n[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 2023.\n\n[44] Jiachen Liu, Jae-Won Chung, Zhiyu Wu, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes: Defining and enhancing quality-of-experience in LLM-based text streaming services. arXiv preprint arXiv:2404.16283, 2024.\n\n[45] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In NeurIPS, 2023.\n\n[46] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noumane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n\n[47] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 2024.\n\n[48] McKinsey & Company. Investing in the rising data center economy. https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy, 2023.\n\n[49] McKinsey & Company. How data centers and the energy sector can sate AI’s hunger for power. https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power, 2024.\n\n[50] Midjourney. Midjourney. https://midjourney.com, 2022.\n\n[51] Sebastian Moss. Meta’s mark zuckerberg says energy constraints are holding back AI data center buildout, 2024.\n\n[52] NVIDIA. NVIDIA DGX A100 datasheet. https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020.\n\n[53] NVIDIA. NVIDIA DGX H200 datasheet. https://resources.nvidia.com/en-us-dgx-systems/dgx-h200-datasheet, 2024.\n\n[54] NVIDIA. NVIDIA DGX B200 datasheet. https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet, 2025.\n\n[55] OpenAI. What are tokens and how to count them? https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them.\n\n[56] OpenAI. ChatGPT. https://chatgpt.com, 2022.\n\n[57] OpenAI. Sora. https://openai.com/index/sora, 2024.", "original_types": ["text"], "id": 209}
{"type": "section", "content": "[58] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. Characterizing power management opportunities for llms in the cloud. ASPLOS, 2024.\n\n[59] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In ISCA, 2024.", "doc_id": "chung2025", "page": 13, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[58] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. Characterizing power management opportunities for llms in the cloud. ASPLOS, 2024.\n\n[59] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In ISCA, 2024.", "original_types": ["text"], "id": 210}
{"type": "section", "content": "[60] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint, 2021.\n\n[61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024.\n\n[62] PromptHero. OpenJourney v4, 2023.\n\n[63] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Başar, and Ravishankar K. Iyer. Power-aware deep learning model serving with u-Serve. In ATC, 2024.\n\n[64] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. In CoLM, 2024.\n\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattaflori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2024.\n\n[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint, 2019.\n\n[68] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing llm inference clusters for performance and energy efficiency. In HPCA, 2025.\n\n[69] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.\n\n[70] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec, Kelly Schaefer, and Scott Huffman. CodeGemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.\n\n[71] Gemma Team. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\n\n[72] ShareGPT Team. ShareGPT. https://sharegpt.com/.\n\n[73] Llama team at Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.", "doc_id": "chung2025", "page": 14, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[60] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint, 2021.\n\n[61] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024.\n\n[62] PromptHero. OpenJourney v4, 2023.\n\n[63] Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Başar, and Ravishankar K. Iyer. Power-aware deep learning model serving with u-Serve. In ATC, 2024.\n\n[64] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. In CoLM, 2024.\n\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n[66] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattaflori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2024.\n\n[67] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint, 2019.\n\n[68] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing llm inference clusters for performance and energy efficiency. In HPCA, 2025.\n\n[69] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.\n\n[70] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec, Kelly Schaefer, and Scott Huffman. CodeGemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.\n\n[71] Gemma Team. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\n\n[72] ShareGPT Team. ShareGPT. https://sharegpt.com/.\n\n[73] Llama team at Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.", "original_types": ["text"], "id": 211}
{"type": "section", "content": "[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint, 2023.\n\n[75] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver Curtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao Pan, Manu Prasad Manmathan, Liz Raymond, Tom St. John, Arjun Suresh, Rowan Taubitz, Sean Zhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi. MLPerf power: Benchmarking the energy efficiency of machine learning systems from uWatts to MWatts for sustainable ai. In HPCA, 2025.\n\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\n\n[77] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas", "doc_id": "chung2025", "page": 14, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint, 2023.\n\n[75] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver Curtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao Pan, Manu Prasad Manmathan, Liz Raymond, Tom St. John, Arjun Suresh, Rowan Taubitz, Sean Zhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi. MLPerf power: Benchmarking the energy efficiency of machine learning systems from uWatts to MWatts for sustainable ai. In HPCA, 2025.\n\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\n\n[77] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas", "original_types": ["text"], "id": 212}
{"type": "section", "content": "Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers.\n\n[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.\n\n[79] Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, and Xin Jin. ServeGen: Workload characterization and generation of large language model serving in production. arXiv preprint arXiv:2505.09999, 2025.\n\n[80] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\n\n[81] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU energy consumption of DNN training. NSDI, 2023.\n\n[82] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In OSDI, 2022.\n\n[83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022.\n\n[84] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2VGen-XL: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023.\n\n[85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.\n\n[86] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving. In OSDI, 2024.", "doc_id": "chung2025", "page": 15, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers.\n\n[78] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.\n\n[79] Yuxing Xiang, Xue Li, Kun Qian, Wenyuan Yu, Ennan Zhai, and Xin Jin. ServeGen: Workload characterization and generation of large language model serving in production. arXiv preprint arXiv:2505.09999, 2025.\n\n[80] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\n\n[81] Jie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU energy consumption of DNN training. NSDI, 2023.\n\n[82] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In OSDI, 2022.\n\n[83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022.\n\n[84] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2VGen-XL: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023.\n\n[85] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.\n\n[86] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving. In OSDI, 2024.", "original_types": ["text"], "id": 213}
{"type": "section", "content": "NeurIPS Paper Checklist\n\n1. Claims\n\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?\n\nAnswer: [Yes]\n\nJustification: The abstract and introduction reflect the paper’s contributions and scope.\n\nGuidelines:\n\n• The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\n• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\n• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n\n2. Limitations\n\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer: [Yes]\n\nJustification: Limitations are discussed in Appendix E.\n\nGuidelines:\n\n• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\n• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\n• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\n• The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\n• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\n• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.", "doc_id": "chung2025", "page": 16, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "NeurIPS Paper Checklist\n\n1. Claims\n\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?\n\nAnswer: [Yes]\n\nJustification: The abstract and introduction reflect the paper’s contributions and scope.\n\nGuidelines:\n\n• The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\n• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\n• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n\n2. Limitations\n\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer: [Yes]\n\nJustification: Limitations are discussed in Appendix E.\n\nGuidelines:\n\n• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\n• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\n• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\n• The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\n• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\n• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.", "original_types": ["text", "header"], "id": 214}
{"type": "section", "content": "• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n\n3. Theory assumptions and proofs\n\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer: [NA]", "doc_id": "chung2025", "page": 16, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n\n3. Theory assumptions and proofs\n\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer: [NA]", "original_types": ["text", "header"], "id": 215}
{"type": "section", "content": "Justification: This paper does not include theoretical results.\n\nGuidelines:\n\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n\n4. Experimental result reproducibility\n\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer: [Yes]\n\nJustification: The benchmark code and the result data that supply the leaderboard are available open-source and documented at https://github.com/ml-energy/leaderboard. The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.energy/leaderboard.\n\nGuidelines:", "doc_id": "chung2025", "page": 17, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Justification: This paper does not include theoretical results.\n\nGuidelines:\n\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n\n4. Experimental result reproducibility\n\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer: [Yes]\n\nJustification: The benchmark code and the result data that supply the leaderboard are available open-source and documented at https://github.com/ml-energy/leaderboard. The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.energy/leaderboard.\n\nGuidelines:", "original_types": ["text", "header", "list"], "id": 216}
{"type": "section", "content": "• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n• (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n• (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n• (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n• (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n\n5. Open access to data and code", "doc_id": "chung2025", "page": 17, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n• (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n• (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n• (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n• (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n\n5. Open access to data and code", "original_types": ["header", "list"], "id": 217}
{"type": "section", "content": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\n\nAnswer: [Yes]\n\nJustification: The benchmark code and the result data that supply the leaderboard are available open-source and documented at https://github.com/ml-energy/leaderboard. The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.energy/leaderboard.\n\nGuidelines:\n\n- The answer NA means that paper does not include experiments requiring code.\n- Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n- While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n\n6. Experimental setting/details\n\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nAnswer: [Yes]\n\nJustification: We mention important details in the main paper and provide more details in the Appendix. Full details are available in the code repository.\n\nGuidelines:\n\n- The answer NA means that the paper does not include experiments.\n- The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n- The full details can be provided either with the code, in appendix, or as supplemental material.\n\n7. Experiment statistical significance\n\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\n\nAnswer: [No]\n\nJustification: We were not able to run the benchmark multiple times due to the high monetary cost of even a single run.\n\nGuidelines:", "doc_id": "chung2025", "page": 18, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\n\nAnswer: [Yes]\n\nJustification: The benchmark code and the result data that supply the leaderboard are available open-source and documented at https://github.com/ml-energy/leaderboard. The full result data can be browsed at the ML.ENERGY Leaderboard at https://ml.energy/leaderboard.\n\nGuidelines:\n\n- The answer NA means that paper does not include experiments requiring code.\n- Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n- While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n\n6. Experimental setting/details\n\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nAnswer: [Yes]\n\nJustification: We mention important details in the main paper and provide more details in the Appendix. Full details are available in the code repository.\n\nGuidelines:\n\n- The answer NA means that the paper does not include experiments.\n- The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n- The full details can be provided either with the code, in appendix, or as supplemental material.\n\n7. Experiment statistical significance\n\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\n\nAnswer: [No]\n\nJustification: We were not able to run the benchmark multiple times due to the high monetary cost of even a single run.\n\nGuidelines:", "original_types": ["text", "header"], "id": 218}
{"type": "section", "content": "- The answer NA means that the paper does not include experiments.\n- The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.", "doc_id": "chung2025", "page": 18, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "- The answer NA means that the paper does not include experiments.\n- The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.", "original_types": ["text"], "id": 219}
{"type": "section", "content": "8. Experiments compute resources\n\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer: [Yes]\n\nJustification: We mention compute resources in the beginning of Section 4.\n\nGuidelines:\n\n• The answer NA means that the paper does not include experiments.\n\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\n• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\n• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).\n\n9. Code of ethics\n\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n\nAnswer: [Yes]\n\nJustification: We confirm that we reviewed the NeurIPS Code of Ethics and that our research conforms to it.\n\nGuidelines:\n\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\n• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\n• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\n10. Broader impacts\n\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer: [Yes]\n\nJustification: We discuss this in Appendix F.\n\nGuidelines:\n\n• The answer NA means that there is no societal impact of the work performed.", "doc_id": "chung2025", "page": 19, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "8. Experiments compute resources\n\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer: [Yes]\n\nJustification: We mention compute resources in the beginning of Section 4.\n\nGuidelines:\n\n• The answer NA means that the paper does not include experiments.\n\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\n• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\n• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).\n\n9. Code of ethics\n\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n\nAnswer: [Yes]\n\nJustification: We confirm that we reviewed the NeurIPS Code of Ethics and that our research conforms to it.\n\nGuidelines:\n\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\n• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\n• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\n10. Broader impacts\n\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer: [Yes]\n\nJustification: We discuss this in Appendix F.\n\nGuidelines:\n\n• The answer NA means that there is no societal impact of the work performed.", "original_types": ["text", "header"], "id": 220}
{"type": "section", "content": "11. Safeguards\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer: [NA]\n\nJustification: We do not believe safeguards are necessary for our work.\n\nGuidelines:\n\n• The answer NA means that the paper poses no such risks.\n\n• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n\n• Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n\n• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\n12. Licenses for existing assets\n\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer: [Yes]\n\nJustification: We extensively use models and datasets created by others in our benchmark. We credit the authors through citation. Appendix A contains a comprehensive table. The benchmark has default request datasets that we recommend, but does not come packaged with any specific model or dataset.\n\nGuidelines:\n\n• The answer NA means that the paper does not use existing assets.\n\n• The authors should cite the original paper that produced the code package or dataset.\n\n• The authors should state which version of the asset is used and, if possible, include a URL.\n\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.", "doc_id": "chung2025", "page": 20, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "11. Safeguards\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer: [NA]\n\nJustification: We do not believe safeguards are necessary for our work.\n\nGuidelines:\n\n• The answer NA means that the paper poses no such risks.\n\n• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n\n• Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n\n• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\n12. Licenses for existing assets\n\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer: [Yes]\n\nJustification: We extensively use models and datasets created by others in our benchmark. We credit the authors through citation. Appendix A contains a comprehensive table. The benchmark has default request datasets that we recommend, but does not come packaged with any specific model or dataset.\n\nGuidelines:\n\n• The answer NA means that the paper does not use existing assets.\n\n• The authors should cite the original paper that produced the code package or dataset.\n\n• The authors should state which version of the asset is used and, if possible, include a URL.\n\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.", "original_types": ["text", "header"], "id": 221}
{"type": "section", "content": "New assets\n\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer: [Yes]\n\nJustification: The benchmark code is available open-source and documented at https://github.com/ml-energy/leaderboard under the Apache-2.0 license.\n\nGuidelines:\n\n• The answer NA means that the paper does not release new assets.\n\n• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n\n• The paper should discuss whether and how consent was obtained from people whose asset is used.\n\n• At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n\nCrowdsourcing and research with human subjects\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\n\nAnswer: [NA]\n\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\n• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\n• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n\nInstitutional review board (IRB) approvals or equivalent for research with human subjects\n\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\n\nAnswer: [NA]\n\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\n• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.", "doc_id": "chung2025", "page": 21, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "New assets\n\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer: [Yes]\n\nJustification: The benchmark code is available open-source and documented at https://github.com/ml-energy/leaderboard under the Apache-2.0 license.\n\nGuidelines:\n\n• The answer NA means that the paper does not release new assets.\n\n• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n\n• The paper should discuss whether and how consent was obtained from people whose asset is used.\n\n• At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n\nCrowdsourcing and research with human subjects\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\n\nAnswer: [NA]\n\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\n• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\n• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n\nInstitutional review board (IRB) approvals or equivalent for research with human subjects\n\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\n\nAnswer: [NA]\n\nJustification: This paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\n• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.", "original_types": ["text", "header"], "id": 222}
{"type": "section", "content": "16. Declaration of LLM usage\n\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.\n\nAnswer: [NA]\n\nJustification: We have used LLMs to assist in editing the paper, generating figures, and writing code snippets, and its use does not impact the core methodology, scientific rigorousness, or originality of the research.\n\nGuidelines:", "doc_id": "chung2025", "page": 22, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "16. Declaration of LLM usage\n\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.\n\nAnswer: [NA]\n\nJustification: We have used LLMs to assist in editing the paper, generating figures, and writing code snippets, and its use does not impact the core methodology, scientific rigorousness, or originality of the research.\n\nGuidelines:", "original_types": ["text", "header"], "id": 223}
{"type": "section", "content": "Tasks, Model Architectures, and Default Request Datasets\n\nTables 2 and 3 list the model architectures and tasks supported by current iteration of the ML.ENERGY Benchmark, along with the default request datasets for each task. We note that models that were fine-tuned based on the supported models are also supported as is, and the benchmark is designed to be extensible (Section 3.4).", "doc_id": "chung2025", "page": 23, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Tasks, Model Architectures, and Default Request Datasets\n\nTables 2 and 3 list the model architectures and tasks supported by current iteration of the ML.ENERGY Benchmark, along with the default request datasets for each task. We note that models that were fine-tuned based on the supported models are also supported as is, and the benchmark is designed to be extensible (Section 3.4).", "original_types": ["text", "header"], "id": 224}
{"type": "table", "content": "Table 2: Model type, task, and default request dataset used in the ML.ENERGY Benchmark.\nMarkdown representation of the table", "doc_id": "chung2025", "page": 23, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Table 2: Model type, task, and default request dataset used in the ML.ENERGY Benchmark.\nMarkdown representation of the table", "id": 225}
{"type": "table", "content": "Table 3: Model architectures supported by the ML.ENERGY Benchmark for each task.\nMarkdown representation of the table", "doc_id": "chung2025", "page": 23, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Table 3: Model architectures supported by the ML.ENERGY Benchmark for each task.\nMarkdown representation of the table", "id": 226}
{"type": "section", "content": "Energy Implication of System Parameters\n\nThis section discusses the energy implication of different system-level configurations. System-level configurations are those that do not change what is computed but rather how it is computed by the underlying software system.", "doc_id": "chung2025", "page": 23, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Energy Implication of System Parameters\n\nThis section discusses the energy implication of different system-level configurations. System-level configurations are those that do not change what is computed but rather how it is computed by the underlying software system.", "original_types": ["text", "header"], "id": 227}
{"type": "figure", "content": "Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.", "doc_id": "chung2025", "page": 24, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 8: Energy consumption per generation while varying the maximum batch size for Mistral Nemo (12B). The LLM inference server’s preemption mechanism is compared.", "id": 228}
{"type": "figure", "content": "Figure 9: Energy consumption per generation while varying batch size for Llama 3.1 8B. The number of NVIDIA A100 GPUs used to run the same model is scaled up.", "doc_id": "chung2025", "page": 24, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 9: Energy consumption per generation while varying batch size for Llama 3.1 8B. The number of NVIDIA A100 GPUs used to run the same model is scaled up.", "id": 229}
{"type": "section", "content": "In Figure 8, we compare the energy consumption per generation of the two preemption mechanisms with the Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size configuration and causing preemption. It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section). Furthermore, as the server gets more and more overloaded, energy consumption generally increases. This is because with higher overload, more preemptions – and thus more recomputation or data movement – occur. Since preemptions do not directly contribute to the completion of the request, the extra energy consumption from preemptions increases the average energy consumption of completing each request.\n\nIn order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary the number of GPUs used (Figure 9). Because the amount of computation executed is the same regardless of the number of GPUs, energy consumption should ideally be constant. Indeed, energy consumption barely changes when scaling from one GPU (no communication) to two, but when scaling further, energy consumption significantly increases. This is because, while the amount of computation decreases for each GPU, additional communication time between the GPUs offsets the reduction in computation time. Since communication time increases with the number of GPUs, using too many GPUs can lead to slowdowns in executing the same amount of computation and increase energy consumption.", "doc_id": "chung2025", "page": 24, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "In Figure 8, we compare the energy consumption per generation of the two preemption mechanisms with the Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size configuration and causing preemption. It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section). Furthermore, as the server gets more and more overloaded, energy consumption generally increases. This is because with higher overload, more preemptions – and thus more recomputation or data movement – occur. Since preemptions do not directly contribute to the completion of the request, the extra energy consumption from preemptions increases the average energy consumption of completing each request.\n\nIn order to ablate the effect of communication, we employ the same Llama 3.1 8B model and vary the number of GPUs used (Figure 9). Because the amount of computation executed is the same regardless of the number of GPUs, energy consumption should ideally be constant. Indeed, energy consumption barely changes when scaling from one GPU (no communication) to two, but when scaling further, energy consumption significantly increases. This is because, while the amount of computation decreases for each GPU, additional communication time between the GPUs offsets the reduction in computation time. Since communication time increases with the number of GPUs, using too many GPUs can lead to slowdowns in executing the same amount of computation and increase energy consumption.", "original_types": ["text"], "id": 230}
{"type": "table", "content": "Table 4: Energy per generation (Joules) and the percentage of decode energy consumption with PD disaggregation.\nFollowing recent trace analysis [79], we sampled input lengths from a Pareto distribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean specified in the table. TP means tensor parallelism degree, and xPyD means it was deployed with x prefill instances and y decode instances, each with TP-many GPUs.", "doc_id": "chung2025", "page": 25, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Table 4: Energy per generation (Joules) and the percentage of decode energy consumption with PD disaggregation.\nFollowing recent trace analysis [79], we sampled input lengths from a Pareto distribution with alpha 2.5, and output lengths from an Exponential distribution, each with mean specified in the table. TP means tensor parallelism degree, and xPyD means it was deployed with x prefill instances and y decode instances, each with TP-many GPUs.", "id": 231}
{"type": "table", "content": "Table 5: Energy per generation (Joules) of Llama 3.1 8B on a synthetic long context request dataset running on H100 GPUs. Following recent trace analysis [79], we sampled input lengths from a Pareto distribution with mean 4,096 and alpha 2.5, and output lengths from an Exponential distribution with mean 512. Note that vLLM does not allow the max number of batched tokens to be smaller than the max batch size, which is why the lower left triangle of the table is empty.", "doc_id": "chung2025", "page": 25, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Table 5: Energy per generation (Joules) of Llama 3.1 8B on a synthetic long context request dataset running on H100 GPUs. Following recent trace analysis [79], we sampled input lengths from a Pareto distribution with mean 4,096 and alpha 2.5, and output lengths from an Exponential distribution with mean 512. Note that vLLM does not allow the max number of batched tokens to be smaller than the max batch size, which is why the lower left triangle of the table is empty.", "id": 232}
{"type": "section", "content": "From this scaling experiment, we can observe that the energy impact of communication overhead can be large. This impact will be even more pronounced in hardware environments without sufficient or state-of-the-art networking infrastructure, which is common in real world settings due to its cost [36].\n\nPrefill–decode (PD) disaggregation is a rising production deployment setting where prefill and decode phases are run on separate GPUs [59, 86]. This allows for independent scaling and optimization of prefill and decode phases based on workload characteristics, and leads to better latency deadline attainment. Table 4 shows energy measurements for different PD disaggregation configurations, where “xPyD” denotes x prefill instances and y decode instances.\n\nOverall, decode consumes the majority of energy, with some amount shifting to prefill when input length is long. In our setup, PD disaggregation configurations did not have a large impact on absolute energy consumption or the energy split as long as the throughput of prefill and decode instances are reasonably balanced.\n\nChunked prefill is a technique where long input prompts are split into chunks and processed alongside decode iterations, improving GPU utilization and reducing the interference between long prefills and decode iterations [5]. For chunked prefill, the max number of batched tokens is a key parameter that controls the chunk size. Table 5 shows the impact of this parameter on energy consumption.\n\nTable 5 shows that the more sequences or tokens you batch, the better the energy amortization you get and energy per generation decreases, and after a certain point, returns diminish.", "doc_id": "chung2025", "page": 25, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "From this scaling experiment, we can observe that the energy impact of communication overhead can be large. This impact will be even more pronounced in hardware environments without sufficient or state-of-the-art networking infrastructure, which is common in real world settings due to its cost [36].\n\nPrefill–decode (PD) disaggregation is a rising production deployment setting where prefill and decode phases are run on separate GPUs [59, 86]. This allows for independent scaling and optimization of prefill and decode phases based on workload characteristics, and leads to better latency deadline attainment. Table 4 shows energy measurements for different PD disaggregation configurations, where “xPyD” denotes x prefill instances and y decode instances.\n\nOverall, decode consumes the majority of energy, with some amount shifting to prefill when input length is long. In our setup, PD disaggregation configurations did not have a large impact on absolute energy consumption or the energy split as long as the throughput of prefill and decode instances are reasonably balanced.\n\nChunked prefill is a technique where long input prompts are split into chunks and processed alongside decode iterations, improving GPU utilization and reducing the interference between long prefills and decode iterations [5]. For chunked prefill, the max number of batched tokens is a key parameter that controls the chunk size. Table 5 shows the impact of this parameter on energy consumption.\n\nTable 5 shows that the more sequences or tokens you batch, the better the energy amortization you get and energy per generation decreases, and after a certain point, returns diminish.", "original_types": ["text"], "id": 233}
{"type": "figure", "content": "Figure 10: Power consumption of various models on A100 and H100 GPUs.", "doc_id": "chung2025", "page": 26, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 10: Power consumption of various models on A100 and H100 GPUs.", "id": 234}
{"type": "figure", "content": "Figure 11: Ratio of a model's power consumption to the maximum GPU power draw across all models.", "doc_id": "chung2025", "page": 26, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 11: Ratio of a model's power consumption to the maximum GPU power draw across all models.", "id": 235}
{"type": "figure", "content": "Figure 11: Ratio of power consumption to maximum GPU power draw across various models.", "doc_id": "chung2025", "page": 27, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 11: Ratio of power consumption to maximum GPU power draw across various models.", "id": 236}
{"type": "figure", "content": "Figure 12: GPU power draw breakdown over time on one NVIDIA H100 GPU. “Entire GPU” and “Only VRAM” (memory) were measured, and the two were subtracted to derive “Entire GPU excluding VRAM.”", "doc_id": "chung2025", "page": 27, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "Figure 12: GPU power draw breakdown over time on one NVIDIA H100 GPU. “Entire GPU” and “Only VRAM” (memory) were measured, and the two were subtracted to derive “Entire GPU excluding VRAM.”", "id": 237}
{"type": "section", "content": "in power draw – when the benchmark begins, the server begins admitting new requests, creating a short period where numerous Prefills are executed back-to-back, leading to high power draw. After the initial spike, power draw repeats a periodic fluctuation. This is because, before each Prefill or Decode, the server must make numerous control decisions, including determining which requests are now finished and which ones should run next. Since these decisions are executed by the CPU, this creates a periodic time gap where the GPU is not running any computation. This GPU idle time leads to the periodic drop in GPU power draw. On the other hand, Stable Video Diffusion XT shows a different power draw pattern. Diffusion models generally have three phases: Encode, Denoise, and Decode. The Encode phase digests the", "doc_id": "chung2025", "page": 27, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "in power draw – when the benchmark begins, the server begins admitting new requests, creating a short period where numerous Prefills are executed back-to-back, leading to high power draw. After the initial spike, power draw repeats a periodic fluctuation. This is because, before each Prefill or Decode, the server must make numerous control decisions, including determining which requests are now finished and which ones should run next. Since these decisions are executed by the CPU, this creates a periodic time gap where the GPU is not running any computation. This GPU idle time leads to the periodic drop in GPU power draw. On the other hand, Stable Video Diffusion XT shows a different power draw pattern. Diffusion models generally have three phases: Encode, Denoise, and Decode. The Encode phase digests the", "original_types": ["text"], "id": 238}
{"type": "section", "content": "The ML.ENERGY Leaderboard and Benchmark\n\nOn July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy leaderboard for modern generative AI models. Our goal was to measure and understand the energy consumption of generative AI models, and we provided a web-based leaderboard to allow everyone to browse the results. The leaderboard started with only LLM chat with tens of different LLMs, but gradually expanded to include more tasks, models, and datasets. Our benchmarking suite to supply data to the leaderboard is what we dub the ML.ENERGY Benchmark. This paper shares our design philosophy and principles we have acquired over time by gradually maintaining and upgrading the ML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained from the early 2025 iteration of the benchmark. Importantly, we plan to continuously update the benchmark and the leaderboard as long as resources allow, and what is presented in this paper is only a snapshot of the current state of the benchmark at the time of writing. We encourage readers to visit the leaderboard website and benchmark repository for the latest results and updates.", "doc_id": "chung2025", "page": 28, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "The ML.ENERGY Leaderboard and Benchmark\n\nOn July 2023, we launched the ML.ENERGY Leaderboard and Benchmark, the first inference energy leaderboard for modern generative AI models. Our goal was to measure and understand the energy consumption of generative AI models, and we provided a web-based leaderboard to allow everyone to browse the results. The leaderboard started with only LLM chat with tens of different LLMs, but gradually expanded to include more tasks, models, and datasets. Our benchmarking suite to supply data to the leaderboard is what we dub the ML.ENERGY Benchmark. This paper shares our design philosophy and principles we have acquired over time by gradually maintaining and upgrading the ML.ENERGY Benchmark and the Leaderboard, and highlights notable results we have obtained from the early 2025 iteration of the benchmark. Importantly, we plan to continuously update the benchmark and the leaderboard as long as resources allow, and what is presented in this paper is only a snapshot of the current state of the benchmark at the time of writing. We encourage readers to visit the leaderboard website and benchmark repository for the latest results and updates.", "original_types": ["text", "header"], "id": 239}
{"type": "section", "content": "which is one factor that determines the cost of running generative AI services at the infrastructure level. By optimizing energy consumption, we can reduce the cost of running generative AI services, which can help democratize access to generative AI.", "doc_id": "chung2025", "page": 29, "url": "https://arxiv.org/pdf/2505.06371", "embedded_text": "which is one factor that determines the cost of running generative AI services at the infrastructure level. By optimizing energy consumption, we can reduce the cost of running generative AI services, which can help democratize access to generative AI.", "original_types": ["text"], "id": 240}
{"type": "section", "content": "Abstract\n\nReducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs' energy consumption during the inference phase without compromising performance, paving the way for further investigations.", "doc_id": "rubei2025", "page": 1, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "Abstract\n\nReducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs' energy consumption during the inference phase without compromising performance, paving the way for further investigations.", "original_types": ["text", "header"], "id": 241}
{"type": "section", "content": "II. BACKGROUND\n\nWhile measuring traditional software impact in terms of emissions is well-established [1], [7], assessing LLMs consumption is still challenging, as High-Performance Computing (HPC) clusters are often required to run the training process, which can last for weeks or even months. Therefore, measuring the energy consumption in terms of carbon emissions is particularly challenging in those environments due to several factors, e.g., parallel jobs or the non-exclusive use of the cluster.\n\nMoreover, even well-maintained LLMs leaderboard benchmarks [19]–[21] do not report energy consumption, focusing instead on accuracy metrics. Figure 1 shows the carbon emissions of the GPT-3 model in different server regions for three big IT players, i.e., Google, Amazon, and Microsoft. For instance, some models emit carbon that is equivalent to the average of five cars over their lifetimes [22], thus underlining significant sustainability concerns, especially when considering the growing scope of LLM-based implementations and their integration into everyday life. This highlights the need to reduce the carbon footprint of LLMs and to examine the details that contribute to the reported figures.\n\nTo address the environmental impact of software, a range of energy monitoring tools [5], [6] has been recently developed to measure the carbon emissions associated with code execution. Among these, the CodeCarbon tool [16] is a widely adopted Python library that estimates the energy consumption of code executions. It can also calculate the carbon footprint by measuring the electricity power consumption of the underlying hardware architecture, i.e., GPU, CPU, and RAM. In addition, it can estimate the carbon intensity of the region where the computing is done. This study focuses on the energy consumption related to GPU usage without considering the carbon emission.\n\nConcerning the inference phase of LLMs, prompt engineering is pivotal to enhancing LLMs’ generation capabilities. The most basic PET is zero-shot, in which the LLM is given a query without any example of outputs, which are expected from the given inputs [23]. In contrast, one-shot prompting provides the model with a single example, offering a minimal context to guide responses. The few-shots prompting [24] involves multiple examples, allowing the model to generalize more effectively with limited supervision [25]. In the scope of this paper, we focus on different shot techniques i.e., zero-shot, one-shot, and few-shots given their efficiency and success in improving the performance of LLMs in source code-related tasks.", "doc_id": "rubei2025", "page": 2, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "II. BACKGROUND\n\nWhile measuring traditional software impact in terms of emissions is well-established [1], [7], assessing LLMs consumption is still challenging, as High-Performance Computing (HPC) clusters are often required to run the training process, which can last for weeks or even months. Therefore, measuring the energy consumption in terms of carbon emissions is particularly challenging in those environments due to several factors, e.g., parallel jobs or the non-exclusive use of the cluster.\n\nMoreover, even well-maintained LLMs leaderboard benchmarks [19]–[21] do not report energy consumption, focusing instead on accuracy metrics. Figure 1 shows the carbon emissions of the GPT-3 model in different server regions for three big IT players, i.e., Google, Amazon, and Microsoft. For instance, some models emit carbon that is equivalent to the average of five cars over their lifetimes [22], thus underlining significant sustainability concerns, especially when considering the growing scope of LLM-based implementations and their integration into everyday life. This highlights the need to reduce the carbon footprint of LLMs and to examine the details that contribute to the reported figures.\n\nTo address the environmental impact of software, a range of energy monitoring tools [5], [6] has been recently developed to measure the carbon emissions associated with code execution. Among these, the CodeCarbon tool [16] is a widely adopted Python library that estimates the energy consumption of code executions. It can also calculate the carbon footprint by measuring the electricity power consumption of the underlying hardware architecture, i.e., GPU, CPU, and RAM. In addition, it can estimate the carbon intensity of the region where the computing is done. This study focuses on the energy consumption related to GPU usage without considering the carbon emission.\n\nConcerning the inference phase of LLMs, prompt engineering is pivotal to enhancing LLMs’ generation capabilities. The most basic PET is zero-shot, in which the LLM is given a query without any example of outputs, which are expected from the given inputs [23]. In contrast, one-shot prompting provides the model with a single example, offering a minimal context to guide responses. The few-shots prompting [24] involves multiple examples, allowing the model to generalize more effectively with limited supervision [25]. In the scope of this paper, we focus on different shot techniques i.e., zero-shot, one-shot, and few-shots given their efficiency and success in improving the performance of LLMs in source code-related tasks.", "original_types": ["text", "header"], "id": 242}
{"type": "section", "content": "Quantization [26] is a technique that reduces the computational and memory requirements of LLMs by lowering the precision of their numerical representations (e.g., from 32-bit to 8-bit). This compression speeds up inference, making LLMs more efficient with minimal impact on accuracy. In this paper, we leverage quantization alongside PETs to minimize the computational cost while maintaining performance in code-related tasks.\n\nWhile developing a comprehensive methodology for measuring LLM energy consumption is beyond this paper’s scope, we focus on reducing these emissions through efficient PETs. By utilizing custom tags, we aim to lower energy consumption in LLMs used for code-related tasks, offering an approach that balances sustainability with performance.", "doc_id": "rubei2025", "page": 2, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "Quantization [26] is a technique that reduces the computational and memory requirements of LLMs by lowering the precision of their numerical representations (e.g., from 32-bit to 8-bit). This compression speeds up inference, making LLMs more efficient with minimal impact on accuracy. In this paper, we leverage quantization alongside PETs to minimize the computational cost while maintaining performance in code-related tasks.\n\nWhile developing a comprehensive methodology for measuring LLM energy consumption is beyond this paper’s scope, we focus on reducing these emissions through efficient PETs. By utilizing custom tags, we aim to lower energy consumption in LLMs used for code-related tasks, offering an approach that balances sustainability with performance.", "original_types": ["text"], "id": 243}
{"type": "section", "content": "III. PERFORMED EXPERIMENTS\n\nFigure 2 depicts the workflow of the experiments we performed to answer the two research questions. Starting from the CodeXGLUE dataset [14] (1), prompt creator (2) translates input prompts into a format that Llama 3 can understand, before augmenting them with tags that we specifically introduced (3). Afterward, the crafted prompts are used to query the LLM locally deployed (4). For each snippet, we executed 75 queries.2 Each Llama run is monitored (5) by the CodeCarbon energy monitoring tool. For each execution, we store three artifacts (question, answer (6), and measurement (7)), to enable both efficiency and accuracy analysis.\n\nA. Dataset\n\nAmong different benchmarks, we select CodeXGLUE as it is tailored for supporting and evaluating LLMs in code-related tasks [27], [28]. In this paper, we consider the code completion task as it is widely supported by LLMs as recently investigated [29], [30]. This task leverages established evaluation methodologies in the literature, enabling straightforward comparisons with ground truth data.\n\nB. Prompt Creator\n\nThis component is responsible for defining and augmenting prompts that have been used to query the model under analysis. In particular, we use standard PETs, i.e., zero-shot, one-shot, and few-shots, as a baseline to evaluate the effect of custom tags in terms of energy impact. The Llama 3 model card3 defines several tokens which form the model’s input. We aim to investigate the impact of custom tags on energy consumption and performance metrics for Llama 3. To this end, we define five distinct prompt configurations. Each prompt comprises two key components: a role attribute and content specification, as illustrated in Listing 1. The role attribute can be assigned as either system or user. In the case of system, the accompanying content attribute specifies the task to be performed, thus clarifying the expected contribution from the model. For example, in Listing 1, the system role is configured to instruct the model on a code completion task for given code fragments. The user role, on the other hand, introduces the input code snippet that the model is expected to complete. According to the different configurations, the content can be enhanced with custom tags or explanations related to the task. The configurations are defined as follows:\n\nListing 1: Example of a zero-shot prompt\n\n{\n  \"role\": \"system\",\n  \"content\": \"You are an AI assistant specialized in code completion for Java. Your task is to complete the provided Java code segment with one line. Give only the code completion.\",\n}, {\n  \"role\": \"user\",\n  \"content\": \"package com.lmax.disruptor.support ; import java.util.concurrent. ThreadFactory; public final\"", "doc_id": "rubei2025", "page": 3, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "III. PERFORMED EXPERIMENTS\n\nFigure 2 depicts the workflow of the experiments we performed to answer the two research questions. Starting from the CodeXGLUE dataset [14] (1), prompt creator (2) translates input prompts into a format that Llama 3 can understand, before augmenting them with tags that we specifically introduced (3). Afterward, the crafted prompts are used to query the LLM locally deployed (4). For each snippet, we executed 75 queries.2 Each Llama run is monitored (5) by the CodeCarbon energy monitoring tool. For each execution, we store three artifacts (question, answer (6), and measurement (7)), to enable both efficiency and accuracy analysis.\n\nA. Dataset\n\nAmong different benchmarks, we select CodeXGLUE as it is tailored for supporting and evaluating LLMs in code-related tasks [27], [28]. In this paper, we consider the code completion task as it is widely supported by LLMs as recently investigated [29], [30]. This task leverages established evaluation methodologies in the literature, enabling straightforward comparisons with ground truth data.\n\nB. Prompt Creator\n\nThis component is responsible for defining and augmenting prompts that have been used to query the model under analysis. In particular, we use standard PETs, i.e., zero-shot, one-shot, and few-shots, as a baseline to evaluate the effect of custom tags in terms of energy impact. The Llama 3 model card3 defines several tokens which form the model’s input. We aim to investigate the impact of custom tags on energy consumption and performance metrics for Llama 3. To this end, we define five distinct prompt configurations. Each prompt comprises two key components: a role attribute and content specification, as illustrated in Listing 1. The role attribute can be assigned as either system or user. In the case of system, the accompanying content attribute specifies the task to be performed, thus clarifying the expected contribution from the model. For example, in Listing 1, the system role is configured to instruct the model on a code completion task for given code fragments. The user role, on the other hand, introduces the input code snippet that the model is expected to complete. According to the different configurations, the content can be enhanced with custom tags or explanations related to the task. The configurations are defined as follows:\n\nListing 1: Example of a zero-shot prompt\n\n{\n  \"role\": \"system\",\n  \"content\": \"You are an AI assistant specialized in code completion for Java. Your task is to complete the provided Java code segment with one line. Give only the code completion.\",\n}, {\n  \"role\": \"user\",\n  \"content\": \"package com.lmax.disruptor.support ; import java.util.concurrent. ThreadFactory; public final\"", "original_types": ["text", "header"], "id": 244}
{"type": "section", "content": "C2 - use of custom tags with explanation\n\nWe embed the meaning of the custom tags in the prompt as shown in Listing 3.\n\nListing 3: Fragment of a prompt including custom tags explanation\n\n{\n  \"role\": \"user\",\n  \"content\": \"The code to analyze is marked by the <code> tag and the line to be completed is marked by the <incomplete> tag. <code> package com.lmax.disruptor.\n  support; import java.util.concurrent.\n  ThreadFactory; </code> <incomplete> public final </incomplete>\"}\n\n\nC3 - custom prompt explained in the system\n\nDifferently from configuration C2, the explanation of custom tags is given in the system role part of the input prompt as shown in Listing 4.\n\nListing 4: Example of a zero-shot prompt including the definition of custom tags\n\n{\n  \"role\": \"system\",\n  \"content\": \"You are an AI assistant specialized in code completion for Java. Your task is to complete the provided Java code segment with one line. Give only the code completion. The code to analyze is marked by the <code> tag and the line to be completed is marked by the <incomplete> tag.\"\",\n}\n\n\n,{\n  \"role\": \"user\",\n  \"content\": \"<code> package com.lmax.disruptor.\n  support; import java.util.concurrent.\n  ThreadFactory; </code><incomplete> public final </incomplete>\"}\n\n\nC4 - no system definition\n\nWith this configuration, we want to assess the effect of the complete absence of the system role definition. Therefore, we provide only the incomplete input snippet and a task definition directly in the prompt without any customization as illustrated in Listing 5.\n\nListing 5: Fragment of a prompt including custom tags\n\n{\n  \"role\": \"system\",\n  \"content\": \"\",\n}\n\n\nD. Execution process\n\nThe experiments have been performed by considering the settings shown in Table I. In particular, we tested 1,000 random", "doc_id": "rubei2025", "page": 4, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "C2 - use of custom tags with explanation\n\nWe embed the meaning of the custom tags in the prompt as shown in Listing 3.\n\nListing 3: Fragment of a prompt including custom tags explanation\n\n{\n  \"role\": \"user\",\n  \"content\": \"The code to analyze is marked by the <code> tag and the line to be completed is marked by the <incomplete> tag. <code> package com.lmax.disruptor.\n  support; import java.util.concurrent.\n  ThreadFactory; </code> <incomplete> public final </incomplete>\"}\n\n\nC3 - custom prompt explained in the system\n\nDifferently from configuration C2, the explanation of custom tags is given in the system role part of the input prompt as shown in Listing 4.\n\nListing 4: Example of a zero-shot prompt including the definition of custom tags\n\n{\n  \"role\": \"system\",\n  \"content\": \"You are an AI assistant specialized in code completion for Java. Your task is to complete the provided Java code segment with one line. Give only the code completion. The code to analyze is marked by the <code> tag and the line to be completed is marked by the <incomplete> tag.\"\",\n}\n\n\n,{\n  \"role\": \"user\",\n  \"content\": \"<code> package com.lmax.disruptor.\n  support; import java.util.concurrent.\n  ThreadFactory; </code><incomplete> public final </incomplete>\"}\n\n\nC4 - no system definition\n\nWith this configuration, we want to assess the effect of the complete absence of the system role definition. Therefore, we provide only the incomplete input snippet and a task definition directly in the prompt without any customization as illustrated in Listing 5.\n\nListing 5: Fragment of a prompt including custom tags\n\n{\n  \"role\": \"system\",\n  \"content\": \"\",\n}\n\n\nD. Execution process\n\nThe experiments have been performed by considering the settings shown in Table I. In particular, we tested 1,000 random", "original_types": ["text", "header", "code"], "id": 245}
{"type": "section", "content": "TABLE I: Summary of the Experimental Settings", "doc_id": "rubei2025", "page": 5, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "TABLE I: Summary of the Experimental Settings", "original_types": ["header"], "id": 246}
{"type": "table", "content": "TABLE I: Summary of the Experimental Settings\n| Model | Llama3 8B - Instruct \n|---|---\n| Snippets | 1,000 \n| PETs | 3 \n| Custom Prompts | 5 \n| Repetitions | 5 \n| Pause | 10 seconds \n| Metrics (Performance) | Energy Consumption, Execution Time \n| Metrics (Accuracy) | Exact Match, Edit Distance", "doc_id": "rubei2025", "page": 5, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "TABLE I: Summary of the Experimental Settings\n| Model | Llama3 8B - Instruct \n|---|---\n| Snippets | 1,000 \n| PETs | 3 \n| Custom Prompts | 5 \n| Repetitions | 5 \n| Pause | 10 seconds \n| Metrics (Performance) | Energy Consumption, Execution Time \n| Metrics (Accuracy) | Exact Match, Edit Distance", "id": 247}
{"type": "section", "content": "The reason is that the model started to generate completely new code snippets when asked to finalize the code given as input. The few-shots technique seems to be less affected by this problem. The sequence of example questions and answers instructed the model on the behaviour despite the lack of the system role specification. \n\nConcerning the execution time, Figure 3b reports the results obtained for all the prompt configurations. Similar to energy consumption, the usage of custom tags provides a general improvement in performance. In particular, the one-shot and few-shots reduced the average time from 1.54 seconds of configuration C0 to 0.74 (-52%) and from 2.1 to 1.09 (-48%), respectively, using configuration C2. The zero-shot technique performed better using C1, reporting an improvement from 0.74 seconds to 0.63 (-14.8%). Similarly, for the energy consumption, in the case of C4, we can notice a remarkable increase in execution time for zero-shot and one-shot. \n\n\n\nAnswer to RQ1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. \n\n\n\nAnswering RQ2: Figure 4 depicts the obtained results in terms of accuracy metrics. In particular, Figure 4a shows the effects of custom tags on exact match performance across different prompt engineering techniques. Overall, we observe an increase in exact matches for configuration C1-C3 in comparison with the default configuration C0. Notably, zero-shot shows the greatest improvement with C1, where exact matches rise from 63 to 82, reflecting a 23% increase. Both one-shot and few-shots see substantial gains with C3, achieving approximately a 44% improvement. Interestingly, with C4, zero-shot fails to achieve any exact matches. \n\nFigure 4b shows the impact of custom tags on edit distance metrics, where an edit distance of 0 indicates a perfect result. Overall, custom tags contributed to a reduction in edit distance, with C2 emerging as the most effective configuration across all prompt engineering techniques. Specifically, zero-shot showed a 24% improvement, one-shot achieved a 64% reduction, and few-shots improved by 70%. Results for zero-shot and one-shot are omitted for C4 because, with this configuration, the LLM produced uncontrolled responses. As a result, it was impossible to calculate edit distance accurately, as the outputs included both code and explanatory text. Despite lacking explicit role definitions, few-shots continued to yield satisfactory results. \n\n\n\nAnswer to RQ2: Prompt customizations enhanced the accuracy of the tested PETs, showing a positive trend with increased exact matches and reduced edit distances. \n\n", "doc_id": "rubei2025", "page": 5, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "The reason is that the model started to generate completely new code snippets when asked to finalize the code given as input. The few-shots technique seems to be less affected by this problem. The sequence of example questions and answers instructed the model on the behaviour despite the lack of the system role specification. \n\nConcerning the execution time, Figure 3b reports the results obtained for all the prompt configurations. Similar to energy consumption, the usage of custom tags provides a general improvement in performance. In particular, the one-shot and few-shots reduced the average time from 1.54 seconds of configuration C0 to 0.74 (-52%) and from 2.1 to 1.09 (-48%), respectively, using configuration C2. The zero-shot technique performed better using C1, reporting an improvement from 0.74 seconds to 0.63 (-14.8%). Similarly, for the energy consumption, in the case of C4, we can notice a remarkable increase in execution time for zero-shot and one-shot. \n\n\n\nAnswer to RQ1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. \n\n\n\nAnswering RQ2: Figure 4 depicts the obtained results in terms of accuracy metrics. In particular, Figure 4a shows the effects of custom tags on exact match performance across different prompt engineering techniques. Overall, we observe an increase in exact matches for configuration C1-C3 in comparison with the default configuration C0. Notably, zero-shot shows the greatest improvement with C1, where exact matches rise from 63 to 82, reflecting a 23% increase. Both one-shot and few-shots see substantial gains with C3, achieving approximately a 44% improvement. Interestingly, with C4, zero-shot fails to achieve any exact matches. \n\nFigure 4b shows the impact of custom tags on edit distance metrics, where an edit distance of 0 indicates a perfect result. Overall, custom tags contributed to a reduction in edit distance, with C2 emerging as the most effective configuration across all prompt engineering techniques. Specifically, zero-shot showed a 24% improvement, one-shot achieved a 64% reduction, and few-shots improved by 70%. Results for zero-shot and one-shot are omitted for C4 because, with this configuration, the LLM produced uncontrolled responses. As a result, it was impossible to calculate edit distance accurately, as the outputs included both code and explanatory text. Despite lacking explicit role definitions, few-shots continued to yield satisfactory results. \n\n\n\nAnswer to RQ2: Prompt customizations enhanced the accuracy of the tested PETs, showing a positive trend with increased exact matches and reduced edit distances. \n\n", "original_types": ["text"], "id": 248}
{"type": "figure", "content": "Fig. 3: Energy consumption with different prompt configurations.", "doc_id": "rubei2025", "page": 6, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "Fig. 3: Energy consumption with different prompt configurations.", "id": 249}
{"type": "figure", "content": "Fig. 4: LLMs accuracy with different prompt configurations.", "doc_id": "rubei2025", "page": 6, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "Fig. 4: LLMs accuracy with different prompt configurations.", "id": 250}
{"type": "section", "content": "V. RELATED WORK\nAssessing LLMs energy consumption: Jagannadharao et al. [36] investigate the usage of time-shifting technique to reduce the energy consumption of LLMs during long-running training sessions. Concretely, the authors estimates the consumption of Llama model by pausing and resuming the training when the carbon emission is below a certain threshold. The results shows that the proposed approach succeed in reducing the carbon emission even though the region may impact the obtained results. Liu and Yin [37] investigate how to reduce and measure the consumption of pre-trained models by combining fine-tuning and efficient tokenizers. In particular, BERT, DistilBERT, and T5 models are compared using SQuAD benchmark [38] in terms of accuracy and carbon emissions. The experimental results reveal that both the T5 and BERT models emitted considerably more CO2 compared to DistilBERT and the T4 GPU contributes in reducing the overall carbon emissions. Samsi et al. [13] compare the inference performance in terms of watts of different Llama models, i.e., evaluating smaller models (7B, 13B) against the largest available version (65B) at the time of writing. In addition, the authors consider different GPUs, i.e., V100 and A100. The study reveals that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model, thus making small models a suitable choice for energy-efficient applications. Cursaro et al. [39] investigate the energy consumption of LLMs during inference and propose a method to reduce the energy consumption by using a combination of hardware and software techniques. The results show that the proposed method can reduce the energy consumption by up to 50% compared to the baseline. In addition, the authors also propose a method to estimate the energy consumption of LLMs during inference, which can be used to optimize the model selection and deployment. Overall, the related work shows that there is a significant opportunity to reduce the energy consumption of LLMs, and various techniques have been proposed to achieve this goal.", "doc_id": "rubei2025", "page": 6, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "V. RELATED WORK\nAssessing LLMs energy consumption: Jagannadharao et al. [36] investigate the usage of time-shifting technique to reduce the energy consumption of LLMs during long-running training sessions. Concretely, the authors estimates the consumption of Llama model by pausing and resuming the training when the carbon emission is below a certain threshold. The results shows that the proposed approach succeed in reducing the carbon emission even though the region may impact the obtained results. Liu and Yin [37] investigate how to reduce and measure the consumption of pre-trained models by combining fine-tuning and efficient tokenizers. In particular, BERT, DistilBERT, and T5 models are compared using SQuAD benchmark [38] in terms of accuracy and carbon emissions. The experimental results reveal that both the T5 and BERT models emitted considerably more CO2 compared to DistilBERT and the T4 GPU contributes in reducing the overall carbon emissions. Samsi et al. [13] compare the inference performance in terms of watts of different Llama models, i.e., evaluating smaller models (7B, 13B) against the largest available version (65B) at the time of writing. In addition, the authors consider different GPUs, i.e., V100 and A100. The study reveals that 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model, thus making small models a suitable choice for energy-efficient applications. Cursaro et al. [39] investigate the energy consumption of LLMs during inference and propose a method to reduce the energy consumption by using a combination of hardware and software techniques. The results show that the proposed method can reduce the energy consumption by up to 50% compared to the baseline. In addition, the authors also propose a method to estimate the energy consumption of LLMs during inference, which can be used to optimize the model selection and deployment. Overall, the related work shows that there is a significant opportunity to reduce the energy consumption of LLMs, and various techniques have been proposed to achieve this goal.", "original_types": ["text"], "id": 251}
{"type": "section", "content": "VI. Threats to validity\n\nThis section discusses threats that may hamper the results of our study and corresponding mitigation strategies.\n\nInternal validity\n\nInternal validity concerns factors that may impact the measurements, i.e., noise interference, background processes, and voltage fluctuations. To mitigate these issues, all the experiments have been conducted in an isolated Linux-based system without parallel or background tasks running on the GPU. In addition, we repeated each experiment five times and a 10-second pause between each query execution to prevent potential performance degradation and statistical anomalies, thus increasing the reliability of measurements.\n\nExternal validity\n\nThreats to external validity are related to the generalizability of the performed experiments, i.e., the obtained results in terms of energy consumption and accuracy may vary considering different tasks and LLMs. Concerning the data, we employed CodeXGLUE, a well-known dataset exploited in several studies. We were forced to cap our dataset to 1,000 snippets, since the time needed to test one snippet has been evaluated to 900 seconds. Finally, the measurements calculated on the inference without any customization are strictly related to the particular task that we decided to study, thus code generation or text summarization might require different energy resources. We mitigated this threat focusing on the effects of the customization.", "doc_id": "rubei2025", "page": 7, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "VI. Threats to validity\n\nThis section discusses threats that may hamper the results of our study and corresponding mitigation strategies.\n\nInternal validity\n\nInternal validity concerns factors that may impact the measurements, i.e., noise interference, background processes, and voltage fluctuations. To mitigate these issues, all the experiments have been conducted in an isolated Linux-based system without parallel or background tasks running on the GPU. In addition, we repeated each experiment five times and a 10-second pause between each query execution to prevent potential performance degradation and statistical anomalies, thus increasing the reliability of measurements.\n\nExternal validity\n\nThreats to external validity are related to the generalizability of the performed experiments, i.e., the obtained results in terms of energy consumption and accuracy may vary considering different tasks and LLMs. Concerning the data, we employed CodeXGLUE, a well-known dataset exploited in several studies. We were forced to cap our dataset to 1,000 snippets, since the time needed to test one snippet has been evaluated to 900 seconds. Finally, the measurements calculated on the inference without any customization are strictly related to the particular task that we decided to study, thus code generation or text summarization might require different energy resources. We mitigated this threat focusing on the effects of the customization.", "original_types": ["text", "header"], "id": 252}
{"type": "section", "content": "R. Tufano, S. Masiero, A. Mastropaolo et al., “Using pre-trained models to boost code review automation,” in Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. New York, NY, USA: Association for Computing Machinery, Jul. 2022, pp. 2291–2302. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510621\n\nA. Mastropaolo, S. Scalabrin, N. Cooper et al., “Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,” in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). Madrid, ES: IEEE, May 2021, pp. 336–347. [Online]. Available: https://ieeexplore.ieee.org/document/9401982/\n\nD. Wang, Z. Jia, S. Li et al., “Bridging pre-trained models and downstream tasks for source code understanding,” in Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. New York, NY, USA: Association for Computing Machinery, Jul. 2022, pp. 287–298. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510062\n\nJ. Castaño, S. Martínez-Fernández, X. Franch et al., “Exploring the Carbon Footprint of Hugging Face’s ML Models: A Repository Mining Study,” in 2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), Oct. 2023, pp. 1–12, arXiv:2305.11164 [cs, stat]. [Online]. Available: http://arxiv.org/abs/2305.11164\n\nS. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, “From words to watts: Benchmarking the energy costs of large language model inference,” in IEEE High Performance Extreme Computing Conference, HPEC 2023, Boston, MA, USA, September 25-29, 2023. IEEE, 2023, pp. 1–9. [Online]. Available: https://doi.org/10.1109/HPEC58863.2023.10363447\n\nS. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue: A machine learning benchmark dataset for code understanding and generation,” in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)\n\nA. Dubey, A. Jauhri, A. Pandey et al., “The llama 3 herd of models,” 2024. [Online]. Available: https://arxiv.org/abs/2407.21783\n\nM. C. Impact, “Codecarbon: A tool to estimate the carbon emissions of machine learning models,” 2024, accessed: 2024-03-05. [Online]. Available: https://mlco2.github.io/codecarbon/\n\nR. A. Husein, H. Aburajouh, and C. Catal, “Large language models for code completion: A systematic literature review,” Comput. Stand. Interfaces, vol. 92, p. 103917, 2025. [Online]. Available: https://doi.org/10.1016/j.csi.2024.103917\n\nS. T. Footprint, “Carbon footprint of training gpt-3 and large language models,” 2023, accessed: 2024-07-22. [Online]. Available: https://shrinkthatfootprint.com/carbon-footprint-of-training-gpt-3-and-large-language-models/\n\nTrustbit, “Llm benchmarks,” 2024, accessed: 2024-07-22. [Online]. Available: https://www.trustbit.tech/en/llm-benchmarks", "doc_id": "rubei2025", "page": 8, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "R. Tufano, S. Masiero, A. Mastropaolo et al., “Using pre-trained models to boost code review automation,” in Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. New York, NY, USA: Association for Computing Machinery, Jul. 2022, pp. 2291–2302. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510621\n\nA. Mastropaolo, S. Scalabrin, N. Cooper et al., “Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,” in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). Madrid, ES: IEEE, May 2021, pp. 336–347. [Online]. Available: https://ieeexplore.ieee.org/document/9401982/\n\nD. Wang, Z. Jia, S. Li et al., “Bridging pre-trained models and downstream tasks for source code understanding,” in Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. New York, NY, USA: Association for Computing Machinery, Jul. 2022, pp. 287–298. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510062\n\nJ. Castaño, S. Martínez-Fernández, X. Franch et al., “Exploring the Carbon Footprint of Hugging Face’s ML Models: A Repository Mining Study,” in 2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), Oct. 2023, pp. 1–12, arXiv:2305.11164 [cs, stat]. [Online]. Available: http://arxiv.org/abs/2305.11164\n\nS. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones, W. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, “From words to watts: Benchmarking the energy costs of large language model inference,” in IEEE High Performance Extreme Computing Conference, HPEC 2023, Boston, MA, USA, September 25-29, 2023. IEEE, 2023, pp. 1–9. [Online]. Available: https://doi.org/10.1109/HPEC58863.2023.10363447\n\nS. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue: A machine learning benchmark dataset for code understanding and generation,” in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)\n\nA. Dubey, A. Jauhri, A. Pandey et al., “The llama 3 herd of models,” 2024. [Online]. Available: https://arxiv.org/abs/2407.21783\n\nM. C. Impact, “Codecarbon: A tool to estimate the carbon emissions of machine learning models,” 2024, accessed: 2024-03-05. [Online]. Available: https://mlco2.github.io/codecarbon/\n\nR. A. Husein, H. Aburajouh, and C. Catal, “Large language models for code completion: A systematic literature review,” Comput. Stand. Interfaces, vol. 92, p. 103917, 2025. [Online]. Available: https://doi.org/10.1016/j.csi.2024.103917\n\nS. T. Footprint, “Carbon footprint of training gpt-3 and large language models,” 2023, accessed: 2024-07-22. [Online]. Available: https://shrinkthatfootprint.com/carbon-footprint-of-training-gpt-3-and-large-language-models/\n\nTrustbit, “Llm benchmarks,” 2024, accessed: 2024-07-22. [Online]. Available: https://www.trustbit.tech/en/llm-benchmarks", "original_types": ["text"], "id": 253}
{"type": "section", "content": "L. Arena, “Lm arena leaderboard,” 2024, accessed: 2024-07-22. [Online]. Available: https://lmarena.ai/?leaderboard\n\nOobabooga, “Oobabooga benchmark,” 2024, accessed: 2024-07-22. [Online]. Available: https://oobabooga.github.io/benchmark.html\n\nE. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in nlp,” 2019. [Online]. Available: https://arxiv.org/abs/1906.02243\n\nB. Romera-Paredes and P. H. S. Torr, “An embarrassingly simple approach to zero-shot learning,” in Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ser. ICML'15. JMLR.org, 2015, p. 2152–2161\n\nR. L. L. I. au2, I. Balažević, E. Wallace, F. Petroni, S. Singh, and S. Riedel, “Cutting down on prompts and parameters: Simple few-shot learning with language models,” 2021. [Online]. Available: https://arxiv.org/abs/2106.13353\n\nX. Li, S. Yuan, X. Gu et al., “Few-shot code translation via task-adapted prompt learning,” Journal of Systems and Software, vol. 212, p. 112002, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0164121224000451\n\nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A survey of quantization methods for efficient neural network inference,” 2021. [Online]. Available: https://arxiv.org/abs/2103.13630", "doc_id": "rubei2025", "page": 8, "url": "https://arxiv.org/pdf/2501.05899", "embedded_text": "L. Arena, “Lm arena leaderboard,” 2024, accessed: 2024-07-22. [Online]. Available: https://lmarena.ai/?leaderboard\n\nOobabooga, “Oobabooga benchmark,” 2024, accessed: 2024-07-22. [Online]. Available: https://oobabooga.github.io/benchmark.html\n\nE. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in nlp,” 2019. [Online]. Available: https://arxiv.org/abs/1906.02243\n\nB. Romera-Paredes and P. H. S. Torr, “An embarrassingly simple approach to zero-shot learning,” in Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ser. ICML'15. JMLR.org, 2015, p. 2152–2161\n\nR. L. L. I. au2, I. Balažević, E. Wallace, F. Petroni, S. Singh, and S. Riedel, “Cutting down on prompts and parameters: Simple few-shot learning with language models,” 2021. [Online]. Available: https://arxiv.org/abs/2106.13353\n\nX. Li, S. Yuan, X. Gu et al., “Few-shot code translation via task-adapted prompt learning,” Journal of Systems and Software, vol. 212, p. 112002, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0164121224000451\n\nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A survey of quantization methods for efficient neural network inference,” 2021. [Online]. Available: https://arxiv.org/abs/2103.13630", "original_types": ["text"], "id": 254}
{"type": "section", "content": "Abstract\n\nAs the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impacts—energy and water usage in data centers, e-waste from frequent hardware upgrades—without addressing the significant indirect effects. This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI’s impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI’s true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI’s role in the climate crisis.\n\nKeywords\n\nArtificial intelligence, Environmental Impacts, Lifecycle Assessment, Rebound Effects, Sustainability\n\nACM Reference Format:\n\nAlexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. In The 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25), June 23–26, 2025, Athens, Greece. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3715275.3732007\n\n1. Introduction", "doc_id": "luccioni2025a", "page": 1, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Abstract\n\nAs the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impacts—energy and water usage in data centers, e-waste from frequent hardware upgrades—without addressing the significant indirect effects. This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI’s impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI’s true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI’s role in the climate crisis.\n\nKeywords\n\nArtificial intelligence, Environmental Impacts, Lifecycle Assessment, Rebound Effects, Sustainability\n\nACM Reference Format:\n\nAlexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. In The 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25), June 23–26, 2025, Athens, Greece. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3715275.3732007\n\n1. Introduction", "original_types": ["text", "header"], "id": 255}
{"type": "section", "content": "As the climate crisis intensifies, the environmental impacts of artificial intelligence (AI) have become the subject of many a polarized debate. The question of whether the potential positive impacts of AI outweigh the negative ones, and how to foster technological advances in AI while minimizing its environmental harms, has divided AI researchers and practitioners alike. Some maintain that its potential to accelerate sustainable breakthroughs will exceed its environmental costs by increasing renewable energy production and transmission or aiding in the design of more sustainable materials [76, 99]. Others point to the soaring resource demands of large-scale AI models and their negative environmental impacts from non-renewable energy use, water consumption, and extraction of minerals [24, 28, 45, 57]. These opposing positions tend to center on the technology’s direct impacts, measured in energy consumption and greenhouse gas (GHG) emissions from data centers, or in the e-waste that accumulates as hardware becomes obsolete. Yet a critical dimension of AI’s climate footprint lies outside these direct resource and emissions calculations. Recent work on indirect impacts [63] warns of potential “rebound effects”, whereby gains in efficiency spur higher overall consumption. These second-order effects challenge the presumption that purely technical optimizations alone will deliver sufficient climate benefits. Cost savings achieved by more efficient AI hardware, for example, can spur increased demand for new AI functionalities, which in turn drive further hardware upgrades and increase costs. Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60]. The addition of increasingly efficient AI to systems from commerce to transportation can have far-reaching effects on our societies, our behaviors, and the future paths available to us in the race against climate change. This system-level complexity underscores the inadequacy of the question, “Is AI net positive or net negative for the climate?” Instead, we adopt an analytic approach that includes the intersecting social, political, and economic contexts in which AI systems are developed and deployed. Although efficiency has been a defining ethos in recent AI research, reflected in “scaling laws” that promise ever-more-powerful models [64], effective climate action requires grappling with how these systems reshape markets, cultural norms, and policy priorities. Thus, understanding rebound effects requires drawing on both qualitative and quantitative methods, drawn from computer science, economics and the social sciences, as they hinge not on algorithmic design but human adaptation and use patterns. Adopting an interdisciplinary approach allows for both a rigorous lifecycle accounting of direct effects as well as understanding the social behaviors that AI can induce or displace. This paper aims to bridge the gap in the existing literature by, first, providing a brief overview of the debate about AI’s direct positive and negative impacts on the environment (§2). We follow this", "doc_id": "luccioni2025a", "page": 1, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "As the climate crisis intensifies, the environmental impacts of artificial intelligence (AI) have become the subject of many a polarized debate. The question of whether the potential positive impacts of AI outweigh the negative ones, and how to foster technological advances in AI while minimizing its environmental harms, has divided AI researchers and practitioners alike. Some maintain that its potential to accelerate sustainable breakthroughs will exceed its environmental costs by increasing renewable energy production and transmission or aiding in the design of more sustainable materials [76, 99]. Others point to the soaring resource demands of large-scale AI models and their negative environmental impacts from non-renewable energy use, water consumption, and extraction of minerals [24, 28, 45, 57]. These opposing positions tend to center on the technology’s direct impacts, measured in energy consumption and greenhouse gas (GHG) emissions from data centers, or in the e-waste that accumulates as hardware becomes obsolete. Yet a critical dimension of AI’s climate footprint lies outside these direct resource and emissions calculations. Recent work on indirect impacts [63] warns of potential “rebound effects”, whereby gains in efficiency spur higher overall consumption. These second-order effects challenge the presumption that purely technical optimizations alone will deliver sufficient climate benefits. Cost savings achieved by more efficient AI hardware, for example, can spur increased demand for new AI functionalities, which in turn drive further hardware upgrades and increase costs. Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60]. The addition of increasingly efficient AI to systems from commerce to transportation can have far-reaching effects on our societies, our behaviors, and the future paths available to us in the race against climate change. This system-level complexity underscores the inadequacy of the question, “Is AI net positive or net negative for the climate?” Instead, we adopt an analytic approach that includes the intersecting social, political, and economic contexts in which AI systems are developed and deployed. Although efficiency has been a defining ethos in recent AI research, reflected in “scaling laws” that promise ever-more-powerful models [64], effective climate action requires grappling with how these systems reshape markets, cultural norms, and policy priorities. Thus, understanding rebound effects requires drawing on both qualitative and quantitative methods, drawn from computer science, economics and the social sciences, as they hinge not on algorithmic design but human adaptation and use patterns. Adopting an interdisciplinary approach allows for both a rigorous lifecycle accounting of direct effects as well as understanding the social behaviors that AI can induce or displace. This paper aims to bridge the gap in the existing literature by, first, providing a brief overview of the debate about AI’s direct positive and negative impacts on the environment (§2). We follow this", "id": 256}
{"type": "section", "content": "AI and the environment\n\nThe environmental impacts of AI were largely overlooked until recent years. Now these issues figure prominently in both scientific debates and public media. Central to this discourse is the question of whether AI’s capacity to help mitigate climate change, e.g. by optimizing energy use or discovering sustainable materials, truly exceeds its environmental costs in terms of energy consumption, water usage, and mineral extraction. Some contend that AI’s potential benefits justify widespread deployment across various climate-focused applications, whereas others caution that unrestrained expansion may ultimately be more harmful than beneficial. In this section, we review these competing perspectives on AI’s positive and negative environmental impacts as presented in academic and industry discussions.\n\nArguments that AI is a net climate negative\n\nResearch into AI’s total usage of natural resources is still nascent, but initial studies have highlighted significant concerns across multiple domains of direct impact. Papers have addressed carbon emissions from training large models [67, 69, 70, 120], water consumption for cooling servers [52, 80], and the mining of minerals for technical infrastructure [24]. In the present section, we present the different directions of study pursued in terms of AI’s negative environmental impacts and discuss the observations that have been put forward.\n\nAI is using increasing amounts of energy, most of it generated from nonrenewable sources. Data from the International Energy Agency (IEA) has provided the strongest international benchmark. They note that the electricity demand from data centers, driven heavily by AI training and inference, is currently at 2% of total global electricity and will more than double by 2026, surpassing Canada’s national power use [45, 56]. This growth is putting strain on energy grids around the world and resulting in many countries, such as Ireland and the Netherlands, placing a moratorium on the construction of new data centers in certain regions. In the United States, the demand for electricity has surged in the past twelve months – according to a recent report into the national grid, electric utilities have nearly doubled their forecasts of how much additional power they will need in the next five years [137].", "doc_id": "luccioni2025a", "page": 2, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "AI and the environment\n\nThe environmental impacts of AI were largely overlooked until recent years. Now these issues figure prominently in both scientific debates and public media. Central to this discourse is the question of whether AI’s capacity to help mitigate climate change, e.g. by optimizing energy use or discovering sustainable materials, truly exceeds its environmental costs in terms of energy consumption, water usage, and mineral extraction. Some contend that AI’s potential benefits justify widespread deployment across various climate-focused applications, whereas others caution that unrestrained expansion may ultimately be more harmful than beneficial. In this section, we review these competing perspectives on AI’s positive and negative environmental impacts as presented in academic and industry discussions.\n\nArguments that AI is a net climate negative\n\nResearch into AI’s total usage of natural resources is still nascent, but initial studies have highlighted significant concerns across multiple domains of direct impact. Papers have addressed carbon emissions from training large models [67, 69, 70, 120], water consumption for cooling servers [52, 80], and the mining of minerals for technical infrastructure [24]. In the present section, we present the different directions of study pursued in terms of AI’s negative environmental impacts and discuss the observations that have been put forward.\n\nAI is using increasing amounts of energy, most of it generated from nonrenewable sources. Data from the International Energy Agency (IEA) has provided the strongest international benchmark. They note that the electricity demand from data centers, driven heavily by AI training and inference, is currently at 2% of total global electricity and will more than double by 2026, surpassing Canada’s national power use [45, 56]. This growth is putting strain on energy grids around the world and resulting in many countries, such as Ireland and the Netherlands, placing a moratorium on the construction of new data centers in certain regions. In the United States, the demand for electricity has surged in the past twelve months – according to a recent report into the national grid, electric utilities have nearly doubled their forecasts of how much additional power they will need in the next five years [137].", "original_types": ["text", "header"], "id": 257}
{"type": "section", "content": "AI data centers are putting stress on already strained aquifers globally. The construction and operation of data centers requires vast quantities of water, which is used for cooling servers to prevent them from overheating by circulating cool water through radiators. This cooling process requires a constant supply of cool, fresh water, which is heated up during the process, causing a significant portion of it to evaporate, whereas the rest has to be cooled and filtered before being reused or discharged back into local aquifers [96]. Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78]. Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68]. By 2050, 50% of the world’s population is projected to live in a region affected by water scarcity [14], but the full impacts of data center water usage is unknown as “the entire data centre industry suffers from a lack of transparency” [80].\n\nThe rare earth minerals needed to produce computing hardware are mined in unsustainable and opaque ways. Metals such as cobalt, lithium, coltan, gallium, copper, tungsten, and germanium are required for AI hardware and infrastructure. Individual studies have looked at the impacts of mining lithium cobalt, copper, and rare earth for consumer devices (smartphones, tablets) to the GPUs and TPUs powering large-scale AI model training and addressed the resulting environmental damage and impact on local conflict and war. One study specifically addressed mining on indigenous lands, with 54% of technology-critical materials drawn from what they describe as Indigenous or peasant territory, while 62% were extracted from drought-prone zones [86]. The mining of these metals also comes with a cost to the environment, given the tonnes of earth that have to be mined, the radiation produced, and the toxic waste created [28]. Taiwan Semiconductor Manufacturing Company (TSMC), the company that manufactures the GPUs designed by NVIDIA (among other companies), citing the complexity of their supply chains and proprietary information of its customers, also does not provide granular data for their suppliers nor their internal processes (such as chemical purification) [136], meaning that it is impossible to carry out a full life cycle analysis of the hundreds of thousands of GPUs that are designed and manufactured each year and used to train and deploy AI models.", "doc_id": "luccioni2025a", "page": 2, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "AI data centers are putting stress on already strained aquifers globally. The construction and operation of data centers requires vast quantities of water, which is used for cooling servers to prevent them from overheating by circulating cool water through radiators. This cooling process requires a constant supply of cool, fresh water, which is heated up during the process, causing a significant portion of it to evaporate, whereas the rest has to be cooled and filtered before being reused or discharged back into local aquifers [96]. Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78]. Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68]. By 2050, 50% of the world’s population is projected to live in a region affected by water scarcity [14], but the full impacts of data center water usage is unknown as “the entire data centre industry suffers from a lack of transparency” [80].\n\nThe rare earth minerals needed to produce computing hardware are mined in unsustainable and opaque ways. Metals such as cobalt, lithium, coltan, gallium, copper, tungsten, and germanium are required for AI hardware and infrastructure. Individual studies have looked at the impacts of mining lithium cobalt, copper, and rare earth for consumer devices (smartphones, tablets) to the GPUs and TPUs powering large-scale AI model training and addressed the resulting environmental damage and impact on local conflict and war. One study specifically addressed mining on indigenous lands, with 54% of technology-critical materials drawn from what they describe as Indigenous or peasant territory, while 62% were extracted from drought-prone zones [86]. The mining of these metals also comes with a cost to the environment, given the tonnes of earth that have to be mined, the radiation produced, and the toxic waste created [28]. Taiwan Semiconductor Manufacturing Company (TSMC), the company that manufactures the GPUs designed by NVIDIA (among other companies), citing the complexity of their supply chains and proprietary information of its customers, also does not provide granular data for their suppliers nor their internal processes (such as chemical purification) [136], meaning that it is impossible to carry out a full life cycle analysis of the hundreds of thousands of GPUs that are designed and manufactured each year and used to train and deploy AI models.", "original_types": ["text"], "id": 258}
{"type": "section", "content": "AI is responsible for a large amount of greenhouse gas (GHG) emissions. Despite significant efforts by AI-driven companies to invest in renewable energy and meet net-zero emissions pledges, all evidence indicates that direct GHG emissions due to AI are on the rise. Many AI companies are reporting significantly higher GHG emissions over earlier baselines, likely driven by increases in development and use of generative AI. For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42], Baidu reports 32.6% increase in GHG emissions over 2021 citing “rapid development of LLMs” posing “severe challenges” to their development of green data centers [9], and Microsoft cites a 29.1%", "doc_id": "luccioni2025a", "page": 2, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "AI is responsible for a large amount of greenhouse gas (GHG) emissions. Despite significant efforts by AI-driven companies to invest in renewable energy and meet net-zero emissions pledges, all evidence indicates that direct GHG emissions due to AI are on the rise. Many AI companies are reporting significantly higher GHG emissions over earlier baselines, likely driven by increases in development and use of generative AI. For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42], Baidu reports 32.6% increase in GHG emissions over 2021 citing “rapid development of LLMs” posing “severe challenges” to their development of green data centers [9], and Microsoft cites a 29.1%", "original_types": ["text"], "id": 259}
{"type": "section", "content": "2.2 Arguments that AI is a net climate positive\n\nThe argument that AI will ultimately be more beneficial than harmful to the environment is primarily based on the narrative that AI can be sustainably developed and used for applications that directly benefit the environment, or that could accelerate the technological advances needed to address climate change. However, claims around AI’s sustainability and significance in enabling new scientific advances remain largely hypothetical, with little explicit evidence or quantitative analysis of how the overall impact of AI applications, which may cause equal or greater harm to the environment or even accelerate climate change, will be ultimately beneficial. We discuss the main assertions supporting AI’s positive role with respect to climate change in more detail below.", "doc_id": "luccioni2025a", "page": 3, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "2.2 Arguments that AI is a net climate positive\n\nThe argument that AI will ultimately be more beneficial than harmful to the environment is primarily based on the narrative that AI can be sustainably developed and used for applications that directly benefit the environment, or that could accelerate the technological advances needed to address climate change. However, claims around AI’s sustainability and significance in enabling new scientific advances remain largely hypothetical, with little explicit evidence or quantitative analysis of how the overall impact of AI applications, which may cause equal or greater harm to the environment or even accelerate climate change, will be ultimately beneficial. We discuss the main assertions supporting AI’s positive role with respect to climate change in more detail below.", "original_types": ["text", "header"], "id": 260}
{"type": "section", "content": "distribution (which, like other transformative changes in the climate space, would also require significant investment in infrastructure and navigating corresponding sociopolitical systems), PPAs and carbon offsets will remain necessary to fill the gap. However, offsetting was only ever meant to serve as a temporary stop-gap to help reduce emissions in the short term, and does not represent a viable replacement to reducing actual emissions. Fundamental limitations to carbon offsetting are: (1) the difficulty of proving additionality, or that the carbon-reducing activity would not have happened regardless of investment or purchase of the offset [17, 46], and (2) that offsetting rarely mitigates localized impacts to the communities where emissions or other environmental degradation is occurring. Increasingly, companies such as Intel and TSMC are reporting annual water withdrawal and consumption as well, and compensating with market-based water offsets which are subject to the same challenges [138].\n\nIncreases in efficiency will negate growth in AI resource consumption. Another common viewpoint is that the direct environmental impacts of AI will diminish over time due to increases in hardware, software and algorithmic efficiency. Patterson et al. [90] argue that “the carbon footprint of machine learning training will plateau, then shrink” thanks to continued innovations in machine learning models, specialized hardware platforms, data center efficiency, scheduling and use patterns, which will reduce overall energy use and emissions. The claim that increasing AI efficiency will lead to an overall reduction in AI’s resource use is a clear example of why a deeper engagement with indirect effects is needed in the work on AI and climate change.", "doc_id": "luccioni2025a", "page": 4, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "distribution (which, like other transformative changes in the climate space, would also require significant investment in infrastructure and navigating corresponding sociopolitical systems), PPAs and carbon offsets will remain necessary to fill the gap. However, offsetting was only ever meant to serve as a temporary stop-gap to help reduce emissions in the short term, and does not represent a viable replacement to reducing actual emissions. Fundamental limitations to carbon offsetting are: (1) the difficulty of proving additionality, or that the carbon-reducing activity would not have happened regardless of investment or purchase of the offset [17, 46], and (2) that offsetting rarely mitigates localized impacts to the communities where emissions or other environmental degradation is occurring. Increasingly, companies such as Intel and TSMC are reporting annual water withdrawal and consumption as well, and compensating with market-based water offsets which are subject to the same challenges [138].\n\nIncreases in efficiency will negate growth in AI resource consumption. Another common viewpoint is that the direct environmental impacts of AI will diminish over time due to increases in hardware, software and algorithmic efficiency. Patterson et al. [90] argue that “the carbon footprint of machine learning training will plateau, then shrink” thanks to continued innovations in machine learning models, specialized hardware platforms, data center efficiency, scheduling and use patterns, which will reduce overall energy use and emissions. The claim that increasing AI efficiency will lead to an overall reduction in AI’s resource use is a clear example of why a deeper engagement with indirect effects is needed in the work on AI and climate change.", "original_types": ["text"], "id": 261}
{"type": "section", "content": "Similarly to Jevons’ Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible. However, as Koomey and Masanet [65] cogently argue, this is not the first time the alarm has been raised about rising energy use due to technology, and that similar projections made in the dot-com boom of the early 2000s failed to materialize. They cite poor data availability, flawed methodology, and inaccurate reporting as causes for inaccurate projections in data center energy use that ultimately did not take into account significant improvements in data center efficiency. Similar uncertainty is clouding the prediction of AI’s energy use, and we are well aligned with Koomey and Masanet [65], Masanet et al. [74] and others in calling for more granular transparent data from technology firms and service providers, and more rigorous analysis of available data. This does not imply, however, that AI will necessarily follow the same trend as past technological advances. Further, although data center energy use is a relatively accessible statistic for approximating the environmental impacts of ICT, it represents only small portion of the diverse direct and indirect negative environmental impacts arising from AI, which extend far beyond GHG emissions due to the energy required to develop and use AI models.", "doc_id": "luccioni2025a", "page": 4, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Similarly to Jevons’ Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible. However, as Koomey and Masanet [65] cogently argue, this is not the first time the alarm has been raised about rising energy use due to technology, and that similar projections made in the dot-com boom of the early 2000s failed to materialize. They cite poor data availability, flawed methodology, and inaccurate reporting as causes for inaccurate projections in data center energy use that ultimately did not take into account significant improvements in data center efficiency. Similar uncertainty is clouding the prediction of AI’s energy use, and we are well aligned with Koomey and Masanet [65], Masanet et al. [74] and others in calling for more granular transparent data from technology firms and service providers, and more rigorous analysis of available data. This does not imply, however, that AI will necessarily follow the same trend as past technological advances. Further, although data center energy use is a relatively accessible statistic for approximating the environmental impacts of ICT, it represents only small portion of the diverse direct and indirect negative environmental impacts arising from AI, which extend far beyond GHG emissions due to the energy required to develop and use AI models.", "original_types": ["text"], "id": 262}
{"type": "section", "content": "There is no reason to focus on AI over other technological advances or sectors. Some would argue that there is no reason to raise concerns around the current or future energy use (and corresponding environmental impacts) of AI specifically, as compared to any other way that energy might be used, such as “for watching television, microwaving popcorn, [or] powering lights” [18]. This line of thought posits that AI will be subject to the same market pressures, such as energy prices, as any other use case, and as a result will benefit from the same innovations, such as a market transitions to renewable energy. This oversimplification ignores the reality that (1) AI is already responsible for non-trivial negative environmental externalities and (2) we can, and likely should, regulate certain uses of AI as appropriate/inappropriate or necessary/unnecessary, and that this does require breaking down the monolith of “AI” into different use cases and corresponding judgments of potential utility. For example, under a limited energy or emissions budget, we might selectively incentivize uses of AI that contribute towards the UN’s sustainable development goals [130] or mitigate national security concerns, over e.g. generating personalized ads for social media (as we discussion in Section 3). Finally, AI does differ substantially from other energy sinks in its potential to engender vast transformations in the economy and society, similar to how the advent of the Internet has undeniably changed the nature of work, education, and social interactions. Purely economic incentives have obviously failed to align well with environmental sustainability in the past, and AI is no different; this does not mean that we should not strive to do better with this new technology.\n\nWhile debates over AI’s role in climate change and sustainability have become increasingly polarized, both sides have tended to focus only on the direct impacts of this technology— positive and negative. But direct effects are not the whole story. As we show in the next section, the integration of AI into tools and systems reshapes social structures and influences human behavior, which ultimately has complex environmental consequences.\n\n3 Indirect Impacts and Rebound Effects", "doc_id": "luccioni2025a", "page": 4, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "There is no reason to focus on AI over other technological advances or sectors. Some would argue that there is no reason to raise concerns around the current or future energy use (and corresponding environmental impacts) of AI specifically, as compared to any other way that energy might be used, such as “for watching television, microwaving popcorn, [or] powering lights” [18]. This line of thought posits that AI will be subject to the same market pressures, such as energy prices, as any other use case, and as a result will benefit from the same innovations, such as a market transitions to renewable energy. This oversimplification ignores the reality that (1) AI is already responsible for non-trivial negative environmental externalities and (2) we can, and likely should, regulate certain uses of AI as appropriate/inappropriate or necessary/unnecessary, and that this does require breaking down the monolith of “AI” into different use cases and corresponding judgments of potential utility. For example, under a limited energy or emissions budget, we might selectively incentivize uses of AI that contribute towards the UN’s sustainable development goals [130] or mitigate national security concerns, over e.g. generating personalized ads for social media (as we discussion in Section 3). Finally, AI does differ substantially from other energy sinks in its potential to engender vast transformations in the economy and society, similar to how the advent of the Internet has undeniably changed the nature of work, education, and social interactions. Purely economic incentives have obviously failed to align well with environmental sustainability in the past, and AI is no different; this does not mean that we should not strive to do better with this new technology.\n\nWhile debates over AI’s role in climate change and sustainability have become increasingly polarized, both sides have tended to focus only on the direct impacts of this technology— positive and negative. But direct effects are not the whole story. As we show in the next section, the integration of AI into tools and systems reshapes social structures and influences human behavior, which ultimately has complex environmental consequences.\n\n3 Indirect Impacts and Rebound Effects", "original_types": ["section", "text"], "id": 263}
{"type": "section", "content": "In economics and lifecycle assessment, direct impacts, such as those described in the previous section, are those engendered by the product during its lifecycle, whereas indirect (or second order) impacts refer to systemic responses to the development of the product in terms of behavioral or structural changes which affect on other processes, structures and lifestyles [22, 97, 136]. Indirect impacts also include rebound effects, which occur when the improvement of one aspect of a product results in unintended negative consequences due to increased adoption, usage, and workloads [13]. These impacts inevitably come with consequences to the environment due to the increased usage, or a redistribution in the usage, of natural resources. Given that AI pervades many different areas of society and economic sectors, it is more difficult to enumerate all of the possible indirect impacts and rebound effects that it can have, which would involve considering the different interactions at play within every sector and between them [23, 53]. Nonetheless, our aim in this section is to propose a way of thinking about the indirect environmental impacts of AI that can help inform discussions around AI’s environmental costs and benefits to make them more complete than those described in Section 2. In Table 1, we take elements from the qualitative taxonomy for second order environmental effects of ICT proposed by Börjesson, Rivera et al. [97] as well as the initial work on the indirect impacts of AI by Kaack et al. [63]. We build upon and adapt both of these approaches to get a better idea of the different types of indirect environmental impacts of AI technologies and, to the extent possible, propose approaches to track and", "doc_id": "luccioni2025a", "page": 4, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "In economics and lifecycle assessment, direct impacts, such as those described in the previous section, are those engendered by the product during its lifecycle, whereas indirect (or second order) impacts refer to systemic responses to the development of the product in terms of behavioral or structural changes which affect on other processes, structures and lifestyles [22, 97, 136]. Indirect impacts also include rebound effects, which occur when the improvement of one aspect of a product results in unintended negative consequences due to increased adoption, usage, and workloads [13]. These impacts inevitably come with consequences to the environment due to the increased usage, or a redistribution in the usage, of natural resources. Given that AI pervades many different areas of society and economic sectors, it is more difficult to enumerate all of the possible indirect impacts and rebound effects that it can have, which would involve considering the different interactions at play within every sector and between them [23, 53]. Nonetheless, our aim in this section is to propose a way of thinking about the indirect environmental impacts of AI that can help inform discussions around AI’s environmental costs and benefits to make them more complete than those described in Section 2. In Table 1, we take elements from the qualitative taxonomy for second order environmental effects of ICT proposed by Börjesson, Rivera et al. [97] as well as the initial work on the indirect impacts of AI by Kaack et al. [63]. We build upon and adapt both of these approaches to get a better idea of the different types of indirect environmental impacts of AI technologies and, to the extent possible, propose approaches to track and", "original_types": ["text"], "id": 264}
{"type": "section", "content": "Material Rebound Effects\n\nAI is impacting the distribution of objects and physical space by changing the way products are made and distributed, and the functionalities that they need to have in order to allow us to interact with AI tools and systems. This has material rebound effects across complex global supply chains, which inexorably changes the way natural resources are exploited, transported, and combined.\n\nFor instance, substitution impacts arise when one product or service replaces another, rendering digital something that was previously analog (i.e. dematerializing it). Recent examples of this include online streaming which has replaced VHS cassettes, vinyl records, CDs and DVDs, as well as e-readers substituting print books and magazines [48]. This can come with both positive and negative impacts in terms of sustainability: Increased capacity and efficiency of new technologies (e.g. many different movies and TV shows being available on a single streaming platform) can be invalidated by the increased materials needed to support the underlying infrastructure needed to host and deliver these technologies [95]. For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103]. In the case of AI, many previously material tools and products are being substituted by digital, AI-enabled systems, such as paper maps having been replaced by digital navigation for routine travel. Substitution impacts may occur at varying speeds and may be more gradual. For instance, only a fraction of illustrations such as photographs and artworks have, as of yet, been replaced by AI-generated images, and combinations of AI tools and analog dictionaries are used for translation and writing tasks. While these substitution impacts are likely environmentally-positive (given that it is no longer necessary to manufacture the equipment and supplies needed to carry out the original task), the environmental impacts of developing and deploying these AI-enabled tools remains under-explored and direct comparisons are seldom performed, or even possible using status quo data and methodology (see Section 3.4).", "doc_id": "luccioni2025a", "page": 5, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Material Rebound Effects\n\nAI is impacting the distribution of objects and physical space by changing the way products are made and distributed, and the functionalities that they need to have in order to allow us to interact with AI tools and systems. This has material rebound effects across complex global supply chains, which inexorably changes the way natural resources are exploited, transported, and combined.\n\nFor instance, substitution impacts arise when one product or service replaces another, rendering digital something that was previously analog (i.e. dematerializing it). Recent examples of this include online streaming which has replaced VHS cassettes, vinyl records, CDs and DVDs, as well as e-readers substituting print books and magazines [48]. This can come with both positive and negative impacts in terms of sustainability: Increased capacity and efficiency of new technologies (e.g. many different movies and TV shows being available on a single streaming platform) can be invalidated by the increased materials needed to support the underlying infrastructure needed to host and deliver these technologies [95]. For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103]. In the case of AI, many previously material tools and products are being substituted by digital, AI-enabled systems, such as paper maps having been replaced by digital navigation for routine travel. Substitution impacts may occur at varying speeds and may be more gradual. For instance, only a fraction of illustrations such as photographs and artworks have, as of yet, been replaced by AI-generated images, and combinations of AI tools and analog dictionaries are used for translation and writing tasks. While these substitution impacts are likely environmentally-positive (given that it is no longer necessary to manufacture the equipment and supplies needed to carry out the original task), the environmental impacts of developing and deploying these AI-enabled tools remains under-explored and direct comparisons are seldom performed, or even possible using status quo data and methodology (see Section 3.4).", "original_types": ["text", "header"], "id": 265}
{"type": "section", "content": "Another type of material rebound effect, referred to as the scale effect, occurs when large-scale production or usage of a product has a lesser environmental impact, therefore reducing some of the environmental impacts incurred. For instance, purchasing raw materials in bulk or wholesale lowers overall production costs for businesses, and manufacturing on a larger scale allows a more efficient use of products that would otherwise be wasted [139]. Scaling is a core part of AI research and practice, and the promise of “methods that continue to scale with increased computation even as the available computation becomes very great” [121] has become a core tenet in the field. This is often guided by so-called scaling laws that predict the optimal model size and number of training steps based on data availability, incentivizing the development and deployment of increasingly large models as more data becomes available [49]. At a more granular level, hardware optimizations such as parallelization, do in fact support these economies of scale; e.g. batched inference on a GPU scales sub-linearly in the number of examples, allowing multiple queries to be performed at once at only a marginally higher cost [109]. In fact, batching, caching and other optimization techniques are routinely used as a means to improve the scaling of AI, allowing more users to interact with ever more powerful technologies [93, 141]. However, it remains unclear to what extent this pursuit of bigger models requiring more computation is counteracted by optimization approaches that are used in parallel, and the consequences this might have on power grids and supply chains worldwide.", "doc_id": "luccioni2025a", "page": 5, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Another type of material rebound effect, referred to as the scale effect, occurs when large-scale production or usage of a product has a lesser environmental impact, therefore reducing some of the environmental impacts incurred. For instance, purchasing raw materials in bulk or wholesale lowers overall production costs for businesses, and manufacturing on a larger scale allows a more efficient use of products that would otherwise be wasted [139]. Scaling is a core part of AI research and practice, and the promise of “methods that continue to scale with increased computation even as the available computation becomes very great” [121] has become a core tenet in the field. This is often guided by so-called scaling laws that predict the optimal model size and number of training steps based on data availability, incentivizing the development and deployment of increasingly large models as more data becomes available [49]. At a more granular level, hardware optimizations such as parallelization, do in fact support these economies of scale; e.g. batched inference on a GPU scales sub-linearly in the number of examples, allowing multiple queries to be performed at once at only a marginally higher cost [109]. In fact, batching, caching and other optimization techniques are routinely used as a means to improve the scaling of AI, allowing more users to interact with ever more powerful technologies [93, 141]. However, it remains unclear to what extent this pursuit of bigger models requiring more computation is counteracted by optimization approaches that are used in parallel, and the consequences this might have on power grids and supply chains worldwide.", "original_types": ["text"], "id": 266}
{"type": "section", "content": "Indirect Effect\n\nSubstitution impacts\n\nSpace rebound effects\n\nScale effects\n\nDirect economic rebound effects\n\nIndirect economic rebound effects\n\nto interact with AI systems\n\nEconomy-wide rebound effects\n\nrenewable energy capacity to be expanded\n\nInduction impacts\n\nTime rebound effects", "doc_id": "luccioni2025a", "page": 6, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Indirect Effect\n\nSubstitution impacts\n\nSpace rebound effects\n\nScale effects\n\nDirect economic rebound effects\n\nIndirect economic rebound effects\n\nto interact with AI systems\n\nEconomy-wide rebound effects\n\nrenewable energy capacity to be expanded\n\nInduction impacts\n\nTime rebound effects", "original_types": ["text", "header"], "id": 267}
{"type": "table", "content": "Table 1: Examples of AI’s indirect impacts and rebound effects, expanding on Börjesson Rivera et al [97].", "doc_id": "luccioni2025a", "page": 6, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Table 1: Examples of AI’s indirect impacts and rebound effects, expanding on Börjesson Rivera et al [97].", "id": 268}
{"type": "section", "content": "Examples for AI’s environmental impacts\n\nArt replaced by AI-generated imagery\n\nEncyclopedias and books replaced by AI-generated content\n\nDatacenters are getting bigger while devices are getting smaller\n\nAI models growing in size and complexity\n\nAI hardware getting more efficient, yet datacenter energy usage is rising\n\nMore consumer devices (speakers, appliances, etc.) that allow users\n\nPower purchase agreements from AI compute providers enable global\n\nTargeted ads that use AI to induce more consumption\n\nAI-optimized navigation\n\nRobots doing household chores", "doc_id": "luccioni2025a", "page": 6, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Examples for AI’s environmental impacts\n\nArt replaced by AI-generated imagery\n\nEncyclopedias and books replaced by AI-generated content\n\nDatacenters are getting bigger while devices are getting smaller\n\nAI models growing in size and complexity\n\nAI hardware getting more efficient, yet datacenter energy usage is rising\n\nMore consumer devices (speakers, appliances, etc.) that allow users\n\nPower purchase agreements from AI compute providers enable global\n\nTargeted ads that use AI to induce more consumption\n\nAI-optimized navigation\n\nRobots doing household chores", "original_types": ["text", "header"], "id": 269}
{"type": "section", "content": "Societal and Behavioral Rebound Effects\n\nAs users of technology and citizens of society, our behaviors are shaped and affected by the technologies we use. This has increasingly become the case for AI as it becomes more intertwined in daily activities such as shopping, travel, household chores, and the workplace. In this section, we describe ways in which AI can impact human behaviors and how those changes, can have broader environmental impacts in turn.", "doc_id": "luccioni2025a", "page": 7, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Societal and Behavioral Rebound Effects\n\nAs users of technology and citizens of society, our behaviors are shaped and affected by the technologies we use. This has increasingly become the case for AI as it becomes more intertwined in daily activities such as shopping, travel, household chores, and the workplace. In this section, we describe ways in which AI can impact human behaviors and how those changes, can have broader environmental impacts in turn.", "original_types": ["text", "header"], "id": 270}
{"type": "section", "content": "3.4 Tracking and Mitigating AI’s Rebound Effects\n\nThe main challenges in measuring and mitigating indirect impacts and rebound effects are their uncertainty and heterogeneity. While tracking direct impacts can largely be achieved by monitoring a finite set of relatively well-defined metrics, such as liters of water consumed or tons of CO2 emitted, rebound effects by definition encompass social, economic, and behavioral impacts across different areas of society [114]. Realizing effective progress in characterizing AI’s rebound effects necessitates moving beyond the binary narrative towards more holistic assessment that incorporates a variety of complementary approaches. We discuss some of these approaches below.", "doc_id": "luccioni2025a", "page": 8, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "3.4 Tracking and Mitigating AI’s Rebound Effects\n\nThe main challenges in measuring and mitigating indirect impacts and rebound effects are their uncertainty and heterogeneity. While tracking direct impacts can largely be achieved by monitoring a finite set of relatively well-defined metrics, such as liters of water consumed or tons of CO2 emitted, rebound effects by definition encompass social, economic, and behavioral impacts across different areas of society [114]. Realizing effective progress in characterizing AI’s rebound effects necessitates moving beyond the binary narrative towards more holistic assessment that incorporates a variety of complementary approaches. We discuss some of these approaches below.", "original_types": ["text", "header"], "id": 271}
{"type": "section", "content": "Discussion\n\nIn the context of climate change, artificial intelligence has emerged as a highly polarizing technology. On the one hand, it is championed as a driver of efficiency that could potentially invent new solutions to the climate crisis or reinvent old ones [5, 21, 99]; on the other hand, it is criticized for its steeply increasing resource demands and carbon footprint [68, 120, 134]. This debate overlooks a significant reality: we still do not have a comprehensive picture of AI’s current environmental impacts on everything from economic systems to individual behaviors. Consequently, the AI field risks simplifying the nuances that must be understood if AI is to be responsibly integrated into environmental policy and practice without exacerbating harms. A more accurate assessment of AI’s environmental outcomes would include a wider range of factors spanning data center operations, supply chains, hardware lifecycles, social behaviors, business incentives, policy commitments, and institutional practices.", "doc_id": "luccioni2025a", "page": 9, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Discussion\n\nIn the context of climate change, artificial intelligence has emerged as a highly polarizing technology. On the one hand, it is championed as a driver of efficiency that could potentially invent new solutions to the climate crisis or reinvent old ones [5, 21, 99]; on the other hand, it is criticized for its steeply increasing resource demands and carbon footprint [68, 120, 134]. This debate overlooks a significant reality: we still do not have a comprehensive picture of AI’s current environmental impacts on everything from economic systems to individual behaviors. Consequently, the AI field risks simplifying the nuances that must be understood if AI is to be responsibly integrated into environmental policy and practice without exacerbating harms. A more accurate assessment of AI’s environmental outcomes would include a wider range of factors spanning data center operations, supply chains, hardware lifecycles, social behaviors, business incentives, policy commitments, and institutional practices.", "original_types": ["text", "header"], "id": 272}
{"type": "section", "content": "5 Conclusion\n\nThis paper argues that the AI field needs to adopt a more detailed and nuanced approach to framing, articulating, and addressing AI’s environmental impacts in order to avoid unhelpful polarization. This requires including AI’s direct impacts—mineral supply chain studies, carbon emissions of training large-scale models, energy and water consumption, and e-waste from hardware—as well as mapping the ways AI innovations reshape economic structures and societal practices that, in turn, drive increased resource usage. Such a comprehensive perspective will empower researchers, policymakers, and industry stakeholders to devise strategies that prevent “tech-solutionism” from overshadowing the urgent need for systemic change. Greater transparency in reporting energy usage, more robust lifecycle assessment tools, and meaningful industry-wide enforceable standards are examples that would foster much-needed progress. Ultimately, what is at stake is clear: There is a scientific consensus that the dangers of climate change are extreme, and the effects are already unfolding globally. The need to limit global warming to below 1.5°C underscores the need for transformative change across sectors, and the technology sector is no exception. If AI is deployed without adequate consideration of its direct and indirect effects, it has the potential to deepen inequalities, accelerate resource depletion, and exacerbate the very climate problems it hopes to address. Conversely, if approached with rigorous assessment, transparent reporting, and supportive policy frameworks, AI could serve as a helpful tool in climate adaptation, environmental monitoring, and sustainable planning. Yet we cannot simply hope for the best outcome. The onus is on the AI industry to ensure technology does not contribute to the problem before producing any future solutions. This requires reckoning with AI’s actual impacts, both direct and indirect, measured comprehensively and contextualized socially, economically, and environmentally.", "doc_id": "luccioni2025a", "page": 10, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "5 Conclusion\n\nThis paper argues that the AI field needs to adopt a more detailed and nuanced approach to framing, articulating, and addressing AI’s environmental impacts in order to avoid unhelpful polarization. This requires including AI’s direct impacts—mineral supply chain studies, carbon emissions of training large-scale models, energy and water consumption, and e-waste from hardware—as well as mapping the ways AI innovations reshape economic structures and societal practices that, in turn, drive increased resource usage. Such a comprehensive perspective will empower researchers, policymakers, and industry stakeholders to devise strategies that prevent “tech-solutionism” from overshadowing the urgent need for systemic change. Greater transparency in reporting energy usage, more robust lifecycle assessment tools, and meaningful industry-wide enforceable standards are examples that would foster much-needed progress. Ultimately, what is at stake is clear: There is a scientific consensus that the dangers of climate change are extreme, and the effects are already unfolding globally. The need to limit global warming to below 1.5°C underscores the need for transformative change across sectors, and the technology sector is no exception. If AI is deployed without adequate consideration of its direct and indirect effects, it has the potential to deepen inequalities, accelerate resource depletion, and exacerbate the very climate problems it hopes to address. Conversely, if approached with rigorous assessment, transparent reporting, and supportive policy frameworks, AI could serve as a helpful tool in climate adaptation, environmental monitoring, and sustainable planning. Yet we cannot simply hope for the best outcome. The onus is on the AI industry to ensure technology does not contribute to the problem before producing any future solutions. This requires reckoning with AI’s actual impacts, both direct and indirect, measured comprehensively and contextualized socially, economically, and environmentally.", "original_types": ["text", "header"], "id": 273}
{"type": "section", "content": "References\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névéol, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. The elephant in the room: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n\n[3] Eshaan Agrawal and Christian Schroeder de Witt. 2025. Testing the Limits of the World’s Largest Control Task: Solar Geoengineering as a Deep Reinforcement Learning Problem. Geoengineering and Climate Change: Methods, Risks, and Governance (2025), 171–205.\n\n[4] Nur Ahmed, Muntasir Wahed, and Neil C. Thompson. 2023. The growing influence of industry in AI research. Science 379, 6635 (2023), 884–886. https://doi.org/10.1126/science.ade2420 arXiv:https://www.science.org/doi/pdf/10.1126/science.ade2420\n\n[5] Sam Altman. 2024. The Intelligence Age. https://ia.samaltman.com/\n\n[6] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud\n\n[7] Apple. 2024. Apple Intelligence is available today on iPhone, iPad, and Mac. https://www.apple.com/ca/newsroom/2024/10/apple-intelligence-is-available-today-on-iphone-ipad-and-mac/\n\n[8] Neha Arora, Theophile Cabannes, Sanjay Ganapathy Subramaniam, Yechen Li, Preston McAfee, Marc Nunkesser, Carolina Osorio, Andrew Tomkins, and Ivel Tsogtsuren. 2021. Quantifying the sustainability impact of Google Maps: A case study of Salt Lake City. Google Report.\n\n[9] Baidu. 2023. 2022 Baidu Sustainability Bond Allocation and Impact Report. https://esg.baidu.com/Uploads/File/2023/12/15/Baidu%E2%80%99s%20Sustainability%20Bond%20Report.20231215162734.pdf\n\n[10] Cornelis P Baldé, Vanessa Forti, Vanessa Gray, Ruediger Kuehr, Paul Stegmann, et al. 2024. The global e-waste monitor 2024. United Nations University (UNU), International Telecommunication Union (ITU) & International Solid Waste Association (ISWA), Bonn/Geneva/Vienna (2024), 1–109. https://ewastemonitor.info/\n\n[11] Lindsay Barbieri, Sonya Ahamed, and Sam Bliss. 2019. Farming within limits. Interactions 26, 5 (2019), 70–73.\n\n[12] Joshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. 2023. Gflownet foundations. The Journal of Machine Learning Research 24, 1 (2023), 10006–10060.\n\n[13] Mathias Binswanger. 2001. Technological progress and sustainable development: what about the rebound effect? Ecological economics 36, 1 (2001), 119–132.\n\n[14] Alberto Boretti and Lorenzo Rosa. 2019. Reassessing the projections of the world water development report. NPJ Clean Water 2, 1 (2019), 15.\n\n[15] Johannes Buhl and José Acosta. 2016. Work less, do less? Working time reductions and rebound effects. Sustainability Science 11 (2016), 261–276.", "doc_id": "luccioni2025a", "page": 11, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "References\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névéol, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. The elephant in the room: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n\n[3] Eshaan Agrawal and Christian Schroeder de Witt. 2025. Testing the Limits of the World’s Largest Control Task: Solar Geoengineering as a Deep Reinforcement Learning Problem. Geoengineering and Climate Change: Methods, Risks, and Governance (2025), 171–205.\n\n[4] Nur Ahmed, Muntasir Wahed, and Neil C. Thompson. 2023. The growing influence of industry in AI research. Science 379, 6635 (2023), 884–886. https://doi.org/10.1126/science.ade2420 arXiv:https://www.science.org/doi/pdf/10.1126/science.ade2420\n\n[5] Sam Altman. 2024. The Intelligence Age. https://ia.samaltman.com/\n\n[6] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud\n\n[7] Apple. 2024. Apple Intelligence is available today on iPhone, iPad, and Mac. https://www.apple.com/ca/newsroom/2024/10/apple-intelligence-is-available-today-on-iphone-ipad-and-mac/\n\n[8] Neha Arora, Theophile Cabannes, Sanjay Ganapathy Subramaniam, Yechen Li, Preston McAfee, Marc Nunkesser, Carolina Osorio, Andrew Tomkins, and Ivel Tsogtsuren. 2021. Quantifying the sustainability impact of Google Maps: A case study of Salt Lake City. Google Report.\n\n[9] Baidu. 2023. 2022 Baidu Sustainability Bond Allocation and Impact Report. https://esg.baidu.com/Uploads/File/2023/12/15/Baidu%E2%80%99s%20Sustainability%20Bond%20Report.20231215162734.pdf\n\n[10] Cornelis P Baldé, Vanessa Forti, Vanessa Gray, Ruediger Kuehr, Paul Stegmann, et al. 2024. The global e-waste monitor 2024. United Nations University (UNU), International Telecommunication Union (ITU) & International Solid Waste Association (ISWA), Bonn/Geneva/Vienna (2024), 1–109. https://ewastemonitor.info/\n\n[11] Lindsay Barbieri, Sonya Ahamed, and Sam Bliss. 2019. Farming within limits. Interactions 26, 5 (2019), 70–73.\n\n[12] Joshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. 2023. Gflownet foundations. The Journal of Machine Learning Research 24, 1 (2023), 10006–10060.\n\n[13] Mathias Binswanger. 2001. Technological progress and sustainable development: what about the rebound effect? Ecological economics 36, 1 (2001), 119–132.\n\n[14] Alberto Boretti and Lorenzo Rosa. 2019. Reassessing the projections of the world water development report. NPJ Clean Water 2, 1 (2019), 15.\n\n[15] Johannes Buhl and José Acosta. 2016. Work less, do less? Working time reductions and rebound effects. Sustainability Science 11 (2016), 261–276.", "original_types": ["text", "header"], "id": 274}
{"type": "section", "content": "[16] Christopher Burns, Barbara Bollard, and Ajit Narayanan. 2022. Machine-learning for mapping and monitoring shallow coral reef habitats. Remote Sensing 14, 11 (2022), 2666.\n\n[17] Jessica Campbell, Irene M Herremans, and Anne Kleffner. 2018. Barriers to achieving additionality in carbon offsets: a regulatory risk perspective. Journal of Environmental Planning and Management 61, 14 (2018), 2570–2589.\n\n[18] Daniel Castro. 2024. Rethinking Concerns About AI’s Energy Use.\n\n[19] Ye Chen, Michael Kapralov, John Canny, and Dmitry Pavlov. 2009. Factor modeling for advertisement targeting. Advances in neural information processing systems 22 (2009).\n\n[20] Jin-A Choi and Kiho Lim. 2020. Identifying machine learning techniques for classification of target advertising. ICT Express 6, 3 (2020), 175–180.\n\n[21] Peter Clutton-Brock, David Rolnick, Priya L Donti, and Lynn Kaack. 2021. Climate change and AI. recommendations for government action. Technical Report. GPAI, Climate Change AI, Centre for AI & Climate.\n\n[22] Vlad C Coroamă, Pernilla Bergmark, Mattias Höjer, and Jens Malmadin. 2020. A methodology for assessing the environmental effects induced by ict services: Part i: Single services. In Proceedings of the 7th International Conference on ICT for Sustainability. 36–45.\n\n[23] Vlad C Coroamă and Daniel Pargman. 2020. Skill rebound: On an unintended effect of digitalization. In Proceedings of the 7th International Conference on ICT for Sustainability. 213–219.\n\n[24] Kate Crawford. 2021. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.\n\n[25] Casey Crownhart. 2024. Why Microsoft made a deal to help restart Three Mile Island . https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/\n\n[26] Rosa Maria Dangelico and Daniele Vocalelli. 2017. “Green Marketing”: An analysis of definitions, strategy steps, and tools through a systematic review of the literature. Journal of Cleaner production 165 (2017), 1263–1279.\n\n[27] Souvik Datta and Massimo Filippini. 2016. Analysing the impact of ENERGY STAR rebate policies in the US. Energy Efficiency 9 (2016), 677–698.\n\n[28] Peter Dauvergne. 2022. Is artificial intelligence greening global supply chains? Exposing the political economy of environmental costs. Review of International Political Economy 29, 3 (2022), 696–718.", "doc_id": "luccioni2025a", "page": 11, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "[16] Christopher Burns, Barbara Bollard, and Ajit Narayanan. 2022. Machine-learning for mapping and monitoring shallow coral reef habitats. Remote Sensing 14, 11 (2022), 2666.\n\n[17] Jessica Campbell, Irene M Herremans, and Anne Kleffner. 2018. Barriers to achieving additionality in carbon offsets: a regulatory risk perspective. Journal of Environmental Planning and Management 61, 14 (2018), 2570–2589.\n\n[18] Daniel Castro. 2024. Rethinking Concerns About AI’s Energy Use.\n\n[19] Ye Chen, Michael Kapralov, John Canny, and Dmitry Pavlov. 2009. Factor modeling for advertisement targeting. Advances in neural information processing systems 22 (2009).\n\n[20] Jin-A Choi and Kiho Lim. 2020. Identifying machine learning techniques for classification of target advertising. ICT Express 6, 3 (2020), 175–180.\n\n[21] Peter Clutton-Brock, David Rolnick, Priya L Donti, and Lynn Kaack. 2021. Climate change and AI. recommendations for government action. Technical Report. GPAI, Climate Change AI, Centre for AI & Climate.\n\n[22] Vlad C Coroamă, Pernilla Bergmark, Mattias Höjer, and Jens Malmadin. 2020. A methodology for assessing the environmental effects induced by ict services: Part i: Single services. In Proceedings of the 7th International Conference on ICT for Sustainability. 36–45.\n\n[23] Vlad C Coroamă and Daniel Pargman. 2020. Skill rebound: On an unintended effect of digitalization. In Proceedings of the 7th International Conference on ICT for Sustainability. 213–219.\n\n[24] Kate Crawford. 2021. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. Yale University Press.\n\n[25] Casey Crownhart. 2024. Why Microsoft made a deal to help restart Three Mile Island . https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/\n\n[26] Rosa Maria Dangelico and Daniele Vocalelli. 2017. “Green Marketing”: An analysis of definitions, strategy steps, and tools through a systematic review of the literature. Journal of Cleaner production 165 (2017), 1263–1279.\n\n[27] Souvik Datta and Massimo Filippini. 2016. Analysing the impact of ENERGY STAR rebate policies in the US. Energy Efficiency 9 (2016), 677–698.\n\n[28] Peter Dauvergne. 2022. Is artificial intelligence greening global supply chains? Exposing the political economy of environmental costs. Review of International Political Economy 29, 3 (2022), 696–718.", "original_types": ["text"], "id": 275}
{"type": "section", "content": "[29] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948\n\n[30] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency. 1877–1894.", "doc_id": "luccioni2025a", "page": 11, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "[29] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948\n\n[30] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency. 1877–1894.", "original_types": ["text"], "id": 276}
{"type": "section", "content": "[31] Ewa Dominowska and Vanja Josifovski. 2008. First workshop on targeting and ranking for online advertising. In Proceedings of the 17th international conference on World Wide Web. 1269–1270.\n\n[32] Dealva Jade Dowd-Hinkle. 2012. Kindle vs. Printed book an environmental analysis. Rochester Institute of Technology.\n\n[33] Aurélien Dumont, Beatriz Mayor, and Elena López-Gunn. 2013. Is the rebound effect or Jevons paradox a useful concept for better management of water resources? Insights from the irrigation modernisation process in Spain. Aquatic proedia 1 (2013), 64–76.\n\n[34] Eclectic Light. 2024. Check Writing Tools using AIR. https://eclecticlight.co/2024/10/29/check-writing-tools-using-air/\n\n[35] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 (2023).\n\n[36] Wolfgang Ertel and Christopher Bonenberger. 2024. Rebound Effects Caused by Artificial Intelligence and Automation in Private Life and Industry. (2024).\n\n[37] Richard Evans and Jim Gao. 2016. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%. https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/\n\n[38] R. Fischer, M. Jakobs, S. Mücke, and K. Morik. 2022. A Unified Framework for Assessing Energy Efficiency of Machine Learning. In Proceedings of the ECML Workshop on Data Science for Social Good.\n\n[39] Jaume Freire-González and Ignasi Puig-Ventosa. 2015. Energy efficiency policies and the Jevons paradox. International Journal of Energy Economics and Policy 5, 1 (2015), 69–79.\n\n[40] Mario Giampietro and Kozo Mayumi. 2018. Unraveling the complexity of the Jevons Paradox: The link between innovation, efficiency, and sustainability. Frontiers in Energy Research 6 (2018), 349753.\n\n[41] Bastien Girod, Peter de Haan, and Roland W Scholz. 2011. Consumption-as-usual instead of ceteris paribus assumption for demand: Integration of potential rebound effects into LCA. The International Journal of Life Cycle Assessment 16 (2011), 3–11.\n\n[42] Google. 2024. Google Environmental Report 2024. https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf", "doc_id": "luccioni2025a", "page": 11, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "[31] Ewa Dominowska and Vanja Josifovski. 2008. First workshop on targeting and ranking for online advertising. In Proceedings of the 17th international conference on World Wide Web. 1269–1270.\n\n[32] Dealva Jade Dowd-Hinkle. 2012. Kindle vs. Printed book an environmental analysis. Rochester Institute of Technology.\n\n[33] Aurélien Dumont, Beatriz Mayor, and Elena López-Gunn. 2013. Is the rebound effect or Jevons paradox a useful concept for better management of water resources? Insights from the irrigation modernisation process in Spain. Aquatic proedia 1 (2013), 64–76.\n\n[34] Eclectic Light. 2024. Check Writing Tools using AIR. https://eclecticlight.co/2024/10/29/check-writing-tools-using-air/\n\n[35] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 (2023).\n\n[36] Wolfgang Ertel and Christopher Bonenberger. 2024. Rebound Effects Caused by Artificial Intelligence and Automation in Private Life and Industry. (2024).\n\n[37] Richard Evans and Jim Gao. 2016. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%. https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/\n\n[38] R. Fischer, M. Jakobs, S. Mücke, and K. Morik. 2022. A Unified Framework for Assessing Energy Efficiency of Machine Learning. In Proceedings of the ECML Workshop on Data Science for Social Good.\n\n[39] Jaume Freire-González and Ignasi Puig-Ventosa. 2015. Energy efficiency policies and the Jevons paradox. International Journal of Energy Economics and Policy 5, 1 (2015), 69–79.\n\n[40] Mario Giampietro and Kozo Mayumi. 2018. Unraveling the complexity of the Jevons Paradox: The link between innovation, efficiency, and sustainability. Frontiers in Energy Research 6 (2018), 349753.\n\n[41] Bastien Girod, Peter de Haan, and Roland W Scholz. 2011. Consumption-as-usual instead of ceteris paribus assumption for demand: Integration of potential rebound effects into LCA. The International Journal of Life Cycle Assessment 16 (2011), 3–11.\n\n[42] Google. 2024. Google Environmental Report 2024. https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf", "original_types": ["text"], "id": 277}
{"type": "section", "content": "[43] Cédric Gossart. 2015. Rebound effects and ICT: a review of the literature. ICT innovations for sustainability (2015), 435–448.\n\n[44] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destruc...\n\n[45] Gianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell, Scott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of United States Data Centers in the Artificial Intelligence Era. arXiv preprint arXiv:2411.09786 (2024).\n\n[46] Robert Hahn and Kenneth Richards. 2013. Understanding the effectiveness of environmental offset policies. Journal of Regulatory Economics 44 (2013), 103–119.\n\n[47] Patrick Hartmann, Aitor Marcos, Juana Castro, and Vanessa Apaolaza. 2023. Perspectives: Advertising and climate change–Part of the problem or part of the solution? International Journal of Advertising 42, 2 (2023), 430–457.\n\n[48] Chris T Hendrickson, Lester B Lave, and H Scott Matthews. 2010. Environmental life cycle assessment of goods and services: an input-output approach. Routledge.\n\n[49] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 (2017).\n\n[50] Lorenz M Hilty, Andreas Köhler, Fabian Von Schéele, Rainer Zah, and Thomas Ruddy. 2006. Rebound effects of progress in information technology. Poiesis & Praxis 4, 1 (2006), 19–38.\n\n[51] Torsten Hoefler, Ariel Hendel, and Duncan Roweth. 2022. The convergence of hyperscale data center and high-performance computing networks. Computer 55, 7 (2022), 29–37.\n\n[52] Mél Hogan. 2015. Data flows and water woes: The Utah Data Center. Big Data & Society 2, 2 (2015), 2053951715592429. https://doi.org/10.1177/2053951715592429 arXiv:https://doi.org/10.1177/2053951715592429\n\n[53] Nathaniel C Horner, Arman Shehabi, and Inês L Azevedo. 2016. Known unknowns: indirect energy effects of information and communication technology. Environmental Research Letters 11, 10 (2016), 103001.\n\n[54] Laura Hostettler Macias, Emmanuel Ravalet, and Patrick Rérat. 2022. Potential rebound effects of teleworking on residential and daily mobility. Geography Compass 16, 9 (2022), e12657.\n\n[55] Kerstin Hötte, Melline Somers, and Angelos Theodorakopoulos. 2023. Technology and jobs: A systematic literature review. Technological Forecasting and Social Change 194 (2023), 122750.\n\n[56] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\n\n[57] International Energy Authority. 2024. World Energy Outlook 2024. https://www.iea.org/reports/world-energy-outlook-2024", "doc_id": "luccioni2025a", "page": 12, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "[43] Cédric Gossart. 2015. Rebound effects and ICT: a review of the literature. ICT innovations for sustainability (2015), 435–448.\n\n[44] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destruc...\n\n[45] Gianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell, Scott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of United States Data Centers in the Artificial Intelligence Era. arXiv preprint arXiv:2411.09786 (2024).\n\n[46] Robert Hahn and Kenneth Richards. 2013. Understanding the effectiveness of environmental offset policies. Journal of Regulatory Economics 44 (2013), 103–119.\n\n[47] Patrick Hartmann, Aitor Marcos, Juana Castro, and Vanessa Apaolaza. 2023. Perspectives: Advertising and climate change–Part of the problem or part of the solution? International Journal of Advertising 42, 2 (2023), 430–457.\n\n[48] Chris T Hendrickson, Lester B Lave, and H Scott Matthews. 2010. Environmental life cycle assessment of goods and services: an input-output approach. Routledge.\n\n[49] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 (2017).\n\n[50] Lorenz M Hilty, Andreas Köhler, Fabian Von Schéele, Rainer Zah, and Thomas Ruddy. 2006. Rebound effects of progress in information technology. Poiesis & Praxis 4, 1 (2006), 19–38.\n\n[51] Torsten Hoefler, Ariel Hendel, and Duncan Roweth. 2022. The convergence of hyperscale data center and high-performance computing networks. Computer 55, 7 (2022), 29–37.\n\n[52] Mél Hogan. 2015. Data flows and water woes: The Utah Data Center. Big Data & Society 2, 2 (2015), 2053951715592429. https://doi.org/10.1177/2053951715592429 arXiv:https://doi.org/10.1177/2053951715592429\n\n[53] Nathaniel C Horner, Arman Shehabi, and Inês L Azevedo. 2016. Known unknowns: indirect energy effects of information and communication technology. Environmental Research Letters 11, 10 (2016), 103001.\n\n[54] Laura Hostettler Macias, Emmanuel Ravalet, and Patrick Rérat. 2022. Potential rebound effects of teleworking on residential and daily mobility. Geography Compass 16, 9 (2022), e12657.\n\n[55] Kerstin Hötte, Melline Somers, and Angelos Theodorakopoulos. 2023. Technology and jobs: A systematic literature review. Technological Forecasting and Social Change 194 (2023), 122750.\n\n[56] International Energy Authority. 2023. Data Centres and Data Transmission Networks. https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\n\n[57] International Energy Authority. 2024. World Energy Outlook 2024. https://www.iea.org/reports/world-energy-outlook-2024", "original_types": ["text"], "id": 278}
{"type": "section", "content": "[58] René Itten, Roland Hischier, Anders SG Andrae, Jan CT Bieser, Livia Cabernard, Annemarie Falke, Hugues Ferreboeuf, Lorenz M Hilty, Regula L Keller, Etienne Lees-Perasso, et al. 2020. Digital transformation—life cycle assessment of digital services, multifunctional devices and cloud computing. The International Journal of Life Cycle Assessment 25 (2020), 2093–2098.\n\n[59] Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube, Michael Keiblunger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles Goddard, et al. 2024. INTELLECT-1 Technical Report. arXiv preprint arXiv:2412.01152 (2024).\n\n[60] W Stanley Jevons. 1866. The coal question. In The Economics of Population. Routledge, 193–204.\n\n[61] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. 2019. Dissecting the graphcore ipu architecture via microbenchmarking. arXiv preprint arXiv:1912.03413 (2019).\n\n[62] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture. 1–12.\n\n[63] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n\n[64] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n\n[65] Jonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls assessing the Internet’s energy and carbon impacts. Joule 5, 7 (2021), 1625–1628. https://doi.org/10.1016/j.joule.2021.05.007\n\n[66] Adam Krechowicz, Maria Krechowicz, and Katarzyna Poczeta. 2022. Machine learning approaches to predict electricity production from renewable energy sources. Energies 15, 23 (2022), 9146.\n\n[67] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).", "doc_id": "luccioni2025a", "page": 12, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "[58] René Itten, Roland Hischier, Anders SG Andrae, Jan CT Bieser, Livia Cabernard, Annemarie Falke, Hugues Ferreboeuf, Lorenz M Hilty, Regula L Keller, Etienne Lees-Perasso, et al. 2020. Digital transformation—life cycle assessment of digital services, multifunctional devices and cloud computing. The International Journal of Life Cycle Assessment 25 (2020), 2093–2098.\n\n[59] Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube, Michael Keiblunger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles Goddard, et al. 2024. INTELLECT-1 Technical Report. arXiv preprint arXiv:2412.01152 (2024).\n\n[60] W Stanley Jevons. 1866. The coal question. In The Economics of Population. Routledge, 193–204.\n\n[61] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. 2019. Dissecting the graphcore ipu architecture via microbenchmarking. arXiv preprint arXiv:1912.03413 (2019).\n\n[62] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture. 1–12.\n\n[63] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n\n[64] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n\n[65] Jonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls assessing the Internet’s energy and carbon impacts. Joule 5, 7 (2021), 1625–1628. https://doi.org/10.1016/j.joule.2021.05.007\n\n[66] Adam Krechowicz, Maria Krechowicz, and Katarzyna Poczeta. 2022. Machine learning approaches to predict electricity production from renewable energy sources. Energies 15, 23 (2022), 9146.\n\n[67] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).", "original_types": ["text"], "id": 279}
{"type": "section", "content": "Hanna Reimers, Anke Jacksohn, Dennis Appenfeller, Wassili Lasarov, Alexandra Hüttel, Katrin Rehdanz, Ingo Balderjahn, and Stefan Hoffmann. 2021. Indirect rebound effects on the consumer level: A state-of-the-art literature review. Cleaner and Responsible Consumption 3 (2021), 100032.\n\nAnnika Rieger. 2021. Does ICT result in dematerialization? The case of Europe, 2005-2017. Environmental Sociology 7, 1 (2021), 64–75.\n\nBora Ristic, Kaveh Madani, and Zen Makuch. 2015. The water footprint of data centers. Sustainability 7, 8 (2015), 11260–11284.\n\nMiriam Börjesson Rivera, Cecilia Håkansson, Åsa Svenfelt, and Göran Finnveden. 2014. Including second order effects in environmental assessments of ICT. Environmental Modelling & Software 56 (2014), 105–115.\n\nBrett H Robinson. 2009. E-waste: an assessment of global production and environmental impacts. Science of the total environment 408, 2 (2009), 183–191.\n\nDavid Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\nTilman Santarius, Johanna Pohl, and Steffen Lange. 2020. Digitalization and the Decoupling Debate: Can ICT Help to Reduce Environmental Impacts While the Economy Keeps Growing? Sustainability 12, 18 (2020). https://doi.org/10.3390/su12187496\n\nHarry Saunders. 2009. Theoretical foundations of the rebound effect. International handbook on the economics of energy (2009).\n\nBerend J Schuit, Joannes D Maasakkers, Pieter Bijl, Gourav Mahapatra, Anne-Wil Van den Berg, Sudhanshu Pandey, Alba Lorente, Tobias Borsdorff, Sander Houweling, Daniel J Varon, et al. 2023. Automated detection and monitoring of methane super-emitters using satellite data. Atmospheric Chemistry and Physics Discussions 2023 (2023), 1–47.\n\nJoy Scrogum. 2009. Books vs. eBooks – A life cycle comparison. https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/\n\nNeil Selwyn. 2024. Digital degrowth: Toward radically sustainable education technology. Learning, Media and Technology 49, 2 (2024), 186–199.\n\nAgam Shah. 2024. Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study. https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/\n\nMadhu Sharma, Kartik Arunachalam, and Dharani Sharma. 2015. Analyzing the data center efficiency by using PUE to make data centers more energy efficient by reducing the electrical consumption and exploring new strategies. Procedia Computer Science 48 (2015), 142–148.\n\nNavin Sharma, Pranshu Sharma, David Irwin, and Prashant Shenoy. 2011. Predicting solar generation from weather forecasts using machine learning. In 2011 IEEE international conference on smart grid communications (SmartGridComm). IEEE, 528–533.", "doc_id": "luccioni2025a", "page": 13, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Hanna Reimers, Anke Jacksohn, Dennis Appenfeller, Wassili Lasarov, Alexandra Hüttel, Katrin Rehdanz, Ingo Balderjahn, and Stefan Hoffmann. 2021. Indirect rebound effects on the consumer level: A state-of-the-art literature review. Cleaner and Responsible Consumption 3 (2021), 100032.\n\nAnnika Rieger. 2021. Does ICT result in dematerialization? The case of Europe, 2005-2017. Environmental Sociology 7, 1 (2021), 64–75.\n\nBora Ristic, Kaveh Madani, and Zen Makuch. 2015. The water footprint of data centers. Sustainability 7, 8 (2015), 11260–11284.\n\nMiriam Börjesson Rivera, Cecilia Håkansson, Åsa Svenfelt, and Göran Finnveden. 2014. Including second order effects in environmental assessments of ICT. Environmental Modelling & Software 56 (2014), 105–115.\n\nBrett H Robinson. 2009. E-waste: an assessment of global production and environmental impacts. Science of the total environment 408, 2 (2009), 183–191.\n\nDavid Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\nTilman Santarius, Johanna Pohl, and Steffen Lange. 2020. Digitalization and the Decoupling Debate: Can ICT Help to Reduce Environmental Impacts While the Economy Keeps Growing? Sustainability 12, 18 (2020). https://doi.org/10.3390/su12187496\n\nHarry Saunders. 2009. Theoretical foundations of the rebound effect. International handbook on the economics of energy (2009).\n\nBerend J Schuit, Joannes D Maasakkers, Pieter Bijl, Gourav Mahapatra, Anne-Wil Van den Berg, Sudhanshu Pandey, Alba Lorente, Tobias Borsdorff, Sander Houweling, Daniel J Varon, et al. 2023. Automated detection and monitoring of methane super-emitters using satellite data. Atmospheric Chemistry and Physics Discussions 2023 (2023), 1–47.\n\nJoy Scrogum. 2009. Books vs. eBooks – A life cycle comparison. https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/\n\nNeil Selwyn. 2024. Digital degrowth: Toward radically sustainable education technology. Learning, Media and Technology 49, 2 (2024), 186–199.\n\nAgam Shah. 2024. Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study. https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/\n\nMadhu Sharma, Kartik Arunachalam, and Dharani Sharma. 2015. Analyzing the data center efficiency by using PUE to make data centers more energy efficient by reducing the electrical consumption and exploring new strategies. Procedia Computer Science 48 (2015), 142–148.\n\nNavin Sharma, Pranshu Sharma, David Irwin, and Prashant Shenoy. 2011. Predicting solar generation from weather forecasts using machine learning. In 2011 IEEE international conference on smart grid communications (SmartGridComm). IEEE, 528–533.", "original_types": ["text"], "id": 280}
{"type": "section", "content": "Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024. 2024 United States Data Center Energy Usage Report. https://eta-publications.lbl.gov/sites/default/files/2024-12/lbnl-2024-united-states-data-center-energy-usage-report.pdf\n\nFranyell Silfa, Jose Maria Arnau, and Antonio González. 2022. E-BATCH: Energy-efficient and high-throughput RNN batching. ACM Transactions on Architecture and Code Optimization (TACO) 19, 1 (2022), 1–23.\n\nAnirbid Sircar, Kriti Yadav, Kamakshi Rayavarapu, Namrata Bist, and Hemangi Oza. 2021. Application of machine learning and artificial intelligence in oil and gas industry. Petroleum Research 6, 4 (2021), 379–391.\n\nSteve Sorrell. 2009. Jevons’ Paradox revisited: The evidence for backfire from improved energy efficiency. Energy policy 37, 4 (2009), 1456–1469.\n\nSteve Sorrell et al. 2007. The Rebound Effect: an assessment of the evidence for economy-wide energy savings from improved energy efficiency.\n\nSteve Sorrell and John Dimitropoulos. 2008. The rebound effect: Microeconomic definitions, limitations and extensions. Ecological Economics 65, 3 (2008), 636–649.\n\nBenjamin K Sovacool, Sarah E Ryan, Paul C Stern, Katy Janda, Gene Rochlin, Daniel Spreng, Martin J Pasqualetti, Harold Wilhite, and Loren Lutzenhiser. 2015. Integrating social science in energy research. Energy Research & Social Science 6 (2015), 95–99.\n\nStatista. 2023. Supply and demand for multitenant data center (MTDC) square footage worldwide from 2013 to 2023. https://www.statista.com/statistics/1008058/multitenant-data-center-square-footage-supply-and-demand-worldwide/\n\nStatista. 2024. Market value of artificial intelligence (AI) in marketing worldwide from 2020 to 2028. https://www.statista.com/statistics/1293758/ai-marketing-revenue-worldwide/\n\nDavid I Stern. 2020. How large is the economy-wide rebound effect? Energy Policy 147 (2020), 111870.\n\nMaddie Stone. 2020. Microsoft’s ambitious climate goal forgets about its oil contracts. https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/\n\nMaddie Stone. 2024. Microsoft employees spent years fighting the tech giant’s oil ties. Now, they’re speaking out. https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Anna Korhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 3645–3650. https://doi.org/10.18653/v1/P19-1355\n\nRichard Sutton. 2019. The bitter lesson. Incomplete Ideas (blog) 13, 1 (2019), 38.", "doc_id": "luccioni2025a", "page": 13, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024. 2024 United States Data Center Energy Usage Report. https://eta-publications.lbl.gov/sites/default/files/2024-12/lbnl-2024-united-states-data-center-energy-usage-report.pdf\n\nFranyell Silfa, Jose Maria Arnau, and Antonio González. 2022. E-BATCH: Energy-efficient and high-throughput RNN batching. ACM Transactions on Architecture and Code Optimization (TACO) 19, 1 (2022), 1–23.\n\nAnirbid Sircar, Kriti Yadav, Kamakshi Rayavarapu, Namrata Bist, and Hemangi Oza. 2021. Application of machine learning and artificial intelligence in oil and gas industry. Petroleum Research 6, 4 (2021), 379–391.\n\nSteve Sorrell. 2009. Jevons’ Paradox revisited: The evidence for backfire from improved energy efficiency. Energy policy 37, 4 (2009), 1456–1469.\n\nSteve Sorrell et al. 2007. The Rebound Effect: an assessment of the evidence for economy-wide energy savings from improved energy efficiency.\n\nSteve Sorrell and John Dimitropoulos. 2008. The rebound effect: Microeconomic definitions, limitations and extensions. Ecological Economics 65, 3 (2008), 636–649.\n\nBenjamin K Sovacool, Sarah E Ryan, Paul C Stern, Katy Janda, Gene Rochlin, Daniel Spreng, Martin J Pasqualetti, Harold Wilhite, and Loren Lutzenhiser. 2015. Integrating social science in energy research. Energy Research & Social Science 6 (2015), 95–99.\n\nStatista. 2023. Supply and demand for multitenant data center (MTDC) square footage worldwide from 2013 to 2023. https://www.statista.com/statistics/1008058/multitenant-data-center-square-footage-supply-and-demand-worldwide/\n\nStatista. 2024. Market value of artificial intelligence (AI) in marketing worldwide from 2020 to 2028. https://www.statista.com/statistics/1293758/ai-marketing-revenue-worldwide/\n\nDavid I Stern. 2020. How large is the economy-wide rebound effect? Energy Policy 147 (2020), 111870.\n\nMaddie Stone. 2020. Microsoft’s ambitious climate goal forgets about its oil contracts. https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/\n\nMaddie Stone. 2024. Microsoft employees spent years fighting the tech giant’s oil ties. Now, they’re speaking out. https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Anna Korhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 3645–3650. https://doi.org/10.18653/v1/P19-1355\n\nRichard Sutton. 2019. The bitter lesson. Incomplete Ideas (blog) 13, 1 (2019), 38.", "original_types": ["text"], "id": 281}
{"type": "section", "content": "Adrian Tantau and Elena Niculescu. 2022. The role of Power Purchase Agreements for the promotion of green energy and the transition to a zero carbon economy. In Proceedings of the International Conference on Business Excellence, Vol. 16. 1237–1245.\n\nZeeshan Tariq, Murtada Saleh Aljawad, Amjed Hasan, Mobeen Murtaza, Emad Mohammed, Ammar El-Husseiny, Sulaiman A Alarifi, Mohamed Mahmoud, and Abdulazeez Abdulraheem. 2021. A systematic review of data science and machine learning applications to the oil and gas industry. Journal of Petroleum Exploration and Production Technology (2021), 1–36.\n\nMichael Terrell. 2024. New nuclear clean energy agreement with Kairos Power. https://blog.google/outreach-initiatives/sustainability/google-kairos-power-nuclear-energy-agreement/\n\nNeil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. Deep Learning’s Diminishing Returns: The Cost of Improvement is Becoming Unsustainable. IEEE Spectrum 58, 10 (2021), 50–55. https://doi.org/10.1109/MSPEC.2021.9563954\n\nJW Thomson. 1972. Methods of traffic limitation in urban areas. Technical Report. 106–112 pages.\n\nSunita Tiwari and Pratibha Singh. 2011. Environmental impacts of e-commerce. In International conference on environment Science and engineering, Vol. 8. 202–207.\n\nBill Tomlinson, Rebecca W Black, Donald J Patterson, and Andrew W Torrance. 2024. The carbon emissions of writing and illustrating are lower for AI than for humans. Scientific Reports 14, 1 (2024), 3732.\n\nAlexandra Tremayne-Pengelly. 2024. Amid the A.I. Boom, These States Have Become Data Center Hubs. https://observer.com/2024/12/ai-demand-where-data-centers-using-most-energy/\n\nUN General Assembly. 2015. Transforming our world : the 2030 Agenda for Sustainable Development. https://www.refworld.org/legal/resolution/unga/2015/en/111816\n\nLaszlo Varro and George Kamiya. 5. 5 ways Big Tech could have big impacts on clean energy transitions. (5).\n\nRicardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature communications 11, 1 (2020), 1–10.\n\nPetro Vorotyntsev, Yuri Gordienko, Oleg Alienin, Oleksandr Rokovyi, and Sergii Stirenko. 2021. Satellite image segmentation using deep learning for deforestation detection. In 2021 IEEE 3rd ukraine conference on electrical and computer engineering (UKRCON). IEEE, 226–231.\n\nPeng Wang, Ling-Yu Zhang, Asaf Tzachor, and Wei-Qiang Chen. 2024. E-waste challenges of generative artificial intelligence. Nature Computational Science (2024), 1–6.\n\nMartina Willenbacher, Torsten Hornauer, and Volker Wohlgemuth. 2021. Rebound effects in methods of artificial intelligence. In Environmental Informatics. Springer, 73–85.\n\nEric Williams. 2011. Environmental effects of information and communications technologies. nature 479, 7373 (2011), 354–358.", "doc_id": "luccioni2025a", "page": 13, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "Adrian Tantau and Elena Niculescu. 2022. The role of Power Purchase Agreements for the promotion of green energy and the transition to a zero carbon economy. In Proceedings of the International Conference on Business Excellence, Vol. 16. 1237–1245.\n\nZeeshan Tariq, Murtada Saleh Aljawad, Amjed Hasan, Mobeen Murtaza, Emad Mohammed, Ammar El-Husseiny, Sulaiman A Alarifi, Mohamed Mahmoud, and Abdulazeez Abdulraheem. 2021. A systematic review of data science and machine learning applications to the oil and gas industry. Journal of Petroleum Exploration and Production Technology (2021), 1–36.\n\nMichael Terrell. 2024. New nuclear clean energy agreement with Kairos Power. https://blog.google/outreach-initiatives/sustainability/google-kairos-power-nuclear-energy-agreement/\n\nNeil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. 2021. Deep Learning’s Diminishing Returns: The Cost of Improvement is Becoming Unsustainable. IEEE Spectrum 58, 10 (2021), 50–55. https://doi.org/10.1109/MSPEC.2021.9563954\n\nJW Thomson. 1972. Methods of traffic limitation in urban areas. Technical Report. 106–112 pages.\n\nSunita Tiwari and Pratibha Singh. 2011. Environmental impacts of e-commerce. In International conference on environment Science and engineering, Vol. 8. 202–207.\n\nBill Tomlinson, Rebecca W Black, Donald J Patterson, and Andrew W Torrance. 2024. The carbon emissions of writing and illustrating are lower for AI than for humans. Scientific Reports 14, 1 (2024), 3732.\n\nAlexandra Tremayne-Pengelly. 2024. Amid the A.I. Boom, These States Have Become Data Center Hubs. https://observer.com/2024/12/ai-demand-where-data-centers-using-most-energy/\n\nUN General Assembly. 2015. Transforming our world : the 2030 Agenda for Sustainable Development. https://www.refworld.org/legal/resolution/unga/2015/en/111816\n\nLaszlo Varro and George Kamiya. 5. 5 ways Big Tech could have big impacts on clean energy transitions. (5).\n\nRicardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature communications 11, 1 (2020), 1–10.\n\nPetro Vorotyntsev, Yuri Gordienko, Oleg Alienin, Oleksandr Rokovyi, and Sergii Stirenko. 2021. Satellite image segmentation using deep learning for deforestation detection. In 2021 IEEE 3rd ukraine conference on electrical and computer engineering (UKRCON). IEEE, 226–231.\n\nPeng Wang, Ling-Yu Zhang, Asaf Tzachor, and Wei-Qiang Chen. 2024. E-waste challenges of generative artificial intelligence. Nature Computational Science (2024), 1–6.\n\nMartina Willenbacher, Torsten Hornauer, and Volker Wohlgemuth. 2021. Rebound effects in methods of artificial intelligence. In Environmental Informatics. Springer, 73–85.\n\nEric Williams. 2011. Environmental effects of information and communications technologies. nature 479, 7373 (2011), 354–358.", "original_types": ["text"], "id": 282}
{"type": "section", "content": "John D Wilson and Zach Zimmerman. 2023. The Era of Flat Power Demand is Over. Grid Strategies (2023).\n\nRichard T Woodward, David A Newburn, and Mariano Mezzatesta. 2016. Additionality and reverse crowding out for pollution offsets in water quality trading. Ecological Economics 128 (2016), 224–231.\n\nXiao Xue, Shufang Wang, and Baoyun Lu. 2015. Computational experiment approach to controlled evolution of procurement pattern in cluster supply chain. Sustainability 7, 2 (2015), 1516–1541.\n\nClaudio Zeni, Robert Pinsler, Daniel Zügner, Andrew Fowler, Xiang Fu Matthew Horton, Zilong Wang, Aliaksandra Shysheya, Jonathan Crabbé, Shoko Ueda, Roberto Sordillo, Lixin Sun, Jake Smith, Bichlien Nguyen, Hannes Schulz, Sarah Lewis, Chin-Wei Huang, Ziheng Lu, Yichi Zhou, Han Yang, Hongxia Hao, Jielan Li, Chunlei Yang, Wenjie Li, Ryota Tomioka, and Tian Xie. 2025. A generative model for inorganic materials design. Nature (2025). https://doi.org/10.1038/s41586-025-08628-5\n\nBanghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao. 2024. Towards Optimal Caching and Model Selection for Large Model Inference. Advances in Neural Information Processing Systems 36 (2024).", "doc_id": "luccioni2025a", "page": 13, "url": "https://arxiv.org/pdf/2501.16548", "embedded_text": "John D Wilson and Zach Zimmerman. 2023. The Era of Flat Power Demand is Over. Grid Strategies (2023).\n\nRichard T Woodward, David A Newburn, and Mariano Mezzatesta. 2016. Additionality and reverse crowding out for pollution offsets in water quality trading. Ecological Economics 128 (2016), 224–231.\n\nXiao Xue, Shufang Wang, and Baoyun Lu. 2015. Computational experiment approach to controlled evolution of procurement pattern in cluster supply chain. Sustainability 7, 2 (2015), 1516–1541.\n\nClaudio Zeni, Robert Pinsler, Daniel Zügner, Andrew Fowler, Xiang Fu Matthew Horton, Zilong Wang, Aliaksandra Shysheya, Jonathan Crabbé, Shoko Ueda, Roberto Sordillo, Lixin Sun, Jake Smith, Bichlien Nguyen, Hannes Schulz, Sarah Lewis, Chin-Wei Huang, Ziheng Lu, Yichi Zhou, Han Yang, Hongxia Hao, Jielan Li, Chunlei Yang, Wenjie Li, Ryota Tomioka, and Tian Xie. 2025. A generative model for inorganic materials design. Nature (2025). https://doi.org/10.1038/s41586-025-08628-5\n\nBanghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao. 2024. Towards Optimal Caching and Model Selection for Large Model Inference. Advances in Neural Information Processing Systems 36 (2024).", "original_types": ["text"], "id": 283}
{"type": "section", "content": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\n\nALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada\nALEX HERNANDEZ-GARCIA, Mila, Université de Montréal, Montreal, Canada\n\nMachine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.", "doc_id": "luccioni2023", "page": 1, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\n\nALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada\nALEX HERNANDEZ-GARCIA, Mila, Université de Montréal, Montreal, Canada\n\nMachine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.", "original_types": ["text", "header"], "id": 284}
{"type": "section", "content": "2 RELATED WORK\n\nMeasuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering momentum in recent years. In the current section, we present several directions pursued in this domain, from empirical studies of specific models to the development of efficient algorithms and hardware.", "doc_id": "luccioni2023", "page": 2, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "2 RELATED WORK\n\nMeasuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering momentum in recent years. In the current section, we present several directions pursued in this domain, from empirical studies of specific models to the development of efficient algorithms and hardware.", "original_types": ["text", "header"], "id": 285}
{"type": "section", "content": "3 METHODOLOGY\n\nAs stated in Section 1, the goal of this paper is descriptive – to observe the evolution of the carbon emissions of our field of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In this section, we present the different aspects and details of our methodology.", "doc_id": "luccioni2023", "page": 3, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "3 METHODOLOGY\n\nAs stated in Section 1, the goal of this paper is descriptive – to observe the evolution of the carbon emissions of our field of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In this section, we present the different aspects and details of our methodology.", "original_types": ["text", "header"], "id": 286}
{"type": "table", "content": "Table 1. Summary of the models analyzed in our study\n```markdown\n| Task | Dataset | Number of Models | Publication dates |\n|------|--------|-----------------|------------------|\n| Image Classification | ImageNet [14] | 35 | 2012-2021 |\n| Machine Translation | WMT2014 [10] | 30 | 2016-2021 |\n| Named Entity Recognition | CoNLL 2003 [41] | 11 | 2015-2021 |\n| Question Answering | SQuAD 1.1 [39] | 10 | 2016-2021 |\n| Object Detection | MS COCO [29] | 9 | 2019-2021 |\n```", "doc_id": "luccioni2023", "page": 3, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Table 1. Summary of the models analyzed in our study\n```markdown\n| Task | Dataset | Number of Models | Publication dates |\n|------|--------|-----------------|------------------|\n| Image Classification | ImageNet [14] | 35 | 2012-2021 |\n| Machine Translation | WMT2014 [10] | 30 | 2016-2021 |\n| Named Entity Recognition | CoNLL 2003 [41] | 11 | 2015-2021 |\n| Question Answering | SQuAD 1.1 [39] | 10 | 2016-2021 |\n| Object Detection | MS COCO [29] | 9 | 2019-2021 |\n```", "id": 287}
{"type": "section", "content": "The models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of conferences and journals. They all represent novel architectures at the time of publication, achieving high performance in their respective tasks: on average, the models are within 8 % of SOTA performance according to Papers With Code leaderboards at the time of their publication . This sample represents the largest amount of information regarding the carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of angles, which we present in Section 4. In the remaining of this section, we describe our method for estimating carbon emissions.", "doc_id": "luccioni2023", "page": 3, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "The models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of conferences and journals. They all represent novel architectures at the time of publication, achieving high performance in their respective tasks: on average, the models are within 8 % of SOTA performance according to Papers With Code leaderboards at the time of their publication . This sample represents the largest amount of information regarding the carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of angles, which we present in Section 4. In the remaining of this section, we describe our method for estimating carbon emissions.", "original_types": ["text"], "id": 288}
{"type": "section", "content": "3.2 Estimating carbon emissions\n\nThe unit of measurement typically used for quantifying and comparing carbon emissions is \\(CO_2\\) equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) \\footnote{For instance, methane is 28 times more potent than CO2 based on its 100-year global warming potential, so energy generation emitting 1 gram of methane per kWh will emit 28 grams of CO2eq per kWh.}.\n\nThe amount of CO2eq (C) emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used (P), the training time (T) and the carbon intensity of the energy grid (I); or equivalently, the energy consumed (E) and the carbon intensity:\n\nC = P × T × I = E × I.", "doc_id": "luccioni2023", "page": 4, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "3.2 Estimating carbon emissions\n\nThe unit of measurement typically used for quantifying and comparing carbon emissions is \\(CO_2\\) equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh) \\footnote{For instance, methane is 28 times more potent than CO2 based on its 100-year global warming potential, so energy generation emitting 1 gram of methane per kWh will emit 28 grams of CO2eq per kWh.}.\n\nThe amount of CO2eq (C) emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used (P), the training time (T) and the carbon intensity of the energy grid (I); or equivalently, the energy consumed (E) and the carbon intensity:\n\nC = P × T × I = E × I.", "original_types": ["text", "header", "equation"], "id": 289}
{"type": "section", "content": "4. DATA ANALYSIS\n\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the main sources of energy used for training (§ 4.1), the order of magnitude of CO2 emissions produced (§ 4.2), the evolution of these emissions over time (§ 4.3) and the relationship between carbon emissions and model performance (§ 4.4)².\n\n4.1 What are the main sources of energy used for training ML models?\n\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity of that grid, in the face of the large differences between energy sources. For instance, renewable energy sources like hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-renewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive (ranging from 360 to 680 gCO2eq/kWh) [24, 44]. That means that the energy source that powers the hardware to train ML models can result in differences of up to 60 times more CO2eq in terms of total emissions.", "doc_id": "luccioni2023", "page": 5, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "4. DATA ANALYSIS\n\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the main sources of energy used for training (§ 4.1), the order of magnitude of CO2 emissions produced (§ 4.2), the evolution of these emissions over time (§ 4.3) and the relationship between carbon emissions and model performance (§ 4.4)².\n\n4.1 What are the main sources of energy used for training ML models?\n\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity of that grid, in the face of the large differences between energy sources. For instance, renewable energy sources like hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas non-renewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive (ranging from 360 to 680 gCO2eq/kWh) [24, 44]. That means that the energy source that powers the hardware to train ML models can result in differences of up to 60 times more CO2eq in terms of total emissions.", "original_types": ["text", "header"], "id": 290}
{"type": "table", "content": "Table 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]", "doc_id": "luccioni2023", "page": 5, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Table 2. Main Energy Sources for the models analyzed and their carbon intensities [24, 52]", "id": 291}
{"type": "table", "content": "Main Energy Sources for the models analyzed and their carbon intensities [24, 52]", "doc_id": "luccioni2023", "page": 5, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Main Energy Sources for the models analyzed and their carbon intensities [24, 52]", "id": 292}
{"type": "table", "content": "|Main energy source|Number of Models|Low-Carbon?|Average Carbon Intensity (gCO2eq/kWh)|\n|---|---|---|---|\n|Coal|38|No|512.3|\n|Natural Gas|23|No|350.5|\n|Hydroelectricity|19|Yes|100.6|\n|Oil|12|No|453.6|\n|Nuclear|3|Yes|147.2|", "doc_id": "luccioni2023", "page": 5, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "|Main energy source|Number of Models|Low-Carbon?|Average Carbon Intensity (gCO2eq/kWh)|\n|---|---|---|---|\n|Coal|38|No|512.3|\n|Natural Gas|23|No|350.5|\n|Hydroelectricity|19|Yes|100.6|\n|Oil|12|No|453.6|\n|Nuclear|3|Yes|147.2|", "id": 293}
{"type": "section", "content": "In Table 2, we show the principal energy source used by the models from our sample, as well as its average carbon intensity. We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and natural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energy ³. While the average carbon intensity used for training the models from our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still leaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss this further in Section 5).\n\nIn Figure 1, we show the model training locations reported by authors on a country-level, with the median carbon intensity of each country indicated below. In terms of the model training locations reported by authors, we found a very imbalanced distribution, with the vast majority of models being trained in a small number of countries – half of the models in our sample were trained in the United States (48), followed by China (18), with the rest of the models distributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on this country-level analysis of energy grids used for training the models in our sample, we found that most common", "doc_id": "luccioni2023", "page": 5, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "In Table 2, we show the principal energy source used by the models from our sample, as well as its average carbon intensity. We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and natural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energy ³. While the average carbon intensity used for training the models from our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still leaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss this further in Section 5).\n\nIn Figure 1, we show the model training locations reported by authors on a country-level, with the median carbon intensity of each country indicated below. In terms of the model training locations reported by authors, we found a very imbalanced distribution, with the vast majority of models being trained in a small number of countries – half of the models in our sample were trained in the United States (48), followed by China (18), with the rest of the models distributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on this country-level analysis of energy grids used for training the models in our sample, we found that most common", "original_types": ["text"], "id": 294}
{"type": "section", "content": "4.1 Countries with the highest carbon intensity\n\ncountries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum, with emissions of 350 gCO2eq/kWh and above. On the other end, the countries with the lowest carbon intensity in our sample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which has a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a total of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly industrialized countries produce the majority of the world’s greenhouse gases [17].", "doc_id": "luccioni2023", "page": 6, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "4.1 Countries with the highest carbon intensity\n\ncountries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum, with emissions of 350 gCO2eq/kWh and above. On the other end, the countries with the lowest carbon intensity in our sample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which has a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a total of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly industrialized countries produce the majority of the world’s greenhouse gases [17].", "original_types": ["text", "header"], "id": 295}
{"type": "figure", "content": "Fig. 1. Map with the countries where the models in the data were trained, as reported by the authors. The colors code the median carbon intensity of the energy used by the models trained in each country. The legend indicates the number of models trained in each country, as well as a colored patch marking the main energy source – see bottom of the legend for the values.", "doc_id": "luccioni2023", "page": 6, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Fig. 1. Map with the countries where the models in the data were trained, as reported by the authors. The colors code the median carbon intensity of the energy used by the models trained in each country. The legend indicates the number of models trained in each country, as well as a colored patch marking the main energy source – see bottom of the legend for the values.", "id": 296}
{"type": "section", "content": "also shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend, with orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models trained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.", "doc_id": "luccioni2023", "page": 7, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "also shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend, with orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models trained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.", "original_types": ["text"], "id": 297}
{"type": "figure", "content": "Fig. 2. Estimated energy consumed (kWh) and CO2 (kg) by each model in the data set, plotted in a log-log scale. Colors indicate the principal energy source, and the size of the dot carbon intensity. While the relationship between energy and carbon emissions is mostly linear, the data show that models trained with less carbon-intensive energy (e.g. hydroelectric) emit orders of magnitude less carbon than those trained using more carbon-intensive energy (e.g. coal).", "doc_id": "luccioni2023", "page": 7, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Fig. 2. Estimated energy consumed (kWh) and CO2 (kg) by each model in the data set, plotted in a log-log scale. Colors indicate the principal energy source, and the size of the dot carbon intensity. While the relationship between energy and carbon emissions is mostly linear, the data show that models trained with less carbon-intensive energy (e.g. hydroelectric) emit orders of magnitude less carbon than those trained using more carbon-intensive energy (e.g. coal).", "id": 298}
{"type": "section", "content": "For instance, honing in on the central bottom portion of Figure 2, it can be seen that the models trained using hydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that consumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange), given that the Y axis is on a logarithmic scale. Furthermore, the size of the dots varies as a function of the carbon intensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend, with the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy consumed. This further supports the analysis carried out in Section 4.1, suggesting that the primary energy source used for training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a low-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.\n\nBesides the primary energy source, carbon emissions are a function of power consumed by the hardware used and the training time. The choice of hardware has a relatively small influence on the large variation of carbon emissions that we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span from 105 kgCO2eq to even less than 10 kgCO2eq (see Section A.2 of the appendix for further details). While using renewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining factor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training time.", "doc_id": "luccioni2023", "page": 7, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "For instance, honing in on the central bottom portion of Figure 2, it can be seen that the models trained using hydroelectricity (the blue dots) are about two orders of magnitude lower in terms of carbon emissions than models that consumed similar amounts of energy from more carbon-intensive sources such as coal (in brown) and gas (in orange), given that the Y axis is on a logarithmic scale. Furthermore, the size of the dots varies as a function of the carbon intensity of the electricity grid used, illustrating two parallel groups of models, both exhibiting a largely linear trend, with the more carbon intensive models positioned higher than the lower carbon models for similar amounts of energy consumed. This further supports the analysis carried out in Section 4.1, suggesting that the primary energy source used for training ML models has a strong impact on the overall resulting emissions from model training, and that choosing a low-carbon energy grid can play a significant role towards reducing the carbon emissions of ML model training.\n\nBesides the primary energy source, carbon emissions are a function of power consumed by the hardware used and the training time. The choice of hardware has a relatively small influence on the large variation of carbon emissions that we observe in our sample , given that the TDP ranges from 180 W to 300 W, while the carbon emissions span from 105 kgCO2eq to even less than 10 kgCO2eq (see Section A.2 of the appendix for further details). While using renewable energy can reduce up to 1,000 the carbon emissions for the same amount of energy used, the remaining factor responsible for the large variation in both energy and carbon emissions in our sample is therefore the training time.", "original_types": ["text"], "id": 299}
{"type": "section", "content": "4.3 How do the CO2 emissions produced by training ML models evolve over time?\n\nSome recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that achieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [50], whereas others have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [37]. Therefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether there are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically compare whether new generations of ML models from our sample consistently used more energy and emitted more carbon than previous ones.", "doc_id": "luccioni2023", "page": 8, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "4.3 How do the CO2 emissions produced by training ML models evolve over time?\n\nSome recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that achieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [50], whereas others have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [37]. Therefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether there are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically compare whether new generations of ML models from our sample consistently used more energy and emitted more carbon than previous ones.", "original_types": ["text", "header"], "id": 300}
{"type": "section", "content": "Does more energy and CO2 lead to better model performance?\n\nA final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of carbon emitted by models to their performance on benchmark tasks such as image classification, machine translation and question answering. We compare the emissions of the models from our sample and their performance on four tasks: image recognition on ImageNet [14] (35 models), machine translation for English-French and English-German on the 2014 WMT Translation tasks [10] (30 models), question answering on the SQuAD 1.1 dataset [39] (10 models), and named entity recognition on the CoNLL 2003 dataset [41] (11 models) \n. Our goal with this analysis is to validate whether, generally speaking, the more carbon-intensive models from our sample achieved better performance on common benchmarks compared to the models with less incurred emissions.\n\nFigure 4 shows the performance of the models in these four tasks and the associated carbon emissions; we also represent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based on our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the models that achieved the best accuracy for a given amount of CO2eq emissions. In other words, all the data points under the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite producing the same or more carbon emissions.\n\nBased on the comparison between carbon emissions and performance, we can observe that the only task in which better performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top right subplot of Figure 4. Still, the relationship is far from being highly correlated (especially given that that the x-axis in on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are also the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance (achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.", "doc_id": "luccioni2023", "page": 9, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Does more energy and CO2 lead to better model performance?\n\nA final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of carbon emitted by models to their performance on benchmark tasks such as image classification, machine translation and question answering. We compare the emissions of the models from our sample and their performance on four tasks: image recognition on ImageNet [14] (35 models), machine translation for English-French and English-German on the 2014 WMT Translation tasks [10] (30 models), question answering on the SQuAD 1.1 dataset [39] (10 models), and named entity recognition on the CoNLL 2003 dataset [41] (11 models) \n. Our goal with this analysis is to validate whether, generally speaking, the more carbon-intensive models from our sample achieved better performance on common benchmarks compared to the models with less incurred emissions.\n\nFigure 4 shows the performance of the models in these four tasks and the associated carbon emissions; we also represent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based on our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the models that achieved the best accuracy for a given amount of CO2eq emissions. In other words, all the data points under the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite producing the same or more carbon emissions.\n\nBased on the comparison between carbon emissions and performance, we can observe that the only task in which better performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top right subplot of Figure 4. Still, the relationship is far from being highly correlated (especially given that that the x-axis in on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are also the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance (achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.", "original_types": ["text", "header"], "id": 301}
{"type": "section", "content": "For other tasks, the trend is even less clear – for instance, for the 30 models evaluated on the WMT translation task (top left plot of Figure 4), there is no clear link between CO2 emissions and BLEU score, for neither English-French or English-German – although the WMT English-French task seems to incur more carbon emissions than the English-German one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger than the English-German one, which can require a longer training time and thus a higher energy consumption. For the final two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and 11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many models from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD dataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).", "doc_id": "luccioni2023", "page": 9, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "For other tasks, the trend is even less clear – for instance, for the 30 models evaluated on the WMT translation task (top left plot of Figure 4), there is no clear link between CO2 emissions and BLEU score, for neither English-French or English-German – although the WMT English-French task seems to incur more carbon emissions than the English-German one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger than the English-German one, which can require a longer training time and thus a higher energy consumption. For the final two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and 11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many models from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD dataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).", "original_types": ["text"], "id": 302}
{"type": "figure", "content": "Figure 4. Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score on the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on ImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) and Named Entity Recognition (bottom right, evaluated using F1 score on the CoNLL dataset) and the CO2 emitted for training models. The black curves correspond to the Pareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that the x axis is in logarithmic scale.", "doc_id": "luccioni2023", "page": 10, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Figure 4. Comparison of the accuracy achieved by each model trained on Machine Translation (top left, evaluated using BLEU score on the English-French and English-German WMT datasets), Image Classification (top right, measured using Top-1 accuracy on ImageNet), Question Answering (bottom left, evaluated using F1 score on SQuAD v.1) and Named Entity Recognition (bottom right, evaluated using F1 score on the CoNLL dataset) and the CO2 emitted for training models. The black curves correspond to the Pareto fronts given the data, that is data points under the line are sub-optimal in terms of performance and CO2 emitted.Note that the x axis is in logarithmic scale.", "id": 303}
{"type": "section", "content": "5.1 Discussion of Results", "doc_id": "luccioni2023", "page": 11, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "5.1 Discussion of Results", "original_types": ["header"], "id": 304}
{"type": "section", "content": "While the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic reporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community to acquire a better understanding of its environmental footprint and how to reduce it [28, 38]. Our study is the first analysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source to performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions associated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is about 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to Beijing. While this may not seem like a large amount, the increase in emissions in recent years – from an average of 487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 – as well as other trends that we observed in Section 4.3, indicate that the overall emissions due to ML model are rising. \n\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training machine learning models is due to the carbon intensity of the primary energy source and the training time, with the power consumption of the hardware having a smaller influence. In terms of training time, the models in our sample range from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours, pointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170 days with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of carbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest training time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the lowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking, we can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more layers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters), whereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the emissions spectrum. Given that Transformer architectures are increasing in popularity – especially in NLP but also for several Computer Vision tasks – having a better idea of their energy consumption, carbon emissions, and the factors that influence them is also crucial part of analyzing the current and future state of our field. \n\nAn important observation from our analysis is that better performance is not generally achieved by using more energy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent years has brought the possibility to train machine learning models efficiently. Image Classification is the task in our sample in which we observed the strongest correlation between performance and emissions. However, even in this task we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the left-hand-side of Figure 4). This highlights the availability of efficient approaches and architectures.", "doc_id": "luccioni2023", "page": 11, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "While the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic reporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community to acquire a better understanding of its environmental footprint and how to reduce it [28, 38]. Our study is the first analysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source to performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions associated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is about 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to Beijing. While this may not seem like a large amount, the increase in emissions in recent years – from an average of 487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 – as well as other trends that we observed in Section 4.3, indicate that the overall emissions due to ML model are rising. \n\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training machine learning models is due to the carbon intensity of the primary energy source and the training time, with the power consumption of the hardware having a smaller influence. In terms of training time, the models in our sample range from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours, pointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170 days with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of carbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest training time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the lowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking, we can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more layers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters), whereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the emissions spectrum. Given that Transformer architectures are increasing in popularity – especially in NLP but also for several Computer Vision tasks – having a better idea of their energy consumption, carbon emissions, and the factors that influence them is also crucial part of analyzing the current and future state of our field. \n\nAn important observation from our analysis is that better performance is not generally achieved by using more energy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent years has brought the possibility to train machine learning models efficiently. Image Classification is the task in our sample in which we observed the strongest correlation between performance and emissions. However, even in this task we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the left-hand-side of Figure 4). This highlights the availability of efficient approaches and architectures.", "id": 305}
{"type": "section", "content": "5.2 Limitations\n\nThe analyses that we have carried out and the insights that they have provided us are useful towards a better understanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for one, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models and architectures that exist and the speed at which our field is evolving. As we discussed in Section 3, despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant", "doc_id": "luccioni2023", "page": 11, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "5.2 Limitations\n\nThe analyses that we have carried out and the insights that they have provided us are useful towards a better understanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for one, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models and architectures that exist and the speed at which our field is evolving. As we discussed in Section 3, despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant", "original_types": ["text", "header"], "id": 306}
{"type": "section", "content": "5.3 Future Work\n\nThere is much interesting and exciting work to be done that would help us better understand the carbon emissions and broader environmental implications of ML. This includes:\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\n\nyears can be attributed to training increasingly deep and computationally expensive models, especially in fields such as natural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from the perspective of their energy consumption [8, 15], the attribution of computing resources [2, 3] or the influence of corporate interests on research directions [1, 9].\n\nWhile discussions regarding the carbon footprint of our daily lives has started to become more common in many communities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat) contribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts of the models we train. We hope that our work encourages better practices and more transparency in reporting the computational needs of the models and details of the energy used, and that our study will be a meaningful contribution towards a better understanding of our impact as ML researchers and practitioners.", "doc_id": "luccioni2023", "page": "12-13", "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "5.3 Future Work\n\nThere is much interesting and exciting work to be done that would help us better understand the carbon emissions and broader environmental implications of ML. This includes:\n\nCounting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\n\nyears can be attributed to training increasingly deep and computationally expensive models, especially in fields such as natural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from the perspective of their energy consumption [8, 15], the attribution of computing resources [2, 3] or the influence of corporate interests on research directions [1, 9].\n\nWhile discussions regarding the carbon footprint of our daily lives has started to become more common in many communities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat) contribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts of the models we train. We hope that our work encourages better practices and more transparency in reporting the computational needs of the models and details of the energy used, and that our study will be a meaningful contribution towards a better understanding of our impact as ML researchers and practitioners.", "original_types": ["text", "header"], "id": 307}
{"type": "section", "content": "REFERENCES\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Oreaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. arXiv preprint arXiv:2110.03036 (2021).\n\n[3] Nur Ahmed and Muntasir Wahed. 2020. The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research. arXiv preprint arXiv:2010.15581 (2020).\n\n[4] Amazon Web Services. 2021. Delivering Progress Every Day : Amazon’s 2021 Sustainability Report. https://sustainability.aboutamazon.com/2021-sustainability-report.pdf\n\n[5] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models. arXiv:2007.03051 [cs.CY]\n\n[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael. 2010. On the causal dynamics between emissions, nuclear energy, renewable energy, and economic growth. Ecological Economics 69, 11 (2010), 2255–2260.\n\n[7] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmithell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610–623.\n\n[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The Values Encoded in Machine Learning Research. arXiv:2106.15590 [cs.LG]\n\n[10] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation. 12–58.\n\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n\n[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308.", "doc_id": "luccioni2023", "page": 14, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "REFERENCES\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Oreaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. arXiv preprint arXiv:2110.03036 (2021).\n\n[3] Nur Ahmed and Muntasir Wahed. 2020. The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research. arXiv preprint arXiv:2010.15581 (2020).\n\n[4] Amazon Web Services. 2021. Delivering Progress Every Day : Amazon’s 2021 Sustainability Report. https://sustainability.aboutamazon.com/2021-sustainability-report.pdf\n\n[5] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models. arXiv:2007.03051 [cs.CY]\n\n[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael. 2010. On the causal dynamics between emissions, nuclear energy, renewable energy, and economic growth. Ecological Economics 69, 11 (2010), 2255–2260.\n\n[7] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmithell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610–623.\n\n[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The Values Encoded in Machine Learning Research. arXiv:2106.15590 [cs.LG]\n\n[10] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation. 12–58.\n\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n\n[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308.", "original_types": ["text", "header"], "id": 308}
{"type": "section", "content": "[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.\n\n[16] Facebook. 2020. Facebook 2020 Sustainability Report. https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf\n\n[17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens. 2020. This interactive chart shows changes in the world’s top 10 emitters. (2020).\n\n[18] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n\n[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 854–867.\n\n[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020), 1–43.\n\n[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging Fairness and Environmental Sustainability in Natural Language Processing. arXiv preprint arXiv:2211.04256 (2022).\n\n[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models. https://doi.org/10.48550/ARXIV.2203.15556\n\n[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).\n\n[24] IEA. 2019. Global Energy & CO2 Status Report 2019. IEA (International Energy Agency): Paris, France (2019). https://www.iea.org/reports/global-energy-co2-status-report-2019\n\n[25] International Telecommunication Union. 2020. Greenhouse gas emissions trajectories for the information and communication technology sector compatible with the UNFCCC Paris agreement: L. 1470. http://handle.itu.int/11.1002/1000/14084\n\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019). Manuscript pending review", "doc_id": "luccioni2023", "page": 14, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.\n\n[16] Facebook. 2020. Facebook 2020 Sustainability Report. https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf\n\n[17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens. 2020. This interactive chart shows changes in the world’s top 10 emitters. (2020).\n\n[18] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n\n[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 854–867.\n\n[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020), 1–43.\n\n[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging Fairness and Environmental Sustainability in Natural Language Processing. arXiv preprint arXiv:2211.04256 (2022).\n\n[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models. https://doi.org/10.48550/ARXIV.2203.15556\n\n[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).\n\n[24] IEA. 2019. Global Energy & CO2 Status Report 2019. IEA (International Energy Agency): Paris, France (2019). https://www.iea.org/reports/global-energy-co2-status-report-2019\n\n[25] International Telecommunication Union. 2020. Greenhouse gas emissions trajectories for the information and communication technology sector compatible with the UNFCCC Paris agreement: L. 1470. http://handle.itu.int/11.1002/1000/14084\n\n[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019). Manuscript pending review", "original_types": ["text"], "id": 309}
{"type": "section", "content": "[27] Loïc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of computation. Advanced Science (2021), 2100707.\n\n[28] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).\n\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European conference on computer vision. Springer, 740–755.\n\n[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic accountability. arXiv preprint arXiv:1911.08354 (2019).\n\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. arXiv preprint arXiv:2211.02001 (2022).\n\n[32] Jens Malmodin and Dag Lundén. 2018. The energy and carbon footprint of the global ICT and E&M sectors 2010–2015. Sustainability 10, 9 (2018), 3027.\n\n[33] Aaditya Mattoo and Arvind Subramanian. 2012. Equity in climate change: an analytical review. World Development 40, 6 (2012), 1083–1097.\n\n[34] Jennifer Morgan and David Waskow. 2014. A new look at climate equity in the UNFCCC. Climate Policy 14, 1 (2014), 17–22.\n\n[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam. 2021. Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. arXiv preprint arXiv:2107.06946 (2021).\n\n[36] Copenhagen Centre on Energy Efficiency. 2020. Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet]. https://c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf\n\n[37] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. arXiv preprint arXiv:2204.05149 (2022).\n\n[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).\n\n[40] Henning Rodhe. 1990. A comparison of the contribution of various gases to the greenhouse effect. Science 248, 4960 (1990), 1217–1219.\n\n[41] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050 (2003).", "doc_id": "luccioni2023", "page": 15, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "[27] Loïc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of computation. Advanced Science (2021), 2100707.\n\n[28] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).\n\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European conference on computer vision. Springer, 740–755.\n\n[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic accountability. arXiv preprint arXiv:1911.08354 (2019).\n\n[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. arXiv preprint arXiv:2211.02001 (2022).\n\n[32] Jens Malmodin and Dag Lundén. 2018. The energy and carbon footprint of the global ICT and E&M sectors 2010–2015. Sustainability 10, 9 (2018), 3027.\n\n[33] Aaditya Mattoo and Arvind Subramanian. 2012. Equity in climate change: an analytical review. World Development 40, 6 (2012), 1083–1097.\n\n[34] Jennifer Morgan and David Waskow. 2014. A new look at climate equity in the UNFCCC. Climate Policy 14, 1 (2014), 17–22.\n\n[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam. 2021. Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. arXiv preprint arXiv:2107.06946 (2021).\n\n[36] Copenhagen Centre on Energy Efficiency. 2020. Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet]. https://c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf\n\n[37] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. arXiv preprint arXiv:2204.05149 (2022).\n\n[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).\n\n[40] Henning Rodhe. 1990. A comparison of the contribution of various gases to the greenhouse effect. Science 248, 4960 (1990), 1217–1219.\n\n[41] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050 (2003).", "original_types": ["text"], "id": 310}
{"type": "section", "content": "[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n\n[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization. https://doi.org/10.48550/ARXIV.2110.08207\n\n[44] Steffen Schlömer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims, Pete Smith, et al. 2014. Annex III: Technology-specific cost and performance parameters. In Climate Change 2014: Mitigation of Climate Change: Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press, 1329–1356.\n\n[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n\n[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.\n\n[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Muñoz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert Nerenberg, Andrea Armani, Kasey M. Faust, and Manish Kumar. 2021. Conference demographics and footprint changed by virtual platforms. Nature Sustainability 2398-9629 (2021).\n\n[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[49] Siddharth Suman. 2018. Hybrid nuclear-renewable energy systems: A review. Journal of Cleaner Production 181 (2018), 166–177.\n\n[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint arXiv:2007.05558 (2020).\n\n[51] Tristan Trébaol. 2020. CUMULATOR — a tool to quantify and report the carbon footprint of machine learning computations and communication in academia and healthcare. Technical Report.\n\n[52] United States Energy Information Administration. 2012-2021. Detailed State Data. https://www.eia.gov/electricity/data/state/", "doc_id": "luccioni2023", "page": 15, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).\n\n[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization. https://doi.org/10.48550/ARXIV.2110.08207\n\n[44] Steffen Schlömer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims, Pete Smith, et al. 2014. Annex III: Technology-specific cost and performance parameters. In Climate Change 2014: Mitigation of Climate Change: Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press, 1329–1356.\n\n[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n\n[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.\n\n[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Muñoz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert Nerenberg, Andrea Armani, Kasey M. Faust, and Manish Kumar. 2021. Conference demographics and footprint changed by virtual platforms. Nature Sustainability 2398-9629 (2021).\n\n[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[49] Siddharth Suman. 2018. Hybrid nuclear-renewable energy systems: A review. Journal of Cleaner Production 181 (2018), 166–177.\n\n[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint arXiv:2007.05558 (2020).\n\n[51] Tristan Trébaol. 2020. CUMULATOR — a tool to quantify and report the carbon footprint of machine learning computations and communication in academia and healthcare. Technical Report.\n\n[52] United States Energy Information Administration. 2012-2021. Detailed State Data. https://www.eia.gov/electricity/data/state/", "original_types": ["text"], "id": 311}
{"type": "section", "content": "[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008.", "doc_id": "luccioni2023", "page": 15, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008.", "original_types": ["text"], "id": 312}
{"type": "section", "content": "[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[55] Guangxuan Xu and Qingyuan Hu. 2022. Can model compression improve NLP fairness. arXiv preprint arXiv:2201.08542 (2022).\n\n[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh. 2021. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. arXiv preprint arXiv:2109.12584 (2021).\n\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. https://doi.org/10.48550/ARXIV.2205.01068\n\n[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for responsible natural language processing. arXiv preprint arXiv:2002.05829 (2020).\n\n[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).", "doc_id": "luccioni2023", "page": 16, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[55] Guangxuan Xu and Qingyuan Hu. 2022. Can model compression improve NLP fairness. arXiv preprint arXiv:2201.08542 (2022).\n\n[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh. 2021. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. arXiv preprint arXiv:2109.12584 (2021).\n\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. https://doi.org/10.48550/ARXIV.2205.01068\n\n[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for responsible natural language processing. arXiv preprint arXiv:2002.05829 (2020).\n\n[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).", "original_types": ["text"], "id": 313}
{"type": "section", "content": "A SUPPLEMENTARY MATERIALS\n\nA.1 Emails sent to authors\n\nSubject: Information Request: Computing Infrastructure Used in your Paper\n\nHello,\n\nMy name is XXXX and I am a researcher working on the environmental impact of Machine Learning. I am trying to gather data regarding the carbon footprint of recent state-of-the-art research papers. This will help the ML community get a better idea of how much CO2 we are emitting when training models. In order to help me on my mission, I was hoping you could give me more information about your paper entitled YYYY. More specifically, could you tell me: - Where it was trained? If it was on a local computing cluster, could you tell me the location of the cluster? And if it was trained on the cloud, could you indicate the provider and server region (e.g. \"Microsoft Azure, us-east1\")? - What hardware you used - The total training time of your models?\n\nThank you very much for this information, XXXX\n\nA.2 Information regarding training hardware", "doc_id": "luccioni2023", "page": 17, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "A SUPPLEMENTARY MATERIALS\n\nA.1 Emails sent to authors\n\nSubject: Information Request: Computing Infrastructure Used in your Paper\n\nHello,\n\nMy name is XXXX and I am a researcher working on the environmental impact of Machine Learning. I am trying to gather data regarding the carbon footprint of recent state-of-the-art research papers. This will help the ML community get a better idea of how much CO2 we are emitting when training models. In order to help me on my mission, I was hoping you could give me more information about your paper entitled YYYY. More specifically, could you tell me: - Where it was trained? If it was on a local computing cluster, could you tell me the location of the cluster? And if it was trained on the cloud, could you indicate the provider and server region (e.g. \"Microsoft Azure, us-east1\")? - What hardware you used - The total training time of your models?\n\nThank you very much for this information, XXXX\n\nA.2 Information regarding training hardware", "original_types": ["text", "header"], "id": 314}
{"type": "table", "content": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "doc_id": "luccioni2023", "page": 17, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "id": 315}
{"type": "table", "content": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "doc_id": "luccioni2023", "page": 17, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "id": 316}
{"type": "table", "content": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "doc_id": "luccioni2023", "page": 17, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).", "id": 317}
{"type": "section", "content": "In Table 3, we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by the number of papers that used them, the range of quantities used, and their TDP. The Tesla V100 was by far the most popular piece of hardware, representing almost a third of the papers, followed by the TPU v3. The TDP of the hardware used in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU v3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and TPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of GPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize a single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly. We analyze the connection between energy usage and performance on different ML tasks in § 4.4, in order to determine whether higher energy consumption helps achieve better performance in different ML tasks.", "doc_id": "luccioni2023", "page": 17, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "In Table 3, we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by the number of papers that used them, the range of quantities used, and their TDP. The Tesla V100 was by far the most popular piece of hardware, representing almost a third of the papers, followed by the TPU v3. The TDP of the hardware used in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU v3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and TPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of GPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize a single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly. We analyze the connection between energy usage and performance on different ML tasks in § 4.4, in order to determine whether higher energy consumption helps achieve better performance in different ML tasks.", "original_types": ["text"], "id": 318}
{"type": "section", "content": "A.3 Energy Consumption by Task\n\nIn Figure 5 below, we plot the same four tasks as in Figure 3, representing the energy consumed instead of the CO2 emitted. We find largely similar trends as the ones we describe in Section 4.4, with better performance on tasks like machine translation and image classification not necessarily being contingent on higher energy consumption.", "doc_id": "luccioni2023", "page": 18, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "A.3 Energy Consumption by Task\n\nIn Figure 5 below, we plot the same four tasks as in Figure 3, representing the energy consumed instead of the CO2 emitted. We find largely similar trends as the ones we describe in Section 4.4, with better performance on tasks like machine translation and image classification not necessarily being contingent on higher energy consumption.", "original_types": ["text", "header"], "id": 319}
{"type": "section", "content": "Carbon intensity over time\n\nIn Figure 6, we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the number of models trained with each energy source. We observe that, despite the need to address the climate crisis by using cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of models trained with cleaner energy. On the contrary, we do observe a stark increase of models trained with coal.", "doc_id": "luccioni2023", "page": 19, "url": "https://arxiv.org/pdf/2302.08476", "embedded_text": "Carbon intensity over time\n\nIn Figure 6, we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the number of models trained with each energy source. We observe that, despite the need to address the climate crisis by using cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of models trained with cleaner energy. On the contrary, we do observe a stark increase of models trained with coal.", "original_types": ["text", "header"], "id": 320}
{"type": "section", "content": "2023 Amazon Sustainability Report\n\nThis report provides an overview of Amazon's sustainability efforts and achievements in 2023.\n\nContents\n\nOn the cover\n\nThe Baldy Mesa Solar and Storage Project (developed and operated by AES), located in Adelanto, California.", "doc_id": "amazon2023", "page": "1-2", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Amazon Sustainability Report\n\nThis report provides an overview of Amazon's sustainability efforts and achievements in 2023.\n\nContents\n\nOn the cover\n\nThe Baldy Mesa Solar and Storage Project (developed and operated by AES), located in Adelanto, California.", "original_types": ["text", "header"], "id": 321}
{"type": "table", "content": "Overview\n3 Introduction\n4 A Letter from Our Chief Sustainability Officer\n5 How We Work\n6 Goals Summary\n7 2023 Year in Review", "doc_id": "amazon2023", "page": 2, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Overview\n3 Introduction\n4 A Letter from Our Chief Sustainability Officer\n5 How We Work\n6 Goals Summary\n7 2023 Year in Review", "id": 322}
{"type": "table", "content": "Environment\n9 Carbon\n24 Carbon-Free Energy\n29 Packaging\n34 Waste and Circularity\n40 Water\n45 Human Rights\n50 Responsible Supply Chain\n58 Sustainable Products and Materials\n64 Supplier Diversity\n67 Community Impact", "doc_id": "amazon2023", "page": 2, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Environment\n9 Carbon\n24 Carbon-Free Energy\n29 Packaging\n34 Waste and Circularity\n40 Water\n45 Human Rights\n50 Responsible Supply Chain\n58 Sustainable Products and Materials\n64 Supplier Diversity\n67 Community Impact", "id": 323}
{"type": "table", "content": "Value Chain\n75 Employee Experience\n81 Health and Safety\n86 Inclusive Experiences", "doc_id": "amazon2023", "page": 2, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Value Chain\n75 Employee Experience\n81 Health and Safety\n86 Inclusive Experiences", "id": 324}
{"type": "table", "content": "People\n94 Sustainability Reporting Topic Assessment\n95 Endnotes\n96 Assurance Statements\n97 Disclaimer and Forward-Looking Statements", "doc_id": "amazon2023", "page": 2, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "People\n94 Sustainability Reporting Topic Assessment\n95 Endnotes\n96 Assurance Statements\n97 Disclaimer and Forward-Looking Statements", "id": 325}
{"type": "section", "content": "Introduction\n\nAmazon is a global company with approximately 1.5 million full- and part-time employees worldwide and operations in Africa, Asia-Pacific, Europe, Latin America, the Middle East, and North America.\n\nAt Amazon, we combine data and science with passion and invention. We set big goals and work backward to achieve them, such as The Climate Pledge, our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement. We apply that same tenacity to how we address some of the world’s biggest environmental and societal challenges, striving to make every day better for our customers, employees, communities, and planet.\n\nHow to Navigate This Report\n\nLook for these symbols throughout the report:", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Introduction\n\nAmazon is a global company with approximately 1.5 million full- and part-time employees worldwide and operations in Africa, Asia-Pacific, Europe, Latin America, the Middle East, and North America.\n\nAt Amazon, we combine data and science with passion and invention. We set big goals and work backward to achieve them, such as The Climate Pledge, our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement. We apply that same tenacity to how we address some of the world’s biggest environmental and societal challenges, striving to make every day better for our customers, employees, communities, and planet.\n\nHow to Navigate This Report\n\nLook for these symbols throughout the report:", "original_types": ["text", "header"], "id": 326}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 327}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 328}
{"type": "section", "content": "In addition to this report, we share on our website our approach to sustainability governance and disclose our 2023 performance against reporting frameworks including the Sustainability Accounting Standards Board (SASB), the United Nations Sustainable Development Goals (SDGs), the Task Force on Climate-related Financial Disclosures (TCFD), and the United Nations Guiding Principles Reporting Framework (UNGPRF).\n\nFramework Disclosures\n\nLearn more in our 2023 Sustainability Reporting Framework Summary", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In addition to this report, we share on our website our approach to sustainability governance and disclose our 2023 performance against reporting frameworks including the Sustainability Accounting Standards Board (SASB), the United Nations Sustainable Development Goals (SDGs), the Task Force on Climate-related Financial Disclosures (TCFD), and the United Nations Guiding Principles Reporting Framework (UNGPRF).\n\nFramework Disclosures\n\nLearn more in our 2023 Sustainability Reporting Framework Summary", "original_types": ["text", "header"], "id": 329}
{"type": "figure", "content": "Figure 2: Title...", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 2: Title...", "id": 330}
{"type": "section", "content": "Employees inside one of our newest office buildings in Bellevue, Washington.", "doc_id": "amazon2023", "page": 3, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employees inside one of our newest office buildings in Bellevue, Washington.", "original_types": ["text"], "id": 331}
{"type": "section", "content": "A Letter from Our Chief Sustainability Officer\n\nAt Amazon, we are always looking for ways to move faster, deliver the best possible customer experience, and innovate. Through it all, we will remain steadfast as we invent, adapt, and will our way to meeting the needs of our customers. We regularly hear from our customers, corporate partners, and employees how much they care about sustainability and social responsibility. For a company as diverse as Amazon, embedding these values into all of our businesses, products, and services—as well as how we deliver for our customers—has been an incredible undertaking, with much of it taking place behind-the-scenes.\n\nWe know that driving change means staying focused on bringing entire industries along with us. Over the past five years, we’ve done this by encouraging companies to join The Climate Pledge—and we’re proud that over 500 have joined us and committed to be net-zero carbon 10 years ahead of the Paris Agreement. But we wanted to do more. Most recently, our sustainability team has been developing resources to share our expertise and help our suppliers build, measure, and act on their sustainability commitments. That’s why we’ve launched the Amazon Sustainability Exchange, a free sustainability resource center that contains a number of Amazon’s previously proprietary playbooks, templates, case studies, and science models, among other sustainability best practices.\n\nLooking ahead, we know our customers look to us to be at the cutting edge of new and growing technologies and enable them for good. We’re already deploying artificial intelligence (AI) in ways that benefit our customers directly, such as using it to right-size packaging and avoid waste. We’re exploring a growing number of AI applications—whether it’s monitoring and optimizing our energy use or helping combat deforestation in Brazil. We also see an opportunity to use AI to address sustainability challenges at an unprecedented scale, all while delivering new solutions for our customers. Much work remains, and we’re excited that Amazon is uniquely positioned to figure out how AI can help us address climate change in a more efficient and responsible way.\n\nMost importantly, we need to continue to invest in talent and hire employees who can lead on sustainability. We’re proud of the wide range of sustainability-focused career paths we offer at Amazon, including engineers, scientists, content creators, building architects, and more. And for those whose jobs aren’t directly within a sustainability field, we offer upskilling programs and affinity groups where our employees can learn more and get involved. It’s thanks to the thousands of professionals working behind-the-scenes across Amazon that we are able to bring all of this amazing work to life.\n\nI’m proud of the work that’s underway, and truly excited for what’s to come.\n\nWith gratitude, Kara Hurst Chief Sustainability Officer", "doc_id": "amazon2023", "page": 4, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "A Letter from Our Chief Sustainability Officer\n\nAt Amazon, we are always looking for ways to move faster, deliver the best possible customer experience, and innovate. Through it all, we will remain steadfast as we invent, adapt, and will our way to meeting the needs of our customers. We regularly hear from our customers, corporate partners, and employees how much they care about sustainability and social responsibility. For a company as diverse as Amazon, embedding these values into all of our businesses, products, and services—as well as how we deliver for our customers—has been an incredible undertaking, with much of it taking place behind-the-scenes.\n\nWe know that driving change means staying focused on bringing entire industries along with us. Over the past five years, we’ve done this by encouraging companies to join The Climate Pledge—and we’re proud that over 500 have joined us and committed to be net-zero carbon 10 years ahead of the Paris Agreement. But we wanted to do more. Most recently, our sustainability team has been developing resources to share our expertise and help our suppliers build, measure, and act on their sustainability commitments. That’s why we’ve launched the Amazon Sustainability Exchange, a free sustainability resource center that contains a number of Amazon’s previously proprietary playbooks, templates, case studies, and science models, among other sustainability best practices.\n\nLooking ahead, we know our customers look to us to be at the cutting edge of new and growing technologies and enable them for good. We’re already deploying artificial intelligence (AI) in ways that benefit our customers directly, such as using it to right-size packaging and avoid waste. We’re exploring a growing number of AI applications—whether it’s monitoring and optimizing our energy use or helping combat deforestation in Brazil. We also see an opportunity to use AI to address sustainability challenges at an unprecedented scale, all while delivering new solutions for our customers. Much work remains, and we’re excited that Amazon is uniquely positioned to figure out how AI can help us address climate change in a more efficient and responsible way.\n\nMost importantly, we need to continue to invest in talent and hire employees who can lead on sustainability. We’re proud of the wide range of sustainability-focused career paths we offer at Amazon, including engineers, scientists, content creators, building architects, and more. And for those whose jobs aren’t directly within a sustainability field, we offer upskilling programs and affinity groups where our employees can learn more and get involved. It’s thanks to the thousands of professionals working behind-the-scenes across Amazon that we are able to bring all of this amazing work to life.\n\nI’m proud of the work that’s underway, and truly excited for what’s to come.\n\nWith gratitude, Kara Hurst Chief Sustainability Officer", "original_types": ["text", "header"], "id": 332}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 4, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 333}
{"type": "section", "content": "Our Mission\n\nTo make customers' lives better and easier every day.\n\nOur Business\n\nWe are committed to addressing sustainability at every stage of our value chain.\n\nOur Operations\n\nWe offer products and services—both Amazon-branded and from many other brands and third-party sellers—in our Amazon stores, leveraging advanced transportation logistics to deliver globally. We engage suppliers from a complex supplier network. We seek to be a good neighbor wherever we operate and to support local people and charitable organizations that meet on-the-ground needs. In particular, we leverage our scale, resources, and expertise to address issues where we can have the greatest impact—namely affordable housing, education, disaster relief, and food security.\n\nOur Supply Chain\n\nWe procure materials, commodities, components, finished goods, and services from a complex supplier network. We engage suppliers globally to align our expectations for respecting human rights; maintaining safe, inclusive workplaces; and promoting sustainable practices.\n\nOur Employees\n\nThe approximately 1.5 million people in Amazon’s global workforce are Amazon-branded and from many other brands and third-party sellers—in our Amazon stores, leveraging advanced transportation logistics to deliver globally. We engage suppliers from a complex supplier network. We seek to be a good neighbor wherever we operate and to support local people and charitable organizations that meet on-the-ground needs. In particular, we leverage our scale, resources, and expertise to address issues where we can have the greatest impact—namely affordable housing, education, disaster relief, and food security.\n\nOur Communities\n\nWe continually seek new and better ways to serve customers, offering lower prices, more convenient services, and a larger selection of more sustainable products. We also help customers advance their businesses and enable digital transformation through AWS, content development services, and advertising options. In addition, we support small businesses with access to Amazon’s tools, resources, and network, helping them reach customers around the world.\n\nOur Customers\n\nWe continually seek new and better ways to serve customers, offering lower prices, more convenient services, and a larger selection of more sustainable products. We also help customers advance their businesses and enable digital transformation through AWS, content development services, and advertising options. In addition, we support small businesses with access to Amazon’s tools, resources, and network, helping them reach customers around the world.\n\nOur Reporting Topics\n\nWe include a number of topics in our reporting. We view these topics as interconnected and recognize that our progress in one area can often help address challenges in another.", "doc_id": "amazon2023", "page": 5, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Mission\n\nTo make customers' lives better and easier every day.\n\nOur Business\n\nWe are committed to addressing sustainability at every stage of our value chain.\n\nOur Operations\n\nWe offer products and services—both Amazon-branded and from many other brands and third-party sellers—in our Amazon stores, leveraging advanced transportation logistics to deliver globally. We engage suppliers from a complex supplier network. We seek to be a good neighbor wherever we operate and to support local people and charitable organizations that meet on-the-ground needs. In particular, we leverage our scale, resources, and expertise to address issues where we can have the greatest impact—namely affordable housing, education, disaster relief, and food security.\n\nOur Supply Chain\n\nWe procure materials, commodities, components, finished goods, and services from a complex supplier network. We engage suppliers globally to align our expectations for respecting human rights; maintaining safe, inclusive workplaces; and promoting sustainable practices.\n\nOur Employees\n\nThe approximately 1.5 million people in Amazon’s global workforce are Amazon-branded and from many other brands and third-party sellers—in our Amazon stores, leveraging advanced transportation logistics to deliver globally. We engage suppliers from a complex supplier network. We seek to be a good neighbor wherever we operate and to support local people and charitable organizations that meet on-the-ground needs. In particular, we leverage our scale, resources, and expertise to address issues where we can have the greatest impact—namely affordable housing, education, disaster relief, and food security.\n\nOur Communities\n\nWe continually seek new and better ways to serve customers, offering lower prices, more convenient services, and a larger selection of more sustainable products. We also help customers advance their businesses and enable digital transformation through AWS, content development services, and advertising options. In addition, we support small businesses with access to Amazon’s tools, resources, and network, helping them reach customers around the world.\n\nOur Customers\n\nWe continually seek new and better ways to serve customers, offering lower prices, more convenient services, and a larger selection of more sustainable products. We also help customers advance their businesses and enable digital transformation through AWS, content development services, and advertising options. In addition, we support small businesses with access to Amazon’s tools, resources, and network, helping them reach customers around the world.\n\nOur Reporting Topics\n\nWe include a number of topics in our reporting. We view these topics as interconnected and recognize that our progress in one area can often help address challenges in another.", "original_types": ["text", "header"], "id": 334}
{"type": "section", "content": "Goals Summary\n\nMaking progress\n\nAchieved\n\nDid not meet", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Goals Summary\n\nMaking progress\n\nAchieved\n\nDid not meet", "original_types": ["text", "header"], "id": 335}
{"type": "table", "content": "Goal\nStatus", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Goal\nStatus", "id": 336}
{"type": "table", "content": "Carbon\nMaking progress", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Carbon\nMaking progress", "id": 337}
{"type": "table", "content": "Employee Experience\nAchieved", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employee Experience\nAchieved", "id": 338}
{"type": "table", "content": "Inclusive Experiences\nOn track to be completed in 2024", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Inclusive Experiences\nOn track to be completed in 2024", "id": 339}
{"type": "table", "content": "At least 100,000 electric delivery vans on the road by 2030, from Rivian and other manufacturers\nOn track to be completed in 2024", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "At least 100,000 electric delivery vans on the road by 2030, from Rivian and other manufacturers\nOn track to be completed in 2024", "id": 340}
{"type": "table", "content": "Deploy 10,000 electric vehicles (EVs) in India by 2025\nAchieved", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Deploy 10,000 electric vehicles (EVs) in India by 2025\nAchieved", "id": 341}
{"type": "table", "content": "Carbon-Free Energy\nAchieved", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Carbon-Free Energy\nAchieved", "id": 342}
{"type": "table", "content": "Community Impact\nInvest $2 billion to create and preserve more than 20,000 affordable homes through 2025", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Community Impact\nInvest $2 billion to create and preserve more than 20,000 affordable homes through 2025", "id": 343}
{"type": "table", "content": "Waste and Circularity\n75% reduction in food waste intensity in Europe operations and 28% in U.S. operations compared to a 2021 baseline§", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Waste and Circularity\n75% reduction in food waste intensity in Europe operations and 28% in U.S. operations compared to a 2021 baseline§", "id": 344}
{"type": "table", "content": "Water\nProvide free artificial intelligence (AI) skills training to 2 million people globally by 2025", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Water\nProvide free artificial intelligence (AI) skills training to 2 million people globally by 2025", "id": 345}
{"type": "table", "content": "Packaging\nAchieved", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Packaging\nAchieved", "id": 346}
{"type": "section", "content": "* Carbon dioxide equivalent.\n\n** In 2022, we reported progress for the Career Choice program in the U.S. In 2023, we expanded our reporting to include all in-scope upskilling programs in the U.S.\n\n† To understand what this goal should encompass, we model and measure the energy consumed by our devices in different types of use, then project their total average global annual electricity consumption.\n\n‡‡ Goal achieved in January 2024. Progress from July 2021 through January 2024.\n\n§§ In January 2024, AWS announced an additional $20 million in funding for the Health Equity Initiative, bringing the company's total commitment to $60 million in cloud credits.\n\n# Water positive means AWS is still working to meet the water positive goal.", "doc_id": "amazon2023", "page": 6, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "* Carbon dioxide equivalent.\n\n** In 2022, we reported progress for the Career Choice program in the U.S. In 2023, we expanded our reporting to include all in-scope upskilling programs in the U.S.\n\n† To understand what this goal should encompass, we model and measure the energy consumed by our devices in different types of use, then project their total average global annual electricity consumption.\n\n‡‡ Goal achieved in January 2024. Progress from July 2021 through January 2024.\n\n§§ In January 2024, AWS announced an additional $20 million in funding for the Health Equity Initiative, bringing the company's total commitment to $60 million in cloud credits.\n\n# Water positive means AWS is still working to meet the water positive goal.", "original_types": ["text"], "id": 347}
{"type": "section", "content": "2023 Year in Review\n\nAs we reflect on 2023, we are proud of the progress we made. We worked hard to reduce our environmental footprint, drive progress throughout our value chain, and create a safer, more inclusive place for people to work.", "doc_id": "amazon2023", "page": 7, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Year in Review\n\nAs we reflect on 2023, we are proud of the progress we made. We worked hard to reduce our environmental footprint, drive progress throughout our value chain, and create a safer, more inclusive place for people to work.", "original_types": ["text", "header"], "id": 348}
{"type": "table", "content": "Environment\n3% Reduction in absolute carbon emissions\n100% Of electricity consumed by Amazon matched with renewable energy sources, up from 90% in 2022\n13% Decrease in carbon intensity\n680M Packages delivered using more than 24,000 electric delivery vehicles globally", "doc_id": "amazon2023", "page": 7, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Environment\n3% Reduction in absolute carbon emissions\n100% Of electricity consumed by Amazon matched with renewable energy sources, up from 90% in 2022\n13% Decrease in carbon intensity\n680M Packages delivered using more than 24,000 electric delivery vehicles globally", "id": 349}
{"type": "table", "content": "Value Chain\n3K Assessments of suppliers of Amazon-branded products on their social and environmental performance\n77 New signatories of The Climate Pledge, bringing the total to 473\n1.16B Items sold that are recognized by certifications in our Climate Pledge Friendly program, a 42% increase from 2022\n75% Reduction in food waste intensity—a measure of food waste as a percentage of total food handled by weight—in Europe operations and 28% reduction in U.S. operations compared to a 2021 baseline\n$20M Catalyzed by founding members of U.S. Agency for International Development (USAID)'s Climate Gender Equity Fund, a public-private partnership that leverages funding to scale climate finance that advances gender-equitable climate action", "doc_id": "amazon2023", "page": 7, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Value Chain\n3K Assessments of suppliers of Amazon-branded products on their social and environmental performance\n77 New signatories of The Climate Pledge, bringing the total to 473\n1.16B Items sold that are recognized by certifications in our Climate Pledge Friendly program, a 42% increase from 2022\n75% Reduction in food waste intensity—a measure of food waste as a percentage of total food handled by weight—in Europe operations and 28% reduction in U.S. operations compared to a 2021 baseline\n$20M Catalyzed by founding members of U.S. Agency for International Development (USAID)'s Climate Gender Equity Fund, a public-private partnership that leverages funding to scale climate finance that advances gender-equitable climate action", "id": 350}
{"type": "table", "content": "People\n358K+ U.S. employees have participated in upskilling programs since we announced our Upskilling Pledge in 2019\n20K Military veterans and military spouses hired in 2023, totaling over 100,000 hired through January 2024\n$1.3B Invested toward pay increases for customer fulfillment and transportation employees in the U.S., bringing the average pay for those roles to over $20.50 per hour\n$16.8M In cloud computing credits distributed to 125 organizations globally to promote equal access to health resources, totaling more than $32 million distributed to 229 organizations since 2021\n12% Of the way toward meeting our AWS water positive goal to return more water to the communities where AWS operates than is used in direct operations\n41% In cloud computing credits distributed to 125 organizations globally to promote equal access to health resources, totaling more than $32 million distributed to 229 organizations since 2021\n60% Improvement in global Lost Time Incident Rate in 2023 versus 2019\nNearly 16.5K Ukrainians globally, including refugees, received training through the AWS program ITSkills4U by the end of 2023", "doc_id": "amazon2023", "page": 7, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "People\n358K+ U.S. employees have participated in upskilling programs since we announced our Upskilling Pledge in 2019\n20K Military veterans and military spouses hired in 2023, totaling over 100,000 hired through January 2024\n$1.3B Invested toward pay increases for customer fulfillment and transportation employees in the U.S., bringing the average pay for those roles to over $20.50 per hour\n$16.8M In cloud computing credits distributed to 125 organizations globally to promote equal access to health resources, totaling more than $32 million distributed to 229 organizations since 2021\n12% Of the way toward meeting our AWS water positive goal to return more water to the communities where AWS operates than is used in direct operations\n41% In cloud computing credits distributed to 125 organizations globally to promote equal access to health resources, totaling more than $32 million distributed to 229 organizations since 2021\n60% Improvement in global Lost Time Incident Rate in 2023 versus 2019\nNearly 16.5K Ukrainians globally, including refugees, received training through the AWS program ITSkills4U by the end of 2023", "id": 351}
{"type": "section", "content": "Environment\n\nAt Amazon, we combine data and science with passion and invention to drive everything we do. We are committed to and invested in sustainability because it’s a win all around—it’s good for the planet, for business, for our customers, and for our communities. We set big goals and work backward to achieve them. We are working to innovate and scale solutions that minimize our emissions, waste, and water usage; increase our use of carbon-free energy; and pioneer new approaches for packaging, materials, and products.", "doc_id": "amazon2023", "page": 8, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Environment\n\nAt Amazon, we combine data and science with passion and invention to drive everything we do. We are committed to and invested in sustainability because it’s a win all around—it’s good for the planet, for business, for our customers, and for our communities. We set big goals and work backward to achieve them. We are working to innovate and scale solutions that minimize our emissions, waste, and water usage; increase our use of carbon-free energy; and pioneer new approaches for packaging, materials, and products.", "original_types": ["text", "header"], "id": 352}
{"type": "section", "content": "Carbon\n\nThe science is clear. Significant carbon emission reductions are required to avoid the most severe effects of climate change, restore biodiversity, protect vulnerable communities, and ensure a habitable planet for future generations. Climate change also has the potential to disrupt global supply chains and change the ways businesses operate today. We have an opportunity—and responsibility—to use our size, scale, and resources to do our part to solve global challenges. In 2019, we co-founded and committed to The Climate Pledge—our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement. We are continually working to reduce emissions throughout our business, as well as partnering across our supply chain and the industries in which we operate to share and scale what we’ve learned.", "doc_id": "amazon2023", "page": 9, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Carbon\n\nThe science is clear. Significant carbon emission reductions are required to avoid the most severe effects of climate change, restore biodiversity, protect vulnerable communities, and ensure a habitable planet for future generations. Climate change also has the potential to disrupt global supply chains and change the ways businesses operate today. We have an opportunity—and responsibility—to use our size, scale, and resources to do our part to solve global challenges. In 2019, we co-founded and committed to The Climate Pledge—our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement. We are continually working to reduce emissions throughout our business, as well as partnering across our supply chain and the industries in which we operate to share and scale what we’ve learned.", "original_types": ["text", "header"], "id": 353}
{"type": "section", "content": "Our Approach\n\nAt Amazon, we think long term, take on grand challenges, and invent solutions to complex problems. These are some of the reasons we co-founded and committed to The Climate Pledge in 2019—our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement—and have invited hundreds of companies to join us.\n\nAmazon has a variety of businesses touching many sectors, some of which rely on carbon-intensive industries to provide critical goods and services to our customers. However, we believe the complexity of our business puts us in a unique position to be a leader in decarbonization strategies. We have an opportunity to demonstrate how achieving net-zero carbon emissions is possible across many sectors, while creating solutions that benefit our business as well as the industries in which we operate.\n\nFoundationally, our strategy relies on embedding decarbonization initiatives and efficiency improvements across our business. Our comprehensive approach to reducing and avoiding carbon emissions focuses on key sectors of our business, including delivery and logistics; building construction and operations; servers and hardware; grocery, products, and devices; and packaging. Teams across Amazon are accountable for setting decarbonization plans that map back to Amazon’s worldwide strategy because ownership and accountability are critical to operational success and managing complexity. We focus on four crosscutting initiatives to decarbonize our business:\n\nWe select lower-carbon alternatives, such as lower-carbon concrete and steel in construction, and lower-emission fuels and vehicles in transportation. We use these alternatives where possible, based on a number of factors including cost, emissions reduction potential, and availability.\n\nWe’re transitioning toward carbon-free electricity, investing in renewables—rooftop solar installations on our buildings, and new, utility-scale wind and solar projects—as well as other carbon-free electricity sources, such as nuclear.\n\nWe engage with suppliers to help reduce emissions from activities beyond our direct operations. We encourage them to set credible decarbonization goals, publicly share progress, and implement carbon reduction strategies throughout their operations and supply chains—and we are providing support to help our supply chain take action.\n\nIn addition to decarbonizing our own business, we are helping drive progress across industries. To do this, we focus on three accelerators:\n\nWe invest in breakthrough technology by adopting ready-to-scale solutions, as well as evaluating and investing in emerging technologies that can help address emissions from hard-to-abate sectors including aviation, shipping, and building construction. Through direct funding, we aim to advance our own progress toward net-zero carbon emissions and help accelerate the widespread adoption of new technologies by making them more affordable and accessible.", "doc_id": "amazon2023", "page": 10, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nAt Amazon, we think long term, take on grand challenges, and invent solutions to complex problems. These are some of the reasons we co-founded and committed to The Climate Pledge in 2019—our goal to reach net-zero carbon emissions by 2040, 10 years ahead of the Paris Agreement—and have invited hundreds of companies to join us.\n\nAmazon has a variety of businesses touching many sectors, some of which rely on carbon-intensive industries to provide critical goods and services to our customers. However, we believe the complexity of our business puts us in a unique position to be a leader in decarbonization strategies. We have an opportunity to demonstrate how achieving net-zero carbon emissions is possible across many sectors, while creating solutions that benefit our business as well as the industries in which we operate.\n\nFoundationally, our strategy relies on embedding decarbonization initiatives and efficiency improvements across our business. Our comprehensive approach to reducing and avoiding carbon emissions focuses on key sectors of our business, including delivery and logistics; building construction and operations; servers and hardware; grocery, products, and devices; and packaging. Teams across Amazon are accountable for setting decarbonization plans that map back to Amazon’s worldwide strategy because ownership and accountability are critical to operational success and managing complexity. We focus on four crosscutting initiatives to decarbonize our business:\n\nWe select lower-carbon alternatives, such as lower-carbon concrete and steel in construction, and lower-emission fuels and vehicles in transportation. We use these alternatives where possible, based on a number of factors including cost, emissions reduction potential, and availability.\n\nWe’re transitioning toward carbon-free electricity, investing in renewables—rooftop solar installations on our buildings, and new, utility-scale wind and solar projects—as well as other carbon-free electricity sources, such as nuclear.\n\nWe engage with suppliers to help reduce emissions from activities beyond our direct operations. We encourage them to set credible decarbonization goals, publicly share progress, and implement carbon reduction strategies throughout their operations and supply chains—and we are providing support to help our supply chain take action.\n\nIn addition to decarbonizing our own business, we are helping drive progress across industries. To do this, we focus on three accelerators:\n\nWe invest in breakthrough technology by adopting ready-to-scale solutions, as well as evaluating and investing in emerging technologies that can help address emissions from hard-to-abate sectors including aviation, shipping, and building construction. Through direct funding, we aim to advance our own progress toward net-zero carbon emissions and help accelerate the widespread adoption of new technologies by making them more affordable and accessible.", "original_types": ["text", "header"], "id": 354}
{"type": "section", "content": "We support policies that drive decarbonization. Amazon works with policymakers, governments, nongovernmental organizations (NGOs), industry associations, coalitions, and other partners on numerous regulatory and policy issues. We seek to advance and incentivize decarbonization by supporting policies that scale lower-emission fuels, drive lower-emission vehicle deployment and infrastructure, advance the transition toward carbon-free energy, modernize the grid, and accelerate investments in clean technologies.\n\nWe catalyze industry action. Through The Climate Pledge, Amazon brings together companies from around the world to drive collective action, cross-sector collaboration, and engagement in initiatives that encourage industry action toward decarbonization.\n\nIn parallel to reducing and avoiding emissions throughout our business, we are also investing in carbon neutralization through additional, quantifiable, real, permanent, and socially beneficial offsets. As part of this effort, we are engaging in science-led collaborations to build credible neutralization initiatives that can be deployed at scale in the future.", "doc_id": "amazon2023", "page": 10, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We support policies that drive decarbonization. Amazon works with policymakers, governments, nongovernmental organizations (NGOs), industry associations, coalitions, and other partners on numerous regulatory and policy issues. We seek to advance and incentivize decarbonization by supporting policies that scale lower-emission fuels, drive lower-emission vehicle deployment and infrastructure, advance the transition toward carbon-free energy, modernize the grid, and accelerate investments in clean technologies.\n\nWe catalyze industry action. Through The Climate Pledge, Amazon brings together companies from around the world to drive collective action, cross-sector collaboration, and engagement in initiatives that encourage industry action toward decarbonization.\n\nIn parallel to reducing and avoiding emissions throughout our business, we are also investing in carbon neutralization through additional, quantifiable, real, permanent, and socially beneficial offsets. As part of this effort, we are engaging in science-led collaborations to build credible neutralization initiatives that can be deployed at scale in the future.", "original_types": ["text"], "id": 355}
{"type": "section", "content": "Our Progress\n\nIn 2023, our absolute carbon emissions decreased by 3%.2 This overall decrease was driven by an 11% reduction in emissions from electricity (Scope 2) and a 5% decrease in indirect and supply chain emissions (Scope 3). We had a 7% increase in emissions from our direct operations (Scope 1), primarily from the use of transportation fuels. Our carbon intensity decreased for the fifth consecutive year, down 13% from 2022 to 2023.3 This metric demonstrates how we are working to decouple emissions growth from business growth. Every year, we aim to serve our customers better, more quickly, and with fewer emissions, but we know our progress may not be linear as our business continues to grow. In 2023, we invested in carbon abatement projects across Amazon. We continue to invent, think long term, and place big bets to accelerate decarbonization efforts year over year. In addition to our direct investment and work to decarbonize our business, we also worked with organizations throughout our supply chain and broader industry to reduce and avoid emissions and create solutions to help decarbonize our value chain. Annually, we also improve our science and data-driven approach to track and measure decarbonization across Amazon.", "doc_id": "amazon2023", "page": 11, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Progress\n\nIn 2023, our absolute carbon emissions decreased by 3%.2 This overall decrease was driven by an 11% reduction in emissions from electricity (Scope 2) and a 5% decrease in indirect and supply chain emissions (Scope 3). We had a 7% increase in emissions from our direct operations (Scope 1), primarily from the use of transportation fuels. Our carbon intensity decreased for the fifth consecutive year, down 13% from 2022 to 2023.3 This metric demonstrates how we are working to decouple emissions growth from business growth. Every year, we aim to serve our customers better, more quickly, and with fewer emissions, but we know our progress may not be linear as our business continues to grow. In 2023, we invested in carbon abatement projects across Amazon. We continue to invent, think long term, and place big bets to accelerate decarbonization efforts year over year. In addition to our direct investment and work to decarbonize our business, we also worked with organizations throughout our supply chain and broader industry to reduce and avoid emissions and create solutions to help decarbonize our value chain. Annually, we also improve our science and data-driven approach to track and measure decarbonization across Amazon.", "original_types": ["text", "header"], "id": 356}
{"type": "table", "content": "Amazon’s Carbon Footprint\nCarbon Intensity (grams of CO2e per $ of gross merchandise sales) | 2019 | 2020 | 2021 | 2022 | 2023 | YoY% | Carbon Intensity (grams of CO2e per $ of gross merchandise sales) | 122.8 | 102.7 | 100.8 | 93.0 | 80.8 | -13% | Emissions Category (MMT CO2e) | Emissions from Direct Operations (Scope 1) | 5.76 | 9.62 | 12.11 | 13.32 | 14.27 | 7% | Fossil fuels | 5.57 | 9.37 | 11.89 | 12.96 | 14.00 | 8% | Refrigerants | 0.19 | 0.25 | 0.22 | 0.36 | 0.27 | -25% | Emissions from Purchased Electricity (Scope 2)* | 5.50 | 5.27 | 4.07 | 3.14 | 2.79 | -11% | Emissions from Indirect Sources (Scope 3)* | 39.91 | 45.75 | 55.36 | 54.28 | 51.76 | -5% | Corporate purchases and Amazon-branded product emissions (e.g., operating expenses, business travel, and Amazon-branded product manufacturing, use phase, and end-of-life) | 15.41 | 16.70 | 19.09 | 19.72 | 19.11 | -3% | Capital goods (e.g., building construction, servers and other hardware, equipment, vehicles) | 8.01 | 10.52 | 15.37 | 10.25 | 8.95 | -13% | Other indirect emissions (e.g., third-party transportation, packaging, upstream energy-related) | 12.44 | 15.77 | 18.00 | 20.90 | 20.07 | -4% | Lifecycle emissions from customer trips to Amazon’s physical stores | 4.05 | 2.77 | 2.91 | 3.41 | 3.63 | 7% | Amazon’s Carbon Footprint | 51.17 | 60.64 | 71.54 | 70.74 | 68.82 | -3% | Greenhouse Gas Protocol Aligned Scope 3 Categories | 2022 | 2023 | Purchased Goods and Services (Amazon corporate purchases made for Amazon’s operations and services, Amazon-branded products) | 20.60 | 19.86 | Capital Goods | 10.25 | 8.95 | Fuel- and Energy-Related Activities | 4.76 | 4.97 | Upstream Transportation and Distribution | 10.65 | 9.30 | Business Travel | 0.61 | 0.63 | Employee Commuting | 2.78 | 2.88 | Downstream Transportation and Distribution | 3.41 | 3.63 | Use of Sold Products (Amazon Devices) | 1.18 | 1.50 | End-of-Life Treatment of Sold Products (Amazon Devices) | 0.04 | 0.04", "doc_id": "amazon2023", "page": 11, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon’s Carbon Footprint\nCarbon Intensity (grams of CO2e per $ of gross merchandise sales) | 2019 | 2020 | 2021 | 2022 | 2023 | YoY% | Carbon Intensity (grams of CO2e per $ of gross merchandise sales) | 122.8 | 102.7 | 100.8 | 93.0 | 80.8 | -13% | Emissions Category (MMT CO2e) | Emissions from Direct Operations (Scope 1) | 5.76 | 9.62 | 12.11 | 13.32 | 14.27 | 7% | Fossil fuels | 5.57 | 9.37 | 11.89 | 12.96 | 14.00 | 8% | Refrigerants | 0.19 | 0.25 | 0.22 | 0.36 | 0.27 | -25% | Emissions from Purchased Electricity (Scope 2)* | 5.50 | 5.27 | 4.07 | 3.14 | 2.79 | -11% | Emissions from Indirect Sources (Scope 3)* | 39.91 | 45.75 | 55.36 | 54.28 | 51.76 | -5% | Corporate purchases and Amazon-branded product emissions (e.g., operating expenses, business travel, and Amazon-branded product manufacturing, use phase, and end-of-life) | 15.41 | 16.70 | 19.09 | 19.72 | 19.11 | -3% | Capital goods (e.g., building construction, servers and other hardware, equipment, vehicles) | 8.01 | 10.52 | 15.37 | 10.25 | 8.95 | -13% | Other indirect emissions (e.g., third-party transportation, packaging, upstream energy-related) | 12.44 | 15.77 | 18.00 | 20.90 | 20.07 | -4% | Lifecycle emissions from customer trips to Amazon’s physical stores | 4.05 | 2.77 | 2.91 | 3.41 | 3.63 | 7% | Amazon’s Carbon Footprint | 51.17 | 60.64 | 71.54 | 70.74 | 68.82 | -3% | Greenhouse Gas Protocol Aligned Scope 3 Categories | 2022 | 2023 | Purchased Goods and Services (Amazon corporate purchases made for Amazon’s operations and services, Amazon-branded products) | 20.60 | 19.86 | Capital Goods | 10.25 | 8.95 | Fuel- and Energy-Related Activities | 4.76 | 4.97 | Upstream Transportation and Distribution | 10.65 | 9.30 | Business Travel | 0.61 | 0.63 | Employee Commuting | 2.78 | 2.88 | Downstream Transportation and Distribution | 3.41 | 3.63 | Use of Sold Products (Amazon Devices) | 1.18 | 1.50 | End-of-Life Treatment of Sold Products (Amazon Devices) | 0.04 | 0.04", "id": 357}
{"type": "section", "content": "Amazon’s Carbon Footprint\n\nIn 2023, our absolute carbon emissions decreased by 3%.2 This overall decrease was driven by an 11% reduction in emissions from electricity (Scope 2) and a 5% decrease in indirect and supply chain emissions (Scope 3). We had a 7% increase in emissions from our direct operations (Scope 1), primarily from the use of transportation fuels. Our carbon intensity decreased for the fifth consecutive year, down 13% from 2022 to 2023.3 This metric demonstrates how we are working to decouple emissions growth from business growth. Every year, we aim to serve our customers better, more quickly, and with fewer emissions, but we know our progress may not be linear as our business continues to grow. In 2023, we invested in carbon abatement projects across Amazon. We continue to invent, think long term, and place big bets to accelerate decarbonization efforts year over year. In addition to our direct investment and work to decarbonize our business, we also worked with organizations throughout our supply chain and broader industry to reduce and avoid emissions and create solutions to help decarbonize our value chain. Annually, we also improve our science and data-driven approach to track and measure decarbonization across Amazon.", "doc_id": "amazon2023", "page": 11, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon’s Carbon Footprint\n\nIn 2023, our absolute carbon emissions decreased by 3%.2 This overall decrease was driven by an 11% reduction in emissions from electricity (Scope 2) and a 5% decrease in indirect and supply chain emissions (Scope 3). We had a 7% increase in emissions from our direct operations (Scope 1), primarily from the use of transportation fuels. Our carbon intensity decreased for the fifth consecutive year, down 13% from 2022 to 2023.3 This metric demonstrates how we are working to decouple emissions growth from business growth. Every year, we aim to serve our customers better, more quickly, and with fewer emissions, but we know our progress may not be linear as our business continues to grow. In 2023, we invested in carbon abatement projects across Amazon. We continue to invent, think long term, and place big bets to accelerate decarbonization efforts year over year. In addition to our direct investment and work to decarbonize our business, we also worked with organizations throughout our supply chain and broader industry to reduce and avoid emissions and create solutions to help decarbonize our value chain. Annually, we also improve our science and data-driven approach to track and measure decarbonization across Amazon.", "original_types": ["text", "header"], "id": 358}
{"type": "section", "content": "Overview\n\nEnvironment\n\nValue Chain\n\nPeople\n\nAppendix\n\n2023 Amazon Sustainability Report\n\nCarbon\n\nCarbon-Free Energy\n\nPackaging\n\nWaste and Circularity\n\nWater\n\nIn 2023, our net sales grew 12% and more than two-thirds of Amazon packages were delivered via Amazon's own logistics network. We decreased emissions per package through operational efficiencies, such as improving truck fill rates (the percentage of truck volume that is utilized), shipping products in their own packaging without additional Amazon packaging, and using artificial intelligence (AI) to optimize packaging types. For example, Amazon data scientists trained an AI model to understand a variety of product attributes, including an item's shape and durability, and to analyze customer feedback on how different packaging options have performed. The model is constantly learning and has helped reduce our use of packaging material since it launched in 2019.\n\nWe strive to keep our packaging lightweight and minimal, while ensuring products reach customers without damage. Lighter, more flexible, and right-sized packaging helps reduce delivery emissions per package by using less material and taking up less space in delivery vehicles. Since 2015, we have reduced the average per-shipment packaging weight by 43% and avoided more than 3 million metric tons of packaging, including more than 446,000 metric tons in 2023 alone.4 Globally, we shipped 12% of products in their own packaging in 2023. This provides a better customer experience by minimizing the packaging materials used for delivery, and avoids incremental carbon emissions associated with additional materials and weight.\n\nLearn more about how we're improving packaging\n\nAs the number of products we deliver has continued to increase, we aim to keep improving the efficiency of the routes our trucks drive. For example, to get packages to customers faster and with fewer emissions, we reorganized our U.S. transportation network from one national network to eight strategic regions in 2023. Regionalization helped us avoid driving nearly 16 million miles last year. We also prioritized shipping products by lower-carbon train and sea routes—instead of trucking—for middle mile deliveries in Europe. We are excited to invest in technologies today that will help reduce our footprint in the future, such as scaling up our use of electric vehicles (EVs) and other lower-carbon vehicles to decrease our Scope 1 emissions.\n\nScope 2: Indirect Emissions from Purchased Electricity", "doc_id": "amazon2023", "page": 12, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Overview\n\nEnvironment\n\nValue Chain\n\nPeople\n\nAppendix\n\n2023 Amazon Sustainability Report\n\nCarbon\n\nCarbon-Free Energy\n\nPackaging\n\nWaste and Circularity\n\nWater\n\nIn 2023, our net sales grew 12% and more than two-thirds of Amazon packages were delivered via Amazon's own logistics network. We decreased emissions per package through operational efficiencies, such as improving truck fill rates (the percentage of truck volume that is utilized), shipping products in their own packaging without additional Amazon packaging, and using artificial intelligence (AI) to optimize packaging types. For example, Amazon data scientists trained an AI model to understand a variety of product attributes, including an item's shape and durability, and to analyze customer feedback on how different packaging options have performed. The model is constantly learning and has helped reduce our use of packaging material since it launched in 2019.\n\nWe strive to keep our packaging lightweight and minimal, while ensuring products reach customers without damage. Lighter, more flexible, and right-sized packaging helps reduce delivery emissions per package by using less material and taking up less space in delivery vehicles. Since 2015, we have reduced the average per-shipment packaging weight by 43% and avoided more than 3 million metric tons of packaging, including more than 446,000 metric tons in 2023 alone.4 Globally, we shipped 12% of products in their own packaging in 2023. This provides a better customer experience by minimizing the packaging materials used for delivery, and avoids incremental carbon emissions associated with additional materials and weight.\n\nLearn more about how we're improving packaging\n\nAs the number of products we deliver has continued to increase, we aim to keep improving the efficiency of the routes our trucks drive. For example, to get packages to customers faster and with fewer emissions, we reorganized our U.S. transportation network from one national network to eight strategic regions in 2023. Regionalization helped us avoid driving nearly 16 million miles last year. We also prioritized shipping products by lower-carbon train and sea routes—instead of trucking—for middle mile deliveries in Europe. We are excited to invest in technologies today that will help reduce our footprint in the future, such as scaling up our use of electric vehicles (EVs) and other lower-carbon vehicles to decrease our Scope 1 emissions.\n\nScope 2: Indirect Emissions from Purchased Electricity", "original_types": ["text", "header"], "id": 359}
{"type": "section", "content": "Our Scope 2 emissions are from electricity used to power Amazon's buildings, including data centers, office buildings, fulfillment centers, and grocery stores, and to charge EVs at our facilities. In 2023, our Scope 2 emissions decreased by 11% compared to 2022 and represented 4% of our total carbon footprint. This decrease resulted from our increased use of electricity from renewable sources, such as wind and onsite solar, as well as from purchasing additional environmental attributes (such as renewable energy credits) including more than 446,000 metric tons of packaging, and avoided more than 3 million metric tons of packaging, including more than 446,000 metric tons in 2023 alone.4 Globally, we shipped 12% of products in their own packaging in 2023. This provides a better customer experience by minimizing the packaging materials used for delivery, and avoids incremental carbon emissions associated with additional materials and weight.\n\nLearn more about our transition to carbon-free energy", "doc_id": "amazon2023", "page": 12, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Scope 2 emissions are from electricity used to power Amazon's buildings, including data centers, office buildings, fulfillment centers, and grocery stores, and to charge EVs at our facilities. In 2023, our Scope 2 emissions decreased by 11% compared to 2022 and represented 4% of our total carbon footprint. This decrease resulted from our increased use of electricity from renewable sources, such as wind and onsite solar, as well as from purchasing additional environmental attributes (such as renewable energy credits) including more than 446,000 metric tons of packaging, and avoided more than 3 million metric tons of packaging, including more than 446,000 metric tons in 2023 alone.4 Globally, we shipped 12% of products in their own packaging in 2023. This provides a better customer experience by minimizing the packaging materials used for delivery, and avoids incremental carbon emissions associated with additional materials and weight.\n\nLearn more about our transition to carbon-free energy", "original_types": ["text"], "id": 360}
{"type": "figure", "content": "Amazon's Carbon Footprint (MMT CO2e*)", "doc_id": "amazon2023", "page": 12, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon's Carbon Footprint (MMT CO2e*)", "id": 361}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 12, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 362}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 12, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 363}
{"type": "section", "content": "Scope 3: Indirect Emissions from Other Sources\n\nScope 3 emissions include emissions from activities that take place beyond our direct operations, including building construction, third-party transportation, and the production of Amazon-branded products and the materials and components used in those products. In 2023, our Scope 3 emissions decreased by 5% from 2022 and represented 75% of our total carbon footprint. This decrease resulted from reductions related to building construction, leased buildings and equipment, and third-party transportation, as more goods were shipped by Amazon’s own logistics providers versus third-party providers than in 2022.\n\nBuilding construction is a significant driver of carbon emissions in many supply chains due to the associated embodied carbon that is emitted. Embodied carbon includes any carbon emissions created during the manufacturing of building materials, the transport of those materials to the job site, and the construction practices used. Embodied carbon is counted in a company’s carbon footprint the year the building is completed and operational. We aim to reduce embodied carbon in building construction by using lower-emission concrete, lower-emission steel, and mass timber. In 2023, 29 Amazon building projects were constructed with lower-carbon concrete and steel, and collectively reduced embodied carbon by 79,500 metric tons of CO2e, equivalent to the emissions generated by 17,200 cars driven for a year.\n\nBecause Scope 3 emissions are beyond our direct operational control, the efforts our suppliers take to reduce their emissions help us progress toward our ambition to achieve net-zero carbon emissions by 2040. We will prioritize working with suppliers who are also committed to decarbonization and reaching net-zero carbon emissions. We have identified a list of the highest-emitting suppliers directly supporting our operations, and expect those suppliers, who collectively contribute more than 50% of emissions globally to Amazon’s Scope 3 footprint, to provide a plan for how they will decarbonize their operations and demonstrate real progress over time. We will prioritize our business toward those who provide their plans and results on their path to net-zero carbon emissions. We are already working with many of these suppliers, and will continue our engagement and share learnings. In addition, we also launched our “Amazon Sustainability Exchange”—a free, publicly available website that democratizes our guidelines, playbooks, science models, and other resources to help other companies make meaningful progress toward net-zero carbon emissions.\n\nLearn more about how we’re engaging suppliers to decarbonize our supply chain", "doc_id": "amazon2023", "page": 13, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Scope 3: Indirect Emissions from Other Sources\n\nScope 3 emissions include emissions from activities that take place beyond our direct operations, including building construction, third-party transportation, and the production of Amazon-branded products and the materials and components used in those products. In 2023, our Scope 3 emissions decreased by 5% from 2022 and represented 75% of our total carbon footprint. This decrease resulted from reductions related to building construction, leased buildings and equipment, and third-party transportation, as more goods were shipped by Amazon’s own logistics providers versus third-party providers than in 2022.\n\nBuilding construction is a significant driver of carbon emissions in many supply chains due to the associated embodied carbon that is emitted. Embodied carbon includes any carbon emissions created during the manufacturing of building materials, the transport of those materials to the job site, and the construction practices used. Embodied carbon is counted in a company’s carbon footprint the year the building is completed and operational. We aim to reduce embodied carbon in building construction by using lower-emission concrete, lower-emission steel, and mass timber. In 2023, 29 Amazon building projects were constructed with lower-carbon concrete and steel, and collectively reduced embodied carbon by 79,500 metric tons of CO2e, equivalent to the emissions generated by 17,200 cars driven for a year.\n\nBecause Scope 3 emissions are beyond our direct operational control, the efforts our suppliers take to reduce their emissions help us progress toward our ambition to achieve net-zero carbon emissions by 2040. We will prioritize working with suppliers who are also committed to decarbonization and reaching net-zero carbon emissions. We have identified a list of the highest-emitting suppliers directly supporting our operations, and expect those suppliers, who collectively contribute more than 50% of emissions globally to Amazon’s Scope 3 footprint, to provide a plan for how they will decarbonize their operations and demonstrate real progress over time. We will prioritize our business toward those who provide their plans and results on their path to net-zero carbon emissions. We are already working with many of these suppliers, and will continue our engagement and share learnings. In addition, we also launched our “Amazon Sustainability Exchange”—a free, publicly available website that democratizes our guidelines, playbooks, science models, and other resources to help other companies make meaningful progress toward net-zero carbon emissions.\n\nLearn more about how we’re engaging suppliers to decarbonize our supply chain", "original_types": ["text", "header"], "id": 364}
{"type": "figure", "content": "Key Milestones on Our Net-Zero Carbon Journey", "doc_id": "amazon2023", "page": 13, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Key Milestones on Our Net-Zero Carbon Journey", "id": 365}
{"type": "section", "content": "100% of the electricity consumed by our global operations was matched with renewable energy—seven years ahead of our original 2030 goal\n\nAmazon co-founded The Climate Pledge with Global Optimism and was the first company to sign on\n\nRight Now Climate Fund A $100 million fund for nature-based solutions to restore and conserve forests, wetlands, and grasslands around the world\n\nPath to 2040\n\nWe know the path to net-zero carbon by 2040 will be challenging, but we are making investments, creating new ways of working, and inventing new solutions to help us decarbonize now and in the future. At Amazon, we think long term, and we’re committed to working collectively with our supply chain and industry partners to create and scale new decarbonization solutions.\n\nWe’re proud of the work we do, not just within our own operations but across the many industries of which we are a part. At the end of 2023, The Climate Pledge included 473 signatories focused on achieving net-zero carbon emissions by 2040. Signatories are working together more than ever before, with five new joint action projects launched in 2023. We are also investing in companies that are building breakthrough technologies and other solutions that could, longer-term, lower the overall cost of decarbonization, even in hard-to-abate sectors. One way we do this is through The Climate Pledge Fund, Amazon’s $2 billion venture investment program, which supports the advancement of sustainability-focused technologies and services that will enable us to meet our net-zero carbon emissions goal.\n\nIn 2022, The Climate Pledge Fund committed at least $50 million to invest in female-founded and female-led climate technology companies\n\nAt least 100,000 electric delivery vans on the road by 2030, from Rivian and other manufacturers\n\nInvest in wind and solar farm capacity equal to the energy use of all active Echo, Fire TV, and Ring devices worldwide by 2025", "doc_id": "amazon2023", "page": 13, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "100% of the electricity consumed by our global operations was matched with renewable energy—seven years ahead of our original 2030 goal\n\nAmazon co-founded The Climate Pledge with Global Optimism and was the first company to sign on\n\nRight Now Climate Fund A $100 million fund for nature-based solutions to restore and conserve forests, wetlands, and grasslands around the world\n\nPath to 2040\n\nWe know the path to net-zero carbon by 2040 will be challenging, but we are making investments, creating new ways of working, and inventing new solutions to help us decarbonize now and in the future. At Amazon, we think long term, and we’re committed to working collectively with our supply chain and industry partners to create and scale new decarbonization solutions.\n\nWe’re proud of the work we do, not just within our own operations but across the many industries of which we are a part. At the end of 2023, The Climate Pledge included 473 signatories focused on achieving net-zero carbon emissions by 2040. Signatories are working together more than ever before, with five new joint action projects launched in 2023. We are also investing in companies that are building breakthrough technologies and other solutions that could, longer-term, lower the overall cost of decarbonization, even in hard-to-abate sectors. One way we do this is through The Climate Pledge Fund, Amazon’s $2 billion venture investment program, which supports the advancement of sustainability-focused technologies and services that will enable us to meet our net-zero carbon emissions goal.\n\nIn 2022, The Climate Pledge Fund committed at least $50 million to invest in female-founded and female-led climate technology companies\n\nAt least 100,000 electric delivery vans on the road by 2030, from Rivian and other manufacturers\n\nInvest in wind and solar farm capacity equal to the energy use of all active Echo, Fire TV, and Ring devices worldwide by 2025", "original_types": ["text"], "id": 366}
{"type": "section", "content": "Delivery and Logistics\n\nWe rely on a complex transportation network to get products from manufacturers and sellers to customers around the globe. Our logistics network uses different modes of transportation, including ships, planes, freight trains, trucks, vans, and bikes, across three transportation legs:\n\n• First mile is the first transportation leg, used for transporting shipments from the manufacturer or supplier to an Amazon facility. First mile includes maritime and airfreight shipping, as well as movement by truck and rail.\n• Middle mile is the intermediate transportation leg, where we move shipments between Amazon facilities. It is also called long-distance transportation. Middle mile includes commercial trucking, aviation, maritime, and intermodal sea and rail.\n• Last mile is the final transportation leg that delivers packages from Amazon to customers. It includes vans, trucks, and micromobility solutions such as e-cargo bikes, e-mopeds, and on-foot deliveries.\n\nWe have adopted a multi-pronged strategy to transport products safely, quickly, efficiently, and more sustainably. We are decarbonizing first, middle, and last mile transportation by:", "doc_id": "amazon2023", "page": 14, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Delivery and Logistics\n\nWe rely on a complex transportation network to get products from manufacturers and sellers to customers around the globe. Our logistics network uses different modes of transportation, including ships, planes, freight trains, trucks, vans, and bikes, across three transportation legs:\n\n• First mile is the first transportation leg, used for transporting shipments from the manufacturer or supplier to an Amazon facility. First mile includes maritime and airfreight shipping, as well as movement by truck and rail.\n• Middle mile is the intermediate transportation leg, where we move shipments between Amazon facilities. It is also called long-distance transportation. Middle mile includes commercial trucking, aviation, maritime, and intermodal sea and rail.\n• Last mile is the final transportation leg that delivers packages from Amazon to customers. It includes vans, trucks, and micromobility solutions such as e-cargo bikes, e-mopeds, and on-foot deliveries.\n\nWe have adopted a multi-pronged strategy to transport products safely, quickly, efficiently, and more sustainably. We are decarbonizing first, middle, and last mile transportation by:", "original_types": ["text", "header", "list"], "id": 367}
{"type": "section", "content": "• Increasing routing and fleet efficiency. In 2023, we restructured our U.S. fulfillment operations by reorganizing our national network of fulfillment centers, intermediate sort centers, last mile delivery hubs, and transportation fleet into eight regions. This shift ensures we’re producing, packaging, and shipping from facilities that are closer to the communities we serve, which reduces the complexity of our shipping network and the miles traveled to get to our customers—helping drive down both carbon emissions and shipping costs. This new model optimizes delivery speed, reduces emissions, and provides the breadth of selection that customers expect. In the fourth quarter of 2023 alone, we shipped nearly 544 million more items from in-region fulfillment centers than we did during the same period of 2022. Shipping from in-region fulfillment centers to our delivery stations also reduces the number of stops per package—avoiding nearly 16 million miles driven in 2023—and decreases our reliance on air transportation.\n• Ocean Freight Zero-emission fuel: Along with opting for more efficient shipping routes, we also prioritize lower-carbon marine biofuels. In 2021, we were a founding member of the First Movers Coalition, a global coalition of companies leveraging their purchasing power to decarbonize the world’s heavy-emitting sectors, such as ocean transportation. Through our active participation, we are supporting First Movers Coalition’s goal to use maritime ships with zero-emission fuels for at least 10% of cargo shipped internationally by 2030. In 2021, we also helped launch Cargo Owners for Scaling use of EVs and micromobility solutions. Sending demand signals and scaling availability of alternative transportation solutions is critical to accelerating industry progress. This includes charging infrastructure, electric trucks, electric delivery vehicles, e-cargo bikes, e-mopeds, and on-foot deliveries from micromobility hubs.", "doc_id": "amazon2023", "page": 14, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "• Increasing routing and fleet efficiency. In 2023, we restructured our U.S. fulfillment operations by reorganizing our national network of fulfillment centers, intermediate sort centers, last mile delivery hubs, and transportation fleet into eight regions. This shift ensures we’re producing, packaging, and shipping from facilities that are closer to the communities we serve, which reduces the complexity of our shipping network and the miles traveled to get to our customers—helping drive down both carbon emissions and shipping costs. This new model optimizes delivery speed, reduces emissions, and provides the breadth of selection that customers expect. In the fourth quarter of 2023 alone, we shipped nearly 544 million more items from in-region fulfillment centers than we did during the same period of 2022. Shipping from in-region fulfillment centers to our delivery stations also reduces the number of stops per package—avoiding nearly 16 million miles driven in 2023—and decreases our reliance on air transportation.\n• Ocean Freight Zero-emission fuel: Along with opting for more efficient shipping routes, we also prioritize lower-carbon marine biofuels. In 2021, we were a founding member of the First Movers Coalition, a global coalition of companies leveraging their purchasing power to decarbonize the world’s heavy-emitting sectors, such as ocean transportation. Through our active participation, we are supporting First Movers Coalition’s goal to use maritime ships with zero-emission fuels for at least 10% of cargo shipped internationally by 2030. In 2021, we also helped launch Cargo Owners for Scaling use of EVs and micromobility solutions. Sending demand signals and scaling availability of alternative transportation solutions is critical to accelerating industry progress. This includes charging infrastructure, electric trucks, electric delivery vehicles, e-cargo bikes, e-mopeds, and on-foot deliveries from micromobility hubs.", "original_types": ["list"], "id": 368}
{"type": "figure", "content": "Transportation Types by Delivery Stage", "doc_id": "amazon2023", "page": 14, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Transportation Types by Delivery Stage", "id": 369}
{"type": "section", "content": "Environment\n\nZero Emission Vessels (coZEV) with the Aspen Institute to support initiatives that increase the availability of zero-emission technologies and fuels while gaining support from shipping lines, cargo owners, ports, and other organizations that can help to enable the energy transition. Through our membership, we are supporting coZEV's goal to use maritime ships with zero-emission fuels for 100% of ocean cargo by 2040.\n\nBiofuel\n\nOur investment in maritime biofuel helps to accelerate the shipping industry's transition to zero-emission fuels by demonstrating demand for lower-carbon fuel alternatives to bunker fuel. In 2023, we transported 10% of our ocean cargo via maritime ships powered by lower-emission biofuels and finalized a 2023–2024 agreement with Maersk through their \"ECO Delivery\" ocean product offering. As part of this collaboration, Amazon piloted shipping cargo on the first methanol-powered vessel, from Singapore to Rotterdam. As availability increases, we will continue to increase the percentage of cargo we transport on these types of ships and leverage additional lower-carbon fuels in 2024 and beyond.\n\nLearn more about this landmark zero-emission voyage\n\nAirfreight\n\nWhile we prioritize ocean transportation, air transportation is an important part of our first mile logistics network, though it represents only 10% of our transoceanic imports.", "doc_id": "amazon2023", "page": 15, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Environment\n\nZero Emission Vessels (coZEV) with the Aspen Institute to support initiatives that increase the availability of zero-emission technologies and fuels while gaining support from shipping lines, cargo owners, ports, and other organizations that can help to enable the energy transition. Through our membership, we are supporting coZEV's goal to use maritime ships with zero-emission fuels for 100% of ocean cargo by 2040.\n\nBiofuel\n\nOur investment in maritime biofuel helps to accelerate the shipping industry's transition to zero-emission fuels by demonstrating demand for lower-carbon fuel alternatives to bunker fuel. In 2023, we transported 10% of our ocean cargo via maritime ships powered by lower-emission biofuels and finalized a 2023–2024 agreement with Maersk through their \"ECO Delivery\" ocean product offering. As part of this collaboration, Amazon piloted shipping cargo on the first methanol-powered vessel, from Singapore to Rotterdam. As availability increases, we will continue to increase the percentage of cargo we transport on these types of ships and leverage additional lower-carbon fuels in 2024 and beyond.\n\nLearn more about this landmark zero-emission voyage\n\nAirfreight\n\nWhile we prioritize ocean transportation, air transportation is an important part of our first mile logistics network, though it represents only 10% of our transoceanic imports.", "original_types": ["text", "header"], "id": 370}
{"type": "section", "content": "Middle Mile Electric Vehicles\n\nScaling up middle mile EVs is another way we aim to decarbonize our logistics network, both on the road and at our logistics facilities. Amazon and delivery service providers (DSPs) deployed more than 245 electric middle mile vehicles in 2023. We also have nearly 18,000 hydrogen-powered forklifts operating at more than 81 fulfillment centers, as well as 110 electric yard hostlers—vehicles that move truck trailers around fulfillment centers—in North America.\n\nLast Mile\n\nLast mile transportation refers to the final part of the delivery journey. This is when products are transported from sort centers and delivery stations to customers via delivery vans, electric delivery vehicles (four-wheel, three-wheel, two-wheel, and e-mopeds), and micromobility solutions, including e-cargo bikes and on-foot deliveries. To help reduce delivery-related emissions, we are investing in EVs and working to optimize our delivery van and package fill rates as products embark on the final leg of their journey. Amazon deliveries are made by DSPs, who are independent contractors that operate their own delivery businesses.\n\nPartnerships Accelerating Middle Mile Decarbonization\n\nTrucking is a challenging area to decarbonize, particularly considering the long-haul distances driven and the requirements for high-power EV charging infrastructure. Identifying and scaling solutions for this important method of transportation cannot be done by one company alone—it requires the know-how, resources, and experience of collaborative partners across industries and sectors.\n\nMiddle Mile Zero-Emission and Lower-Carbon Fuels\n\nIn 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Middle Mile Electric Vehicles\n\nScaling up middle mile EVs is another way we aim to decarbonize our logistics network, both on the road and at our logistics facilities. Amazon and delivery service providers (DSPs) deployed more than 245 electric middle mile vehicles in 2023. We also have nearly 18,000 hydrogen-powered forklifts operating at more than 81 fulfillment centers, as well as 110 electric yard hostlers—vehicles that move truck trailers around fulfillment centers—in North America.\n\nLast Mile\n\nLast mile transportation refers to the final part of the delivery journey. This is when products are transported from sort centers and delivery stations to customers via delivery vans, electric delivery vehicles (four-wheel, three-wheel, two-wheel, and e-mopeds), and micromobility solutions, including e-cargo bikes and on-foot deliveries. To help reduce delivery-related emissions, we are investing in EVs and working to optimize our delivery van and package fill rates as products embark on the final leg of their journey. Amazon deliveries are made by DSPs, who are independent contractors that operate their own delivery businesses.\n\nPartnerships Accelerating Middle Mile Decarbonization\n\nTrucking is a challenging area to decarbonize, particularly considering the long-haul distances driven and the requirements for high-power EV charging infrastructure. Identifying and scaling solutions for this important method of transportation cannot be done by one company alone—it requires the know-how, resources, and experience of collaborative partners across industries and sectors.\n\nMiddle Mile Zero-Emission and Lower-Carbon Fuels\n\nIn 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.", "original_types": ["text", "header"], "id": 371}
{"type": "section", "content": "To reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "To reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 372}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 373}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 374}
{"type": "figure", "content": "Figure 2: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 2: Title...", "id": 375}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 376}
{"type": "figure", "content": "Figure 3: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 3: Title...", "id": 377}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 378}
{"type": "figure", "content": "Figure 4: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 4: Title...", "id": 379}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 380}
{"type": "figure", "content": "Figure 5: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 5: Title...", "id": 381}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 382}
{"type": "figure", "content": "Figure 6: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 6: Title...", "id": 383}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 384}
{"type": "figure", "content": "Figure 7: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 7: Title...", "id": 385}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 386}
{"type": "figure", "content": "Figure 8: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 8: Title...", "id": 387}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 388}
{"type": "figure", "content": "Figure 9: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 9: Title...", "id": 389}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 390}
{"type": "figure", "content": "Figure 10: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 10: Title...", "id": 391}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 392}
{"type": "figure", "content": "Figure 11: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 11: Title...", "id": 393}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 394}
{"type": "figure", "content": "Figure 12: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 12: Title...", "id": 395}
{"type": "section", "content": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon participated in the Smart Freight Centre (SFC) Exchange Network, a nonprofit organization whose mission is to accelerate the reduction of logistics emissions by fostering collaboration. As part of its work, SFC is with more information about the right FCV size to maximize performance, determine optimal hydrogen storage and power management, and understand the viability of these vehicles for middle mile transportation. In 2024, we aim to increase the number of FCVs deployed in our transportation network thanks to the widespread network of trains that transport millions of people and freight across the country each day. In 2019, Amazon became the first e-commerce company to leverage this vast train network by entering into an operational engagement with Indian Railways to meet our customer promise of fast and reliable deliveries. We have continued this collaboration, increasing our use of Indian Railways' electric locomotives to ship packages in 2023. Renewable diesel is made from waste fats, greases, and other oils. And in India, we are collaborating with eFast—India's first national freight electrification platform led by the government policy think tank NITI Aayog—to engage policymakers on policies that can help decarbonize rail freight.\n\nTo reduce our logistics-related emissions, Amazon is also increasing the availability of compressed natural gas vehicles (CNGs). CNGs are trucks powered by natural gas instead of gasoline or diesel fuel. They can reduce carbon emissions by at least 75% compared to diesel when refueled with RNG. Globally, Amazon had 4,400 CNGs on the road in 2023, using 30 million gallons of RNG. This growth was driven by the opening of seven permanent RNG fueling stations across our North American logistics network in partnership with Clean Energy, the largest provider of RNG for the transportation industry in North America.", "original_types": ["text"], "id": 396}
{"type": "figure", "content": "Figure 13: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 13: Title...", "id": 397}
{"type": "section", "content": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Additionally, in 2023, The Climate Pledge and C40 Cities (C40) developed Laneshift, a partnership to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America.\n\nLearn more about Laneshift", "original_types": ["text"], "id": 398}
{"type": "figure", "content": "Figure 14: Title...", "doc_id": "amazon2023", "page": 16, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 14: Title...", "id": 399}
{"type": "section", "content": "Overview\n\nIn 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles, including 19,000 electric delivery vans, around the world.\n\nUnited States\n\nOur U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.\n\nWe delivered 431 million packages via EVs.\n\nEurope\n\nWe deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles.\n\nWe delivered 150 million packages via EVs.\n\nIndia\n\nWe nearly doubled the number of EVs in our Indian delivery fleet to more than 7,200, including 3,600 electric delivery vans, and more than 3,600 two-, three-, and four-wheel vehicles.\n\nWe delivered 81 million packages via EVs.\n\nWe installed charging infrastructure near more than 100 Amazon locations.\n\nZero-Emission Deliveries in London\n\nIn August 2023, we began transporting packages in London using both zero-emission middle mile vehicles (electric heavy-duty trucks) and last mile vehicles (electric vans).\n\nWe also scaled the Amazon Hub Delivery program—which partners with small and medium-sized businesses to deliver packages in local neighborhoods—to over 2,000 partners that delivered nearly 14 million shipments in 2023, around half of which were delivered on foot or bike.\n\nIn Japan, we delivered over 23 million packages using e-cargo bikes and on-foot delivery. We also scaled the Amazon Hub Delivery program—which partners with small and medium-sized businesses to deliver packages in local neighborhoods—to over 2,000 partners that delivered nearly 14 million shipments in 2023, around half of which were delivered on foot or bike.\n\nLearn more about how we are reducing our packaging footprint.\n\nBuilding Construction and Operations\n\nOur building portfolio comprises thousands of owned and leased facilities in more than 60 countries, including operations buildings, grocery stores, corporate offices, and data centers. The construction, operation, and decommissioning of these buildings accounted for one fifth of Amazon’s total carbon emissions in 2023, which is why we’re committed to implementing and scaling decarbonization solutions and processes to reduce the footprint of this sector of our business.\n\nCarbon emissions connected to our buildings fall into two categories: embodied emissions and operational emissions. Embodied emissions in the buildings sector are generated from the manufacture, transportation, installation, maintenance, and disposal of building materials. Operational emissions refer to resources consumed by day-to-day processes needed to run our business, including computing, lighting, heating, cooling, ventilation, refrigeration systems, and operating other equipment.", "doc_id": "amazon2023", "page": 17, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Overview\n\nIn 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles, including 19,000 electric delivery vans, around the world.\n\nUnited States\n\nOur U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.\n\nWe delivered 431 million packages via EVs.\n\nEurope\n\nWe deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles.\n\nWe delivered 150 million packages via EVs.\n\nIndia\n\nWe nearly doubled the number of EVs in our Indian delivery fleet to more than 7,200, including 3,600 electric delivery vans, and more than 3,600 two-, three-, and four-wheel vehicles.\n\nWe delivered 81 million packages via EVs.\n\nWe installed charging infrastructure near more than 100 Amazon locations.\n\nZero-Emission Deliveries in London\n\nIn August 2023, we began transporting packages in London using both zero-emission middle mile vehicles (electric heavy-duty trucks) and last mile vehicles (electric vans).\n\nWe also scaled the Amazon Hub Delivery program—which partners with small and medium-sized businesses to deliver packages in local neighborhoods—to over 2,000 partners that delivered nearly 14 million shipments in 2023, around half of which were delivered on foot or bike.\n\nIn Japan, we delivered over 23 million packages using e-cargo bikes and on-foot delivery. We also scaled the Amazon Hub Delivery program—which partners with small and medium-sized businesses to deliver packages in local neighborhoods—to over 2,000 partners that delivered nearly 14 million shipments in 2023, around half of which were delivered on foot or bike.\n\nLearn more about how we are reducing our packaging footprint.\n\nBuilding Construction and Operations\n\nOur building portfolio comprises thousands of owned and leased facilities in more than 60 countries, including operations buildings, grocery stores, corporate offices, and data centers. The construction, operation, and decommissioning of these buildings accounted for one fifth of Amazon’s total carbon emissions in 2023, which is why we’re committed to implementing and scaling decarbonization solutions and processes to reduce the footprint of this sector of our business.\n\nCarbon emissions connected to our buildings fall into two categories: embodied emissions and operational emissions. Embodied emissions in the buildings sector are generated from the manufacture, transportation, installation, maintenance, and disposal of building materials. Operational emissions refer to resources consumed by day-to-day processes needed to run our business, including computing, lighting, heating, cooling, ventilation, refrigeration systems, and operating other equipment.", "original_types": ["text", "header"], "id": 400}
{"type": "section", "content": "Implementing Foundational Efficiency Initiatives\n\nWe are instituting and improving data collection practices to better track our performance and inform our efforts to improve efficiency and reduce energy use and carbon emissions across our buildings portfolio.\n\nScaling renewable energy and lower-carbon approaches to heating and cooling\n\nWe use on-site renewables, such as rooftop solar installations on buildings we operate, as well as renewable energy from the grid to power our buildings. We’re also scaling up our use of refrigerants with low global warming potential (GWP) and utilizing alternative fuels as backup power sources and to cool data centers.\n\nCreating industry solutions to reduce embodied carbon\n\nWe collaborate with suppliers, industry partners, signatories of The Climate Pledge, and governing bodies to develop and implement standards, alternative materials, and solutions that address environmental challenges specific to the buildings sector.\n\nValidating Our Progress\n\nAs teams across Amazon progress toward the decarbonization of our buildings, we’ve begun to validate our efforts using the International Living Future Institute’s Zero Carbon Certification (ZCC). This third-party certification program validates improvements to our building decarbonization efforts and provides clear accountability to develop and implement standards, alternative materials, and solutions that address environmental challenges specific to the buildings sector.", "doc_id": "amazon2023", "page": 18, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Implementing Foundational Efficiency Initiatives\n\nWe are instituting and improving data collection practices to better track our performance and inform our efforts to improve efficiency and reduce energy use and carbon emissions across our buildings portfolio.\n\nScaling renewable energy and lower-carbon approaches to heating and cooling\n\nWe use on-site renewables, such as rooftop solar installations on buildings we operate, as well as renewable energy from the grid to power our buildings. We’re also scaling up our use of refrigerants with low global warming potential (GWP) and utilizing alternative fuels as backup power sources and to cool data centers.\n\nCreating industry solutions to reduce embodied carbon\n\nWe collaborate with suppliers, industry partners, signatories of The Climate Pledge, and governing bodies to develop and implement standards, alternative materials, and solutions that address environmental challenges specific to the buildings sector.\n\nValidating Our Progress\n\nAs teams across Amazon progress toward the decarbonization of our buildings, we’ve begun to validate our efforts using the International Living Future Institute’s Zero Carbon Certification (ZCC). This third-party certification program validates improvements to our building decarbonization efforts and provides clear accountability to develop and implement standards, alternative materials, and solutions that address environmental challenges specific to the buildings sector.", "original_types": ["text", "header"], "id": 401}
{"type": "figure", "content": "Figure 1: Sustainability at Our Second Headquarters", "doc_id": "amazon2023", "page": 18, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Sustainability at Our Second Headquarters", "id": 402}
{"type": "section", "content": "We want our corporate offices to be both inspiring places to work and models of what is possible when it comes to sustainable design and construction.\n\nWe built our second headquarters (HQ2) in Arlington, Virginia, with sustainability in mind. Opened in May 2023, HQ2 runs on 100% renewable electricity and achieves energy savings of 24% relative to a comparable Leadership in Energy and Environmental Design (LEED) baseline. In March 2024, it became the largest project (by building square footage) in the U.S. to receive LEED v4 Platinum certification.\n\nUsing an advanced lower-carbon concrete mix design that included CarbonCure, which we invested in through The Climate Pledge Fund, we achieved a 20% reduction in HQ2’s concrete structure carbon footprint compared to the industry baseline. This avoided 14,700 metric tons of CO2e, which is the equivalent of taking 3,500 cars off the road in the U.S. for an entire year. More than 40 Amazon sites globally now use this same CarbonCure technology.\n\nDesign elements at HQ2 incorporate native flora and fauna and use earth tones and natural materials, such as wood and stone, to create a warm and inviting atmosphere.", "doc_id": "amazon2023", "page": 18, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We want our corporate offices to be both inspiring places to work and models of what is possible when it comes to sustainable design and construction.\n\nWe built our second headquarters (HQ2) in Arlington, Virginia, with sustainability in mind. Opened in May 2023, HQ2 runs on 100% renewable electricity and achieves energy savings of 24% relative to a comparable Leadership in Energy and Environmental Design (LEED) baseline. In March 2024, it became the largest project (by building square footage) in the U.S. to receive LEED v4 Platinum certification.\n\nUsing an advanced lower-carbon concrete mix design that included CarbonCure, which we invested in through The Climate Pledge Fund, we achieved a 20% reduction in HQ2’s concrete structure carbon footprint compared to the industry baseline. This avoided 14,700 metric tons of CO2e, which is the equivalent of taking 3,500 cars off the road in the U.S. for an entire year. More than 40 Amazon sites globally now use this same CarbonCure technology.\n\nDesign elements at HQ2 incorporate native flora and fauna and use earth tones and natural materials, such as wood and stone, to create a warm and inviting atmosphere.", "original_types": ["text"], "id": 403}
{"type": "section", "content": "Scaling Renewable Energy and Lower-Carbon Approaches to Heating and Cooling\n\nWe are working to reduce the emissions associated with operating our buildings by using renewable energy and other lower-carbon alternatives.\n\nScaling Renewable Energy\n\nAt the end of 2023, Amazon had 270 rooftop solar projects at our facilities around the globe. We brought 50 new on-site solar energy systems online in 2023, for a total capacity of 58 MW. These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.\n\nLearn more about how we are scaling carbon-free energy\n\nIn 2023, 29 Amazon building projects were constructed with lower-carbon concrete and steel, collectively reducing embodied carbon by over 79,000 metric tons of CO2e. Amazon used mass timber when building HQ2 and, in 2023, progressed in the design and construction of three new buildings that will use mass timber structural elements.\n\nLower-Emission Refrigerants\n\nRefrigerants are cooling agents used in air conditioners, heat pumps, refrigerators, and freezers. Refrigerants with high GWP—such as hydrofluorocarbons (HFCs)—contribute to climate change by trapping more heat in the atmosphere than CO2. That’s why Amazon Fresh and Whole Foods Market use low-GWP refrigerants, such as CO2 refrigerants, to help grocery stores reduce emissions.\n\nCreating Industry Solutions to Reduce Embodied Carbon\n\nDecarbonizing the buildings sector is a challenge no one company or organization can solve alone. We can, however, accelerate progress by engaging with industry partners to develop and scale innovations that reduce carbon emissions generated from driving over 11,100 cars in the U.S. for one year. We will continue working with suppliers to achieve even greater carbon savings in future Amazon buildings by increasing the use of lower-carbon materials in their construction.\n\nUsing Lower-Carbon Materials\n\nWe are working to reduce embodied carbon in our infrastructure by increasing the use of lower-carbon materials in Amazon buildings, such as lower-carbon steel, lower-carbon concrete, and mass timber, a lower-carbon structural wood product that can replace concrete and steel in building construction.\n\nServers and Hardware\n\nAs the world’s most comprehensive and broadly adopted cloud provider, AWS is committed to building a lower-carbon business for its customers and the planet. AWS designs its data centers—including servers and hardware—for efficiency, resiliency, and a lower carbon footprint.", "doc_id": "amazon2023", "page": 19, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Scaling Renewable Energy and Lower-Carbon Approaches to Heating and Cooling\n\nWe are working to reduce the emissions associated with operating our buildings by using renewable energy and other lower-carbon alternatives.\n\nScaling Renewable Energy\n\nAt the end of 2023, Amazon had 270 rooftop solar projects at our facilities around the globe. We brought 50 new on-site solar energy systems online in 2023, for a total capacity of 58 MW. These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.\n\nLearn more about how we are scaling carbon-free energy\n\nIn 2023, 29 Amazon building projects were constructed with lower-carbon concrete and steel, collectively reducing embodied carbon by over 79,000 metric tons of CO2e. Amazon used mass timber when building HQ2 and, in 2023, progressed in the design and construction of three new buildings that will use mass timber structural elements.\n\nLower-Emission Refrigerants\n\nRefrigerants are cooling agents used in air conditioners, heat pumps, refrigerators, and freezers. Refrigerants with high GWP—such as hydrofluorocarbons (HFCs)—contribute to climate change by trapping more heat in the atmosphere than CO2. That’s why Amazon Fresh and Whole Foods Market use low-GWP refrigerants, such as CO2 refrigerants, to help grocery stores reduce emissions.\n\nCreating Industry Solutions to Reduce Embodied Carbon\n\nDecarbonizing the buildings sector is a challenge no one company or organization can solve alone. We can, however, accelerate progress by engaging with industry partners to develop and scale innovations that reduce carbon emissions generated from driving over 11,100 cars in the U.S. for one year. We will continue working with suppliers to achieve even greater carbon savings in future Amazon buildings by increasing the use of lower-carbon materials in their construction.\n\nUsing Lower-Carbon Materials\n\nWe are working to reduce embodied carbon in our infrastructure by increasing the use of lower-carbon materials in Amazon buildings, such as lower-carbon steel, lower-carbon concrete, and mass timber, a lower-carbon structural wood product that can replace concrete and steel in building construction.\n\nServers and Hardware\n\nAs the world’s most comprehensive and broadly adopted cloud provider, AWS is committed to building a lower-carbon business for its customers and the planet. AWS designs its data centers—including servers and hardware—for efficiency, resiliency, and a lower carbon footprint.", "original_types": ["text", "header"], "id": 404}
{"type": "section", "content": "Overview\n\nAWS’s scale allows for higher resource utilization and energy efficiency than the typical on-premises data center. From the infrastructure that powers its servers to the techniques that keep them cool, efficiency is a primary goal for every part of the AWS Global Cloud Infrastructure.\n\nValue Chain\n\nThe AWS Global Cloud Infrastructure is built on AWS’s own custom hardware and optimized for workloads run by AWS customers. Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.\n\nPeople\n\nAWS is reducing emissions from transporting hardware, including racks and their related components, by using more sustainable fuels and less carbon-intensive modes of shipping where possible.\n\nAppendix\n\nIn 2023, we worked with our top suppliers to develop carbon footprints for the products responsible for over half the greenhouse gas (GHG) emissions of our private brands. These insights informed the creation of joint abatement plans, which help avoid future product emissions through actions including moving to recycled materials, using renewable energy to power manufacturing facilities, and reducing packaging.\n\nCarbon\n\nReducing Emissions from the Transportation of Data Center Hardware\n\nMeanwhile, AWS Graviton4 is the latest generation of chips designed by AWS and the most powerful and energy-efficient chip AWS has built as of 2023. Graviton4 provides up to 30% better computing performance, 50% more cores, and 75% more memory bandwidth than Graviton3 processors while being more energy efficient.\n\nPartnering to Reduce Carbon Emissions in the Semiconductor Industry\n\nIn 2023, AWS transported approximately 6,600 metric tons of hardware components on cargo ships, avoiding approximately 65,000 metric tons of CO2e by reducing airfreight in favor of ocean freight where possible.\n\nInvesting in lower-carbon fuels: AWS is encouraging its suppliers to decarbonize long-haul transportation, including through the use of SAF. In 2023, AWS purchased over 6 million liters of SAF, which avoided approximately 15,600 metric tons of CO2e compared to conventional aviation fuel.\n\nLearn more about how we focus on sustainability throughout the lifecycle of our products\n\nAmazon Grocery and Whole Foods Market\n\nMaking ground deliveries using EVs: AWS is increasing the use of EVs for equipment-related ground deliveries. In Dublin and Singapore, for example, AWS worked with transportation providers to transport racks, loose gear, and other components to data center locations using electric trucks.\n\nProducts and Devices\n\nAmazon Private Brands", "doc_id": "amazon2023", "page": 20, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Overview\n\nAWS’s scale allows for higher resource utilization and energy efficiency than the typical on-premises data center. From the infrastructure that powers its servers to the techniques that keep them cool, efficiency is a primary goal for every part of the AWS Global Cloud Infrastructure.\n\nValue Chain\n\nThe AWS Global Cloud Infrastructure is built on AWS’s own custom hardware and optimized for workloads run by AWS customers. Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.\n\nPeople\n\nAWS is reducing emissions from transporting hardware, including racks and their related components, by using more sustainable fuels and less carbon-intensive modes of shipping where possible.\n\nAppendix\n\nIn 2023, we worked with our top suppliers to develop carbon footprints for the products responsible for over half the greenhouse gas (GHG) emissions of our private brands. These insights informed the creation of joint abatement plans, which help avoid future product emissions through actions including moving to recycled materials, using renewable energy to power manufacturing facilities, and reducing packaging.\n\nCarbon\n\nReducing Emissions from the Transportation of Data Center Hardware\n\nMeanwhile, AWS Graviton4 is the latest generation of chips designed by AWS and the most powerful and energy-efficient chip AWS has built as of 2023. Graviton4 provides up to 30% better computing performance, 50% more cores, and 75% more memory bandwidth than Graviton3 processors while being more energy efficient.\n\nPartnering to Reduce Carbon Emissions in the Semiconductor Industry\n\nIn 2023, AWS transported approximately 6,600 metric tons of hardware components on cargo ships, avoiding approximately 65,000 metric tons of CO2e by reducing airfreight in favor of ocean freight where possible.\n\nInvesting in lower-carbon fuels: AWS is encouraging its suppliers to decarbonize long-haul transportation, including through the use of SAF. In 2023, AWS purchased over 6 million liters of SAF, which avoided approximately 15,600 metric tons of CO2e compared to conventional aviation fuel.\n\nLearn more about how we focus on sustainability throughout the lifecycle of our products\n\nAmazon Grocery and Whole Foods Market\n\nMaking ground deliveries using EVs: AWS is increasing the use of EVs for equipment-related ground deliveries. In Dublin and Singapore, for example, AWS worked with transportation providers to transport racks, loose gear, and other components to data center locations using electric trucks.\n\nProducts and Devices\n\nAmazon Private Brands", "original_types": ["text", "header"], "id": 405}
{"type": "section", "content": "Two of our private brands, Amazon Essentials and Amazon Basics, make products across many categories, including clothing, bedroom furniture and mattresses, kitchen appliances and cookware, toys, pet essentials, and workout gear. Amazon works with suppliers around the world to manufacture these products at a high quality and great value.", "doc_id": "amazon2023", "page": 20, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Two of our private brands, Amazon Essentials and Amazon Basics, make products across many categories, including clothing, bedroom furniture and mattresses, kitchen appliances and cookware, toys, pet essentials, and workout gear. Amazon works with suppliers around the world to manufacture these products at a high quality and great value.", "original_types": ["text"], "id": 406}
{"type": "section", "content": "Whole Foods Market also actively supports and finances projects that have the potential to strengthen ecosystem services, promote biodiversity, and improve soil health within our supply chain. These projects include manure management with dairy farmers, planting native grasses on farms in collaboration with grain and legume suppliers, and process electrification and feed efficiency with seafood suppliers.\n\nEngaging Suppliers\n\nAs we progress on our net-zero carbon emissions journey, Amazon is continuing to engage suppliers in the critical work of reducing their operational emissions and working with their own upstream supply chains to do the same. We have identified a list of the highest-emitting suppliers directly supporting our operations, and expect those suppliers, who collectively contribute more than 50% of emissions globally to Amazon’s Scope 3 footprint, to provide a plan for how they will decarbonize their operations and demonstrate real progress over time. We will prioritize our business toward those who provide their plans and results on their path to net-zero carbon emissions. In addition, we also launched our collaboration with the Carbon Trust, which partners with businesses, governments, and organizations around the world to accelerate the transition along with product sustainability fact sheets for each device. We work with the Carbon Fact sheets provide customers with a detailed breakdown of emissions throughout the device’s lifecycle, including those resulting from the extraction, production, and transportation of raw materials and device parts; the energy associated with device use; and end-of-life processing. We also published the Amazon Devices Product Carbon Footprint Methodology to give customers insight into how we calculate the carbon footprint of our devices.\n\nCarbon Neutralization", "doc_id": "amazon2023", "page": 21, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Whole Foods Market also actively supports and finances projects that have the potential to strengthen ecosystem services, promote biodiversity, and improve soil health within our supply chain. These projects include manure management with dairy farmers, planting native grasses on farms in collaboration with grain and legume suppliers, and process electrification and feed efficiency with seafood suppliers.\n\nEngaging Suppliers\n\nAs we progress on our net-zero carbon emissions journey, Amazon is continuing to engage suppliers in the critical work of reducing their operational emissions and working with their own upstream supply chains to do the same. We have identified a list of the highest-emitting suppliers directly supporting our operations, and expect those suppliers, who collectively contribute more than 50% of emissions globally to Amazon’s Scope 3 footprint, to provide a plan for how they will decarbonize their operations and demonstrate real progress over time. We will prioritize our business toward those who provide their plans and results on their path to net-zero carbon emissions. In addition, we also launched our collaboration with the Carbon Trust, which partners with businesses, governments, and organizations around the world to accelerate the transition along with product sustainability fact sheets for each device. We work with the Carbon Fact sheets provide customers with a detailed breakdown of emissions throughout the device’s lifecycle, including those resulting from the extraction, production, and transportation of raw materials and device parts; the energy associated with device use; and end-of-life processing. We also published the Amazon Devices Product Carbon Footprint Methodology to give customers insight into how we calculate the carbon footprint of our devices.\n\nCarbon Neutralization", "original_types": ["text", "header"], "id": 407}
{"type": "section", "content": "Reducing embodied carbon in our hardware requires us to decarbonize all parts of the device lifecycle, including manufacturing. We encourage Amazon device suppliers to reduce their manufacturing emissions, including by using renewable energy. As of the end of 2023, we’ve received commitments from 49 device suppliers to work with us on decarbonization, up from 28 suppliers in 2022. We also helped 21 suppliers develop renewable energy plans, including final assembly suppliers that make up over 70% of our direct manufacturing spend for Echo, Kindle, Fire tablet, Fire TV, Ring, Blink, and eero devices and accessories. Amazon supports the development of new renewable energy capacity to match the electricity used by our customers’ devices. This renewable energy comes from investments in off-site wind and solar farms. Amazon may choose to purchase additional environmental attributes, such as renewable energy certificates in the U.S. and Guarantees of Origin in the EU, to signal our support for renewable energy in the grids where we operate, in line with the expected energy generation of the projects we’ve contracted. By the end of 2022, we had contracted enough renewable energy capacity through new wind and solar farms to equal the expected electricity used by all active Echo, Fire TV, and Ring devices globally by 2025. Some of these wind and solar projects are operational today; others are currently under construction and expected to begin operating in 2024.", "doc_id": "amazon2023", "page": 21, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Reducing embodied carbon in our hardware requires us to decarbonize all parts of the device lifecycle, including manufacturing. We encourage Amazon device suppliers to reduce their manufacturing emissions, including by using renewable energy. As of the end of 2023, we’ve received commitments from 49 device suppliers to work with us on decarbonization, up from 28 suppliers in 2022. We also helped 21 suppliers develop renewable energy plans, including final assembly suppliers that make up over 70% of our direct manufacturing spend for Echo, Kindle, Fire tablet, Fire TV, Ring, Blink, and eero devices and accessories. Amazon supports the development of new renewable energy capacity to match the electricity used by our customers’ devices. This renewable energy comes from investments in off-site wind and solar farms. Amazon may choose to purchase additional environmental attributes, such as renewable energy certificates in the U.S. and Guarantees of Origin in the EU, to signal our support for renewable energy in the grids where we operate, in line with the expected energy generation of the projects we’ve contracted. By the end of 2022, we had contracted enough renewable energy capacity through new wind and solar farms to equal the expected electricity used by all active Echo, Fire TV, and Ring devices globally by 2025. Some of these wind and solar projects are operational today; others are currently under construction and expected to begin operating in 2024.", "original_types": ["text"], "id": 408}
{"type": "section", "content": "Accelerating Forest finance (LEAF) Coalition—a public-private initiative that is mobilizing more than $1 billion to protect the world’s tropical forests and surrounding communities by supporting government policies and programs that reduce emissions from deforestation at national or large sub-national scale.\n\nAdditionally, Amazon provided funding and technology to help the State of Pará, Brazil, institute traceability in the cattle sector, advance alternative livelihoods for family farmers, deter illegal land use by streamlining and digitizing proper land titling, and reclaim illegally deforested state lands. AWS is supporting the government of the State of Pará in designing and deploying SeloVerde (Green Seal), a cutting-edge AI tool to address climate change challenges and traceability in supply chains with a high risk of deforestation. SeloVerde combines government databases, innovative map services, and land-use data from high spatial resolution satellite imagery. This allows industry stakeholders access to information that helps them make environmentally responsive, data-based purchasing decisions for commodities such as cattle and soy.\n\nScaling Up Carbon Removal Technologies\n\nIn 2023, we signed an agreement to support what is expected to be the world’s largest deployment of direct air capture (DAC). DAC is an emerging set of technologies that chemically scrub CO2 from the air. The captured CO2 is then stored deep underground or used in applications such as building materials (including concrete, brick, and cement) and low-carbon fuels. We committed to purchasing carbon dioxide removal credits equaling 250,000 metric tons of CO2 from 1PointFive, which is currently constructing its first DAC plant in Texas. When fully operational, the plant is expected to net-zero carbon emissions, signing The Climate Pledge reinforces their commitment to sustainability, holds them accountable to their goals, and provides new opportunities for collaboration and a collective knowledge base to accelerate their progress. At the end of 2023, The Climate Pledge represented 473 signatories in 42 countries and 59 industries.\n\nAdvancing the Removal of Carbon from the Atmosphere with Nature-Based Solutions\n\nAmazon also supports CarbonCapture Inc., a climate technology startup recognized for its pioneering modular DAC systems. The company’s patented modular open system architecture allows the swapping of new sorbents—materials used to absorb CO2—as they become available. In 2023, 77 additional companies signed The Climate Pledge, including Mastercard, Sony, and T-Mobile. Amazon encourages our suppliers to sign The Climate Pledge, and the cost of DAC over time to unlock scale. CarbonCapture Inc. in 2023, nine Amazon transportation and logistics suppliers will make up to 100,000 carbon removal credits available to Amazon. The Climate Pledge Fund also made an equity investment in CarbonCapture Inc. to help accelerate its growth and scale its operations.\n\nThe Climate Pledge", "doc_id": "amazon2023", "page": 22, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Accelerating Forest finance (LEAF) Coalition—a public-private initiative that is mobilizing more than $1 billion to protect the world’s tropical forests and surrounding communities by supporting government policies and programs that reduce emissions from deforestation at national or large sub-national scale.\n\nAdditionally, Amazon provided funding and technology to help the State of Pará, Brazil, institute traceability in the cattle sector, advance alternative livelihoods for family farmers, deter illegal land use by streamlining and digitizing proper land titling, and reclaim illegally deforested state lands. AWS is supporting the government of the State of Pará in designing and deploying SeloVerde (Green Seal), a cutting-edge AI tool to address climate change challenges and traceability in supply chains with a high risk of deforestation. SeloVerde combines government databases, innovative map services, and land-use data from high spatial resolution satellite imagery. This allows industry stakeholders access to information that helps them make environmentally responsive, data-based purchasing decisions for commodities such as cattle and soy.\n\nScaling Up Carbon Removal Technologies\n\nIn 2023, we signed an agreement to support what is expected to be the world’s largest deployment of direct air capture (DAC). DAC is an emerging set of technologies that chemically scrub CO2 from the air. The captured CO2 is then stored deep underground or used in applications such as building materials (including concrete, brick, and cement) and low-carbon fuels. We committed to purchasing carbon dioxide removal credits equaling 250,000 metric tons of CO2 from 1PointFive, which is currently constructing its first DAC plant in Texas. When fully operational, the plant is expected to net-zero carbon emissions, signing The Climate Pledge reinforces their commitment to sustainability, holds them accountable to their goals, and provides new opportunities for collaboration and a collective knowledge base to accelerate their progress. At the end of 2023, The Climate Pledge represented 473 signatories in 42 countries and 59 industries.\n\nAdvancing the Removal of Carbon from the Atmosphere with Nature-Based Solutions\n\nAmazon also supports CarbonCapture Inc., a climate technology startup recognized for its pioneering modular DAC systems. The company’s patented modular open system architecture allows the swapping of new sorbents—materials used to absorb CO2—as they become available. In 2023, 77 additional companies signed The Climate Pledge, including Mastercard, Sony, and T-Mobile. Amazon encourages our suppliers to sign The Climate Pledge, and the cost of DAC over time to unlock scale. CarbonCapture Inc. in 2023, nine Amazon transportation and logistics suppliers will make up to 100,000 carbon removal credits available to Amazon. The Climate Pledge Fund also made an equity investment in CarbonCapture Inc. to help accelerate its growth and scale its operations.\n\nThe Climate Pledge", "original_types": ["text", "header"], "id": 409}
{"type": "section", "content": "We also created the Agroforestry and Restoration Accelerator, which aims to restore degraded lands in ways that both remove carbon from the atmosphere and improve the livelihoods of local communities. The Accelerator experiments with scalable business models, landholder engagement strategies, and measurement techniques to support agroforestry and native restoration on small-scale family farms in Brazil.\n\nThe Climate Pledge by the Numbers", "doc_id": "amazon2023", "page": 22, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We also created the Agroforestry and Restoration Accelerator, which aims to restore degraded lands in ways that both remove carbon from the atmosphere and improve the livelihoods of local communities. The Accelerator experiments with scalable business models, landholder engagement strategies, and measurement techniques to support agroforestry and native restoration on small-scale family farms in Brazil.\n\nThe Climate Pledge by the Numbers", "original_types": ["text", "header"], "id": 410}
{"type": "table", "content": "Table 1: The Climate Pledge by the Numbers\n2021 | 2022 | 2023 | YoY%\n\nSignatories | 300 | 396 | 473 | 19%\n\nCountries | 29 | 36 | 42 | 17%\n\nIndustries | 51 | 55 | 59 | 7%", "doc_id": "amazon2023", "page": 22, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: The Climate Pledge by the Numbers\n2021 | 2022 | 2023 | YoY%\n\nSignatories | 300 | 396 | 473 | 19%\n\nCountries | 29 | 36 | 42 | 17%\n\nIndustries | 51 | 55 | 59 | 7%", "id": 411}
{"type": "section", "content": "Partnering with Others to Scale Progress\nWith 473 signatories, The Climate Pledge focused more on cross-industry collaboration in 2023 than ever before. In 2023, The Climate Pledge launched five joint action projects involving its signatories to accelerate progress to net-zero carbon emissions by 2040. Working together, signatories addressed tough problems in hard-to-abate sectors to promote technological innovation, send demand signals, address supply chain conundrums, and encourage the integration of climate justice business practices. So far, 24 signatories have joined The Climate Pledge projects related to lower-emission supply chain transportation solutions, circularity, increasing urban charging infrastructure, electrifying middle mile freight, and data transparency.\nLaneshift is one example of The Climate Pledge’s joint action projects. In 2023, The Climate Pledge and C40, a global network of nearly 100 mayors of the world’s leading cities, developed Laneshift to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America. Laneshift is creating a roadmap for the freight industry to partner with rapidly urbanizing cities to utilize cleaner transportation solutions. This joint action project involves:\n\nLearn more about how global companies are working together to address the climate crisis through The Climate Pledge", "doc_id": "amazon2023", "page": 22, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Partnering with Others to Scale Progress\nWith 473 signatories, The Climate Pledge focused more on cross-industry collaboration in 2023 than ever before. In 2023, The Climate Pledge launched five joint action projects involving its signatories to accelerate progress to net-zero carbon emissions by 2040. Working together, signatories addressed tough problems in hard-to-abate sectors to promote technological innovation, send demand signals, address supply chain conundrums, and encourage the integration of climate justice business practices. So far, 24 signatories have joined The Climate Pledge projects related to lower-emission supply chain transportation solutions, circularity, increasing urban charging infrastructure, electrifying middle mile freight, and data transparency.\nLaneshift is one example of The Climate Pledge’s joint action projects. In 2023, The Climate Pledge and C40, a global network of nearly 100 mayors of the world’s leading cities, developed Laneshift to accelerate the transition to zero-emission electric trucks and charging infrastructure across major cities in India and Latin America. Laneshift is creating a roadmap for the freight industry to partner with rapidly urbanizing cities to utilize cleaner transportation solutions. This joint action project involves:\n\nLearn more about how global companies are working together to address the climate crisis through The Climate Pledge", "original_types": ["text"], "id": 412}
{"type": "section", "content": "Supporting Policies That Drive Decarbonization\n\nAchieving global decarbonization requires robust and clear policies to reduce the cost gap between established and emerging lower-carbon technologies, as well as an enabling environment to transition multiple sectors simultaneously. At Amazon, we are contributing to policymaking processes and informing public officials of our stances on issues that matter to our customers, stakeholders, and businesses, such as carbon-free energy and climate action. Our public policy team works with policymakers, multilateral organizations, industry associations, coalitions, and other partners on numerous regulatory and policy issues. Specifically, we seek to advance and incentivize decarbonization, supporting policies that scale zero-emission fuels, advance zero-emission vehicle deployment and associated infrastructure, drive the deployment of carbon-free energy, modernize the grid, and accelerate investments in clean technologies.\n\nAccelerating Climate Solutions from Female Founders", "doc_id": "amazon2023", "page": 23, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supporting Policies That Drive Decarbonization\n\nAchieving global decarbonization requires robust and clear policies to reduce the cost gap between established and emerging lower-carbon technologies, as well as an enabling environment to transition multiple sectors simultaneously. At Amazon, we are contributing to policymaking processes and informing public officials of our stances on issues that matter to our customers, stakeholders, and businesses, such as carbon-free energy and climate action. Our public policy team works with policymakers, multilateral organizations, industry associations, coalitions, and other partners on numerous regulatory and policy issues. Specifically, we seek to advance and incentivize decarbonization, supporting policies that scale zero-emission fuels, advance zero-emission vehicle deployment and associated infrastructure, drive the deployment of carbon-free energy, modernize the grid, and accelerate investments in clean technologies.\n\nAccelerating Climate Solutions from Female Founders", "original_types": ["text", "header"], "id": 413}
{"type": "section", "content": "In 2023, CGEF selected the first cohort of women-led organizations to receive grants, each of which is focused on advancing gender-equitable climate solutions. In 2022, it committed at least $50 million to invest in female-led climate technology companies. As part of The Climate Pledge Fund’s broader dedication to accelerate female-led climate solutions, in 2022, it committed at least $50 million to invest in female-led climate technology companies. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As", "doc_id": "amazon2023", "page": 23, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, CGEF selected the first cohort of women-led organizations to receive grants, each of which is focused on advancing gender-equitable climate solutions. In 2022, it committed at least $50 million to invest in female-led climate technology companies. As part of The Climate Pledge Fund’s broader dedication to accelerate female-led climate solutions, in 2022, it committed at least $50 million to invest in female-led climate technology companies. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As part of this, The Climate Pledge Fund launched Passport, an online community for its signatories that provides practical tools and industry expert connections to help them meet their emission reduction goals. As", "id": 414}
{"type": "section", "content": "Carbon-Free Energy\n\nTransitioning to carbon-free energy sources—which include renewable energy sources such as wind and solar as well as other sources such as nuclear power—is one of the most effective ways to lower Scope 2 emissions. It can also create real economic growth in communities where energy projects are built and operate, while helping advance the modernization and management of energy infrastructure. As our customers' needs for computing power, products, and services grow, so does our demand for energy. That means we must diversify our energy portfolio with additional reliable carbon-free sources, so we remain on track to reach net-zero carbon emissions by 2040. Our goal to match 100% of the electricity consumed by our global operations with renewable energy by 2025 is a milestone that is now part of our broader carbon-free energy strategy.", "doc_id": "amazon2023", "page": 24, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Carbon-Free Energy\n\nTransitioning to carbon-free energy sources—which include renewable energy sources such as wind and solar as well as other sources such as nuclear power—is one of the most effective ways to lower Scope 2 emissions. It can also create real economic growth in communities where energy projects are built and operate, while helping advance the modernization and management of energy infrastructure. As our customers' needs for computing power, products, and services grow, so does our demand for energy. That means we must diversify our energy portfolio with additional reliable carbon-free sources, so we remain on track to reach net-zero carbon emissions by 2040. Our goal to match 100% of the electricity consumed by our global operations with renewable energy by 2025 is a milestone that is now part of our broader carbon-free energy strategy.", "original_types": ["text", "header"], "id": 415}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 24, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 416}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 24, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 417}
{"type": "section", "content": "Our Approach\n\nWe use broad carbon-free energy options to support our continued growth, enabling us to deploy and grow new technologies such as artificial intelligence (AI). By scaling carbon-free energy, we aim to make Amazon a more resilient and more sustainable business, drive a global transition to cleaner energy, and achieve our commitment to The Climate Pledge to reach net-zero carbon emissions by 2040. Carbon-free energy includes existing renewable energy technologies, such as wind and solar farms and on-site rooftop solar systems, as well as nuclear reactors that generate carbon-free energy and other sources such as hydroelectric and geothermal. It also includes site energy contracts and green tariffs with local utilities that result in new wind and solar projects being added to the grid. There is not a one-size-fits-all solution when it comes to transitioning to carbon-free energy, and we believe that all viable and scalable options should be considered. Shifting to carbon-free energy requires a targeted, multifaceted approach. We are taking action to achieve our goals in the following ways:\n\nEnergy efficiency: We innovate to continuously improve the energy efficiency of our operations and devices. We're also improving data collection and analysis to better understand our operational energy requirements. This informs our carbon-free procurement decisions and energy optimization initiatives.\n\nLearn more about how we are increasing energy efficiency\n\nScaling renewable energy projects: We invest in on-site renewables through rooftop solar installations on buildings we operate, as well as in off-site renewables through power purchase agreements (PPAs) that fund new utility-scale wind and solar projects. We also participate in green tariff programs with utilities and pursue new renewable projects through competitive site energy contracts. Renewable energy certificates in the U.S. and Guarantees of Origin in the EU help us bridge the gap between project inception and operation—when the project actually starts to power our buildings or feed renewable energy into the grid.\n\nOur Progress\n\nWind and solar will continue to be a critical piece of our energy strategy as we expand our investments into additional forms of carbon-free energy, including nuclear. We'll continue to evaluate our energy strategy to meet the needs of our customers and achieve our commitment to cleaner energy, and achieve our commitment to The Climate Pledge to reach net-zero carbon emissions by 2040.\n\nScaling Renewable Energy", "doc_id": "amazon2023", "page": 25, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nWe use broad carbon-free energy options to support our continued growth, enabling us to deploy and grow new technologies such as artificial intelligence (AI). By scaling carbon-free energy, we aim to make Amazon a more resilient and more sustainable business, drive a global transition to cleaner energy, and achieve our commitment to The Climate Pledge to reach net-zero carbon emissions by 2040. Carbon-free energy includes existing renewable energy technologies, such as wind and solar farms and on-site rooftop solar systems, as well as nuclear reactors that generate carbon-free energy and other sources such as hydroelectric and geothermal. It also includes site energy contracts and green tariffs with local utilities that result in new wind and solar projects being added to the grid. There is not a one-size-fits-all solution when it comes to transitioning to carbon-free energy, and we believe that all viable and scalable options should be considered. Shifting to carbon-free energy requires a targeted, multifaceted approach. We are taking action to achieve our goals in the following ways:\n\nEnergy efficiency: We innovate to continuously improve the energy efficiency of our operations and devices. We're also improving data collection and analysis to better understand our operational energy requirements. This informs our carbon-free procurement decisions and energy optimization initiatives.\n\nLearn more about how we are increasing energy efficiency\n\nScaling renewable energy projects: We invest in on-site renewables through rooftop solar installations on buildings we operate, as well as in off-site renewables through power purchase agreements (PPAs) that fund new utility-scale wind and solar projects. We also participate in green tariff programs with utilities and pursue new renewable projects through competitive site energy contracts. Renewable energy certificates in the U.S. and Guarantees of Origin in the EU help us bridge the gap between project inception and operation—when the project actually starts to power our buildings or feed renewable energy into the grid.\n\nOur Progress\n\nWind and solar will continue to be a critical piece of our energy strategy as we expand our investments into additional forms of carbon-free energy, including nuclear. We'll continue to evaluate our energy strategy to meet the needs of our customers and achieve our commitment to cleaner energy, and achieve our commitment to The Climate Pledge to reach net-zero carbon emissions by 2040.\n\nScaling Renewable Energy", "original_types": ["text", "header"], "id": 418}
{"type": "section", "content": "In 2014, Amazon made our first investment in a renewable energy project at a time when corporate procurement for solar and wind power was just beginning. In 2019, we set an ambitious goal to match 100% of the electricity we use with renewable energy by 2030. This goal includes all data centers, logistics facilities, physical stores, and corporate offices, as well as on-site charging points and our financially integrated subsidiaries. We are proud to have achieved this goal in 2023, seven years early, with 100% of the electricity consumed by Amazon matched with renewable energy sources, up from 90% in 2022. This achievement is an important step on our journey to achieve net-zero carbon emissions by 2040, and we will continue to focus on reducing emissions through carbon-free energy as part of our commitment to The Climate Pledge.\n\nOur journey has included enabling major solar, wind, and battery storage projects around the world, including the first wind farm in Mississippi, and becoming the first corporate partnerships and advocacy for carbon-free energy solutions: We engage in partnerships, industry initiatives, and public policy advocacy to advance access to and the expansion of carbon-free energy for Amazon, our customers, and the communities where we operate.\n\nAs of January 2024, Amazon had announced:\n\n513 Global renewable energy projects\n\n243 Utility-scale wind and solar projects\n\n270 Solar rooftops at our facilities and stores\n\nThe Baldy Mesa Solar and Storage Project in Adelanto, California (developed and operated by AES), represents one of the solar projects that we added to our portfolio that includes storage capacity.", "doc_id": "amazon2023", "page": 25, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2014, Amazon made our first investment in a renewable energy project at a time when corporate procurement for solar and wind power was just beginning. In 2019, we set an ambitious goal to match 100% of the electricity we use with renewable energy by 2030. This goal includes all data centers, logistics facilities, physical stores, and corporate offices, as well as on-site charging points and our financially integrated subsidiaries. We are proud to have achieved this goal in 2023, seven years early, with 100% of the electricity consumed by Amazon matched with renewable energy sources, up from 90% in 2022. This achievement is an important step on our journey to achieve net-zero carbon emissions by 2040, and we will continue to focus on reducing emissions through carbon-free energy as part of our commitment to The Climate Pledge.\n\nOur journey has included enabling major solar, wind, and battery storage projects around the world, including the first wind farm in Mississippi, and becoming the first corporate partnerships and advocacy for carbon-free energy solutions: We engage in partnerships, industry initiatives, and public policy advocacy to advance access to and the expansion of carbon-free energy for Amazon, our customers, and the communities where we operate.\n\nAs of January 2024, Amazon had announced:\n\n513 Global renewable energy projects\n\n243 Utility-scale wind and solar projects\n\n270 Solar rooftops at our facilities and stores\n\nThe Baldy Mesa Solar and Storage Project in Adelanto, California (developed and operated by AES), represents one of the solar projects that we added to our portfolio that includes storage capacity.", "original_types": ["text"], "id": 419}
{"type": "section", "content": "Amazon Renewable Energy Projects*\n\nProjects announced as of January 2024.", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon Renewable Energy Projects*\n\nProjects announced as of January 2024.", "original_types": ["text", "header"], "id": 420}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 421}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 422}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 423}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 424}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 425}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 426}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 427}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 428}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 429}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 430}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 431}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 432}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 433}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 434}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 435}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 436}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 437}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 438}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 439}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 440}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 441}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 442}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 443}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 444}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 445}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 446}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 447}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 448}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 449}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 450}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 451}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 452}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 453}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 454}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 455}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 456}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 457}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 458}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 459}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 460}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 461}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 462}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 463}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 464}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 465}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 466}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 467}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 468}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 469}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 470}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 471}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 472}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 473}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 474}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 475}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 476}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 477}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 478}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 479}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 480}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 481}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 482}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 483}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 484}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 485}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 486}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 487}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 488}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 489}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 490}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 491}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 492}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 493}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 494}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 495}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 496}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 497}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 498}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 499}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 500}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 501}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 502}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 503}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 504}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 505}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 506}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 507}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 508}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 509}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 510}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 511}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 512}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 513}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 514}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 515}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 516}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 517}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 518}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 519}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 520}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 521}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 522}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 523}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 524}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 525}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 526}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 527}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 528}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 529}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 530}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 531}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 532}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 533}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 534}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 535}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 536}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 537}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 538}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 539}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 540}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 541}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 542}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 543}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 544}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 545}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 546}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 547}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 548}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 549}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 550}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 551}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 552}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 553}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 554}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 555}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 556}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 557}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 558}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 559}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 560}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 561}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 562}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 563}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 564}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 565}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 566}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 567}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 568}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 569}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 570}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 571}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 572}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 573}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 574}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 575}
{"type": "table", "content": "Project Location\nNumber of Projects Total MW Capacity†", "doc_id": "amazon2023", "page": 26, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Project Location\nNumber of Projects Total MW Capacity†", "id": 576}
{"type": "section", "content": "Brownfield Solar Projects\n\nThere is an emerging opportunity to repurpose previously polluted, unused land to deliver economic and environmental benefits for local communities. Brownfields are pieces of land that have been abandoned due to pollution, such as mines, factories, and landfills. The U.S. Environmental Protection Agency estimates that more than 450,000 brownfields could be revitalized for solar energy projects across the country. Brownfields are often located near power lines and public roads, making it easier to connect these types of projects to the grid.", "doc_id": "amazon2023", "page": 27, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Brownfield Solar Projects\n\nThere is an emerging opportunity to repurpose previously polluted, unused land to deliver economic and environmental benefits for local communities. Brownfields are pieces of land that have been abandoned due to pollution, such as mines, factories, and landfills. The U.S. Environmental Protection Agency estimates that more than 450,000 brownfields could be revitalized for solar energy projects across the country. Brownfields are often located near power lines and public roads, making it easier to connect these types of projects to the grid.", "original_types": ["text", "header"], "id": 577}
{"type": "figure", "content": "Brownfield solar projects repurpose previously polluted land into solar farms.", "doc_id": "amazon2023", "page": 27, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Brownfield solar projects repurpose previously polluted land into solar farms.", "id": 578}
{"type": "section", "content": "Carbon-Free Energy\n\nplanners to deploy grid-enhancing technologies. These technologies can unlock capacity for and improve efficiencies around renewable energy by reducing congestion on existing grid infrastructure. We are also working with a number of organizations, including PJM Interconnection, the regional grid operator for the mid-Atlantic, and the Rocky Mountain Institute, a nonprofit working to transform the global energy system to secure a clean, prosperous, zero-carbon future, to improve capacity on existing lines and bring renewable energy to the grid at a much lower cost than that of traditional solutions.\n\nPromoting Robust Renewable Energy Policies We work with decision-makers around the world to advance policies that scale up renewable energy. In 2023, we backed a new global renewable energy goal, urging world leaders at COP28 to triple renewable energy capacity to at least 11,000 GW by 2030. We continued our advocacy work with U.S. regulators in 2023 to improve transmission planning, permitting, and interconnection processes, including calling for federal transmission permitting reform in Congress. We also joined advocacy groups such as Western Freedom and Renewable Northwest in support of their efforts to create an organized wholesale electricity market in the western U.S. Amazon publicly supported and advocated for SB-410 (Powering Up Californians Act), a new law to speed up utility interconnection processes, which passed in the California legislature with bipartisan support and was signed into law by the governor.", "doc_id": "amazon2023", "page": 28, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Carbon-Free Energy\n\nplanners to deploy grid-enhancing technologies. These technologies can unlock capacity for and improve efficiencies around renewable energy by reducing congestion on existing grid infrastructure. We are also working with a number of organizations, including PJM Interconnection, the regional grid operator for the mid-Atlantic, and the Rocky Mountain Institute, a nonprofit working to transform the global energy system to secure a clean, prosperous, zero-carbon future, to improve capacity on existing lines and bring renewable energy to the grid at a much lower cost than that of traditional solutions.\n\nPromoting Robust Renewable Energy Policies We work with decision-makers around the world to advance policies that scale up renewable energy. In 2023, we backed a new global renewable energy goal, urging world leaders at COP28 to triple renewable energy capacity to at least 11,000 GW by 2030. We continued our advocacy work with U.S. regulators in 2023 to improve transmission planning, permitting, and interconnection processes, including calling for federal transmission permitting reform in Congress. We also joined advocacy groups such as Western Freedom and Renewable Northwest in support of their efforts to create an organized wholesale electricity market in the western U.S. Amazon publicly supported and advocated for SB-410 (Powering Up Californians Act), a new law to speed up utility interconnection processes, which passed in the California legislature with bipartisan support and was signed into law by the governor.", "original_types": ["text", "header"], "id": 579}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 28, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 580}
{"type": "section", "content": "In the U.S., we are focusing on accelerating the addition of new renewable energy projects to the grid. In 2023, we advocated for grid planners to be required to prioritize projects that are “ready to be brought online,” instead of projects that submitted queue applications first. FERC adopted this shift to a “first-ready, first-served” model in 2023. We also encouraged the Bonneville Power Administration in the Pacific Northwest to adopt first-ready, first-served queue reforms, which they did in 2024, facilitating increased production of clean energy across the region. In Ireland, we encouraged the Commission for Regulation of Utilities (CRU) to create a pathway for offshore wind projects to be delivered through corporate purchasing. CRU agreed and extended the timeline for offshore wind projects to be available for corporate buyers such as Amazon.\n\nAnticipating Future Energy Demand Powered by AWS Because permitting for renewable energy projects is one key factor delaying clean energy deployment, we are working with legislators, advocacy organizations, and corporate partners to drive permitting efficiencies across Europe and help reduce delays. In Poland, we called for policymakers to loosen restrictions on permitting and locating wind projects as its government works to increase the amount of renewable energy on its grid. By running on AWS, Duke Energy’s Intelligent Grid Services will provide more accurate forecasting of electricity needs in a matter of minutes, rather than the weeks it would take with traditional information technology hardware. This in turn enables Duke Energy to make smarter decisions, including about where to replace equipment or implement non-wire alternatives, which are electrical grid investments intended to defer or eliminate the need to construct or upgrade components.\n\nLooking Forward We remain focused on scaling carbon-free energy across our company and the communities we operate in. In 2024 and beyond, we will continue to add more carbon-free energy generation and storage capacity to our portfolio as we scale up new technologies such as AI and work toward our goal to achieve net-zero carbon by 2040. This will include investing in more solar and wind energy projects, such as our Moray West offshore wind farm in Scotland, and nuclear projects. As we progress toward decarbonizing our business, we will also engage with our partners, industry associations, and regulators around the world to scale our work, share what we’ve learned, and advocate for infrastructure and policies that help create a carbon-free energy future. We’ve always said that the path to net-zero carbon won’t be linear. As with any long-term goal, we’ll continue to evolve our strategy over time to do what is right for our business and the planet, to meet the needs of our customers, and to reach net-zero carbon by 2040.", "doc_id": "amazon2023", "page": 28, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In the U.S., we are focusing on accelerating the addition of new renewable energy projects to the grid. In 2023, we advocated for grid planners to be required to prioritize projects that are “ready to be brought online,” instead of projects that submitted queue applications first. FERC adopted this shift to a “first-ready, first-served” model in 2023. We also encouraged the Bonneville Power Administration in the Pacific Northwest to adopt first-ready, first-served queue reforms, which they did in 2024, facilitating increased production of clean energy across the region. In Ireland, we encouraged the Commission for Regulation of Utilities (CRU) to create a pathway for offshore wind projects to be delivered through corporate purchasing. CRU agreed and extended the timeline for offshore wind projects to be available for corporate buyers such as Amazon.\n\nAnticipating Future Energy Demand Powered by AWS Because permitting for renewable energy projects is one key factor delaying clean energy deployment, we are working with legislators, advocacy organizations, and corporate partners to drive permitting efficiencies across Europe and help reduce delays. In Poland, we called for policymakers to loosen restrictions on permitting and locating wind projects as its government works to increase the amount of renewable energy on its grid. By running on AWS, Duke Energy’s Intelligent Grid Services will provide more accurate forecasting of electricity needs in a matter of minutes, rather than the weeks it would take with traditional information technology hardware. This in turn enables Duke Energy to make smarter decisions, including about where to replace equipment or implement non-wire alternatives, which are electrical grid investments intended to defer or eliminate the need to construct or upgrade components.\n\nLooking Forward We remain focused on scaling carbon-free energy across our company and the communities we operate in. In 2024 and beyond, we will continue to add more carbon-free energy generation and storage capacity to our portfolio as we scale up new technologies such as AI and work toward our goal to achieve net-zero carbon by 2040. This will include investing in more solar and wind energy projects, such as our Moray West offshore wind farm in Scotland, and nuclear projects. As we progress toward decarbonizing our business, we will also engage with our partners, industry associations, and regulators around the world to scale our work, share what we’ve learned, and advocate for infrastructure and policies that help create a carbon-free energy future. We’ve always said that the path to net-zero carbon won’t be linear. As with any long-term goal, we’ll continue to evolve our strategy over time to do what is right for our business and the planet, to meet the needs of our customers, and to reach net-zero carbon by 2040.", "original_types": ["text"], "id": 581}
{"type": "section", "content": "Packaging\n\nEvery day, we ship millions of orders around the globe, working hard to make sure our products reach customers safely and with the least amount of packaging necessary. Our customers want right-sized, recyclable packaging that minimizes waste and ensures damage-free delivery, which is why we aim to avoid unnecessary packaging whenever possible. When this is not an option, we optimize the type, material, and weight of our packaging to increase circularity, avoid waste, and reduce carbon emissions—without sacrificing safety or functionality.", "doc_id": "amazon2023", "page": 29, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Packaging\n\nEvery day, we ship millions of orders around the globe, working hard to make sure our products reach customers safely and with the least amount of packaging necessary. Our customers want right-sized, recyclable packaging that minimizes waste and ensures damage-free delivery, which is why we aim to avoid unnecessary packaging whenever possible. When this is not an option, we optimize the type, material, and weight of our packaging to increase circularity, avoid waste, and reduce carbon emissions—without sacrificing safety or functionality.", "original_types": ["text", "header"], "id": 582}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 29, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 583}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 29, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 584}
{"type": "section", "content": "Our Approach\n\nAmazon takes a science-based approach to packaging that combines lab testing, machine learning, materials science, and manufacturing partnerships to optimize our packaging. We think locally, taking into consideration factors such as location and geography, average delivery distance to customers, vehicle type, and delivery method. As we regionalize our network, products travel shorter distances to customers and need less protective packaging. We move quickly to invest in responsible shipping and packaging innovations that can scale our program to ship products without additional packaging.\n\nAs we continue to learn and evolve our packaging strategy, we focus on the following priorities:\n\nWe avoid unnecessary packaging whenever possible through our Ships in Product Packaging program and work with suppliers to re-engineer their packaging to qualify for the program.\n\nWhen supplemental Amazon packaging is required, we select lighter, right-sized options to reduce our packaging footprint while still protecting the item. We use artificial intelligence (AI) to optimize packaging types—from bags to boxes—and sizes, depending on the level of protection needed.\n\nWe prioritize materials that are household recyclable. We are also working on innovations and solutions to avoid single-use plastics, and we're reporting on our progress.\n\nWe work across the public and private sectors to create and scale recyclable and biodegradable materials, find circular solutions, and improve recycling infrastructure globally.\n\nOur Progress\n\nShips in Product Packaging Many products can be shipped safely without protection from additional Amazon delivery bags or boxes, which is the main idea behind our Ships in Product Packaging program. Through this program, we deliver eligible items in the manufacturers' original packaging without supplemental Amazon delivery packaging, allowing us to avoid unnecessary material use and reduce the weight of deliveries. Our aim is to continue to increase the number of products shipped without additional packaging, including 13% in the U.S. and Canada, 9% in Europe, 3% in Japan, 19% in India, and 8% in Brazil, Mexico, and Singapore. This progress is a result of continuing to identify products that qualify for the program, working with selling partners to make packaging updates that allow their products to ship in their original packaging, and implementing regionalization. The last is Amazon’s new delivery approach that utilizes shorter routes across the U.S., expanding eligibility for products to ship in their own packaging without risking damage.\n\nLearn more about the Ships in Product Packaging program", "doc_id": "amazon2023", "page": 30, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nAmazon takes a science-based approach to packaging that combines lab testing, machine learning, materials science, and manufacturing partnerships to optimize our packaging. We think locally, taking into consideration factors such as location and geography, average delivery distance to customers, vehicle type, and delivery method. As we regionalize our network, products travel shorter distances to customers and need less protective packaging. We move quickly to invest in responsible shipping and packaging innovations that can scale our program to ship products without additional packaging.\n\nAs we continue to learn and evolve our packaging strategy, we focus on the following priorities:\n\nWe avoid unnecessary packaging whenever possible through our Ships in Product Packaging program and work with suppliers to re-engineer their packaging to qualify for the program.\n\nWhen supplemental Amazon packaging is required, we select lighter, right-sized options to reduce our packaging footprint while still protecting the item. We use artificial intelligence (AI) to optimize packaging types—from bags to boxes—and sizes, depending on the level of protection needed.\n\nWe prioritize materials that are household recyclable. We are also working on innovations and solutions to avoid single-use plastics, and we're reporting on our progress.\n\nWe work across the public and private sectors to create and scale recyclable and biodegradable materials, find circular solutions, and improve recycling infrastructure globally.\n\nOur Progress\n\nShips in Product Packaging Many products can be shipped safely without protection from additional Amazon delivery bags or boxes, which is the main idea behind our Ships in Product Packaging program. Through this program, we deliver eligible items in the manufacturers' original packaging without supplemental Amazon delivery packaging, allowing us to avoid unnecessary material use and reduce the weight of deliveries. Our aim is to continue to increase the number of products shipped without additional packaging, including 13% in the U.S. and Canada, 9% in Europe, 3% in Japan, 19% in India, and 8% in Brazil, Mexico, and Singapore. This progress is a result of continuing to identify products that qualify for the program, working with selling partners to make packaging updates that allow their products to ship in their original packaging, and implementing regionalization. The last is Amazon’s new delivery approach that utilizes shorter routes across the U.S., expanding eligibility for products to ship in their own packaging without risking damage.\n\nLearn more about the Ships in Product Packaging program", "original_types": ["text", "header", "list"], "id": 585}
{"type": "table", "content": "2023 Delivery Packaging by Region\nShips in Product Packaging\nCorrugated Boxes\n\nRegion\nEurope\nU.S. and Canada\nIndia\n\n9%\n47%\n44%\n\n13%\n51%\n36%\n\n19%\n38%\n43%\n", "doc_id": "amazon2023", "page": 30, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Delivery Packaging by Region\nShips in Product Packaging\nCorrugated Boxes\n\nRegion\nEurope\nU.S. and Canada\nIndia\n\n9%\n47%\n44%\n\n13%\n51%\n36%\n\n19%\n38%\n43%\n", "id": 586}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 30, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 587}
{"type": "section", "content": "Inspiring Selling Partners to Reduce Packaging Our Ships in Product Packaging program doesn’t just offer environmental benefits. Less packaging also means lower costs, and Amazon believes in sharing these cost savings with our selling partners. In 2023, we tested decreasing the cost of Fulfillment by Amazon (FBA) for sellers who adopt the program. In early 2024, we expanded this fee reduction to all FBA sellers with eligible products. The program is available to sellers in seven marketplaces: Canada, France, Germany, Italy, Spain, the UK, and the U.S.", "doc_id": "amazon2023", "page": 30, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Inspiring Selling Partners to Reduce Packaging Our Ships in Product Packaging program doesn’t just offer environmental benefits. Less packaging also means lower costs, and Amazon believes in sharing these cost savings with our selling partners. In 2023, we tested decreasing the cost of Fulfillment by Amazon (FBA) for sellers who adopt the program. In early 2024, we expanded this fee reduction to all FBA sellers with eligible products. The program is available to sellers in seven marketplaces: Canada, France, Germany, Italy, Spain, the UK, and the U.S.", "original_types": ["text"], "id": 588}
{"type": "section", "content": "2023 Packaging Use by Region", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Packaging Use by Region", "original_types": ["header"], "id": 589}
{"type": "table", "content": "2023 Packaging Use by Region\nAverage Packaging Weight per Shipment (g/shipment) Metric Tons of Packaging Material Eliminated", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Packaging Use by Region\nAverage Packaging Weight per Shipment (g/shipment) Metric Tons of Packaging Material Eliminated", "id": 590}
{"type": "section", "content": "Europe 96 34,817 U.S. and Canada 111 33,529 India 112 9,910\n\nSingle-Use Plastic Packaging by Region", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Europe 96 34,817 U.S. and Canada 111 33,529 India 112 9,910\n\nSingle-Use Plastic Packaging by Region", "original_types": ["text", "header"], "id": 591}
{"type": "table", "content": "Single-Use Plastic Packaging by Region\nSingle-Use Plastic Packaging Used in 2023 (MT) Single-Use Plastic Packaging Avoided since 2020 (MT)", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Single-Use Plastic Packaging by Region\nSingle-Use Plastic Packaging Used in 2023 (MT) Single-Use Plastic Packaging Avoided since 2020 (MT)", "id": 592}
{"type": "section", "content": "Europe 877 14,600 U.S. and Canada 83,513 41,600 India 1,015 9,100 Japan 931 13,400 Rest of World17 2,362 1,800 Total 88,698 80,500", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Europe 877 14,600 U.S. and Canada 83,513 41,600 India 1,015 9,100 Japan 931 13,400 Rest of World17 2,362 1,800 Total 88,698 80,500", "original_types": ["text"], "id": 593}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 32, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 594}
{"type": "section", "content": "We’re testing new packaging machines in Europe and the U.S. that build made-to-fit paper bags around individual items on demand.\n\nMinimizing and Optimizing Packaging in Grocery\n\nTo make sure groceries ordered from Amazon and Whole Foods Market arrive in good condition, we look for ways to protect products during delivery while still minimizing our packaging footprint.\n\nIncreasing the Recyclability of Device Packaging\n\nAmazon’s strategy to optimize and increase the recyclability of packaging extends to our devices. In 2020, we set an ambitious goal to make our device packaging 100% recyclable by 2023—a first for the consumer electronics industry. To work toward this goal, we collaborated with our suppliers to develop paper-based wraps and films that protect devices and are compatible with paper recycling streams. These paper-based wraps are recyclable in the U.S. where recycling programs are available.\n\nMinimizing and Optimizing Packaging in Grocery\n\nIn 2023, Amazon Fresh reduced our use of insulation packaging material, which avoided approximately 1,180 metric tons of packaging. Similarly, Whole Foods Market avoided 1,300 metric tons of packaging by using daily variable insulation, a practice that tracks and adjusts the amount of insulation packaging used in deliveries in North America to ensure it is used only when temperatures exceed a specific threshold.\n\nMinimizing and Optimizing Packaging in Grocery\n\nGroceries are also being delivered in packaging that is easier to recycle. In 2023, the grocery delivery packaging used by Amazon Fresh was 88% recyclable by weight.\n\nMinimizing and Optimizing Packaging in Grocery\n\nWhole Foods Market published an updated version of its Packaging Guidelines in 2023 to help merchants, procurement teams, and suppliers understand how to make more sustainable choices for product packaging. In alignment with the Whole Foods Market Sustainability Strategy, these guidelines are meant to reduce the amount of packaging it uses, improve the source material, and design for end-of-life and lower greenhouse gas (GHG) emissions. Smarter packaging choices can also help reduce food waste and divert packaging waste from landfills. Whole Foods Market’s Packaging Guidelines are grounded in four core concepts (or \"Principles\") aimed at balancing safety, performance, and sustainability: material safety, material performance and efficiency, design for recovery, and source responsibly.\n\nMinimizing and Optimizing Packaging in Grocery\n\nWe support the European Commission’s goal to tackle excessive packaging via legislation such as the EU Packaging and Packaging Waste Regulation (PPWR). In Europe, we continue to improve our packaging using a science-based approach and will further reduce our environmental impact by investing in new materials, processes, and technologies that drive void and materials reduction at scale.\n\nMinimizing and Optimizing Packaging in Grocery", "doc_id": "amazon2023", "page": "32-33", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We’re testing new packaging machines in Europe and the U.S. that build made-to-fit paper bags around individual items on demand.\n\nMinimizing and Optimizing Packaging in Grocery\n\nTo make sure groceries ordered from Amazon and Whole Foods Market arrive in good condition, we look for ways to protect products during delivery while still minimizing our packaging footprint.\n\nIncreasing the Recyclability of Device Packaging\n\nAmazon’s strategy to optimize and increase the recyclability of packaging extends to our devices. In 2020, we set an ambitious goal to make our device packaging 100% recyclable by 2023—a first for the consumer electronics industry. To work toward this goal, we collaborated with our suppliers to develop paper-based wraps and films that protect devices and are compatible with paper recycling streams. These paper-based wraps are recyclable in the U.S. where recycling programs are available.\n\nMinimizing and Optimizing Packaging in Grocery\n\nIn 2023, Amazon Fresh reduced our use of insulation packaging material, which avoided approximately 1,180 metric tons of packaging. Similarly, Whole Foods Market avoided 1,300 metric tons of packaging by using daily variable insulation, a practice that tracks and adjusts the amount of insulation packaging used in deliveries in North America to ensure it is used only when temperatures exceed a specific threshold.\n\nMinimizing and Optimizing Packaging in Grocery\n\nGroceries are also being delivered in packaging that is easier to recycle. In 2023, the grocery delivery packaging used by Amazon Fresh was 88% recyclable by weight.\n\nMinimizing and Optimizing Packaging in Grocery\n\nWhole Foods Market published an updated version of its Packaging Guidelines in 2023 to help merchants, procurement teams, and suppliers understand how to make more sustainable choices for product packaging. In alignment with the Whole Foods Market Sustainability Strategy, these guidelines are meant to reduce the amount of packaging it uses, improve the source material, and design for end-of-life and lower greenhouse gas (GHG) emissions. Smarter packaging choices can also help reduce food waste and divert packaging waste from landfills. Whole Foods Market’s Packaging Guidelines are grounded in four core concepts (or \"Principles\") aimed at balancing safety, performance, and sustainability: material safety, material performance and efficiency, design for recovery, and source responsibly.\n\nMinimizing and Optimizing Packaging in Grocery\n\nWe support the European Commission’s goal to tackle excessive packaging via legislation such as the EU Packaging and Packaging Waste Regulation (PPWR). In Europe, we continue to improve our packaging using a science-based approach and will further reduce our environmental impact by investing in new materials, processes, and technologies that drive void and materials reduction at scale.\n\nMinimizing and Optimizing Packaging in Grocery", "original_types": ["text", "header"], "id": 595}
{"type": "section", "content": "We remain committed to enhancing our packaging for both performance and sustainability. We will continue working to reduce our packaging footprint by increasing uptake of the Ships in Product Packaging program, leveraging machine learning and other innovative technologies to optimize packaging size and shape, and transitioning to more circular packaging materials instead of single-use plastics. In fact, in February 2024, we announced that nearly 50% of customer orders from our India fulfillment network now come with reduced or no added packaging. We will also keep prioritizing our engagement with selling partners and larger-scale coalitions to accelerate collective progress.", "doc_id": "amazon2023", "page": 33, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We remain committed to enhancing our packaging for both performance and sustainability. We will continue working to reduce our packaging footprint by increasing uptake of the Ships in Product Packaging program, leveraging machine learning and other innovative technologies to optimize packaging size and shape, and transitioning to more circular packaging materials instead of single-use plastics. In fact, in February 2024, we announced that nearly 50% of customer orders from our India fulfillment network now come with reduced or no added packaging. We will also keep prioritizing our engagement with selling partners and larger-scale coalitions to accelerate collective progress.", "original_types": ["text"], "id": 596}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 33, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 597}
{"type": "section", "content": "We use paper-based wraps and films where we can to avoid plastic in our device packaging.\n\nWaste and Circularity\n\nAround the world, natural resource extraction and waste generation have grown significantly. In the last six years alone, the global economy consumed over half a trillion tons of materials—nearly as much as the materials consumed throughout the entire 20th century.19 As resources continue to be extracted, it is imperative for businesses to do all they can to prevent and reduce waste. At Amazon, we strive to be a responsible steward of our planet’s finite resources. We know that contributing to a circular economy will help mitigate the effects of climate change, reduce biodiversity loss, and alleviate other global challenges by decoupling economic activity from resource consumption. With this in mind, we are working to increase what we resell, reuse, and recycle across our business and to reduce what we ultimately send to landfills.", "doc_id": "amazon2023", "page": "33-34", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We use paper-based wraps and films where we can to avoid plastic in our device packaging.\n\nWaste and Circularity\n\nAround the world, natural resource extraction and waste generation have grown significantly. In the last six years alone, the global economy consumed over half a trillion tons of materials—nearly as much as the materials consumed throughout the entire 20th century.19 As resources continue to be extracted, it is imperative for businesses to do all they can to prevent and reduce waste. At Amazon, we strive to be a responsible steward of our planet’s finite resources. We know that contributing to a circular economy will help mitigate the effects of climate change, reduce biodiversity loss, and alleviate other global challenges by decoupling economic activity from resource consumption. With this in mind, we are working to increase what we resell, reuse, and recycle across our business and to reduce what we ultimately send to landfills.", "original_types": ["text", "header"], "id": 598}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 34, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 599}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 34, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 600}
{"type": "section", "content": "Our Approach\n\nOur waste hierarchy sets out our guiding principles for preventing, managing, and reducing waste. Amazon has programs in place to optimize our inventory, reduce food surplus, and source materials that help us prevent waste in the first place. Where possible, we look for ways to reduce, reuse, recycle, or compost these materials.\n\nWe work hard to make it easy for customers to discover products they love. They usually do, but just like with any retailer, sometimes customers want to return something that they purchased from us. If customers do need to return something, Amazon strives to give items a second life through resale, donation, or recycling—in that order of priority. Returned items undergo inspection to determine whether they can be resold. We aim to donate items that cannot be sold, but when they are not suitable for sale, resale, or donation, often due to damage or expiration, we prioritize their recycling.\n\nWe amplify our efforts by considering how customers use our products through end-of-life and by providing them with opportunities to repair or resell products. Our circular approach keeps valuable resources in use longer, avoiding waste and reducing related effects such as carbon emissions and pollution.\n\nWe know that avoiding waste is an ongoing process and that we cannot do it on our own. We engage with suppliers to reduce waste related to our products, partner with other organizations to scale our efforts to transition to a more circular economy, and work with local municipalities to improve recycling infrastructure where we can.\n\nOur Approach to Managing and Preventing Waste\n\nThis hierarchy is an industry framework that guides our approach to managing and preventing waste. It moves from the most preferred option at the top to the least preferred at the bottom. Materials in this hierarchy may be recycled, reclaimed, or otherwise reused in some way and thus may not end up as waste in landfills. We use this framework to better manage our waste, pursuing opportunities that are more preferred before moving down the hierarchy.\n\nPrevent\nWe improve product and packaging design, inventory management, materials sourcing, and resource use to prevent waste from the start.\n\nReduce\nWe pursue avenues such as increased product durability and resale to reduce waste. We also work to prevent waste generation by helping customers make informed decisions, which reduces product returns, and offer several ways for them to support product circularity.\n\nReuse\nWe repair, repurpose, and donate usable items where possible.\n\nRecycle\nWe recycle and compost to recover raw materials where possible, including food waste.\n\nIncineration with Energy Recovery\nWhere prevention, reduction, reuse, or recycling are not possible, we target energy recovery.", "doc_id": "amazon2023", "page": 35, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nOur waste hierarchy sets out our guiding principles for preventing, managing, and reducing waste. Amazon has programs in place to optimize our inventory, reduce food surplus, and source materials that help us prevent waste in the first place. Where possible, we look for ways to reduce, reuse, recycle, or compost these materials.\n\nWe work hard to make it easy for customers to discover products they love. They usually do, but just like with any retailer, sometimes customers want to return something that they purchased from us. If customers do need to return something, Amazon strives to give items a second life through resale, donation, or recycling—in that order of priority. Returned items undergo inspection to determine whether they can be resold. We aim to donate items that cannot be sold, but when they are not suitable for sale, resale, or donation, often due to damage or expiration, we prioritize their recycling.\n\nWe amplify our efforts by considering how customers use our products through end-of-life and by providing them with opportunities to repair or resell products. Our circular approach keeps valuable resources in use longer, avoiding waste and reducing related effects such as carbon emissions and pollution.\n\nWe know that avoiding waste is an ongoing process and that we cannot do it on our own. We engage with suppliers to reduce waste related to our products, partner with other organizations to scale our efforts to transition to a more circular economy, and work with local municipalities to improve recycling infrastructure where we can.\n\nOur Approach to Managing and Preventing Waste\n\nThis hierarchy is an industry framework that guides our approach to managing and preventing waste. It moves from the most preferred option at the top to the least preferred at the bottom. Materials in this hierarchy may be recycled, reclaimed, or otherwise reused in some way and thus may not end up as waste in landfills. We use this framework to better manage our waste, pursuing opportunities that are more preferred before moving down the hierarchy.\n\nPrevent\nWe improve product and packaging design, inventory management, materials sourcing, and resource use to prevent waste from the start.\n\nReduce\nWe pursue avenues such as increased product durability and resale to reduce waste. We also work to prevent waste generation by helping customers make informed decisions, which reduces product returns, and offer several ways for them to support product circularity.\n\nReuse\nWe repair, repurpose, and donate usable items where possible.\n\nRecycle\nWe recycle and compost to recover raw materials where possible, including food waste.\n\nIncineration with Energy Recovery\nWhere prevention, reduction, reuse, or recycling are not possible, we target energy recovery.", "original_types": ["text", "header"], "id": 601}
{"type": "section", "content": "Landfill and Incineration without Energy Recovery\nBoth landfill and incineration without energy recovery are disposal methods strictly used as a last resort for waste that is either ineligible for or cannot be diverted to better recovery pathways.", "doc_id": "amazon2023", "page": 35, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Landfill and Incineration without Energy Recovery\nBoth landfill and incineration without energy recovery are disposal methods strictly used as a last resort for waste that is either ineligible for or cannot be diverted to better recovery pathways.", "original_types": ["text"], "id": 602}
{"type": "section", "content": "Preventing and Managing Waste\n\nOur external (indirect) waste footprint includes the following areas:\n\nIn 2023, we improved our ability to gather accurate, timely, and robust data. This allows us to enhance waste tracking and management as our business changes and as we add new sites and onboard waste vendors. One enabler of this progress was the creation of a central data collection tool when disposed of. Beyond the work we do to help customers keep items in use longer, we also offer several ways for them to support product circularity and prevent waste through our Amazon Second Chance program. We will continue to build these processes and tools throughout 2024.\n\nOur internal (direct) waste footprint includes:\n\n• Customer waste is generated during product use or in the form of product and delivery packaging, including when disposed of. Beyond the work we do to help customers keep items in use longer, we also offer several ways for them to support product circularity and prevent waste through our Amazon Second Chance program.\n\n• Construction waste comes from activities related to building our facilities. This consists mainly of commingled construction and demolition waste such as metals, masonry, asphalt, and drywall.\n\nFood Waste\n\nIn the U.S., nearly 98% of Amazon’s food handled goes to human and animal consumption, and in Europe, more than 99% does. While these figures are encouraging, we are always exploring new ways to reduce food loss and waste as we work toward our goal to reduce our food waste by 50% across our U.S. and Europe operations by 2030. This commitment is reflective of our membership in the U.S. Environmental Protection Agency’s Food Loss and Waste 2030 Champions, which we joined in 2020. We then extended our commitment to reducing food waste to our Europe operations in 2021. We measure our progress against this commitment with a food waste intensity metric that calculates the amount of food waste generated as a percentage of total food handled by weight within Amazon. As of December 2023, our food waste intensity had decreased by 28% in the U.S. and by 75% in Europe compared to a 2021 baseline.\n\nWe also advocate for policies that would help prevent usable food from being wasted. In 2023, Amazon and Whole Foods Market supported the U.S. Food Date Labeling Act because confusion around food expiration date labeling can result in usable food being unnecessarily thrown out. Our engagement included advocating to Congress for standardizing and clarifying expiration date labels on food as part of a coalition led by the World Wildlife Fund and the Zero Food Waste Coalition.\n\nIn November 2023, we opened a pop-up Second Chance Store in London, where customers could shop second-hand products from Amazon.", "doc_id": "amazon2023", "page": 36, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Preventing and Managing Waste\n\nOur external (indirect) waste footprint includes the following areas:\n\nIn 2023, we improved our ability to gather accurate, timely, and robust data. This allows us to enhance waste tracking and management as our business changes and as we add new sites and onboard waste vendors. One enabler of this progress was the creation of a central data collection tool when disposed of. Beyond the work we do to help customers keep items in use longer, we also offer several ways for them to support product circularity and prevent waste through our Amazon Second Chance program. We will continue to build these processes and tools throughout 2024.\n\nOur internal (direct) waste footprint includes:\n\n• Customer waste is generated during product use or in the form of product and delivery packaging, including when disposed of. Beyond the work we do to help customers keep items in use longer, we also offer several ways for them to support product circularity and prevent waste through our Amazon Second Chance program.\n\n• Construction waste comes from activities related to building our facilities. This consists mainly of commingled construction and demolition waste such as metals, masonry, asphalt, and drywall.\n\nFood Waste\n\nIn the U.S., nearly 98% of Amazon’s food handled goes to human and animal consumption, and in Europe, more than 99% does. While these figures are encouraging, we are always exploring new ways to reduce food loss and waste as we work toward our goal to reduce our food waste by 50% across our U.S. and Europe operations by 2030. This commitment is reflective of our membership in the U.S. Environmental Protection Agency’s Food Loss and Waste 2030 Champions, which we joined in 2020. We then extended our commitment to reducing food waste to our Europe operations in 2021. We measure our progress against this commitment with a food waste intensity metric that calculates the amount of food waste generated as a percentage of total food handled by weight within Amazon. As of December 2023, our food waste intensity had decreased by 28% in the U.S. and by 75% in Europe compared to a 2021 baseline.\n\nWe also advocate for policies that would help prevent usable food from being wasted. In 2023, Amazon and Whole Foods Market supported the U.S. Food Date Labeling Act because confusion around food expiration date labeling can result in usable food being unnecessarily thrown out. Our engagement included advocating to Congress for standardizing and clarifying expiration date labels on food as part of a coalition led by the World Wildlife Fund and the Zero Food Waste Coalition.\n\nIn November 2023, we opened a pop-up Second Chance Store in London, where customers could shop second-hand products from Amazon.", "original_types": ["text", "header"], "id": 603}
{"type": "section", "content": "Unlocking New Insights through Continuous Learning\n\nAs we continue on our journey to reduce waste—including our commitment to reduce food waste by 50% in our U.S. and Europe operations by 2030—we are exploring different ways to learn and grow. As of the end of 2023, four Amazon Fresh facilities had achieved UL’s Zero Waste to Landfill certification at Silver or Gold level. We secured this certification by improving operational waste sorting processes, standardizing recycling practices at all sites, introducing new associate trainings, and establishing new diversion pathways, including for food waste. Through the certification process, we learned new ways of working and identified potential opportunities for Amazon. We are excited to leverage what we have learned to improve our overall waste footprint.\n\nReducing and Managing Inventory and Customer Waste\n\nPrevent\nWhen it comes to product inventory, our priority is to prevent waste, both by helping customers make more informed shopping decisions—which helps reduce customer returns—and by reducing the number of products damaged in handling.\n\nReduce and Reuse\nInventory that is not resold or donated may still be used, reused, or reclaimed through our inventory recycling program. We leverage existing recycling streams where possible and work closely with recycling partners to find\n\nReducing overstock\n\nBefore we remove overstock from our inventory, we try to sell it at a deep discount on our Amazon Outlet storefront. When we cannot sell these items, inventory is returned to vendors or sold to wholesalers to be offered on secondary markets.\n\nRepair and resell\n\nWe carefully inspect and evaluate all returned and undelivered items at our Amazon return centers. If returned items meet Amazon’s high bar for sale as new, we re-list them. For items that are not eligible for re-listing as new but meet Amazon’s standards to be sold as used, we (or our specialized repair vendors) test, clean, repair, and repackage them prior to re-listing the item for sale. We continue to expand our repair capabilities, repairing 24% more products worldwide in 2023 than in 2022. Amazon customers viewing items listed for sale as used will see a condition (e.g., Like New, Very Good, Good, Acceptable) and reason(s) for the condition (e.g., missing a user manual, cosmetic damage), allowing them to make the appropriate purchasing decision.\n\nAmazon Second Chance Store\n\nIn November 2023, Amazon opened a pop-up Second Chance Store in central London, where shoppers could buy quality returned, refurbished, and open-box items in-person. More than 6,500 shoppers visited our store, browsing for kitchen and household appliances, books, games and toys, electronics, and more. While the physical Second Chance Store was temporary, customers can benefit from our Second Chance program online year-round.\n\nReCommerce for Amazon sellers", "doc_id": "amazon2023", "page": 37, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Unlocking New Insights through Continuous Learning\n\nAs we continue on our journey to reduce waste—including our commitment to reduce food waste by 50% in our U.S. and Europe operations by 2030—we are exploring different ways to learn and grow. As of the end of 2023, four Amazon Fresh facilities had achieved UL’s Zero Waste to Landfill certification at Silver or Gold level. We secured this certification by improving operational waste sorting processes, standardizing recycling practices at all sites, introducing new associate trainings, and establishing new diversion pathways, including for food waste. Through the certification process, we learned new ways of working and identified potential opportunities for Amazon. We are excited to leverage what we have learned to improve our overall waste footprint.\n\nReducing and Managing Inventory and Customer Waste\n\nPrevent\nWhen it comes to product inventory, our priority is to prevent waste, both by helping customers make more informed shopping decisions—which helps reduce customer returns—and by reducing the number of products damaged in handling.\n\nReduce and Reuse\nInventory that is not resold or donated may still be used, reused, or reclaimed through our inventory recycling program. We leverage existing recycling streams where possible and work closely with recycling partners to find\n\nReducing overstock\n\nBefore we remove overstock from our inventory, we try to sell it at a deep discount on our Amazon Outlet storefront. When we cannot sell these items, inventory is returned to vendors or sold to wholesalers to be offered on secondary markets.\n\nRepair and resell\n\nWe carefully inspect and evaluate all returned and undelivered items at our Amazon return centers. If returned items meet Amazon’s high bar for sale as new, we re-list them. For items that are not eligible for re-listing as new but meet Amazon’s standards to be sold as used, we (or our specialized repair vendors) test, clean, repair, and repackage them prior to re-listing the item for sale. We continue to expand our repair capabilities, repairing 24% more products worldwide in 2023 than in 2022. Amazon customers viewing items listed for sale as used will see a condition (e.g., Like New, Very Good, Good, Acceptable) and reason(s) for the condition (e.g., missing a user manual, cosmetic damage), allowing them to make the appropriate purchasing decision.\n\nAmazon Second Chance Store\n\nIn November 2023, Amazon opened a pop-up Second Chance Store in central London, where shoppers could buy quality returned, refurbished, and open-box items in-person. More than 6,500 shoppers visited our store, browsing for kitchen and household appliances, books, games and toys, electronics, and more. While the physical Second Chance Store was temporary, customers can benefit from our Second Chance program online year-round.\n\nReCommerce for Amazon sellers", "original_types": ["text", "header"], "id": 604}
{"type": "section", "content": "When returned items are not eligible for resale as new or used, those items are returned to sellers, liquidated, donated, or recycled. ReCommerce is the selling of previously owned items to buyers who reuse, recycle, or resell them. We provide ReCommerce services to Amazon sellers by grading their returned items and enabling recovery through programs such as grade and resell, liquidation, or donation. In 2023, we helped sellers resell, liquidate, or donate nearly 368 million of their items in the U.S. and Europe, a 42% increase compared to 2022.\n\nDonation\n\nWe donate items that are safe to use but remain in our inventory after we try to reuse, resell, or repair them. In 2023, we donated or helped our sellers donate over 162 million items worldwide.", "doc_id": "amazon2023", "page": 37, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "When returned items are not eligible for resale as new or used, those items are returned to sellers, liquidated, donated, or recycled. ReCommerce is the selling of previously owned items to buyers who reuse, recycle, or resell them. We provide ReCommerce services to Amazon sellers by grading their returned items and enabling recovery through programs such as grade and resell, liquidation, or donation. In 2023, we helped sellers resell, liquidate, or donate nearly 368 million of their items in the U.S. and Europe, a 42% increase compared to 2022.\n\nDonation\n\nWe donate items that are safe to use but remain in our inventory after we try to reuse, resell, or repair them. In 2023, we donated or helped our sellers donate over 162 million items worldwide.", "original_types": ["text", "header"], "id": 605}
{"type": "section", "content": "AWS Circular Economy\n\nAWS embraces circular economy principles for its server racks by designing reusable and lower-carbon rack systems from the outset. In addition, AWS works to keep equipment operating efficiently and to recover value from securely decommissioned equipment through reuse, repair, and recycling. By working to maximize resource value for as long as possible, AWS reduces waste generation from its global operations, decreases the use of raw materials, and reduces carbon emissions across its supply chain. This program is now operational and AWS expects it to grow, avoiding even more inventory or to third parties to be sold for reuse. They also enable AWS to optimize component reuse across its data centers, taking decommissioned equipment from one facility and redeploying it to serve demand elsewhere. In 2023, AWS expanded its reverse logistics global square footage and capacity by making investments in three additional sites located in the U.S., Europe, and Asia. The added scale and enhanced capabilities provide global coverage for AWS's decommissioned server and network hardware to be tested, repaired, reused, or recycled. As a result, in 2023, 14.6 million hardware components were diverted from landfills by being recycled or sold into the secondary market for reuse.\n\nReducing and Managing Noninventory Waste\n\nNoninventory waste is made up of the materials we use to run our businesses, including cardboard packaging, shipping pallets, plastic stretch film, and paper labels. Our goal is to segregate these materials as much as possible so they can be more easily recycled. Cardboard is one of our largest noninventory waste streams, so we work closely with third-party recycling partners, including the companies that we source our cardboard packaging from, to recycle these materials.\n\nIn 2023, we:\n\n• Improved our ability to collect and consolidate waste data from our vendors. This gives us a better understanding of our footprint and has allowed us to identify opportunities to increase waste diversion rates across our complex business.\n• Increased our rate of operational waste diverted from disposal, placing emphasis on recycling and composting.\n• Expanded our recycling efforts to include hard-to-recycle materials such as adhesive label backing, bulk cargo bags, and even personal protective equipment (such as masks and gloves).\n• Replaced single-use cardboard shipping containers with reusable outbound carts to reduce cardboard waste at certain sites.\n\nAWS Hard Drive Consolidation", "doc_id": "amazon2023", "page": 38, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "AWS Circular Economy\n\nAWS embraces circular economy principles for its server racks by designing reusable and lower-carbon rack systems from the outset. In addition, AWS works to keep equipment operating efficiently and to recover value from securely decommissioned equipment through reuse, repair, and recycling. By working to maximize resource value for as long as possible, AWS reduces waste generation from its global operations, decreases the use of raw materials, and reduces carbon emissions across its supply chain. This program is now operational and AWS expects it to grow, avoiding even more inventory or to third parties to be sold for reuse. They also enable AWS to optimize component reuse across its data centers, taking decommissioned equipment from one facility and redeploying it to serve demand elsewhere. In 2023, AWS expanded its reverse logistics global square footage and capacity by making investments in three additional sites located in the U.S., Europe, and Asia. The added scale and enhanced capabilities provide global coverage for AWS's decommissioned server and network hardware to be tested, repaired, reused, or recycled. As a result, in 2023, 14.6 million hardware components were diverted from landfills by being recycled or sold into the secondary market for reuse.\n\nReducing and Managing Noninventory Waste\n\nNoninventory waste is made up of the materials we use to run our businesses, including cardboard packaging, shipping pallets, plastic stretch film, and paper labels. Our goal is to segregate these materials as much as possible so they can be more easily recycled. Cardboard is one of our largest noninventory waste streams, so we work closely with third-party recycling partners, including the companies that we source our cardboard packaging from, to recycle these materials.\n\nIn 2023, we:\n\n• Improved our ability to collect and consolidate waste data from our vendors. This gives us a better understanding of our footprint and has allowed us to identify opportunities to increase waste diversion rates across our complex business.\n• Increased our rate of operational waste diverted from disposal, placing emphasis on recycling and composting.\n• Expanded our recycling efforts to include hard-to-recycle materials such as adhesive label backing, bulk cargo bags, and even personal protective equipment (such as masks and gloves).\n• Replaced single-use cardboard shipping containers with reusable outbound carts to reduce cardboard waste at certain sites.\n\nAWS Hard Drive Consolidation", "original_types": ["text", "header", "list"], "id": 606}
{"type": "figure", "content": "Figure 1: AWS Hard Drive Consolidation", "doc_id": "amazon2023", "page": 38, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: AWS Hard Drive Consolidation", "id": 607}
{"type": "section", "content": "Supply Chain Waste\n\nAmazon works with suppliers to reduce waste related to the manufacture of our devices. For example, our devices teams engage suppliers to decrease waste and material use as part of our work to encourage more efficient manufacturing. As of 2023, 42 supplier sites of Amazon devices had achieved UL’s Zero Waste to Landfill certification at Silver or better, an increase from 10 supplier sites when we first launched the program in 2021. All final assembly sites worldwide and all China-based packaging sites for Echo, Kindle, Fire tablet, Fire TV devices, cables, and adapters have also secured Zero Waste to Landfill certification at Silver or better.\n\nAdvocating for Waste Shipment Efficiency\n\nIn 2023, we co-led an industry effort to streamline and modernize waste shipment procedures, so that waste materials reach destinations with the best possible sustainability outcomes, including increased and more efficient recovery of critical raw materials. The Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal requires the origin country, any transit countries, and the destination country to approve certain shipments of waste. This process, known as prior informed consent (PIC), can be complex, time-consuming, and burdensome for under-resourced governments. In many countries, PIC documents are mainly transmitted by post, fax, and email. We are working with other industry stakeholders and international nongovernmental organizations to launch and scale the Ellen MacArthur Foundation Network, which is dedicated to creating a circular economy, we work to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. This collaboration focuses on certifications for products with circular features, providing customers with the information they need to make more efficient to countries with capacity for environmentally sound management of waste and maximum resource recovery potential. These certifications will also encourage brands, OEMs, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.\n\nPartnering for Circularity", "doc_id": "amazon2023", "page": 39, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supply Chain Waste\n\nAmazon works with suppliers to reduce waste related to the manufacture of our devices. For example, our devices teams engage suppliers to decrease waste and material use as part of our work to encourage more efficient manufacturing. As of 2023, 42 supplier sites of Amazon devices had achieved UL’s Zero Waste to Landfill certification at Silver or better, an increase from 10 supplier sites when we first launched the program in 2021. All final assembly sites worldwide and all China-based packaging sites for Echo, Kindle, Fire tablet, Fire TV devices, cables, and adapters have also secured Zero Waste to Landfill certification at Silver or better.\n\nAdvocating for Waste Shipment Efficiency\n\nIn 2023, we co-led an industry effort to streamline and modernize waste shipment procedures, so that waste materials reach destinations with the best possible sustainability outcomes, including increased and more efficient recovery of critical raw materials. The Basel Convention on the Control of Transboundary Movements of Hazardous Wastes and Their Disposal requires the origin country, any transit countries, and the destination country to approve certain shipments of waste. This process, known as prior informed consent (PIC), can be complex, time-consuming, and burdensome for under-resourced governments. In many countries, PIC documents are mainly transmitted by post, fax, and email. We are working with other industry stakeholders and international nongovernmental organizations to launch and scale the Ellen MacArthur Foundation Network, which is dedicated to creating a circular economy, we work to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. This collaboration focuses on certifications for products with circular features, providing customers with the information they need to make more efficient to countries with capacity for environmentally sound management of waste and maximum resource recovery potential. These certifications will also encourage brands, OEMs, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.\n\nPartnering for Circularity", "original_types": ["text", "header"], "id": 608}
{"type": "section", "content": "We cannot solve all the challenges that underpin the broader shift to a circular economy alone, so we are working with multiple industry partners to innovate and bring about change at scale. In 2023, we expanded our collaboration with the Ellen MacArthur Foundation to drive scalable, industry-wide solutions for a circular economy. As a Strategic Partner of the Ellen MacArthur Foundation Network, which is dedicated to creating a circular economy, we work to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. This collaboration focuses on certifications for products with circular features, providing customers with the information they need to make more efficient to countries with capacity for environmentally sound management of waste and maximum resource recovery potential. These certifications will also encourage brands, OEMs, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.\n\nLooking Forward\n\nReducing waste across Amazon is an ongoing journey. We will strive to advance our work across our waste hierarchy to prevent waste, and we’ll continue scaling waste reduction and product circularity programs. Doing so in a manner that can keep up with the pace at which resource use is growing will require adopting new technologies that enable us to make progress more quickly and at a larger scale. In 2024, The Climate Pledge Fund invested in Glacier, a company using AI-powered robots to automate the sorting of recyclables and collect real-time data on recycling streams. This technology helps recycling centers sort materials more effectively so that recyclable items stay out of landfills. We are also excited to collaborate with Glacier on a pilot project to sort novel biomaterials, which would enable recycling for materials that currently cannot be reused.", "doc_id": "amazon2023", "page": 39, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We cannot solve all the challenges that underpin the broader shift to a circular economy alone, so we are working with multiple industry partners to innovate and bring about change at scale. In 2023, we expanded our collaboration with the Ellen MacArthur Foundation to drive scalable, industry-wide solutions for a circular economy. As a Strategic Partner of the Ellen MacArthur Foundation Network, which is dedicated to creating a circular economy, we work to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. This collaboration focuses on certifications for products with circular features, providing customers with the information they need to make more efficient to countries with capacity for environmentally sound management of waste and maximum resource recovery potential. These certifications will also encourage brands, OEMs, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.\n\nLooking Forward\n\nReducing waste across Amazon is an ongoing journey. We will strive to advance our work across our waste hierarchy to prevent waste, and we’ll continue scaling waste reduction and product circularity programs. Doing so in a manner that can keep up with the pace at which resource use is growing will require adopting new technologies that enable us to make progress more quickly and at a larger scale. In 2024, The Climate Pledge Fund invested in Glacier, a company using AI-powered robots to automate the sorting of recyclables and collect real-time data on recycling streams. This technology helps recycling centers sort materials more effectively so that recyclable items stay out of landfills. We are also excited to collaborate with Glacier on a pilot project to sort novel biomaterials, which would enable recycling for materials that currently cannot be reused.", "original_types": ["text", "header"], "id": 609}
{"type": "section", "content": "Water\n\nMore than 2 billion people around the globe do not have access to safe drinking water, and roughly half the world’s population experiences severe water scarcity for at least part of the year, due to climate change, population growth, and economic development.23 Amazon knows that responsible water management practices can mitigate water stress, which is a risk to not only our employees, customers, and communities, but also our business. We are committed to doing our part to help solve this rapidly growing challenge in the communities where we operate, as investment in local water resources is known to improve health, empower women, enable access to education, increase family income, and improve overall quality of life. To foster a more sustainable and resilient future, we are reducing our water footprint by conserving and reusing water across our on-site operations and throughout our communities. We’re also working with nonprofit and public partners to increase fresh water availability in water-scarce regions.", "doc_id": "amazon2023", "page": 40, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Water\n\nMore than 2 billion people around the globe do not have access to safe drinking water, and roughly half the world’s population experiences severe water scarcity for at least part of the year, due to climate change, population growth, and economic development.23 Amazon knows that responsible water management practices can mitigate water stress, which is a risk to not only our employees, customers, and communities, but also our business. We are committed to doing our part to help solve this rapidly growing challenge in the communities where we operate, as investment in local water resources is known to improve health, empower women, enable access to education, increase family income, and improve overall quality of life. To foster a more sustainable and resilient future, we are reducing our water footprint by conserving and reusing water across our on-site operations and throughout our communities. We’re also working with nonprofit and public partners to increase fresh water availability in water-scarce regions.", "original_types": ["text", "header"], "id": 610}
{"type": "section", "content": "Our Approach\n\nWe take a proactive approach to water stewardship, analyzing our footprint, driving operational resilience, and building more efficient systems that reduce our impact on local water sources. Our global water use includes water used in both our direct operations and wider supply chain.\n\nOur Progress\n\nWater Positive in Data Centers\n\nIn 2022, AWS announced its commitment to being water positive by 2030. To meet this goal, AWS is delivering on a water program that we can tailor to specific needs across our company. This enables us to better forecast future water demand, determine the best ways to conserve water, and create more efficient systems that will benefit local watersheds over time.\n\nWater use effectiveness: AWS is continually working to optimize its water consumption and aims to improve its overall WUE by reducing how much incoming water it uses. AWS uses cloud technologies such as Internet of Things (IoT) to analyze real-time water use and identify leaks.\n\nWater Risk Assessment\n\nIn 2023, Amazon conducted an in-depth, site-level global water risk assessment of our retail, operations, and corporate buildings to determine which regions to prioritize for intervention. We ranked regions by looking at their intrinsic water scarcity based on hazards such as water stress, groundwater table decline, and variability of water supply. Through this goal, AWS is focused on reducing overall water withdrawal and replenishing water in basins facing water scarcity around data centers.\n\nReusing cooling water in communities: Data centers use non-contact cooling water to keep our servers from overheating. AWS finds ways to return the water to the community, including conveying the water to third parties for use in irrigation.\n\nWater Use Effectiveness\n\nAWS minimizes water use by using real-time data to identify leaks, piloting new treatment technologies, and exploring a range of operational changes, such as installing sensors and alerts to track water use and detect anomalies.\n\nGlobal teams deploy water monitoring technology in AWS data centers to determine where they need to take action to maintain or improve WUE. In 2023, AWS installed thousands of sensors in its data centers to track water use. Automatic alerts inform AWS of any anomalies so that operators can investigate in near-real time. AWS also invested in on-site water treatment systems that remove scale-forming minerals and allow AWS to recycle more water on-site and minimize the water consumed for cooling. These technologies helped", "doc_id": "amazon2023", "page": 41, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nWe take a proactive approach to water stewardship, analyzing our footprint, driving operational resilience, and building more efficient systems that reduce our impact on local water sources. Our global water use includes water used in both our direct operations and wider supply chain.\n\nOur Progress\n\nWater Positive in Data Centers\n\nIn 2022, AWS announced its commitment to being water positive by 2030. To meet this goal, AWS is delivering on a water program that we can tailor to specific needs across our company. This enables us to better forecast future water demand, determine the best ways to conserve water, and create more efficient systems that will benefit local watersheds over time.\n\nWater use effectiveness: AWS is continually working to optimize its water consumption and aims to improve its overall WUE by reducing how much incoming water it uses. AWS uses cloud technologies such as Internet of Things (IoT) to analyze real-time water use and identify leaks.\n\nWater Risk Assessment\n\nIn 2023, Amazon conducted an in-depth, site-level global water risk assessment of our retail, operations, and corporate buildings to determine which regions to prioritize for intervention. We ranked regions by looking at their intrinsic water scarcity based on hazards such as water stress, groundwater table decline, and variability of water supply. Through this goal, AWS is focused on reducing overall water withdrawal and replenishing water in basins facing water scarcity around data centers.\n\nReusing cooling water in communities: Data centers use non-contact cooling water to keep our servers from overheating. AWS finds ways to return the water to the community, including conveying the water to third parties for use in irrigation.\n\nWater Use Effectiveness\n\nAWS minimizes water use by using real-time data to identify leaks, piloting new treatment technologies, and exploring a range of operational changes, such as installing sensors and alerts to track water use and detect anomalies.\n\nGlobal teams deploy water monitoring technology in AWS data centers to determine where they need to take action to maintain or improve WUE. In 2023, AWS installed thousands of sensors in its data centers to track water use. Automatic alerts inform AWS of any anomalies so that operators can investigate in near-real time. AWS also invested in on-site water treatment systems that remove scale-forming minerals and allow AWS to recycle more water on-site and minimize the water consumed for cooling. These technologies helped", "original_types": ["text", "header"], "id": 611}
{"type": "section", "content": "AWS Water Use Effectiveness", "doc_id": "amazon2023", "page": 42, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "AWS Water Use Effectiveness", "original_types": ["header"], "id": 612}
{"type": "table", "content": "AWS Water Use Effectiveness\n2021 2022 2023 YoY% \nWater use effectiveness (L/kWh) 0.25 0.19 0.18 -5%", "doc_id": "amazon2023", "page": 42, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "AWS Water Use Effectiveness\n2021 2022 2023 YoY% \nWater use effectiveness (L/kWh) 0.25 0.19 0.18 -5%", "id": 613}
{"type": "section", "content": "improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.\n\nFIDO Tech, a cloud-based water leak detection company that uses AWS technology, to identify and reduce leakages in the water system in Spain’s Villanueva de Gállego community. This municipality is located in the province of Zaragoza, Aragon, between Barcelona and Madrid, where AWS data centers are located.\n\nDelivering Water Replenishment\n\nBy the end of 2023, AWS had invested in 15 water replenishment activities, returning water to the communities where it operates across 12 of our global infrastructure regions. These investments have expanded AWS replenishment activities to three new countries, bringing the total to 10: Australia, Brazil, India, Indonesia, Ireland, South Africa, Spain, Sweden, the UK, and the U.S. These activities help recharge groundwater, build wetlands, improve water quality, and reduce water loss in utility systems. In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.\n\nSupplying Water to Farmers in India and Indonesia\n\nWatershed Restoration in Australia\n\nAWS is working with the Great Eastern Ranges (GER)—a nongovernmental organization whose aim is to protect, connect, and regenerate wildlife habitats across eastern Australia—to enhance the health and functioning of the major watershed serving Sydney. Catastrophic bushfires in late 2019 and early 2020 damaged large areas of this watershed. This joint project is restoring the local environment in key locations to improve water yield and quality, boost biodiversity, and enhance the resilience of local communities against the effects of climate change. Together, AWS and GER have supported initiatives that are reducing polluted stormwater runoff, increasing groundwater recharge, enhancing local biodiversity, and supporting wildlife, including 15 endangered species in the affected area. Once complete, the project is expected to deliver an additional 32 million liters of water each year to the Sydney watershed.", "doc_id": "amazon2023", "page": 42, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "improve AWS’s industry-leading global data center WUE to 0.18 liters of water per kilowatt-hour (L/kWh) in 2023 from 0.19 L/kWh in 2022—a 5% improvement year over year and a 28% improvement since 2021.\n\nFIDO Tech, a cloud-based water leak detection company that uses AWS technology, to identify and reduce leakages in the water system in Spain’s Villanueva de Gállego community. This municipality is located in the province of Zaragoza, Aragon, between Barcelona and Madrid, where AWS data centers are located.\n\nDelivering Water Replenishment\n\nBy the end of 2023, AWS had invested in 15 water replenishment activities, returning water to the communities where it operates across 12 of our global infrastructure regions. These investments have expanded AWS replenishment activities to three new countries, bringing the total to 10: Australia, Brazil, India, Indonesia, Ireland, South Africa, Spain, Sweden, the UK, and the U.S. These activities help recharge groundwater, build wetlands, improve water quality, and reduce water loss in utility systems. In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.\n\nSupplying Water to Farmers in India and Indonesia\n\nWatershed Restoration in Australia\n\nAWS is working with the Great Eastern Ranges (GER)—a nongovernmental organization whose aim is to protect, connect, and regenerate wildlife habitats across eastern Australia—to enhance the health and functioning of the major watershed serving Sydney. Catastrophic bushfires in late 2019 and early 2020 damaged large areas of this watershed. This joint project is restoring the local environment in key locations to improve water yield and quality, boost biodiversity, and enhance the resilience of local communities against the effects of climate change. Together, AWS and GER have supported initiatives that are reducing polluted stormwater runoff, increasing groundwater recharge, enhancing local biodiversity, and supporting wildlife, including 15 endangered species in the affected area. Once complete, the project is expected to deliver an additional 32 million liters of water each year to the Sydney watershed.", "original_types": ["text"], "id": 614}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 42, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 615}
{"type": "section", "content": "In collaboration with the Washington Water Trust, AWS supported a drought relief program that enhances stream flows to help ensure healthy salmon runs in the Dungeness River in Washington state’s Olympic Peninsula.", "doc_id": "amazon2023", "page": 42, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In collaboration with the Washington Water Trust, AWS supported a drought relief program that enhances stream flows to help ensure healthy salmon runs in the Dungeness River in Washington state’s Olympic Peninsula.", "original_types": ["text"], "id": 616}
{"type": "section", "content": "Water Use and Replenishment across Amazon\n\nIn addition to moving toward our water positive goal for AWS, Amazon is scaling best practices for water stewardship across our business more broadly. We strive to reduce water use in our operations, which include logistics sites such as fulfillment centers, as well as in our corporate offices and grocery stores.\n\nAcross our global logistics network, our teams actively look for ways to conserve and reuse water. In 2023, we gathered robust data to analyze how much water our facilities use and created a baseline for our water footprint to track progress going forward. We leveraged our water risk dashboard to monitor water hazards and water-related business risks to prioritize improvements. To inform the investments we make in water infrastructure, we also performed cost-benefit analyses for an array of water conservation, reuse, and harvesting projects.\n\nOperations\n\nIn 2023, Amazon scaled water replenishment, harvesting, and reuse solutions across our operations in countries around the world.\n\nIndia ranks at the top of our list of water-stressed regions, and our water stewardship efforts there reflect the urgent need to take action. In 2023, Amazon teams recharged local aquifers with harvested rainwater, installed low-flow fixtures and waterless urinals, and built in-house sewage treatment plants to reuse wastewater for flushing and gardening. In addition, these teams diverted runoff water from the rooftops of 53% of our fulfillment centers to return as much stormwater to the watershed as possible.\n\nCorporate Offices\n\nOur corporate offices increased water recycling and the use of low-flow fixtures in 2023. This work is part of our efforts to set new standards for design as we strive to reduce indoor water use by more than 30% globally versus a Leadership in Energy and Environmental Design (LEED) baseline. Our operations there additionally saved an estimated 3,700 cubic meters of water by installing aerators in faucets, shifting from conventional to sensor faucets, reducing toilet flush tank capacity, and recirculating water used in fire pump testing rather than draining it. In North America, we added faucet aerators in 90% of our fulfillment centers, sort centers, and grocery logistics sites.\n\nAcross the Middle East and Africa, we installed low-flow fixtures at our facilities. Our operations there additionally saved an estimated 3,700 cubic meters of water by installing aerators in faucets, shifting from conventional to sensor faucets, reducing toilet flush tank capacity, and recirculating water used in fire pump testing rather than draining it. In India and China, for example, replaced existing water flow fixtures in bathrooms and utilities areas with low-flow fixtures, reducing water use by 16%, or more than 15.6 million gallons, compared to a 2022 baseline.\n\nLooking Forward", "doc_id": "amazon2023", "page": 43, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Water Use and Replenishment across Amazon\n\nIn addition to moving toward our water positive goal for AWS, Amazon is scaling best practices for water stewardship across our business more broadly. We strive to reduce water use in our operations, which include logistics sites such as fulfillment centers, as well as in our corporate offices and grocery stores.\n\nAcross our global logistics network, our teams actively look for ways to conserve and reuse water. In 2023, we gathered robust data to analyze how much water our facilities use and created a baseline for our water footprint to track progress going forward. We leveraged our water risk dashboard to monitor water hazards and water-related business risks to prioritize improvements. To inform the investments we make in water infrastructure, we also performed cost-benefit analyses for an array of water conservation, reuse, and harvesting projects.\n\nOperations\n\nIn 2023, Amazon scaled water replenishment, harvesting, and reuse solutions across our operations in countries around the world.\n\nIndia ranks at the top of our list of water-stressed regions, and our water stewardship efforts there reflect the urgent need to take action. In 2023, Amazon teams recharged local aquifers with harvested rainwater, installed low-flow fixtures and waterless urinals, and built in-house sewage treatment plants to reuse wastewater for flushing and gardening. In addition, these teams diverted runoff water from the rooftops of 53% of our fulfillment centers to return as much stormwater to the watershed as possible.\n\nCorporate Offices\n\nOur corporate offices increased water recycling and the use of low-flow fixtures in 2023. This work is part of our efforts to set new standards for design as we strive to reduce indoor water use by more than 30% globally versus a Leadership in Energy and Environmental Design (LEED) baseline. Our operations there additionally saved an estimated 3,700 cubic meters of water by installing aerators in faucets, shifting from conventional to sensor faucets, reducing toilet flush tank capacity, and recirculating water used in fire pump testing rather than draining it. In North America, we added faucet aerators in 90% of our fulfillment centers, sort centers, and grocery logistics sites.\n\nAcross the Middle East and Africa, we installed low-flow fixtures at our facilities. Our operations there additionally saved an estimated 3,700 cubic meters of water by installing aerators in faucets, shifting from conventional to sensor faucets, reducing toilet flush tank capacity, and recirculating water used in fire pump testing rather than draining it. In India and China, for example, replaced existing water flow fixtures in bathrooms and utilities areas with low-flow fixtures, reducing water use by 16%, or more than 15.6 million gallons, compared to a 2022 baseline.\n\nLooking Forward", "original_types": ["text", "header"], "id": 617}
{"type": "section", "content": "To build on progress made in 2023, we will continue to analyze our water footprint and water risk assessment in both our direct operations and supply chain, as well as develop a comprehensive water management strategy tailored to unique geographic and business unit needs. AWS will focus on making ongoing progress toward its water positive goal, investing in efforts to restore watersheds and increase access to water, sanitation, and hygiene for people who need it most. Amazon plans to leverage the learnings from this work, applying them companywide to strengthen our efforts to conserve and reuse water across our operations, within our communities, and in water-scarce regions around the world.\n\nWater.org Water & Climate Fund\n\nIn 2022, Amazon announced a partnership with Water.org to help launch the Water.org Water & Climate Fund, which focuses on climate-resilient water and sanitation solutions. Amazon’s $10 million contribution will directly empower 1 million people with access to clean water by 2025, providing 3 billion liters of water per year to areas facing water scarcity.\n\nAlong with scaling water conservation, harvesting, and reuse solutions, Amazon has seeded a global water metering and leak detection program, with pilots at logistics, grocery, and data center sites in the U.S. and Europe supported by Amazon-developed software. Using machine learning, the program automatically sends a repair ticket to our engineering team when it detects abnormal water flows.\n\nAs part of Amazon’s Water.org Water & Climate Fund work, more than 400,000 people were connected to piped water supply and sanitation systems across India. The volume of fresh water saved by operational efficiencies, replenished through groundwater recharge, and made accessible through our Water.org initiatives together accounted for 93% of the direct water footprint of our logistics sites in India in 2023.", "doc_id": "amazon2023", "page": 43, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "To build on progress made in 2023, we will continue to analyze our water footprint and water risk assessment in both our direct operations and supply chain, as well as develop a comprehensive water management strategy tailored to unique geographic and business unit needs. AWS will focus on making ongoing progress toward its water positive goal, investing in efforts to restore watersheds and increase access to water, sanitation, and hygiene for people who need it most. Amazon plans to leverage the learnings from this work, applying them companywide to strengthen our efforts to conserve and reuse water across our operations, within our communities, and in water-scarce regions around the world.\n\nWater.org Water & Climate Fund\n\nIn 2022, Amazon announced a partnership with Water.org to help launch the Water.org Water & Climate Fund, which focuses on climate-resilient water and sanitation solutions. Amazon’s $10 million contribution will directly empower 1 million people with access to clean water by 2025, providing 3 billion liters of water per year to areas facing water scarcity.\n\nAlong with scaling water conservation, harvesting, and reuse solutions, Amazon has seeded a global water metering and leak detection program, with pilots at logistics, grocery, and data center sites in the U.S. and Europe supported by Amazon-developed software. Using machine learning, the program automatically sends a repair ticket to our engineering team when it detects abnormal water flows.\n\nAs part of Amazon’s Water.org Water & Climate Fund work, more than 400,000 people were connected to piped water supply and sanitation systems across India. The volume of fresh water saved by operational efficiencies, replenished through groundwater recharge, and made accessible through our Water.org initiatives together accounted for 93% of the direct water footprint of our logistics sites in India in 2023.", "original_types": ["text", "header"], "id": 618}
{"type": "section", "content": "Value Chain\n\nMillions of people work alongside Amazon to serve our customers, including our employees, partners, suppliers, and people living in the communities where we operate. We respect the dignity, rights, and well-being of everyone connected to our global business. We aim to provide products and services responsibly, work with more diverse suppliers, and deliver positive impact in our communities.", "doc_id": "amazon2023", "page": 44, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Value Chain\n\nMillions of people work alongside Amazon to serve our customers, including our employees, partners, suppliers, and people living in the communities where we operate. We respect the dignity, rights, and well-being of everyone connected to our global business. We aim to provide products and services responsibly, work with more diverse suppliers, and deliver positive impact in our communities.", "original_types": ["text", "header"], "id": 619}
{"type": "table", "content": "In This Section", "doc_id": "amazon2023", "page": 44, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In This Section", "id": 620}
{"type": "section", "content": "45 Human Rights\n\n50 Responsible Supply Chain\n\n58 Sustainable Products and Materials\n\n64 Supplier Diversity\n\n67 Community Impact", "doc_id": "amazon2023", "page": 44, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "45 Human Rights\n\n50 Responsible Supply Chain\n\n58 Sustainable Products and Materials\n\n64 Supplier Diversity\n\n67 Community Impact", "original_types": ["text"], "id": 621}
{"type": "figure", "content": "Employees at BlueHenry, an Amazon selling partner that specializes in all-natural cocktail garnishes.", "doc_id": "amazon2023", "page": 44, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employees at BlueHenry, an Amazon selling partner that specializes in all-natural cocktail garnishes.", "id": 622}
{"type": "section", "content": "Human Rights\n\nLaunched a cross-functional initiative to help employees identify and proactively address forced labor and modern slavery risks, with a focus on ensuring the safety of potential human trafficking survivors\n\nConducted a human rights saliency assessment for Amazon Private Brands to improve our understanding of the company’s commitment to human rights, as well as courses on forced labor awareness\n\nEmployees at an Amazon supplier in Bengaluru, India.", "doc_id": "amazon2023", "page": 45, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Human Rights\n\nLaunched a cross-functional initiative to help employees identify and proactively address forced labor and modern slavery risks, with a focus on ensuring the safety of potential human trafficking survivors\n\nConducted a human rights saliency assessment for Amazon Private Brands to improve our understanding of the company’s commitment to human rights, as well as courses on forced labor awareness\n\nEmployees at an Amazon supplier in Bengaluru, India.", "original_types": ["text", "header"], "id": 623}
{"type": "section", "content": "Our Approach\n\nRespect for human rights is woven throughout our business activities and relationships, and we work to engage with partners and suppliers that are also committed to respecting human and labor rights. Just as we are committed to promoting the rights of our employees, we are committed to working with our suppliers to embed respect for human rights in their operations and supply chains and to help us further our goal to support the fundamental dignity of everyone we work with.\n\nEmbedding Human Rights into Our Business\n\nOur strategy to deliver on these commitments is based on the UNGPs and has five pillars:\n\nDevelop and maintain strong policies and standards: Our policies and standards are the backbone of our human rights strategy, spanning a diverse array of people-centric topics.\n\nEmbed human rights into our business operations and decision-making: We are committed to embedding human rights considerations into decision-making across our business and into our policy and governance framework, including by raising awareness among our employees on human rights issues. We have a central team that works across the company to conduct human rights due diligence and integrate human rights considerations into business decisions, and we strive to embed our human rights principles in employees’ everyday work by offering training curricula.\n\nDeveloping and Maintaining Strong Policies and Standards\n\nAmazon’s human rights strategy is anchored in policies that apply across our business. We regularly review our strategy, principles, and supporting policies to identify opportunities for improvement. By engaging external stakeholders and monitoring evolving international and human rights standards, regulations, and industry best practices, we are able to effectively and continuously improve our own policies and standards to better support our employees and suppliers.\n\nAssess, prioritize, and address risk: We have robust processes that allow us to effectively identify, prioritize, and ultimately address human rights risks across our business and supply chain.\n\nEngage with stakeholders: We are committed to driving best practices in human rights due diligence by expanding our stakeholder engagement. Through collaboration with stakeholders including supply chain workers, employees and contractors, customers, and communities, we are creating effective, sustainable solutions of and expectations for suppliers. They apply to all suppliers and are grounded in the ILO Declaration on Fundamental Principles and Rights at Work.\n\nImprove access to effective grievance mechanisms and remediation: We offer multiple channels for our employees to voice grievances and concerns. We work hard to raise employee awareness of these grievance mechanisms and engage with suppliers to do the same for their workers.", "doc_id": "amazon2023", "page": 46, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nRespect for human rights is woven throughout our business activities and relationships, and we work to engage with partners and suppliers that are also committed to respecting human and labor rights. Just as we are committed to promoting the rights of our employees, we are committed to working with our suppliers to embed respect for human rights in their operations and supply chains and to help us further our goal to support the fundamental dignity of everyone we work with.\n\nEmbedding Human Rights into Our Business\n\nOur strategy to deliver on these commitments is based on the UNGPs and has five pillars:\n\nDevelop and maintain strong policies and standards: Our policies and standards are the backbone of our human rights strategy, spanning a diverse array of people-centric topics.\n\nEmbed human rights into our business operations and decision-making: We are committed to embedding human rights considerations into decision-making across our business and into our policy and governance framework, including by raising awareness among our employees on human rights issues. We have a central team that works across the company to conduct human rights due diligence and integrate human rights considerations into business decisions, and we strive to embed our human rights principles in employees’ everyday work by offering training curricula.\n\nDeveloping and Maintaining Strong Policies and Standards\n\nAmazon’s human rights strategy is anchored in policies that apply across our business. We regularly review our strategy, principles, and supporting policies to identify opportunities for improvement. By engaging external stakeholders and monitoring evolving international and human rights standards, regulations, and industry best practices, we are able to effectively and continuously improve our own policies and standards to better support our employees and suppliers.\n\nAssess, prioritize, and address risk: We have robust processes that allow us to effectively identify, prioritize, and ultimately address human rights risks across our business and supply chain.\n\nEngage with stakeholders: We are committed to driving best practices in human rights due diligence by expanding our stakeholder engagement. Through collaboration with stakeholders including supply chain workers, employees and contractors, customers, and communities, we are creating effective, sustainable solutions of and expectations for suppliers. They apply to all suppliers and are grounded in the ILO Declaration on Fundamental Principles and Rights at Work.\n\nImprove access to effective grievance mechanisms and remediation: We offer multiple channels for our employees to voice grievances and concerns. We work hard to raise employee awareness of these grievance mechanisms and engage with suppliers to do the same for their workers.", "original_types": ["text", "header"], "id": 624}
{"type": "section", "content": "Mechanisms to Assess, Prioritize, and Address Risk\n\nWe are committed to assessing, prioritizing, and addressing adverse human rights impacts connected to our business, and we continuously work to improve our approach.\n\nWithin Amazon's own operations, we deploy a variety of mechanisms to assess and respond to risks specific to Amazon businesses, including the sectors and the countries where we operate. We have a centralized team of experts who work across the company to conduct human rights and environmental due diligence. With support from this team, Amazon businesses work toward integrating our human rights principles into their operations and business relationships by conducting human rights risk assessments and refining identified issues.\n\nIn our supply chains, we assess and respond to risk by leveraging internal and external data and guidance from stakeholders, including industry experts, civil society groups, and nongovernmental organizations. We also engage directly with suppliers and conduct independent audits to verify compliance with our Supply Chain Standards. We work with suppliers on appropriate remediation measures and offer partnerships and programs to help them address human rights risks and invest in worker well-being. Our supply chain efforts focus on six priority commitment areas: Safe and Healthy Workplaces, Gender Equity, Fair Wages, Responsible Recruitment and Freely Chosen Employment, Environmental Protection, and Access to Effective Grievance Mechanisms.\n\nLearn more about our supply chain due diligence work.\n\nHuman Rights Assessments\n\nIdentifying and prioritizing the most salient risks connected to Amazon operations and business relationships is central to our human rights due diligence practices.26 As we continuously improve and expand these practices, we use human rights assessment methodologies to identify and mitigate human rights risks. These assessments help us understand the causes of systemic issues, enhance ongoing engagement with critical stakeholders, implement the correct risk-based mitigation measures, and refine strategies for ongoing risk management across our supply chain. Examples of human rights assessment methodologies include:\n\nDiversity, Equity, and Inclusion\n\nSafe and Healthy Working Conditions\n\nModern Slavery and Forced Labor\n\nFair Wages and Hours\n\nFreedom of Association\n\nFuture of Work\n\nRight to Privacy\n\nProduct Safety and Security\n\nSocial, Economic, and Environmental Justice\n\nLearn more about our salient human rights risks on our website.\n\nIdentifying and Addressing Modern Slavery Indicators across Amazon\n\nAddressing modern slavery requires a holistic approach that includes commitments, resources, and innovative solutions from governments, international organizations, the private sector, and civil society. At Amazon, we understand the important role we can play in eradicating this issue and are committed to expanding our work to understand and address any modern slavery risks that may arise in our business.", "doc_id": "amazon2023", "page": 47, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Mechanisms to Assess, Prioritize, and Address Risk\n\nWe are committed to assessing, prioritizing, and addressing adverse human rights impacts connected to our business, and we continuously work to improve our approach.\n\nWithin Amazon's own operations, we deploy a variety of mechanisms to assess and respond to risks specific to Amazon businesses, including the sectors and the countries where we operate. We have a centralized team of experts who work across the company to conduct human rights and environmental due diligence. With support from this team, Amazon businesses work toward integrating our human rights principles into their operations and business relationships by conducting human rights risk assessments and refining identified issues.\n\nIn our supply chains, we assess and respond to risk by leveraging internal and external data and guidance from stakeholders, including industry experts, civil society groups, and nongovernmental organizations. We also engage directly with suppliers and conduct independent audits to verify compliance with our Supply Chain Standards. We work with suppliers on appropriate remediation measures and offer partnerships and programs to help them address human rights risks and invest in worker well-being. Our supply chain efforts focus on six priority commitment areas: Safe and Healthy Workplaces, Gender Equity, Fair Wages, Responsible Recruitment and Freely Chosen Employment, Environmental Protection, and Access to Effective Grievance Mechanisms.\n\nLearn more about our supply chain due diligence work.\n\nHuman Rights Assessments\n\nIdentifying and prioritizing the most salient risks connected to Amazon operations and business relationships is central to our human rights due diligence practices.26 As we continuously improve and expand these practices, we use human rights assessment methodologies to identify and mitigate human rights risks. These assessments help us understand the causes of systemic issues, enhance ongoing engagement with critical stakeholders, implement the correct risk-based mitigation measures, and refine strategies for ongoing risk management across our supply chain. Examples of human rights assessment methodologies include:\n\nDiversity, Equity, and Inclusion\n\nSafe and Healthy Working Conditions\n\nModern Slavery and Forced Labor\n\nFair Wages and Hours\n\nFreedom of Association\n\nFuture of Work\n\nRight to Privacy\n\nProduct Safety and Security\n\nSocial, Economic, and Environmental Justice\n\nLearn more about our salient human rights risks on our website.\n\nIdentifying and Addressing Modern Slavery Indicators across Amazon\n\nAddressing modern slavery requires a holistic approach that includes commitments, resources, and innovative solutions from governments, international organizations, the private sector, and civil society. At Amazon, we understand the important role we can play in eradicating this issue and are committed to expanding our work to understand and address any modern slavery risks that may arise in our business.", "original_types": ["text", "header"], "id": 625}
{"type": "section", "content": "We place a specific focus on vulnerable workers, including foreign and domestic migrant workers; contract, agency, and temporary workers; refugees; asylum seekers; ethnic/religious minorities and displaced persons; Indigenous peoples; and young or student workers.\n\nIn 2023, we launched a new initiative to identify and proactively address forced labor and modern slavery risk indicators across our business. The program brings together teams across our company to identify and eliminate any instances of exploitation—with a focus on victim safety.\n\nWorking with both internal and external experts in human trafficking, the cross-functional team is establishing clear protocols, mandatory training, and policies on identifying potential victims of forced labor or modern slavery within our business operations, as well as detailing what employees should do if they observe any indicators of human trafficking. These include guidance on ensuring victim safety and providing immediate support and assistance to them, as well as involving relevant authorities and support organizations. More broadly, we are working to help our businesses understand how to address the root causes of any exploitation they identify within our supply chain and business practices.\n\nLearn more about our policies and principles to assess and address the risks of modern slavery within our supply chain in our Responsible Supply Chain section and in our Modern Slavery Statement.", "doc_id": "amazon2023", "page": 47, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We place a specific focus on vulnerable workers, including foreign and domestic migrant workers; contract, agency, and temporary workers; refugees; asylum seekers; ethnic/religious minorities and displaced persons; Indigenous peoples; and young or student workers.\n\nIn 2023, we launched a new initiative to identify and proactively address forced labor and modern slavery risk indicators across our business. The program brings together teams across our company to identify and eliminate any instances of exploitation—with a focus on victim safety.\n\nWorking with both internal and external experts in human trafficking, the cross-functional team is establishing clear protocols, mandatory training, and policies on identifying potential victims of forced labor or modern slavery within our business operations, as well as detailing what employees should do if they observe any indicators of human trafficking. These include guidance on ensuring victim safety and providing immediate support and assistance to them, as well as involving relevant authorities and support organizations. More broadly, we are working to help our businesses understand how to address the root causes of any exploitation they identify within our supply chain and business practices.\n\nLearn more about our policies and principles to assess and address the risks of modern slavery within our supply chain in our Responsible Supply Chain section and in our Modern Slavery Statement.", "original_types": ["text"], "id": 626}
{"type": "section", "content": "Meaningful Consultation with Stakeholders\n\nHuman rights risks are systemic and complex, and addressing them requires us to reach well outside of our own operations. Engagement with external stakeholders is essential to identifying positive outcomes for people across our business and key to our human rights due diligence approach.\n\nWe collaborate with credible, knowledgeable, and innovative partner organizations around the world who share our vision. Together, we examine assessment findings, help remediate our salient human rights risks, and advance effective solutions that improve working conditions for people throughout our supply chain. We also rely on experts and affected rights-holders to inform our approach and validate our efforts. Examples of our partnerships include:", "doc_id": "amazon2023", "page": 48, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Meaningful Consultation with Stakeholders\n\nHuman rights risks are systemic and complex, and addressing them requires us to reach well outside of our own operations. Engagement with external stakeholders is essential to identifying positive outcomes for people across our business and key to our human rights due diligence approach.\n\nWe collaborate with credible, knowledgeable, and innovative partner organizations around the world who share our vision. Together, we examine assessment findings, help remediate our salient human rights risks, and advance effective solutions that improve working conditions for people throughout our supply chain. We also rely on experts and affected rights-holders to inform our approach and validate our efforts. Examples of our partnerships include:", "original_types": ["text", "header"], "id": 627}
{"type": "table", "content": "Table 1: Title...", "doc_id": "amazon2023", "page": 48, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...", "id": 628}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 48, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 629}
{"type": "section", "content": "Thorn, a nonprofit that builds technology to combat child sexual abuse. Amazon provides millions of dollars in AWS credits to power Thorn’s tools, and Thorn leverages a variety of AWS solutions to support Safer, a tool that uses advanced artificial intelligence (AI) and machine learning models to detect child sexual abuse material (CSAM) at scale. Safer helps companies identify, review, and report CSAM from content-hosting platforms, detecting over 3.8 million CSAM files in 2023.\n\nPolaris, a nonprofit AWS customer that leads a survivor-centered, justice- and equity-driven movement to end human trafficking in the U.S. AWS provides financial and technical support to enhance Polaris’s data collection and operations and improve trafficking identification and prevention. In 2023, AWS delivered hundreds of thousands of dollars in AWS credits to help fund Polaris’s work. Since 2007, Polaris has identified over 82,300 situations of human trafficking.\n\nTruckers Against Trafficking, an organization that stands committed to educate, equip, empower, and mobilize members of the trucking, bus, and energy industries to address human trafficking. In addition to being a Truckers Against Trafficking corporate sponsor, we include Truckers Against Trafficking modules in our training for internal fleet drivers to help them identify and respond to potential human trafficking victims. In 2023, we trained 9,970 Amazon transportation associates.\n\nNational Center for Missing and Exploited Children (NCMEC), a child protection organization committed to aiding the search for missing children, reducing child sexual exploitation, and preventing child victimization. NCMEC utilizes AWS solutions to support several of its programs and an AWS representative serves on NCMEC’s Board of Directors. In 2023, NCMEC received hundreds of thousands of dollars in AWS credits to support critical applications that help make sure every child has a safe childhood. Additionally, Amazon’s subsidiary Ring works with NCMEC to distribute geo-targeted missing child posters in its app and trained vendors in Saudi Arabia and the United Arab Emirates on responsible recruitment practices.\n\nTech Against Trafficking (TAT), a coalition of companies and global experts working to eradicate human trafficking using technology. An Amazon representative serves as a member of its steering committee. In 2023, we worked with Polaris and the Issara Institute—both participants of the TAT Accelerator—to scale their technology solutions to address human trafficking.\n\nThe Mekong Club, a nonprofit that works with the private sector to address modern slavery. Through the organization, we developed a supplier-facing remediation guideline and trained vendors in Pakistan and India. Amazon is supporting YESS’s efforts to scale and expand the program to Vietnam and Bangladesh.", "doc_id": "amazon2023", "page": 48, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Thorn, a nonprofit that builds technology to combat child sexual abuse. Amazon provides millions of dollars in AWS credits to power Thorn’s tools, and Thorn leverages a variety of AWS solutions to support Safer, a tool that uses advanced artificial intelligence (AI) and machine learning models to detect child sexual abuse material (CSAM) at scale. Safer helps companies identify, review, and report CSAM from content-hosting platforms, detecting over 3.8 million CSAM files in 2023.\n\nPolaris, a nonprofit AWS customer that leads a survivor-centered, justice- and equity-driven movement to end human trafficking in the U.S. AWS provides financial and technical support to enhance Polaris’s data collection and operations and improve trafficking identification and prevention. In 2023, AWS delivered hundreds of thousands of dollars in AWS credits to help fund Polaris’s work. Since 2007, Polaris has identified over 82,300 situations of human trafficking.\n\nTruckers Against Trafficking, an organization that stands committed to educate, equip, empower, and mobilize members of the trucking, bus, and energy industries to address human trafficking. In addition to being a Truckers Against Trafficking corporate sponsor, we include Truckers Against Trafficking modules in our training for internal fleet drivers to help them identify and respond to potential human trafficking victims. In 2023, we trained 9,970 Amazon transportation associates.\n\nNational Center for Missing and Exploited Children (NCMEC), a child protection organization committed to aiding the search for missing children, reducing child sexual exploitation, and preventing child victimization. NCMEC utilizes AWS solutions to support several of its programs and an AWS representative serves on NCMEC’s Board of Directors. In 2023, NCMEC received hundreds of thousands of dollars in AWS credits to support critical applications that help make sure every child has a safe childhood. Additionally, Amazon’s subsidiary Ring works with NCMEC to distribute geo-targeted missing child posters in its app and trained vendors in Saudi Arabia and the United Arab Emirates on responsible recruitment practices.\n\nTech Against Trafficking (TAT), a coalition of companies and global experts working to eradicate human trafficking using technology. An Amazon representative serves as a member of its steering committee. In 2023, we worked with Polaris and the Issara Institute—both participants of the TAT Accelerator—to scale their technology solutions to address human trafficking.\n\nThe Mekong Club, a nonprofit that works with the private sector to address modern slavery. Through the organization, we developed a supplier-facing remediation guideline and trained vendors in Pakistan and India. Amazon is supporting YESS’s efforts to scale and expand the program to Vietnam and Bangladesh.", "original_types": ["text"], "id": 630}
{"type": "section", "content": "Access to Effective Grievance Mechanisms and Remediation\n\nBy listening to the people connected to our business, we can better understand their experiences and concerns, address risks they face, remedy issues, and ultimately improve our workplace experience. Our grievance policies and practices are designed to promote respect for the rights of freedom of association and collective bargaining and comply with the legal requirements of the countries where we operate.\n\nWe empower and encourage all our employees to share their concerns and communicate openly and candidly with us through various channels, including grievance mechanisms and avenues for effective two-way dialogue with leadership. We have processes in place to promptly review and address employee suggestions, concerns, and grievances—making improvements and providing remedies in response.\n\nLearn more about our grievance mechanisms and remediation processes for employees\n\nLooking Forward\n\nOur commitment to respecting human rights is unwavering. In 2024, Amazon will work with internal and external human rights experts to update our Global Human Rights Principles. We will continue raising awareness among our employees, suppliers, and partners about the importance we place on advancing human rights throughout our business and supply chain, as well as our expectation that respect for human rights guide their everyday work and business decisions. We will also uphold our engagement and collaboration with these stakeholders to help us deliver on our priorities and raise the bar for human rights across our industries.\n\nIn 2024, Amazon will continue to scale our human rights and environmental due diligence programs using systems that enable our businesses to get even more granular about their human rights risks—identifying business-specific priority issues and better targeting risk management actions most relevant to their sectors and activities.\n\nLearn more about our grievance mechanisms and remediation processes for suppliers", "doc_id": "amazon2023", "page": 49, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Access to Effective Grievance Mechanisms and Remediation\n\nBy listening to the people connected to our business, we can better understand their experiences and concerns, address risks they face, remedy issues, and ultimately improve our workplace experience. Our grievance policies and practices are designed to promote respect for the rights of freedom of association and collective bargaining and comply with the legal requirements of the countries where we operate.\n\nWe empower and encourage all our employees to share their concerns and communicate openly and candidly with us through various channels, including grievance mechanisms and avenues for effective two-way dialogue with leadership. We have processes in place to promptly review and address employee suggestions, concerns, and grievances—making improvements and providing remedies in response.\n\nLearn more about our grievance mechanisms and remediation processes for employees\n\nLooking Forward\n\nOur commitment to respecting human rights is unwavering. In 2024, Amazon will work with internal and external human rights experts to update our Global Human Rights Principles. We will continue raising awareness among our employees, suppliers, and partners about the importance we place on advancing human rights throughout our business and supply chain, as well as our expectation that respect for human rights guide their everyday work and business decisions. We will also uphold our engagement and collaboration with these stakeholders to help us deliver on our priorities and raise the bar for human rights across our industries.\n\nIn 2024, Amazon will continue to scale our human rights and environmental due diligence programs using systems that enable our businesses to get even more granular about their human rights risks—identifying business-specific priority issues and better targeting risk management actions most relevant to their sectors and activities.\n\nLearn more about our grievance mechanisms and remediation processes for suppliers", "original_types": ["text", "header"], "id": 631}
{"type": "section", "content": "Responsible Supply Chain\n\nLaunched a new peer-learning workshop series, convening factory management to hear from industry experts and share best practices to solve common challenges\n\n100% Of supplier employee grievances were investigated and resolved at supplier sites across seven countries using effective Amazon-operated worker grievance mechanisms\n\n458 Launched i4Equality, a new Amazon-owned capacity-building program that helps suppliers achieve gender equity\n\nIn early 2024, joined Nirapon to create and sustain a culture of workplace safety in Bangladesh factories, as well as Life and Building Safety (LABS) Initiative to support safety monitoring and workplace safety in our supply chain\n\nAmazon works with thousands of suppliers around the world. From the way our suppliers source materials to the way they treat their employees, we understand our opportunity and responsibility to support safe working conditions, fair pay, and environmental protection—well beyond our direct operations. To address supply chain challenges and enable safe, equitable, fair, and sustainable supply chains, we are building long-term relationships with suppliers that align with our values, partnering with them to consistently improve conditions for workers. We also engage business partners and industry peers to expand our efforts and drive improvements on a wider scale.", "doc_id": "amazon2023", "page": 50, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Responsible Supply Chain\n\nLaunched a new peer-learning workshop series, convening factory management to hear from industry experts and share best practices to solve common challenges\n\n100% Of supplier employee grievances were investigated and resolved at supplier sites across seven countries using effective Amazon-operated worker grievance mechanisms\n\n458 Launched i4Equality, a new Amazon-owned capacity-building program that helps suppliers achieve gender equity\n\nIn early 2024, joined Nirapon to create and sustain a culture of workplace safety in Bangladesh factories, as well as Life and Building Safety (LABS) Initiative to support safety monitoring and workplace safety in our supply chain\n\nAmazon works with thousands of suppliers around the world. From the way our suppliers source materials to the way they treat their employees, we understand our opportunity and responsibility to support safe working conditions, fair pay, and environmental protection—well beyond our direct operations. To address supply chain challenges and enable safe, equitable, fair, and sustainable supply chains, we are building long-term relationships with suppliers that align with our values, partnering with them to consistently improve conditions for workers. We also engage business partners and industry peers to expand our efforts and drive improvements on a wider scale.", "original_types": ["text", "header"], "id": 632}
{"type": "section", "content": "Our Approach\n\nAt Amazon, we are laser focused on requiring safe and healthy working conditions throughout our supply chain. We have dedicated teams in key sourcing regions that engage directly with suppliers to communicate our standards, evaluate risks in our supply chain, and help suppliers build their capacities to provide working environments that are safe and respectful of human rights. We also work closely with strategic partners across the globe to enhance our influence.\n\nWe continue to focus our efforts on our six priority commitment areas:\n\nSafe and healthy workplaces\nGender equity\nFair wages\nResponsible recruitment and freely chosen employment\nEnvironmental protection\nAccess to effective grievance mechanisms\n\nSelling Partners and Our Standards\n\nSelling partners are third-party sellers and retail vendors that sell or provide products and services in Amazon stores. Our Supply Chain Standards apply to all products and services offered in our stores. We encourage selling partners to perform due diligence to help ensure their products and services are produced and supplied in ways that respect human rights and the environment and protect the fundamental dignity of workers.\n\nOur Supply Chain Standards are the backbone of our efforts to enable a responsible supply chain.28 They apply to all suppliers of goods and services for Amazon, including service providers, vendors, selling partners, contractors, and subcontractors (collectively, “suppliers”). Products sold in Amazon stores, as well as products and services provided to us, must be manufactured, produced, or provided in accordance with these standards.\n\nSupplier Assessments\n\nWe assess suppliers of Amazon-branded products globally during onboarding, and periodically thereafter, to evaluate their compliance with our Supply Chain Standards in four categories: Labor Rights, Ethics, Environment, and Health and Safety. These categories include subcategories such as nondiscrimination, emergency preparedness, hazardous substances, and transparency. By assessing the performance of our suppliers, we improve working conditions and strengthen the resilience of our supply chain.\n\nSupply Chain Transparency\n\nSupply chain transparency is a valuable tool to address risks in our supply chain and identify opportunities for collaboration on systemic issues. That’s why we publish a supplier list and interactive supply chain map with details on finished-product suppliers of Amazon-branded apparel, consumer electronics, food and beverage, and home goods products. Our 2023 supplier list included nearly 2,230 finished-product suppliers. We update our supply chain map annually to provide customers and external stakeholders more current visibility into where we source. In 2024, we will begin to expand our supplier list and supply chain map to include some apparel component suppliers.", "doc_id": "amazon2023", "page": 51, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nAt Amazon, we are laser focused on requiring safe and healthy working conditions throughout our supply chain. We have dedicated teams in key sourcing regions that engage directly with suppliers to communicate our standards, evaluate risks in our supply chain, and help suppliers build their capacities to provide working environments that are safe and respectful of human rights. We also work closely with strategic partners across the globe to enhance our influence.\n\nWe continue to focus our efforts on our six priority commitment areas:\n\nSafe and healthy workplaces\nGender equity\nFair wages\nResponsible recruitment and freely chosen employment\nEnvironmental protection\nAccess to effective grievance mechanisms\n\nSelling Partners and Our Standards\n\nSelling partners are third-party sellers and retail vendors that sell or provide products and services in Amazon stores. Our Supply Chain Standards apply to all products and services offered in our stores. We encourage selling partners to perform due diligence to help ensure their products and services are produced and supplied in ways that respect human rights and the environment and protect the fundamental dignity of workers.\n\nOur Supply Chain Standards are the backbone of our efforts to enable a responsible supply chain.28 They apply to all suppliers of goods and services for Amazon, including service providers, vendors, selling partners, contractors, and subcontractors (collectively, “suppliers”). Products sold in Amazon stores, as well as products and services provided to us, must be manufactured, produced, or provided in accordance with these standards.\n\nSupplier Assessments\n\nWe assess suppliers of Amazon-branded products globally during onboarding, and periodically thereafter, to evaluate their compliance with our Supply Chain Standards in four categories: Labor Rights, Ethics, Environment, and Health and Safety. These categories include subcategories such as nondiscrimination, emergency preparedness, hazardous substances, and transparency. By assessing the performance of our suppliers, we improve working conditions and strengthen the resilience of our supply chain.\n\nSupply Chain Transparency\n\nSupply chain transparency is a valuable tool to address risks in our supply chain and identify opportunities for collaboration on systemic issues. That’s why we publish a supplier list and interactive supply chain map with details on finished-product suppliers of Amazon-branded apparel, consumer electronics, food and beverage, and home goods products. Our 2023 supplier list included nearly 2,230 finished-product suppliers. We update our supply chain map annually to provide customers and external stakeholders more current visibility into where we source. In 2024, we will begin to expand our supplier list and supply chain map to include some apparel component suppliers.", "original_types": ["text", "header", "list"], "id": 633}
{"type": "section", "content": "When we identify a gap between a supplier’s practices and our Supply Chain Standards, we track remediation and conduct follow-up assessments as needed. Between assessments, our central team directly engages with the supplier to discuss open issues and remediation progress and provides them with issue-specific remediation guidebooks. Failure to implement the actions listed in the corrective action plan may prevent the supplier from continuing production or providing services or labor to us.\n\nWe also contribute our supplier list to the Open Supply Hub to foster brand collaboration and action in the industry. Open Supply Hub is an accessible, collaborative supply chain mapping platform, used and populated by stakeholders across sectors and supply chains. In 2023, we utilized tools in Open Supply Hub’s map function to list facilities that produce Amazon-branded products, allowing users to clearly and easily view our supply chain and interact with facility data.\n\nLearn more about our auditing and remediation processes\n\nLearn more about our ongoing engagement with critical stakeholders to advance human rights in our supply chain\n\nSupplier Assessments\n\nWe assess suppliers of Amazon-branded products globally during onboarding, and periodically thereafter, to evaluate their compliance with our Supply Chain Standards in four categories: Labor Rights, Ethics, Environment, and Health and Safety. These categories include subcategories such as nondiscrimination, emergency preparedness, hazardous substances, and transparency. By assessing the performance of our suppliers, we improve working conditions and strengthen the resilience of our supply chain.\n\nSupply Chain Transparency\n\nSupply chain transparency is a valuable tool to address risks in our supply chain and identify opportunities for collaboration on systemic issues. That’s why we publish a supplier list and interactive supply chain map with details on finished-product suppliers of Amazon-branded apparel, consumer electronics, food and beverage, and home goods products. Our 2023 supplier list included nearly 2,230 finished-product suppliers. We update our supply chain map annually to provide customers and external stakeholders more current visibility into where we source. In 2024, we will begin to expand our supplier list and supply chain map to include some apparel component suppliers.\n\nWhen we identify a gap between a supplier’s practices and our Supply Chain Standards, we track remediation and conduct follow-up assessments as needed. Between assessments, our central team directly engages with the supplier to discuss open issues and remediation progress and provides them with issue-specific remediation guidebooks. Failure to implement the actions listed in the corrective action plan may prevent the supplier from continuing production or providing services or labor to us.", "doc_id": "amazon2023", "page": 51, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "When we identify a gap between a supplier’s practices and our Supply Chain Standards, we track remediation and conduct follow-up assessments as needed. Between assessments, our central team directly engages with the supplier to discuss open issues and remediation progress and provides them with issue-specific remediation guidebooks. Failure to implement the actions listed in the corrective action plan may prevent the supplier from continuing production or providing services or labor to us.\n\nWe also contribute our supplier list to the Open Supply Hub to foster brand collaboration and action in the industry. Open Supply Hub is an accessible, collaborative supply chain mapping platform, used and populated by stakeholders across sectors and supply chains. In 2023, we utilized tools in Open Supply Hub’s map function to list facilities that produce Amazon-branded products, allowing users to clearly and easily view our supply chain and interact with facility data.\n\nLearn more about our auditing and remediation processes\n\nLearn more about our ongoing engagement with critical stakeholders to advance human rights in our supply chain\n\nSupplier Assessments\n\nWe assess suppliers of Amazon-branded products globally during onboarding, and periodically thereafter, to evaluate their compliance with our Supply Chain Standards in four categories: Labor Rights, Ethics, Environment, and Health and Safety. These categories include subcategories such as nondiscrimination, emergency preparedness, hazardous substances, and transparency. By assessing the performance of our suppliers, we improve working conditions and strengthen the resilience of our supply chain.\n\nSupply Chain Transparency\n\nSupply chain transparency is a valuable tool to address risks in our supply chain and identify opportunities for collaboration on systemic issues. That’s why we publish a supplier list and interactive supply chain map with details on finished-product suppliers of Amazon-branded apparel, consumer electronics, food and beverage, and home goods products. Our 2023 supplier list included nearly 2,230 finished-product suppliers. We update our supply chain map annually to provide customers and external stakeholders more current visibility into where we source. In 2024, we will begin to expand our supplier list and supply chain map to include some apparel component suppliers.\n\nWhen we identify a gap between a supplier’s practices and our Supply Chain Standards, we track remediation and conduct follow-up assessments as needed. Between assessments, our central team directly engages with the supplier to discuss open issues and remediation progress and provides them with issue-specific remediation guidebooks. Failure to implement the actions listed in the corrective action plan may prevent the supplier from continuing production or providing services or labor to us.", "original_types": ["text"], "id": 634}
{"type": "section", "content": "We also contribute our supplier list to the Open Supply Hub to foster brand collaboration and action in the industry. Open Supply Hub is an accessible, collaborative supply chain mapping platform, used and populated by stakeholders across sectors and supply chains. In 2023, we utilized tools in Open Supply Hub’s map function to list facilities that produce Amazon-branded products, allowing users to clearly and easily view our supply chain and interact with facility data.\n\nLearn more about our auditing and remediation processes\n\nLearn more about our ongoing engagement with critical stakeholders to advance human rights in our supply chain", "doc_id": "amazon2023", "page": 51, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We also contribute our supplier list to the Open Supply Hub to foster brand collaboration and action in the industry. Open Supply Hub is an accessible, collaborative supply chain mapping platform, used and populated by stakeholders across sectors and supply chains. In 2023, we utilized tools in Open Supply Hub’s map function to list facilities that produce Amazon-branded products, allowing users to clearly and easily view our supply chain and interact with facility data.\n\nLearn more about our auditing and remediation processes\n\nLearn more about our ongoing engagement with critical stakeholders to advance human rights in our supply chain", "original_types": ["text"], "id": 635}
{"type": "section", "content": "Our Supply Chain Standards\n\nOur Supply Chain Standards are the backbone of our efforts to enable a more responsible supply chain. They apply to all suppliers of goods and services for Amazon, including service providers, vendors, selling partners, contractors, and subcontractors (collectively, “suppliers”). Products sold in Amazon stores, as well as products and services provided to us, must be manufactured, produced, or provided in accordance with these standards.", "doc_id": "amazon2023", "page": 52, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Supply Chain Standards\n\nOur Supply Chain Standards are the backbone of our efforts to enable a more responsible supply chain. They apply to all suppliers of goods and services for Amazon, including service providers, vendors, selling partners, contractors, and subcontractors (collectively, “suppliers”). Products sold in Amazon stores, as well as products and services provided to us, must be manufactured, produced, or provided in accordance with these standards.", "original_types": ["text", "header"], "id": 636}
{"type": "section", "content": "Our Progress\n\nEnabling safe, equitable, fair, and sustainable supply chains through our six priority commitment areas goes well beyond setting supplier standards. We want to ensure they comply with our Supply Chain Standards and continuously improve. We assess the performance of our suppliers, take action when we identify concerns, and offer capacity-building programs to prevent identified issues from happening again.\n\nSupplier Assessments\n\nOur approach to enabling a more responsible supply chain is rooted in our commitment to workers and based on a model of continuous improvement. We offer on-site and remote training to support supplier remediation, and we expect suppliers to act within an agreed-upon timeline, remove harm, act in the best interests of workers, and commit to preventing similar issues in the future. We know that change does not happen overnight and are committed to supporting our suppliers in remediating issues over time.", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Progress\n\nEnabling safe, equitable, fair, and sustainable supply chains through our six priority commitment areas goes well beyond setting supplier standards. We want to ensure they comply with our Supply Chain Standards and continuously improve. We assess the performance of our suppliers, take action when we identify concerns, and offer capacity-building programs to prevent identified issues from happening again.\n\nSupplier Assessments\n\nOur approach to enabling a more responsible supply chain is rooted in our commitment to workers and based on a model of continuous improvement. We offer on-site and remote training to support supplier remediation, and we expect suppliers to act within an agreed-upon timeline, remove harm, act in the best interests of workers, and commit to preventing similar issues in the future. We know that change does not happen overnight and are committed to supporting our suppliers in remediating issues over time.", "original_types": ["text", "header"], "id": 637}
{"type": "table", "content": "Percentage of High- and Medium-Level Assessment Findings by Subcategory", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Percentage of High- and Medium-Level Assessment Findings by Subcategory", "id": 638}
{"type": "table", "content": "Type of Assessment", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Type of Assessment", "id": 639}
{"type": "table", "content": "Subcategory", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Subcategory", "id": 640}
{"type": "table", "content": "2021", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2021", "id": 641}
{"type": "table", "content": "2022", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2022", "id": 642}
{"type": "table", "content": "2023", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023", "id": 643}
{"type": "figure", "content": "Supplier Assessments29", "doc_id": "amazon2023", "page": 53, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supplier Assessments29", "id": 644}
{"type": "section", "content": "Supplier Training and Awareness\n\nThroughout our relationship with our suppliers, we provide resources and training to help them strengthen their ability to respect human rights and the environment within their own workplace and supply chain.\n\nIn 2023, we hosted in-person and online training events on our Supply Chain Standards. These trainings reached 458 suppliers in 14 countries/regions.\n\nWe also partner with global organizations that are dedicated to maintaining safe and healthy workplaces in the countries where we operate. In February 2024, we joined Nirapon (safe place in Bangla), an industry-led nonprofit that works with global brands, retailers, manufacturers, and other nongovernmental organizations (NGOs) to create and sustain a culture of workplace safety in factories in Bangladesh.\n\nGender Equity\n\nAmazon has a long-standing commitment to gender equity, which is a fundamental human right and necessary foundation of a sustainable supply chain. We strive to engage with more women-owned businesses throughout our supply chains and support women in making their own decisions on health, finances, and career development.\n\nBuilding, Electrical, and Fire Safety\n\nBuilding, electrical, and fire safety is a critical part of our commitment to maintain safe and healthy workplaces. In 2023, we began the process to join Life and Building Safety (LABS) Initiative, officially becoming members in April 2024. LABS is an industry-driven program working to mitigate preventable fire, electrical, and structural building safety risks in key apparel- and footwear-producing countries.\n\nIn 2023, we joined the advisory board of the Resilience Fund for Women in Global Value Chains, created to support women’s economic resilience, health, and well-being.\n\nProgress on wages and working hours requires us to continuously evaluate and strengthen our own processes, including our purchasing practices. Based on anonymous supplier feedback collected through the Better Buying Institute, we implemented new programs to improve our forecasting, communication, and payments systems, and moving forward, we will use the Better Buying survey to measure whether these programs had a positive impact.\n\nMeasuring Good Health\n\nExamining the outcomes of our health and safety programs and partnerships helps build evidence for continued investment in worker health that can support ongoing and future efforts throughout our supply chain. In 2023, nearly 1,750 workers, including over 1,180 female workers, had completed the training in China and India. In China, 90% of participants reported that they were aware of their workplace’s sexual harassment prevention policies, compared to 48% before we launched the program.\n\nSafe and Healthy Workplaces", "doc_id": "amazon2023", "page": 54, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supplier Training and Awareness\n\nThroughout our relationship with our suppliers, we provide resources and training to help them strengthen their ability to respect human rights and the environment within their own workplace and supply chain.\n\nIn 2023, we hosted in-person and online training events on our Supply Chain Standards. These trainings reached 458 suppliers in 14 countries/regions.\n\nWe also partner with global organizations that are dedicated to maintaining safe and healthy workplaces in the countries where we operate. In February 2024, we joined Nirapon (safe place in Bangla), an industry-led nonprofit that works with global brands, retailers, manufacturers, and other nongovernmental organizations (NGOs) to create and sustain a culture of workplace safety in factories in Bangladesh.\n\nGender Equity\n\nAmazon has a long-standing commitment to gender equity, which is a fundamental human right and necessary foundation of a sustainable supply chain. We strive to engage with more women-owned businesses throughout our supply chains and support women in making their own decisions on health, finances, and career development.\n\nBuilding, Electrical, and Fire Safety\n\nBuilding, electrical, and fire safety is a critical part of our commitment to maintain safe and healthy workplaces. In 2023, we began the process to join Life and Building Safety (LABS) Initiative, officially becoming members in April 2024. LABS is an industry-driven program working to mitigate preventable fire, electrical, and structural building safety risks in key apparel- and footwear-producing countries.\n\nIn 2023, we joined the advisory board of the Resilience Fund for Women in Global Value Chains, created to support women’s economic resilience, health, and well-being.\n\nProgress on wages and working hours requires us to continuously evaluate and strengthen our own processes, including our purchasing practices. Based on anonymous supplier feedback collected through the Better Buying Institute, we implemented new programs to improve our forecasting, communication, and payments systems, and moving forward, we will use the Better Buying survey to measure whether these programs had a positive impact.\n\nMeasuring Good Health\n\nExamining the outcomes of our health and safety programs and partnerships helps build evidence for continued investment in worker health that can support ongoing and future efforts throughout our supply chain. In 2023, nearly 1,750 workers, including over 1,180 female workers, had completed the training in China and India. In China, 90% of participants reported that they were aware of their workplace’s sexual harassment prevention policies, compared to 48% before we launched the program.\n\nSafe and Healthy Workplaces", "original_types": ["text", "header"], "id": 645}
{"type": "section", "content": "Everyone has the right to a safe and healthy workplace with appropriate rules and practices for reporting and preventing accidents, injuries, and unsafe conditions. We work with suppliers globally to support worker health and safety and Catalyst Management Services). Across both countries, we helped 26 suppliers plan and manage their responses to the pandemic and provide health care services to approximately 60,000 frontline workers between January 2022 and March 2023.\n\nLearn more about how we embed health and safety considerations into our own workplace operations.\n\nFair Wages\n\nWe believe everyone has a right to be paid fairly for the work they perform, and ensuring workers receive fair pay remains a global, cross-industry issue. That’s why fair wages is one of our six priority commitment areas.\n\nWe monitor wage payments throughout our supply chains to inform more meaningful supplier engagements, enhance our human rights due diligence processes, and promote continuous improvement in the fair and on-time payment of wages. In accordance with our Supply Chain Standards, our suppliers must pay legally required compensation, including overtime and benefits, and we support them in evaluating whether their workers earn enough to meet their basic needs and those of their families. In 2023, we began working with select suppliers to benchmark worker compensation against regionally specific fair wages, focusing on electronics manufacturing and packaging—two areas where there is a lack of industry-wide data on worker wages. In 2024, we will expand these efforts to apparel manufacturing.\n\nLearn more about our approach to promoting equity across our value chain and enhancing supplier diversity.", "doc_id": "amazon2023", "page": 54, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Everyone has the right to a safe and healthy workplace with appropriate rules and practices for reporting and preventing accidents, injuries, and unsafe conditions. We work with suppliers globally to support worker health and safety and Catalyst Management Services). Across both countries, we helped 26 suppliers plan and manage their responses to the pandemic and provide health care services to approximately 60,000 frontline workers between January 2022 and March 2023.\n\nLearn more about how we embed health and safety considerations into our own workplace operations.\n\nFair Wages\n\nWe believe everyone has a right to be paid fairly for the work they perform, and ensuring workers receive fair pay remains a global, cross-industry issue. That’s why fair wages is one of our six priority commitment areas.\n\nWe monitor wage payments throughout our supply chains to inform more meaningful supplier engagements, enhance our human rights due diligence processes, and promote continuous improvement in the fair and on-time payment of wages. In accordance with our Supply Chain Standards, our suppliers must pay legally required compensation, including overtime and benefits, and we support them in evaluating whether their workers earn enough to meet their basic needs and those of their families. In 2023, we began working with select suppliers to benchmark worker compensation against regionally specific fair wages, focusing on electronics manufacturing and packaging—two areas where there is a lack of industry-wide data on worker wages. In 2024, we will expand these efforts to apparel manufacturing.\n\nLearn more about our approach to promoting equity across our value chain and enhancing supplier diversity.", "original_types": ["text", "header"], "id": 646}
{"type": "section", "content": "Partnering to Drive Workplace Health and Safety\n\nIn 2023, we collaborated with organizations around the world to support workers' rights to safe and healthy workplaces. Examples of our progress and impact include:", "doc_id": "amazon2023", "page": 55, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Partnering to Drive Workplace Health and Safety\n\nIn 2023, we collaborated with organizations around the world to support workers' rights to safe and healthy workplaces. Examples of our progress and impact include:", "original_types": ["text", "header"], "id": 647}
{"type": "table", "content": "Table 1: Title...\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 55, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Table 1: Title...\nMarkdown representation of the table", "id": 648}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 55, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 649}
{"type": "section", "content": "Responding to Violations of Our Supply Chain Standards in Saudi Arabia", "doc_id": "amazon2023", "page": 56, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Responding to Violations of Our Supply Chain Standards in Saudi Arabia", "original_types": ["header"], "id": 650}
{"type": "section", "content": "In 2023, Amazon found violations of our Supply Chain Standards at a third-party licensed temporary labor agency (“third-party vendor”) in Saudi Arabia through an independent audit. The violations ranged from recruitment fees paid by migrant workers to worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward", "doc_id": "amazon2023", "page": 56, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In 2023, Amazon found violations of our Supply Chain Standards at a third-party licensed temporary labor agency (“third-party vendor”) in Saudi Arabia through an independent audit. The violations ranged from recruitment fees paid by migrant workers to worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward, and reimburse affected workers in full and in a manner that protects them about worker housing. Additionally, we secured the vendor’s commitment that even after its employees no longer work at Amazon, it will pay them in line with their contracts and won’t move them to a new point in the hiring process moving forward", "id": 651}
{"type": "section", "content": "Value Chain\n\nModule (FEM), a self-assessment owned and developed by Cascale, exclusively licensed to the Worldly sustainability data and insights platform. The assessment evaluates performance and prioritizes opportunities for improvement across seven areas: air emissions, carbon emissions, chemicals management, environmental management systems, waste, wastewater, and water. In 2023, 67% of Amazon’s Private Brands Tier 1 apparel suppliers completed the Higg FEM, helping them better understand their environmental performance and practices.\n\nLooking Forward\n\nReport to Resolution: The Amader Kotha Process in Action\n\nBuilding responsible supply chains is an ongoing endeavor—one that requires collective effort and continuous improvement from Amazon, our suppliers, and our network of expert partners. We are investing in identifying and addressing the most severe human rights and environmental risks throughout our supply chain. We will continue engaging internal and external stakeholders in our efforts to make progress across our six priority commitment areas and, in doing so, enable working conditions that respect supply chain workers and the environments where we operate.\n\nAccess to Effective Grievance Mechanisms\n\nWe want our supply chain workers to have the ability to voice their concerns about the workplace in a safe and confidential manner. To do this, we connect suppliers and service providers with trusted tools, products, and systems to hear directly from workers about their experiences and support the resolution of issues from workers’ perspectives. We believe this is fundamental to supporting the safety and well-being of the workers in our supply chain.\n\nIn 2023, we expanded our grievance mechanisms program to include additional supplier segments across our supply chain. In Japan, we partnered with the Japan Platform for Migrant Workers Towards Responsible and Inclusive Society (JP-MIRAI) so that certain suppliers in Japan could provide their workers access to JP-MIRAI’s independent grievance mechanism.\n\nIn the Middle East, North Africa, Mexico, the UK, and the U.S., we helped connect supplier sites across Bangladesh, Cambodia, China, India, Malaysia, Pakistan, and Thailand with independent grievance mechanisms. Of the nearly 800 worker grievances raised using these mechanisms, 100% were investigated and resolved.\n\nThe Amader Kotha Helpline and Ulula are two examples of effective grievance mechanisms that we introduced to our suppliers. The Amader Kotha Helpline is an independent helpline serving the ready-made garment sector in Bangladesh, while Ulula uses a range of grievance mechanisms—including helplines to messaging platforms—", "doc_id": "amazon2023", "page": 57, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Value Chain\n\nModule (FEM), a self-assessment owned and developed by Cascale, exclusively licensed to the Worldly sustainability data and insights platform. The assessment evaluates performance and prioritizes opportunities for improvement across seven areas: air emissions, carbon emissions, chemicals management, environmental management systems, waste, wastewater, and water. In 2023, 67% of Amazon’s Private Brands Tier 1 apparel suppliers completed the Higg FEM, helping them better understand their environmental performance and practices.\n\nLooking Forward\n\nReport to Resolution: The Amader Kotha Process in Action\n\nBuilding responsible supply chains is an ongoing endeavor—one that requires collective effort and continuous improvement from Amazon, our suppliers, and our network of expert partners. We are investing in identifying and addressing the most severe human rights and environmental risks throughout our supply chain. We will continue engaging internal and external stakeholders in our efforts to make progress across our six priority commitment areas and, in doing so, enable working conditions that respect supply chain workers and the environments where we operate.\n\nAccess to Effective Grievance Mechanisms\n\nWe want our supply chain workers to have the ability to voice their concerns about the workplace in a safe and confidential manner. To do this, we connect suppliers and service providers with trusted tools, products, and systems to hear directly from workers about their experiences and support the resolution of issues from workers’ perspectives. We believe this is fundamental to supporting the safety and well-being of the workers in our supply chain.\n\nIn 2023, we expanded our grievance mechanisms program to include additional supplier segments across our supply chain. In Japan, we partnered with the Japan Platform for Migrant Workers Towards Responsible and Inclusive Society (JP-MIRAI) so that certain suppliers in Japan could provide their workers access to JP-MIRAI’s independent grievance mechanism.\n\nIn the Middle East, North Africa, Mexico, the UK, and the U.S., we helped connect supplier sites across Bangladesh, Cambodia, China, India, Malaysia, Pakistan, and Thailand with independent grievance mechanisms. Of the nearly 800 worker grievances raised using these mechanisms, 100% were investigated and resolved.\n\nThe Amader Kotha Helpline and Ulula are two examples of effective grievance mechanisms that we introduced to our suppliers. The Amader Kotha Helpline is an independent helpline serving the ready-made garment sector in Bangladesh, while Ulula uses a range of grievance mechanisms—including helplines to messaging platforms—", "original_types": ["text", "header"], "id": 652}
{"type": "section", "content": "Sustainable Products and Materials\n\nCustomers want products that align with their values, and this often includes products created with sustainability in mind. We believe it is important to offer more sustainable products to our customers without compromising on quality, safety, or cost. We are working to do this within our own brands by incorporating sourcing and design practices that support responsible supply chains, circular economy principles, decarbonization, and the use of safer chemicals. At the same time, we're working closely with our selling partners to help them offer more products that qualify for at least one of the 55 certifications in our Climate Pledge Friendly program. To help them get started with Amazon's sustainability programs, we launched the Sustainability Solutions Hub. For our customers, this means access to more products recognized by certifications in the Climate Pledge Friendly program that meet their needs, as well as new ways to easily shop for and discover them.", "doc_id": "amazon2023", "page": 58, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Sustainable Products and Materials\n\nCustomers want products that align with their values, and this often includes products created with sustainability in mind. We believe it is important to offer more sustainable products to our customers without compromising on quality, safety, or cost. We are working to do this within our own brands by incorporating sourcing and design practices that support responsible supply chains, circular economy principles, decarbonization, and the use of safer chemicals. At the same time, we're working closely with our selling partners to help them offer more products that qualify for at least one of the 55 certifications in our Climate Pledge Friendly program. To help them get started with Amazon's sustainability programs, we launched the Sustainability Solutions Hub. For our customers, this means access to more products recognized by certifications in the Climate Pledge Friendly program that meet their needs, as well as new ways to easily shop for and discover them.", "original_types": ["text", "header"], "id": 653}
{"type": "section", "content": "Our Approach\n\nWe seek to offer our customers more products that align with their preferences and our sustainability goals.\n\nWe are improving the materials and sourcing standards for our products, as well as building on our responsible sourcing roadmap developed in 2022 to improve visibility across our supply chains. This roadmap focuses on key materials used for Amazon private brands products and Amazon devices. We continue to work toward our sourcing goals, including our commitment to reduce deforestation risks associated with raw materials and ingredients used in Amazon and Whole Foods Market private brands products. In addition to these commitments, Amazon also complies with European regulations to encourage responsible sourcing and reduce deforestation. We also incorporate recycled materials into many new products and product packaging, particularly in Amazon devices, where feasible.\n\nWhile many of our sourcing commitments identify materials we seek to include in our private brands products, we also identify those we seek to avoid. Our Formulated Products Restricted Substances List is an extensive list of chemicals we seek to exclude in Amazon private brands baby, household cleaning, personal care, and beauty products in the U.S. and Europe. Our Restricted Substances List for packaging outlines the chemicals of concern we seek to avoid in the packaging of Amazon private brands food and beverage products sold in North America. Our private brands apparel, accessory, footwear, and home textile products in North America, Europe, and Japan seek to comply with the Apparel and Footwear International RSL Management (AFIRM) Group’s Restricted Substances List. Our compliance teams are responsible for enforcing these restricted substances lists with our suppliers.\n\nOur sustainability efforts are not limited to our own operations. We look for ways to encourage our selling partners to develop products with sustainability in mind, such as through our Climate Pledge Friendly program and the Sustainability Solutions Hub. We also participate in industry partnerships and working groups to drive greater adoption of sustainable and responsible practices.\n\nAdditionally, we use our success and scale to engage with certified diverse suppliers and small businesses and share their products with our customers.31 Through our Supplier Diversity Initiative, customers can shop at storefronts that support women-owned businesses and Black-owned businesses, as well as at the Amazon Saheli Store, which displays products made and sold by women supported by nongovernmental organizations (NGOs) in India.\n\nLearn more about how we create a more inclusive and equitable business environment for our suppliers and empower small businesses.\n\nAll cotton for Amazon private brands apparel is sourced from recycled materials, from farms certified as producing organic cotton, or through the Better Cotton Initiative.", "doc_id": "amazon2023", "page": 59, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nWe seek to offer our customers more products that align with their preferences and our sustainability goals.\n\nWe are improving the materials and sourcing standards for our products, as well as building on our responsible sourcing roadmap developed in 2022 to improve visibility across our supply chains. This roadmap focuses on key materials used for Amazon private brands products and Amazon devices. We continue to work toward our sourcing goals, including our commitment to reduce deforestation risks associated with raw materials and ingredients used in Amazon and Whole Foods Market private brands products. In addition to these commitments, Amazon also complies with European regulations to encourage responsible sourcing and reduce deforestation. We also incorporate recycled materials into many new products and product packaging, particularly in Amazon devices, where feasible.\n\nWhile many of our sourcing commitments identify materials we seek to include in our private brands products, we also identify those we seek to avoid. Our Formulated Products Restricted Substances List is an extensive list of chemicals we seek to exclude in Amazon private brands baby, household cleaning, personal care, and beauty products in the U.S. and Europe. Our Restricted Substances List for packaging outlines the chemicals of concern we seek to avoid in the packaging of Amazon private brands food and beverage products sold in North America. Our private brands apparel, accessory, footwear, and home textile products in North America, Europe, and Japan seek to comply with the Apparel and Footwear International RSL Management (AFIRM) Group’s Restricted Substances List. Our compliance teams are responsible for enforcing these restricted substances lists with our suppliers.\n\nOur sustainability efforts are not limited to our own operations. We look for ways to encourage our selling partners to develop products with sustainability in mind, such as through our Climate Pledge Friendly program and the Sustainability Solutions Hub. We also participate in industry partnerships and working groups to drive greater adoption of sustainable and responsible practices.\n\nAdditionally, we use our success and scale to engage with certified diverse suppliers and small businesses and share their products with our customers.31 Through our Supplier Diversity Initiative, customers can shop at storefronts that support women-owned businesses and Black-owned businesses, as well as at the Amazon Saheli Store, which displays products made and sold by women supported by nongovernmental organizations (NGOs) in India.\n\nLearn more about how we create a more inclusive and equitable business environment for our suppliers and empower small businesses.\n\nAll cotton for Amazon private brands apparel is sourced from recycled materials, from farms certified as producing organic cotton, or through the Better Cotton Initiative.", "original_types": ["text", "header"], "id": 654}
{"type": "section", "content": "Our Progress\n\nMaterials and Agricultural Commodities Sourcing\n\n2023 Progress (% of in-scope products that meet our goal or ambition)", "doc_id": "amazon2023", "page": 60, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Progress\n\nMaterials and Agricultural Commodities Sourcing\n\n2023 Progress (% of in-scope products that meet our goal or ambition)", "original_types": ["text", "header"], "id": 655}
{"type": "table", "content": "Materials and Agricultural Commodities Sourcing\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 60, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Materials and Agricultural Commodities Sourcing\nMarkdown representation of the table", "id": 656}
{"type": "section", "content": "Sustainable Seafood and Animal Welfare Goals and Ambitions\n\nSeafood\n\nSource Responsibly Farmed or sustainable wild-caught fresh and frozen seafood\n\nn/a\n\nn/a\n\nMaterials and Agricultural Commodities Sourcing", "doc_id": "amazon2023", "page": "60-61", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Sustainable Seafood and Animal Welfare Goals and Ambitions\n\nSeafood\n\nSource Responsibly Farmed or sustainable wild-caught fresh and frozen seafood\n\nn/a\n\nn/a\n\nMaterials and Agricultural Commodities Sourcing", "original_types": ["text", "header"], "id": 657}
{"type": "table", "content": "Commodity or Material\nGoal or Ambition\n2023 Progress (% of in-scope products that meet our goal or ambition)", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Commodity or Material\nGoal or Ambition\n2023 Progress (% of in-scope products that meet our goal or ambition)", "id": 658}
{"type": "table", "content": "Sustainable Seafood and Animal Welfare Goals and Ambitions\nCommodity or Material\nGoal or Ambition\n2023 Progress (% of in-scope products that meet our goal or ambition)", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Sustainable Seafood and Animal Welfare Goals and Ambitions\nCommodity or Material\nGoal or Ambition\n2023 Progress (% of in-scope products that meet our goal or ambition)", "id": 659}
{"type": "table", "content": "Seafood (cont'd)\nSource Amazon private brands seafood products that have a third-party sustainability certification or are actively working toward certification or engaged in a fishery improvement project (FIP). \nAmazon private brands North America", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Seafood (cont'd)\nSource Amazon private brands seafood products that have a third-party sustainability certification or are actively working toward certification or engaged in a fishery improvement project (FIP). \nAmazon private brands North America", "id": 660}
{"type": "table", "content": "Eggs\nSource shell and liquid egg products to a cage-free or higher animal welfare standard.\nAmazon private brands and national brands shell and liquid egg selections sold in North America are cage-free.", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Eggs\nSource shell and liquid egg products to a cage-free or higher animal welfare standard.\nAmazon private brands and national brands shell and liquid egg selections sold in North America are cage-free.", "id": 661}
{"type": "table", "content": "Pork\nSource fresh pork sold in the Whole Foods Market meat department in the U.S. and Canada that is crate-free and certified by the Global Animal Partnership.\nWhole Foods Market", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Pork\nSource fresh pork sold in the Whole Foods Market meat department in the U.S. and Canada that is crate-free and certified by the Global Animal Partnership.\nWhole Foods Market", "id": 662}
{"type": "table", "content": "Other Animal Proteins\nSource all fresh beef, pork, chicken, turkey (excluding kosher turkey), and lamb sold in the meat department to Whole Foods Market’s Animal Welfare Standards \nWhole Foods Market", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Other Animal Proteins\nSource all fresh beef, pork, chicken, turkey (excluding kosher turkey), and lamb sold in the meat department to Whole Foods Market’s Animal Welfare Standards \nWhole Foods Market", "id": 663}
{"type": "table", "content": "Apparel Goals and Ambitions\nCotton\nSource all cotton for Amazon private brands apparel products from more sustainable sources, which we define as being sourced from recycled materials, from farms certified as producing organic cotton, or through the Better Cotton Initiative.\nAmazon private brands apparel products", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Apparel Goals and Ambitions\nCotton\nSource all cotton for Amazon private brands apparel products from more sustainable sources, which we define as being sourced from recycled materials, from farms certified as producing organic cotton, or through the Better Cotton Initiative.\nAmazon private brands apparel products", "id": 664}
{"type": "table", "content": "Leather\nSource leather apparel and shoe products from more sustainable sources, which we define as being sourced from tanneries that meet the Leather Working Group’s Bronze level or higher.\nAmazon did not source any private brands apparel or shoes made from leather in 2023.", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Leather\nSource leather apparel and shoe products from more sustainable sources, which we define as being sourced from tanneries that meet the Leather Working Group’s Bronze level or higher.\nAmazon did not source any private brands apparel or shoes made from leather in 2023.", "id": 665}
{"type": "table", "content": "Manufactured Cellulosic Fibers\nSource manufactured cellulosic fibers used in Amazon private brands apparel products—including rayon, viscose, lyocell, and modal—from more sustainable sources. We use the nonprofit Canopy’s tools and reports to help avoid fibers sourced from endangered forests, endangered species’ habitats, or other controversial sources.\nAmazon private brands apparel products", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Manufactured Cellulosic Fibers\nSource manufactured cellulosic fibers used in Amazon private brands apparel products—including rayon, viscose, lyocell, and modal—from more sustainable sources. We use the nonprofit Canopy’s tools and reports to help avoid fibers sourced from endangered forests, endangered species’ habitats, or other controversial sources.\nAmazon private brands apparel products", "id": 666}
{"type": "table", "content": "Recycled Fabrics\nIncrease the use of recycled fabrics in Amazon private brands apparel products, including moving from conventional to recycled polyester and launching products made from innovative recycled fibers.\nPolyester in Amazon private brands apparel products is recycled polyester.", "doc_id": "amazon2023", "page": 61, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Recycled Fabrics\nIncrease the use of recycled fabrics in Amazon private brands apparel products, including moving from conventional to recycled polyester and launching products made from innovative recycled fibers.\nPolyester in Amazon private brands apparel products is recycled polyester.", "id": 667}
{"type": "section", "content": "Amazon Private Brands\n\nWe consider sustainability from the beginning stages of design for our private brands products. Through our commodities sourcing goals, we prioritize using recycled content, reducing material use, and improving recyclability of our products. We also consider the use phase of our products and have programs in place to help customers keep their products in use for longer. In 2023, we expanded these efforts by launching a spare parts and repair program in Europe for our private brands products.\n\nIn 2023, we partnered with the U.S. Agency for International Development (USAID), BHP Foundation, and the Chandler Foundation for the Powering a Just Energy Transition Green Minerals Challenge (JET Minerals Challenge). Amazon non-grocery private brands offers everything from beauty and skin care to diapers and infant wipes. In 2023, we published a Formulated Products Restricted Substances List for cleaning, beauty, and personal care. We also expanded our compliance with AFIRM’s Restricted Substances List to home textile products, which we actively enforce with our suppliers.\n\nLearn more about our device packaging and device recycling\n\nResponsible Mineral Sourcing\n\nWhile Amazon does not engage in direct sourcing from mine sites and smelters, we are committed to avoiding the use of minerals that have fueled conflict. We strive to have 100% of tin, tungsten, tantalum, and gold (3TG) smelters and refiners in our supply chain conform with a recognized minerals certification program. These certifications, such as the Responsible Minerals Initiative (RMI)’s Responsible Minerals Assurance Process (RMAP), are critical to enabling responsible raw material sourcing and processing upstream in the supply chain. We support suppliers in their efforts to increase smelter audits through RMAP, and we expect our suppliers to remove nonconformant smelters from our supply chain.\n\nClimate Pledge Friendly\n\nWe expanded our collaboration with the Ellen MacArthur Foundation to drive scalable, industry-wide solutions for a circular economy in 2023. As a Strategic Partner of the Ellen MacArthur Foundation Network, we are working to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. Through this partnership, we are working to develop certifications for products with circular features, providing customers with the information they need to make a more circular choice. These certifications encourage brands, original equipment manufacturers, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.", "doc_id": "amazon2023", "page": 62, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon Private Brands\n\nWe consider sustainability from the beginning stages of design for our private brands products. Through our commodities sourcing goals, we prioritize using recycled content, reducing material use, and improving recyclability of our products. We also consider the use phase of our products and have programs in place to help customers keep their products in use for longer. In 2023, we expanded these efforts by launching a spare parts and repair program in Europe for our private brands products.\n\nIn 2023, we partnered with the U.S. Agency for International Development (USAID), BHP Foundation, and the Chandler Foundation for the Powering a Just Energy Transition Green Minerals Challenge (JET Minerals Challenge). Amazon non-grocery private brands offers everything from beauty and skin care to diapers and infant wipes. In 2023, we published a Formulated Products Restricted Substances List for cleaning, beauty, and personal care. We also expanded our compliance with AFIRM’s Restricted Substances List to home textile products, which we actively enforce with our suppliers.\n\nLearn more about our device packaging and device recycling\n\nResponsible Mineral Sourcing\n\nWhile Amazon does not engage in direct sourcing from mine sites and smelters, we are committed to avoiding the use of minerals that have fueled conflict. We strive to have 100% of tin, tungsten, tantalum, and gold (3TG) smelters and refiners in our supply chain conform with a recognized minerals certification program. These certifications, such as the Responsible Minerals Initiative (RMI)’s Responsible Minerals Assurance Process (RMAP), are critical to enabling responsible raw material sourcing and processing upstream in the supply chain. We support suppliers in their efforts to increase smelter audits through RMAP, and we expect our suppliers to remove nonconformant smelters from our supply chain.\n\nClimate Pledge Friendly\n\nWe expanded our collaboration with the Ellen MacArthur Foundation to drive scalable, industry-wide solutions for a circular economy in 2023. As a Strategic Partner of the Ellen MacArthur Foundation Network, we are working to leverage our reach, technology, and innovation capabilities and the foundation’s subject-matter expertise to launch and scale circular economy solutions. Through this partnership, we are working to develop certifications for products with circular features, providing customers with the information they need to make a more circular choice. These certifications encourage brands, original equipment manufacturers, suppliers, retailers, and e-commerce platforms to create products that are designed in accordance with circular economy principles.", "original_types": ["text", "header"], "id": 668}
{"type": "section", "content": "We collaborate with our selling partners to identify opportunities for them to develop more sustainable products and apply for certifications that recognize the strides they are already making in sustainability. In 2023, we worked with a variety of selling partners—such as L’Oréal, Levi’s, and Patagonia—to develop and launch Climate Pledge Friendly products.", "doc_id": "amazon2023", "page": 62, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We collaborate with our selling partners to identify opportunities for them to develop more sustainable products and apply for certifications that recognize the strides they are already making in sustainability. In 2023, we worked with a variety of selling partners—such as L’Oréal, Levi’s, and Patagonia—to develop and launch Climate Pledge Friendly products.", "original_types": ["text"], "id": 669}
{"type": "section", "content": "The Sustainability Solutions Hub\n\nWe launched the Sustainability Solutions Hub in 2023 for Amazon selling partners in France, Germany, Italy, Spain, the UK, and the U.S. The Hub helps them get started with Amazon’s sustainability programs, including Climate Pledge Friendly, Amazon Renewed, and Ships in Product Packaging. It includes a curated set of third-party service providers to support selling partners in designing and certifying more sustainable products and packaging, as well as personalized reporting dashboards.\n\nDiscover Product Sustainability Features\n\nHelping customers discover products with sustainability features throughout the online shopping experience.", "doc_id": "amazon2023", "page": 63, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "The Sustainability Solutions Hub\n\nWe launched the Sustainability Solutions Hub in 2023 for Amazon selling partners in France, Germany, Italy, Spain, the UK, and the U.S. The Hub helps them get started with Amazon’s sustainability programs, including Climate Pledge Friendly, Amazon Renewed, and Ships in Product Packaging. It includes a curated set of third-party service providers to support selling partners in designing and certifying more sustainable products and packaging, as well as personalized reporting dashboards.\n\nDiscover Product Sustainability Features\n\nHelping customers discover products with sustainability features throughout the online shopping experience.", "original_types": ["text", "header"], "id": 670}
{"type": "table", "content": "Number of Products Recognized by One or More Certification in Our Climate Pledge Friendly Program\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 63, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Number of Products Recognized by One or More Certification in Our Climate Pledge Friendly Program\nMarkdown representation of the table", "id": 671}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 63, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 672}
{"type": "section", "content": "As part of the Hub, we launched new dashboards that provide selling partners with personalized, relevant data and information so they can:\n\n- Track their monthly sales in Amazon’s sustainability programs over the past 12 months.\n\n- Identify their other products that are eligible or candidates for our sustainability programs.\n\n- Highlight products that are currently enrolled in Amazon’s sustainability programs.\n\nIn 2023, over 200,000 selling partners participated in Amazon’s sustainability programs.\n\nLooking Forward\n\nWe continue to evolve the customer shopping experience and make the discovery of products with sustainability features even easier. In March 2024, we launched a new online shopping experience that highlights these features (see visual, right), such as recycled materials and organic content, in products that are recognized by one or more certification in the Climate Pledge Friendly program. We also look forward to expanding and refining the Sustainability Solutions Hub to support our efforts to welcome our selling partners into our sustainability programs and give them the resources they need to succeed in sharing more sustainable products with our customers.", "doc_id": "amazon2023", "page": 63, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "As part of the Hub, we launched new dashboards that provide selling partners with personalized, relevant data and information so they can:\n\n- Track their monthly sales in Amazon’s sustainability programs over the past 12 months.\n\n- Identify their other products that are eligible or candidates for our sustainability programs.\n\n- Highlight products that are currently enrolled in Amazon’s sustainability programs.\n\nIn 2023, over 200,000 selling partners participated in Amazon’s sustainability programs.\n\nLooking Forward\n\nWe continue to evolve the customer shopping experience and make the discovery of products with sustainability features even easier. In March 2024, we launched a new online shopping experience that highlights these features (see visual, right), such as recycled materials and organic content, in products that are recognized by one or more certification in the Climate Pledge Friendly program. We also look forward to expanding and refining the Sustainability Solutions Hub to support our efforts to welcome our selling partners into our sustainability programs and give them the resources they need to succeed in sharing more sustainable products with our customers.", "original_types": ["text", "header"], "id": 673}
{"type": "section", "content": "Supplier Diversity\n\nBuilding diverse and inclusive supply chains drives innovation, supports competitiveness, meets customers' expectations, and spurs local economic growth through community investment and job creation. For Amazon, a diverse and inclusive supply chain not only strengthens the resilience of our business but also drives the ability to innovate on behalf of our customers through the diverse perspectives and knowledge of people from all backgrounds. We are dedicated to advancing supplier diversity and inclusion (SDI) throughout our supply chain—engaging with diverse-owned and small businesses and driving long-term economic sustainability in the communities we serve.", "doc_id": "amazon2023", "page": 64, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supplier Diversity\n\nBuilding diverse and inclusive supply chains drives innovation, supports competitiveness, meets customers' expectations, and spurs local economic growth through community investment and job creation. For Amazon, a diverse and inclusive supply chain not only strengthens the resilience of our business but also drives the ability to innovate on behalf of our customers through the diverse perspectives and knowledge of people from all backgrounds. We are dedicated to advancing supplier diversity and inclusion (SDI) throughout our supply chain—engaging with diverse-owned and small businesses and driving long-term economic sustainability in the communities we serve.", "original_types": ["text", "header"], "id": 674}
{"type": "section", "content": "Our Approach\n\nThrough Amazon’s global supplier diversity and inclusion (SDI) strategy, we strive to build a more inclusive and equitable supply chain that fully reflects the diversity of our customers and employees. To achieve this, we are prioritizing three actions:\n\n- Driving economic benefits by scaling existing and onboarding new certified diverse suppliers, which are businesses certified to be at least 51% owned and operated by an individual or group that is part of a traditionally underrepresented or underserved community\n\n- Expanding SDI initiatives across the globe\n\n- Partnering with advocacy organizations to further engage with certified Tier 1 diverse suppliers worldwide\n\nDriving Economic Benefits with Diverse Businesses\n\nAmazon believes that the intentional inclusion of diversity in our supply chain is a powerful catalyst for economic growth in the communities where we operate—helping increase job creation, wealth distribution, and local development.\n\nThe Positive Effects of Supplier Diversity in Amazon’s Supply Chain", "doc_id": "amazon2023", "page": 65, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nThrough Amazon’s global supplier diversity and inclusion (SDI) strategy, we strive to build a more inclusive and equitable supply chain that fully reflects the diversity of our customers and employees. To achieve this, we are prioritizing three actions:\n\n- Driving economic benefits by scaling existing and onboarding new certified diverse suppliers, which are businesses certified to be at least 51% owned and operated by an individual or group that is part of a traditionally underrepresented or underserved community\n\n- Expanding SDI initiatives across the globe\n\n- Partnering with advocacy organizations to further engage with certified Tier 1 diverse suppliers worldwide\n\nDriving Economic Benefits with Diverse Businesses\n\nAmazon believes that the intentional inclusion of diversity in our supply chain is a powerful catalyst for economic growth in the communities where we operate—helping increase job creation, wealth distribution, and local development.\n\nThe Positive Effects of Supplier Diversity in Amazon’s Supply Chain", "original_types": ["text", "header"], "id": 675}
{"type": "table", "content": "Supplier Diversity by the Numbers", "doc_id": "amazon2023", "page": 65, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supplier Diversity by the Numbers", "id": 676}
{"type": "table", "content": "2023\nNumber of Diverse Suppliers | 500+\nSpend on Diverse Suppliers | $4.3B\nJobs Supported for Diverse Suppliers | 30,000\nWages Earned for Diverse Suppliers | $2.8B\nNumber of Countries with SDI Programs | 8", "doc_id": "amazon2023", "page": 65, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023\nNumber of Diverse Suppliers | 500+\nSpend on Diverse Suppliers | $4.3B\nJobs Supported for Diverse Suppliers | 30,000\nWages Earned for Diverse Suppliers | $2.8B\nNumber of Countries with SDI Programs | 8", "id": 677}
{"type": "section", "content": "Expanding SDI Globally\n\nAs a global company, we are able to drive economic empowerment for more businesses around the world. While Amazon's SDI initiative began in the U.S., we are focused on expanding these efforts internationally, integrating more certified diverse suppliers into our global supply chain. We are committed to an inclusive global supply chain and insist on the highest bar of equivalent certification standards and processes, as utilized in the U.S.", "doc_id": "amazon2023", "page": 65, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Expanding SDI Globally\n\nAs a global company, we are able to drive economic empowerment for more businesses around the world. While Amazon's SDI initiative began in the U.S., we are focused on expanding these efforts internationally, integrating more certified diverse suppliers into our global supply chain. We are committed to an inclusive global supply chain and insist on the highest bar of equivalent certification standards and processes, as utilized in the U.S.", "original_types": ["text", "header"], "id": 678}
{"type": "section", "content": "Partnering with Diversity Advocacy Organizations\n\nserve more communities globally. Outside the U.S., we spent over $74 million with more than 100 certified Tier 1 global diverse suppliers in 2023.\n\nTo support Amazon’s global expansion, we participated in SDI events in Australia, Canada, the UK, and the Netherlands, including conferences hosted by Supply Nation (Australia), the Canadian Aboriginal and Minority Supplier Council, and Minority Supplier Development UK. We also participated in the European Supplier Diversity Program (ESDP)’s inaugural procurement conference in the Netherlands. We are committed to supporting ESDP as they work to empower ethnic minority- and immigrant-owned businesses across European markets.\n\nLooking Forward\n\nAmazon’s supplier diversity program is guided by our commitment to scale and engage with certified diverse-owned or small businesses and drive economic value in our communities. In 2023, we were inducted into the Billion Dollar Roundtable (BDR), a group of corporations dedicated to spending $1 billion each annually with certified U.S. Tier 1 diverse suppliers. As a BDR member, an Amazon representative was elected to the European Supplier Diversity Program (ESDP)’s inaugural procurement conference in the Netherlands. We are committed to supporting ESDP as they work to empower ethnic minority- and immigrant-owned businesses across European markets.\n\nDiversity advocacy organizations play a crucial role in our SDI strategy. In 2023, we sponsored events and programs to develop diverse suppliers as well as participated in panel discussions and fireside chats to share information about SDI at Amazon. Being recognized as a diverse supplier by Amazon requires valid certification. That is why we annually confirm that each of our diverse suppliers has been certified by one of the following agencies:\n\nAwards and Recognitions\n\nIn 2023, Amazon earned the following recognitions for our SDI efforts:\n\n- E-Mega Award: NMSDC Dallas-Fort Worth Minority Supplier Development Council\n\n- 2023 Class IV Corporation of the Year Finalist: NMSDC\n\n- Buying Entity of the Year Nomination: NMSDC Dallas-Fort Worth Minority Supplier Development Council\n\n- Corporation of the Year Nomination: NMSDC New York/New Jersey Minority Supplier Development Council\n\n- Outstanding Corporation Award Nomination: WBENC New York\n\n- Leading Global Corporation for Diverse Business Practices Award: The Maryland Washington Minority Companies Association\n\nCountries with SDI Initiatives\n\nFirst Country with an SDI Initiative\n\nCountries That Launched SDI Initiatives in 2023\n\n- Canadian Aboriginal and Minority Supplier Council (CAMSC)\n\n- Department of Transportation Disadvantaged Business Enterprise (DBE) Program\n\n- Disability:IN\n\n- Minority Supplier Development UK (MSDUK)\n\n- National LGBT Chamber of Commerce (NGLCC)\n\n- National Minority Supplier Development Council (NMSDC) or a regional affiliate\n\n- National Veteran Business Development Council (NVBDC)", "doc_id": "amazon2023", "page": 66, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Partnering with Diversity Advocacy Organizations\n\nserve more communities globally. Outside the U.S., we spent over $74 million with more than 100 certified Tier 1 global diverse suppliers in 2023.\n\nTo support Amazon’s global expansion, we participated in SDI events in Australia, Canada, the UK, and the Netherlands, including conferences hosted by Supply Nation (Australia), the Canadian Aboriginal and Minority Supplier Council, and Minority Supplier Development UK. We also participated in the European Supplier Diversity Program (ESDP)’s inaugural procurement conference in the Netherlands. We are committed to supporting ESDP as they work to empower ethnic minority- and immigrant-owned businesses across European markets.\n\nLooking Forward\n\nAmazon’s supplier diversity program is guided by our commitment to scale and engage with certified diverse-owned or small businesses and drive economic value in our communities. In 2023, we were inducted into the Billion Dollar Roundtable (BDR), a group of corporations dedicated to spending $1 billion each annually with certified U.S. Tier 1 diverse suppliers. As a BDR member, an Amazon representative was elected to the European Supplier Diversity Program (ESDP)’s inaugural procurement conference in the Netherlands. We are committed to supporting ESDP as they work to empower ethnic minority- and immigrant-owned businesses across European markets.\n\nDiversity advocacy organizations play a crucial role in our SDI strategy. In 2023, we sponsored events and programs to develop diverse suppliers as well as participated in panel discussions and fireside chats to share information about SDI at Amazon. Being recognized as a diverse supplier by Amazon requires valid certification. That is why we annually confirm that each of our diverse suppliers has been certified by one of the following agencies:\n\nAwards and Recognitions\n\nIn 2023, Amazon earned the following recognitions for our SDI efforts:\n\n- E-Mega Award: NMSDC Dallas-Fort Worth Minority Supplier Development Council\n\n- 2023 Class IV Corporation of the Year Finalist: NMSDC\n\n- Buying Entity of the Year Nomination: NMSDC Dallas-Fort Worth Minority Supplier Development Council\n\n- Corporation of the Year Nomination: NMSDC New York/New Jersey Minority Supplier Development Council\n\n- Outstanding Corporation Award Nomination: WBENC New York\n\n- Leading Global Corporation for Diverse Business Practices Award: The Maryland Washington Minority Companies Association\n\nCountries with SDI Initiatives\n\nFirst Country with an SDI Initiative\n\nCountries That Launched SDI Initiatives in 2023\n\n- Canadian Aboriginal and Minority Supplier Council (CAMSC)\n\n- Department of Transportation Disadvantaged Business Enterprise (DBE) Program\n\n- Disability:IN\n\n- Minority Supplier Development UK (MSDUK)\n\n- National LGBT Chamber of Commerce (NGLCC)\n\n- National Minority Supplier Development Council (NMSDC) or a regional affiliate\n\n- National Veteran Business Development Council (NVBDC)", "original_types": ["text", "header"], "id": 679}
{"type": "section", "content": "- Small Business Administration (SBA) HUBZone or 8(a) Business Development Program\n\n- South African Supplier Diversity Council (SASDC)\n\n- Supply Nation (Australia)\n\n- WEConnect International\n\nCommunity Impact\n\nCompanies have an important opportunity to create meaningful, tangible change in the communities where they operate. Locally driven programs for communities benefit both residents and businesses, spurring innovation and economic growth. Amazon has a presence in thousands of communities around the world, and with this broad scale comes broad responsibility. We strive to leverage our size, reach, and ability to innovate quickly to strengthen the communities where our employees live and work. An important part of this is collaboration, which is why we work side by side with local partners to find solutions to our communities’ most pressing challenges and build long-term, innovative programs that have a lasting, positive influence.", "doc_id": "amazon2023", "page": "66-67", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "- Small Business Administration (SBA) HUBZone or 8(a) Business Development Program\n\n- South African Supplier Diversity Council (SASDC)\n\n- Supply Nation (Australia)\n\n- WEConnect International\n\nCommunity Impact\n\nCompanies have an important opportunity to create meaningful, tangible change in the communities where they operate. Locally driven programs for communities benefit both residents and businesses, spurring innovation and economic growth. Amazon has a presence in thousands of communities around the world, and with this broad scale comes broad responsibility. We strive to leverage our size, reach, and ability to innovate quickly to strengthen the communities where our employees live and work. An important part of this is collaboration, which is why we work side by side with local partners to find solutions to our communities’ most pressing challenges and build long-term, innovative programs that have a lasting, positive influence.", "original_types": ["text", "header"], "id": 680}
{"type": "section", "content": "Our Approach\n\nAmazon’s culture is built around finding effective solutions to difficult problems, and we apply this thinking to support the communities where our employees live and work. We leverage our people, technology, and logistics networks to build programs and products that help enable future generations to thrive in our communities. We focus on several priority areas across our business and respond to emergent needs where we are uniquely positioned to make a positive difference. Our work especially emphasizes historically underrepresented communities, because we envision a world that embraces diverse perspectives and where all individuals have equitable opportunities.\n\nAmazon’s community impact efforts are focused on seven key areas:\n\n- Supporting economic impact in communities: Amazon adds economic vibrancy to the communities where we operate through investments and job creation. Our investments in economies across the globe expand opportunities for local residents and small businesses and catalyze local economic activity.\n\n- Creating and preserving affordable housing: All people should have access to housing they can afford, but many communities in the U.S. are facing an affordable housing shortage, with low-income and minority families disproportionately affected. That is why we created the Amazon Housing Equity Fund, a $2 billion commitment to use our scale and reach to support innovative housing solutions in the communities we call home.\n\n- Addressing food insecurity and basic needs: Amazon aims to improve access to healthy, affordable food for customers in underserved communities across the U.S. Each year, we donate groceries to local food banks and leverage our logistics network to provide free delivery of groceries to families in need. Additionally, the Whole Foods Market Foundation seeks to enhance livelihoods in its sourcing communities, bring more nutritious foods into children’s schools and homes, and improve local food systems and healthy food access. It is also investing in producer loan and accelerator programs that empower small-scale, local, and emerging producers to grow their businesses.\n\n- Addressing health equity: We are focused on providing equitable access to health resources. In 2021, AWS launched the AWS Health Equity Initiative, a three-year program with the goal of enhancing health outcomes for underserved and historically marginalized communities. We aim to distribute up to $60 million in cloud computing credits to support organizations promoting health equity globally by the end of 2024.", "doc_id": "amazon2023", "page": 68, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nAmazon’s culture is built around finding effective solutions to difficult problems, and we apply this thinking to support the communities where our employees live and work. We leverage our people, technology, and logistics networks to build programs and products that help enable future generations to thrive in our communities. We focus on several priority areas across our business and respond to emergent needs where we are uniquely positioned to make a positive difference. Our work especially emphasizes historically underrepresented communities, because we envision a world that embraces diverse perspectives and where all individuals have equitable opportunities.\n\nAmazon’s community impact efforts are focused on seven key areas:\n\n- Supporting economic impact in communities: Amazon adds economic vibrancy to the communities where we operate through investments and job creation. Our investments in economies across the globe expand opportunities for local residents and small businesses and catalyze local economic activity.\n\n- Creating and preserving affordable housing: All people should have access to housing they can afford, but many communities in the U.S. are facing an affordable housing shortage, with low-income and minority families disproportionately affected. That is why we created the Amazon Housing Equity Fund, a $2 billion commitment to use our scale and reach to support innovative housing solutions in the communities we call home.\n\n- Addressing food insecurity and basic needs: Amazon aims to improve access to healthy, affordable food for customers in underserved communities across the U.S. Each year, we donate groceries to local food banks and leverage our logistics network to provide free delivery of groceries to families in need. Additionally, the Whole Foods Market Foundation seeks to enhance livelihoods in its sourcing communities, bring more nutritious foods into children’s schools and homes, and improve local food systems and healthy food access. It is also investing in producer loan and accelerator programs that empower small-scale, local, and emerging producers to grow their businesses.\n\n- Addressing health equity: We are focused on providing equitable access to health resources. In 2021, AWS launched the AWS Health Equity Initiative, a three-year program with the goal of enhancing health outcomes for underserved and historically marginalized communities. We aim to distribute up to $60 million in cloud computing credits to support organizations promoting health equity globally by the end of 2024.", "original_types": ["text", "header"], "id": 681}
{"type": "section", "content": "- Empowering students and adults through education and skills training: We believe in the transformative power of education, which is why we invest in programs that help young learners, higher education students, and adults unlock their full potential. Our goal is to help 29 million people globally grow their technical skills with free cloud computing training by 2025 through AWS-designed programs. Additionally, with the rapid growth of artificial intelligence (AI) and the need for an AI-savvy workforce, Amazon has pledged to provide free AI skills training to 2 million people globally by 2025.\n\n- Supporting disaster relief and response efforts: Our global logistics capabilities, combined with AWS technology, make us uniquely suited to help people when natural disasters strike. Our disaster relief and response efforts provide fast, effective aid for affected communities.\n\n- Funding nature in our communities: We support the restoration and conservation of natural spaces in communities across the globe. Through Amazon’s Right Now Climate Fund, we invest in projects that aim to mitigate the effects of climate change, add green space to urban areas, improve biodiversity, and enhance livelihoods.\n\nThrough support from the Right Now Climate Fund, CIFAL Málaga-UNITAR and Amazon launched the Green Helmets educational initiative, designed to teach young people in Spain how to develop and implement sustainability projects.", "doc_id": "amazon2023", "page": 68, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "- Empowering students and adults through education and skills training: We believe in the transformative power of education, which is why we invest in programs that help young learners, higher education students, and adults unlock their full potential. Our goal is to help 29 million people globally grow their technical skills with free cloud computing training by 2025 through AWS-designed programs. Additionally, with the rapid growth of artificial intelligence (AI) and the need for an AI-savvy workforce, Amazon has pledged to provide free AI skills training to 2 million people globally by 2025.\n\n- Supporting disaster relief and response efforts: Our global logistics capabilities, combined with AWS technology, make us uniquely suited to help people when natural disasters strike. Our disaster relief and response efforts provide fast, effective aid for affected communities.\n\n- Funding nature in our communities: We support the restoration and conservation of natural spaces in communities across the globe. Through Amazon’s Right Now Climate Fund, we invest in projects that aim to mitigate the effects of climate change, add green space to urban areas, improve biodiversity, and enhance livelihoods.\n\nThrough support from the Right Now Climate Fund, CIFAL Málaga-UNITAR and Amazon launched the Green Helmets educational initiative, designed to teach young people in Spain how to develop and implement sustainability projects.", "original_types": ["text"], "id": 682}
{"type": "section", "content": "Supporting Economic Impact in Communities\n\nTo continue serving and innovating for our customers, we invest heavily in the communities where we operate. These investments spur new economic activity that, in turn, results in expanded opportunities for local residents and small businesses. Amazon has made significant investments in the U.S. economy since 2010, which have contributed more than $880 billion to national gross domestic product (GDP). We have supported 4.5 million American jobs through full- and part-time jobs created directly by Amazon, jobs indirectly supported by our investments in areas like construction and logistics, and jobs created by small and medium-sized businesses selling on Amazon.com. In fact, Amazon has created more jobs in the past decade than any other U.S. company. We understand the value of developing opportunities for community members to climb the economic ladder and believe in helping them every step of the way.", "doc_id": "amazon2023", "page": 69, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supporting Economic Impact in Communities\n\nTo continue serving and innovating for our customers, we invest heavily in the communities where we operate. These investments spur new economic activity that, in turn, results in expanded opportunities for local residents and small businesses. Amazon has made significant investments in the U.S. economy since 2010, which have contributed more than $880 billion to national gross domestic product (GDP). We have supported 4.5 million American jobs through full- and part-time jobs created directly by Amazon, jobs indirectly supported by our investments in areas like construction and logistics, and jobs created by small and medium-sized businesses selling on Amazon.com. In fact, Amazon has created more jobs in the past decade than any other U.S. company. We understand the value of developing opportunities for community members to climb the economic ladder and believe in helping them every step of the way.", "original_types": ["text", "header"], "id": 683}
{"type": "section", "content": "Addressing Food Insecurity and Basic Needs\n\nWe strive to bring healthy, affordable food to our communities through our delivery service providers (DSPs), donations, funding, and partnerships with organizations helping individuals and families faced with food insecurity.\n\nOur Community Delivery program leverages our network of last mile fleet DSPs to provide free meal deliveries from food banks and other local organizations to households in need. Through this program, in 2023, we delivered over 10 million meals to underserved families to help alleviate food insecurity across the U.S.\n\nIn partnership with local food banks, we are scaling our food redistribution efforts to contribute to food security in local communities. As of the end of 2023, 100% of our grocery facilities in North America and Europe partner with a local food bank, over 40 of which are Feeding America affiliates. As a proud partner of Feeding America, Amazon Fresh also donated $250,000 to the organization’s efforts to increase food security in the U.S.\n\nWhole Foods Market\n\nThe Whole Foods Market foundations (Whole Planet Foundation, Whole Kids Foundation, and Whole Cities Foundation) further fulfill the company’s higher purpose to nourish people and the planet by helping to advance healthy food access, nutrition, and economic opportunities in local and global communities. See the full list of 2023 grantees.\n\nLocal and Emerging Accelerator Program\n\nWhole Foods Market launched the Local and Emerging Accelerator Program (LEAP) in 2022 to support local and emerging suppliers through mentorship opportunities and tailored educational programming. In 2023, 20 cohort participants received a 10-week educational curriculum taught by industry experts, a year-long mentorship with a Whole Foods Market local forager, and the potential for financial support to promote business growth. Upon successful completion of the program, participants have the opportunity to become Whole Foods Market suppliers in their hometowns and potentially beyond.\n\nSupporting Local Communities through Our Grocery Stores", "doc_id": "amazon2023", "page": 70, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Addressing Food Insecurity and Basic Needs\n\nWe strive to bring healthy, affordable food to our communities through our delivery service providers (DSPs), donations, funding, and partnerships with organizations helping individuals and families faced with food insecurity.\n\nOur Community Delivery program leverages our network of last mile fleet DSPs to provide free meal deliveries from food banks and other local organizations to households in need. Through this program, in 2023, we delivered over 10 million meals to underserved families to help alleviate food insecurity across the U.S.\n\nIn partnership with local food banks, we are scaling our food redistribution efforts to contribute to food security in local communities. As of the end of 2023, 100% of our grocery facilities in North America and Europe partner with a local food bank, over 40 of which are Feeding America affiliates. As a proud partner of Feeding America, Amazon Fresh also donated $250,000 to the organization’s efforts to increase food security in the U.S.\n\nWhole Foods Market\n\nThe Whole Foods Market foundations (Whole Planet Foundation, Whole Kids Foundation, and Whole Cities Foundation) further fulfill the company’s higher purpose to nourish people and the planet by helping to advance healthy food access, nutrition, and economic opportunities in local and global communities. See the full list of 2023 grantees.\n\nLocal and Emerging Accelerator Program\n\nWhole Foods Market launched the Local and Emerging Accelerator Program (LEAP) in 2022 to support local and emerging suppliers through mentorship opportunities and tailored educational programming. In 2023, 20 cohort participants received a 10-week educational curriculum taught by industry experts, a year-long mentorship with a Whole Foods Market local forager, and the potential for financial support to promote business growth. Upon successful completion of the program, participants have the opportunity to become Whole Foods Market suppliers in their hometowns and potentially beyond.\n\nSupporting Local Communities through Our Grocery Stores", "original_types": ["text", "header"], "id": 684}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 70, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 685}
{"type": "section", "content": "We have a hyperlocal approach to engage and give back to the communities where we operate our grocery stores that prioritizes causes that address food insecurity and sustainability. We enable stores to identify local causes that are important to their communities, which allows our grocery teams to drive meaningful benefits within each store’s region.\n\nOne hyperlocal activation supported in 2023 was in Fullerton, California, where a store partnered with their local Boys & Girls Club. Through various Boys & Girls Club initiatives—including participating in the annual Community Fair, filling backpacks with academic supplies for the start of the school year, and providing meals for Thanksgiving dinners—the store gave back to the specific needs of its community.\n\nIn collaboration with the San Francisco-Marin Food Bank, Amazon Flex drivers help deliver a monthly box of healthy, shelf-stable food to seniors who are part of their Supplemental Food Program.", "doc_id": "amazon2023", "page": 70, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "We have a hyperlocal approach to engage and give back to the communities where we operate our grocery stores that prioritizes causes that address food insecurity and sustainability. We enable stores to identify local causes that are important to their communities, which allows our grocery teams to drive meaningful benefits within each store’s region.\n\nOne hyperlocal activation supported in 2023 was in Fullerton, California, where a store partnered with their local Boys & Girls Club. Through various Boys & Girls Club initiatives—including participating in the annual Community Fair, filling backpacks with academic supplies for the start of the school year, and providing meals for Thanksgiving dinners—the store gave back to the specific needs of its community.\n\nIn collaboration with the San Francisco-Marin Food Bank, Amazon Flex drivers help deliver a monthly box of healthy, shelf-stable food to seniors who are part of their Supplemental Food Program.", "original_types": ["text"], "id": 686}
{"type": "section", "content": "Nourishing Our Neighborhoods Program\n\nNourishing Our Neighborhoods was created by Whole Foods Market to expand capacity and capability for community-based food rescue organizations to move food from where it is available to where it is needed most. Marking its fourth year in 2023, Nourishing Our Neighborhoods has donated 44 refrigerated vans to community-based food rescue and redistribution programs that transport food to low-income communities across the U.S. and Canada.\n\nIn 2023, Whole Foods Market launched Nourishing Our Neighborhoods Mobile Pantry, pop-up farmers-market-style events at local schools. Each event provides fresh produce and pantry staples to 700 families. In addition, Stuff the Van events were introduced for each of the Nourishing Our Neighborhoods food rescue partners. These events were held at Whole Foods Market stores, where team members filled each of the vans with a mix of fresh produce and pantry staples. Through these events, Whole Foods Market donated over 57,585 pounds of food—enough for nearly 48,000 meals.\n\nEmpowering Students and Adults through Education and Skills Training\n\nAmazon invests in programs to increase youth and adult access to science, technology, engineering, and math (STEM) opportunities. Our aim is to help young learners and workforce professionals alike develop valuable skills needed for careers in STEM fields through a broad range of education and training programs.\n\nFor young learners, Amazon’s programs are designed to excite curiosity about career opportunities in STEM, computer science, and beyond.\n\nAWS InCommunities\n\nAWS InCommunities is a community outreach program that aims to positively influence the communities where AWS builds and operates its global infrastructure. Four pillars anchor this work: Science, Technology, Engineering, Arts, and Math (STEAM) Education; Local Skills Development; Sustainability; and Hyperlocal Social Impact.\n\nThrough its programming, in 2023, AWS InCommunities drove nearly 6 million positive community interactions—defined as an engagement with a community member that results in a benefit for the recipient. One of its most notable initiatives is the AWS InCommunities Fund, a microgrant program that supports local individuals and organizations driving positive change. AWS launched 10 of these microgrants in 2023, helping renovate schools and rural hospitals in India, establish community health care facilities in Indonesia, implement food access programs in Australia, support sustainability programs such as textile recycling in the U.S., and more.\n\nSince its launch in 2021, AWS InCommunities has allocated over $2.1 million in microgrants to bolster hyperlocal projects in communities around the world. In 2023 alone, more than 5,800 AWS employees volunteered nearly 22,000 hours to support AWS InCommunities programs across its four areas of focus.", "doc_id": "amazon2023", "page": 71, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Nourishing Our Neighborhoods Program\n\nNourishing Our Neighborhoods was created by Whole Foods Market to expand capacity and capability for community-based food rescue organizations to move food from where it is available to where it is needed most. Marking its fourth year in 2023, Nourishing Our Neighborhoods has donated 44 refrigerated vans to community-based food rescue and redistribution programs that transport food to low-income communities across the U.S. and Canada.\n\nIn 2023, Whole Foods Market launched Nourishing Our Neighborhoods Mobile Pantry, pop-up farmers-market-style events at local schools. Each event provides fresh produce and pantry staples to 700 families. In addition, Stuff the Van events were introduced for each of the Nourishing Our Neighborhoods food rescue partners. These events were held at Whole Foods Market stores, where team members filled each of the vans with a mix of fresh produce and pantry staples. Through these events, Whole Foods Market donated over 57,585 pounds of food—enough for nearly 48,000 meals.\n\nEmpowering Students and Adults through Education and Skills Training\n\nAmazon invests in programs to increase youth and adult access to science, technology, engineering, and math (STEM) opportunities. Our aim is to help young learners and workforce professionals alike develop valuable skills needed for careers in STEM fields through a broad range of education and training programs.\n\nFor young learners, Amazon’s programs are designed to excite curiosity about career opportunities in STEM, computer science, and beyond.\n\nAWS InCommunities\n\nAWS InCommunities is a community outreach program that aims to positively influence the communities where AWS builds and operates its global infrastructure. Four pillars anchor this work: Science, Technology, Engineering, Arts, and Math (STEAM) Education; Local Skills Development; Sustainability; and Hyperlocal Social Impact.\n\nThrough its programming, in 2023, AWS InCommunities drove nearly 6 million positive community interactions—defined as an engagement with a community member that results in a benefit for the recipient. One of its most notable initiatives is the AWS InCommunities Fund, a microgrant program that supports local individuals and organizations driving positive change. AWS launched 10 of these microgrants in 2023, helping renovate schools and rural hospitals in India, establish community health care facilities in Indonesia, implement food access programs in Australia, support sustainability programs such as textile recycling in the U.S., and more.\n\nSince its launch in 2021, AWS InCommunities has allocated over $2.1 million in microgrants to bolster hyperlocal projects in communities around the world. In 2023 alone, more than 5,800 AWS employees volunteered nearly 22,000 hours to support AWS InCommunities programs across its four areas of focus.", "original_types": ["text", "header"], "id": 687}
{"type": "section", "content": "Skills Programs for Adults\n\nFor adults, Amazon offers a range of free courses for individuals with technical and non-technical backgrounds to help them learn new skills and accelerate their careers in cloud computing and other cloud-enabled fields, including AI.\n\nAWS Skills Centers\n\nAWS Skills Centers, in-person learning that offers no-cost, on-site and virtual cloud computing classes, physical learning spaces, and networking events. Skills Centers are located in Seattle, Washington; Arlington, Virginia; and Cape Town, South Africa.\n\nAmazon AI Ready\n\nIn 2023, Amazon announced AI Ready, a new commitment to provide free AI skills training to 2 million people globally by 2025. As part of this initiative, we launched eight new, free AI and generative AI courses; an AWS Generative AI Scholarship that provides high school and university students with access to a new generative AI course on Udacity; and a new collaboration with Code.org designed to help students learn about generative AI.\n\nLearn more about the launch of our first international AWS Skills Center in South Africa\n\nAmazon brings AWS cloud technology to areas hard-hit by natural disasters to support mapping, establish connectivity, and quickly increase capacity for emergency call centers. In 2023, AWS responded to 14 disasters and provided 103 affected business customers with $3.5 million worth of cloud credit donations to facilitate emergency services. AWS also provided its technology to assist responders in Hawaii, helping coordinate support and make it possible for community members to contact loved ones following the historic wildfires in Maui.\n\nSupporting Disaster Relief and Response Efforts\n\nWe mobilize the full breadth of our infrastructure, cloud technology, and global logistics network to help communities affected by natural disasters. Disaster Relief by Amazon delivers speed in the form of logistics and inventory, while AWS Disaster Response delivers information through access to connectivity and data.\n\nAWS Cloud Institute\n\nAWS Cloud Institute, a new, structured, hands-on training program that helps learners launch their cloud careers in as little as one year—regardless of their technical background. The first AWS Cloud Institute cohort began classes in January 2024.\n\nAWS re/Start\n\nAWS re/Start, a free-to-learner, cohort-based workforce development training program that helps unemployed or underemployed individuals with little or no tech experience build the skills needed for entry-level cloud careers. In 2023, AWS re/Start was delivered to more than 200 cities in 60 countries. Through this program, 90% of participants have been connected to job interview opportunities.\n\nAWS Skill Builder\n\nAWS Skill Builder, an online learning center that offers more than 600 free, on-demand cloud courses in up to 17 languages. Skill Builder provides subscriptions with access to game-based learning, labs, scenario-based challenges, and practice exams for select AWS Certifications.\n\nGiving Back through Volunteering Efforts", "doc_id": "amazon2023", "page": 72, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Skills Programs for Adults\n\nFor adults, Amazon offers a range of free courses for individuals with technical and non-technical backgrounds to help them learn new skills and accelerate their careers in cloud computing and other cloud-enabled fields, including AI.\n\nAWS Skills Centers\n\nAWS Skills Centers, in-person learning that offers no-cost, on-site and virtual cloud computing classes, physical learning spaces, and networking events. Skills Centers are located in Seattle, Washington; Arlington, Virginia; and Cape Town, South Africa.\n\nAmazon AI Ready\n\nIn 2023, Amazon announced AI Ready, a new commitment to provide free AI skills training to 2 million people globally by 2025. As part of this initiative, we launched eight new, free AI and generative AI courses; an AWS Generative AI Scholarship that provides high school and university students with access to a new generative AI course on Udacity; and a new collaboration with Code.org designed to help students learn about generative AI.\n\nLearn more about the launch of our first international AWS Skills Center in South Africa\n\nAmazon brings AWS cloud technology to areas hard-hit by natural disasters to support mapping, establish connectivity, and quickly increase capacity for emergency call centers. In 2023, AWS responded to 14 disasters and provided 103 affected business customers with $3.5 million worth of cloud credit donations to facilitate emergency services. AWS also provided its technology to assist responders in Hawaii, helping coordinate support and make it possible for community members to contact loved ones following the historic wildfires in Maui.\n\nSupporting Disaster Relief and Response Efforts\n\nWe mobilize the full breadth of our infrastructure, cloud technology, and global logistics network to help communities affected by natural disasters. Disaster Relief by Amazon delivers speed in the form of logistics and inventory, while AWS Disaster Response delivers information through access to connectivity and data.\n\nAWS Cloud Institute\n\nAWS Cloud Institute, a new, structured, hands-on training program that helps learners launch their cloud careers in as little as one year—regardless of their technical background. The first AWS Cloud Institute cohort began classes in January 2024.\n\nAWS re/Start\n\nAWS re/Start, a free-to-learner, cohort-based workforce development training program that helps unemployed or underemployed individuals with little or no tech experience build the skills needed for entry-level cloud careers. In 2023, AWS re/Start was delivered to more than 200 cities in 60 countries. Through this program, 90% of participants have been connected to job interview opportunities.\n\nAWS Skill Builder\n\nAWS Skill Builder, an online learning center that offers more than 600 free, on-demand cloud courses in up to 17 languages. Skill Builder provides subscriptions with access to game-based learning, labs, scenario-based challenges, and practice exams for select AWS Certifications.\n\nGiving Back through Volunteering Efforts", "original_types": ["text", "header"], "id": 688}
{"type": "section", "content": "Amazon encourages and enables our global teams to volunteer in their communities and support the causes they are passionate about. In 2023, over 170,000 volunteers from 54 countries participated in more than 25,000 events with more than 5,700 nonprofit and community organizations. This included over 76,000 Amazon volunteers who joined our second Global Month of Volunteering. During this initiative, team members volunteered for more than 4,500 events supporting over 1,200 nonprofit and community organizations—cleaning up parks, donating school supplies and clothing to students, building houses for families experiencing homelessness, assembling wish boxes for kids, and delivering meals to local food banks.", "doc_id": "amazon2023", "page": 72, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon encourages and enables our global teams to volunteer in their communities and support the causes they are passionate about. In 2023, over 170,000 volunteers from 54 countries participated in more than 25,000 events with more than 5,700 nonprofit and community organizations. This included over 76,000 Amazon volunteers who joined our second Global Month of Volunteering. During this initiative, team members volunteered for more than 4,500 events supporting over 1,200 nonprofit and community organizations—cleaning up parks, donating school supplies and clothing to students, building houses for families experiencing homelessness, assembling wish boxes for kids, and delivering meals to local food banks.", "original_types": ["text"], "id": 689}
{"type": "section", "content": "Addressing Health Equity\n\nAWS is harnessing the power of the cloud to advance health equity globally. Through the AWS Health Equity Initiative (HEI), AWS has pledged to provide up to $60 million in cloud computing credits to organizations that are using cloud computing technology to address health disparities that impact underserved or underrepresented communities around the world. AWS has already awarded over $32 million and supported 229 global organizations to promote equal access to health resources, up from 90 organizations in 2022.\n\nIn December 2023, AWS joined the Health Electrification and Telecommunications Alliance (HETA), Power Africa’s initiative for health facility electrification and digital connectivity in sub-Saharan Africa. Power Africa is part of the United States Agency for International Development (USAID) and harnesses the collective resources of public and private sectors to expand electricity in sub-Saharan Africa. AWS has pledged to support efforts across the initiative’s target countries, contributing cash and AWS promotional credits to help health facilities keep the lights on for nighttime services, reliably provide patients with oxygen and other lifesaving care, refrigerate vaccines and other temperature-sensitive medical commodities, and use the digital tools that modern medicine relies on. AWS is also collaborating with HETA to develop a cloud-based solution for real-time monitoring, analytics, predictive maintenance of energy and connectivity infrastructure.\n\nLooking Forward\n\nAs we move forward, Amazon aims to continue supporting the communities where we operate, focusing on the areas where we can make the biggest change: education, food security, disaster response, affordable housing equity, and health equity. Our goal is to keep using our infrastructure, technology, and passion for invention—along with the uniquely-Amazon skills and know-how of our global workforce—to unlock new opportunities for our communities across the globe.", "doc_id": "amazon2023", "page": 73, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Addressing Health Equity\n\nAWS is harnessing the power of the cloud to advance health equity globally. Through the AWS Health Equity Initiative (HEI), AWS has pledged to provide up to $60 million in cloud computing credits to organizations that are using cloud computing technology to address health disparities that impact underserved or underrepresented communities around the world. AWS has already awarded over $32 million and supported 229 global organizations to promote equal access to health resources, up from 90 organizations in 2022.\n\nIn December 2023, AWS joined the Health Electrification and Telecommunications Alliance (HETA), Power Africa’s initiative for health facility electrification and digital connectivity in sub-Saharan Africa. Power Africa is part of the United States Agency for International Development (USAID) and harnesses the collective resources of public and private sectors to expand electricity in sub-Saharan Africa. AWS has pledged to support efforts across the initiative’s target countries, contributing cash and AWS promotional credits to help health facilities keep the lights on for nighttime services, reliably provide patients with oxygen and other lifesaving care, refrigerate vaccines and other temperature-sensitive medical commodities, and use the digital tools that modern medicine relies on. AWS is also collaborating with HETA to develop a cloud-based solution for real-time monitoring, analytics, predictive maintenance of energy and connectivity infrastructure.\n\nLooking Forward\n\nAs we move forward, Amazon aims to continue supporting the communities where we operate, focusing on the areas where we can make the biggest change: education, food security, disaster response, affordable housing equity, and health equity. Our goal is to keep using our infrastructure, technology, and passion for invention—along with the uniquely-Amazon skills and know-how of our global workforce—to unlock new opportunities for our communities across the globe.", "original_types": ["text", "header"], "id": 690}
{"type": "section", "content": "People\n\nWe aim to be Earth’s best employer and the safest place to work in our industries—a mindset that inspires us to take a people-first approach and prioritize making our workforce feel valued and supported, while building a culture of inclusive experiences everywhere.", "doc_id": "amazon2023", "page": 74, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "People\n\nWe aim to be Earth’s best employer and the safest place to work in our industries—a mindset that inspires us to take a people-first approach and prioritize making our workforce feel valued and supported, while building a culture of inclusive experiences everywhere.", "original_types": ["text", "header"], "id": 691}
{"type": "table", "content": "In This Section", "doc_id": "amazon2023", "page": 74, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In This Section", "id": 692}
{"type": "section", "content": "75 Employee Experience\n81 Health and Safety\n86 Inclusive Experiences", "doc_id": "amazon2023", "page": 74, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "75 Employee Experience\n81 Health and Safety\n86 Inclusive Experiences", "original_types": ["text"], "id": 693}
{"type": "figure", "content": "Dynamo, Amazon’s newest office building in Bellevue, Washington, provides office and collaboration space for employees working across various teams at Amazon.", "doc_id": "amazon2023", "page": 74, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Dynamo, Amazon’s newest office building in Bellevue, Washington, provides office and collaboration space for employees working across various teams at Amazon.", "id": 694}
{"type": "section", "content": "Employee Experience\n\nPassionate, engaged employees are an important part of what makes Amazon successful. We strive to be Earth’s best employer, which is why investing in the physical, mental, and emotional well-being of our employees is a top priority. To achieve this ambition and ensure our workforce has the tools and support they need to succeed, we invest in our employees and their futures through comprehensive benefits, competitive compensation, prepaid education, and upskilling opportunities to help them build fulfilling careers. We listen to and learn from our employees through continuous engagement and communication channels that aim to improve their day-to-day experiences at work. We aim to ensure our benefits, compensation, and career development programs are equitable, empowering all employees to reach their full potential.", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employee Experience\n\nPassionate, engaged employees are an important part of what makes Amazon successful. We strive to be Earth’s best employer, which is why investing in the physical, mental, and emotional well-being of our employees is a top priority. To achieve this ambition and ensure our workforce has the tools and support they need to succeed, we invest in our employees and their futures through comprehensive benefits, competitive compensation, prepaid education, and upskilling opportunities to help them build fulfilling careers. We listen to and learn from our employees through continuous engagement and communication channels that aim to improve their day-to-day experiences at work. We aim to ensure our benefits, compensation, and career development programs are equitable, empowering all employees to reach their full potential.", "original_types": ["text", "header"], "id": 695}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 696}
{"type": "section", "content": "U.S. employees have participated in upskilling programs since we announced our Upskilling Pledge in 2019\n\nBuilt for employees, the Spheres provide a tranquil green workspace in the middle of Amazon’s Seattle, Washington, headquarters.", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "U.S. employees have participated in upskilling programs since we announced our Upskilling Pledge in 2019\n\nBuilt for employees, the Spheres provide a tranquil green workspace in the middle of Amazon’s Seattle, Washington, headquarters.", "original_types": ["text"], "id": 697}
{"type": "figure", "content": "Figure 2: Title...", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 2: Title...", "id": 698}
{"type": "section", "content": "Global employees have participated in Career Choice since it launched in 2012, up from 110,000 global employees in 2022", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Global employees have participated in Career Choice since it launched in 2012, up from 110,000 global employees in 2022", "original_types": ["text"], "id": 699}
{"type": "figure", "content": "Figure 3: Title...", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 3: Title...", "id": 700}
{"type": "section", "content": "In supplementary retirement benefits provided by Amazon to over 1 million U.S. employees in 2023", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In supplementary retirement benefits provided by Amazon to over 1 million U.S. employees in 2023", "original_types": ["text"], "id": 701}
{"type": "figure", "content": "Figure 4: Title...", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 4: Title...", "id": 702}
{"type": "section", "content": "Invested toward pay increases for customer fulfillment and transportation employees in the U.S., bringing the average pay for those roles to over $20.50 per hour", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Invested toward pay increases for customer fulfillment and transportation employees in the U.S., bringing the average pay for those roles to over $20.50 per hour", "original_types": ["text"], "id": 703}
{"type": "figure", "content": "Figure 5: Title...", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 5: Title...", "id": 704}
{"type": "section", "content": "New countries where we partner with Twill Therapeutics to offer employees self-guided tools for everyday mental health", "doc_id": "amazon2023", "page": 75, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "New countries where we partner with Twill Therapeutics to offer employees self-guided tools for everyday mental health", "original_types": ["text"], "id": 705}
{"type": "section", "content": "Our Approach\n\nAmazon works hard to attract and recruit the best employees and create a valuable experience for them every day. We do this in part by listening to our employees to understand how we can design benefits that work for them and their families and that are optimized for affordability, flexibility, choice, and personalization. We strive to provide competitive and equitable compensation that rewards our employees for their achievements.\n\nWe also want our employees to have the opportunity to grow and thrive with us. As the skills needed for jobs in our sector rapidly evolve, we are preparing our employees by investing in training and development programs with real-world applications. We do this by analyzing the labor market to identify in-demand jobs and providing education and training—including through career coaching and our mentoring network—that reflect the skills needed to be successful in these roles.\n\nIn everything we do, we aim to create a culture of mutual respect and progress, underpinned by candid, constructive communication. We offer multiple ways to ensure employees' voices are heard and that they can share concerns and ask questions, including our “open door” policy, whereby employees can reach out to anyone in management with suggestions, concerns, or feedback. Input from our employees, collected from multiple channels, is used to continuously improve our workplace.\n\nOur Benefits\n\nWe offer a range of benefits that support our employees and eligible family members, including their children and domestic partners. These comprehensive benefits begin on day one and include health care insurance coverage, paid parental leave, ways to save for the future, our upskilling programs, and other resources to improve health and well-being.\n\nNew Benefits Added in 2023\n\nExpanded access to a virtual care family-building program to more than 54 countries\n\nLaunched charitable giving benefit*\n\nLaunched emergency savings and financial assistance programs for customer fulfillment and transportation employees*\n\nLaunched payroll-deducted emergency savings benefit in the U.S. and Puerto Rico\n\nAdded supplementary retirement benefits in Kenya and Belgium\n\nFoundational Benefits\n\n401(k) retirement plan with company match*\n\nLife and accidental death and dismemberment insurance*\n\nAnytime Pay, a free wage access program*\n\nUp to 20 weeks of paid leave for birthing parents*\n\nRamp Back, a reduced schedule for up to eight weeks after the birth or adoption of a child*\n\nMost corporate employees are given Amazon shares as part of their compensation package\n\nAnnual discount on products sold and shipped by Amazon\n\nNon-U.S. supplementary retirement benefits\n\nAccess to financial counseling and estate planning\n\nExpanded options for pharmacies able to fill 90-day supplies of medication*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "doc_id": "amazon2023", "page": 76, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nAmazon works hard to attract and recruit the best employees and create a valuable experience for them every day. We do this in part by listening to our employees to understand how we can design benefits that work for them and their families and that are optimized for affordability, flexibility, choice, and personalization. We strive to provide competitive and equitable compensation that rewards our employees for their achievements.\n\nWe also want our employees to have the opportunity to grow and thrive with us. As the skills needed for jobs in our sector rapidly evolve, we are preparing our employees by investing in training and development programs with real-world applications. We do this by analyzing the labor market to identify in-demand jobs and providing education and training—including through career coaching and our mentoring network—that reflect the skills needed to be successful in these roles.\n\nIn everything we do, we aim to create a culture of mutual respect and progress, underpinned by candid, constructive communication. We offer multiple ways to ensure employees' voices are heard and that they can share concerns and ask questions, including our “open door” policy, whereby employees can reach out to anyone in management with suggestions, concerns, or feedback. Input from our employees, collected from multiple channels, is used to continuously improve our workplace.\n\nOur Benefits\n\nWe offer a range of benefits that support our employees and eligible family members, including their children and domestic partners. These comprehensive benefits begin on day one and include health care insurance coverage, paid parental leave, ways to save for the future, our upskilling programs, and other resources to improve health and well-being.\n\nNew Benefits Added in 2023\n\nExpanded access to a virtual care family-building program to more than 54 countries\n\nLaunched charitable giving benefit*\n\nLaunched emergency savings and financial assistance programs for customer fulfillment and transportation employees*\n\nLaunched payroll-deducted emergency savings benefit in the U.S. and Puerto Rico\n\nAdded supplementary retirement benefits in Kenya and Belgium\n\nFoundational Benefits\n\n401(k) retirement plan with company match*\n\nLife and accidental death and dismemberment insurance*\n\nAnytime Pay, a free wage access program*\n\nUp to 20 weeks of paid leave for birthing parents*\n\nRamp Back, a reduced schedule for up to eight weeks after the birth or adoption of a child*\n\nMost corporate employees are given Amazon shares as part of their compensation package\n\nAnnual discount on products sold and shipped by Amazon\n\nNon-U.S. supplementary retirement benefits\n\nAccess to financial counseling and estate planning\n\nExpanded options for pharmacies able to fill 90-day supplies of medication*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "original_types": ["text", "header"], "id": 706}
{"type": "section", "content": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "doc_id": "amazon2023", "page": 76, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "original_types": ["text"], "id": 707}
{"type": "section", "content": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "doc_id": "amazon2023", "page": 76, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "original_types": ["text"], "id": 708}
{"type": "section", "content": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "doc_id": "amazon2023", "page": 76, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "original_types": ["text"], "id": 709}
{"type": "section", "content": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "doc_id": "amazon2023", "page": 76, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Access to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year\n\nAccess to upskilling programs, including prepaid education benefits at hundreds of colleges, universities, and education partners globally\n\nTravel and lodging reimbursement for select medical issues*\n\nMedical, prescription, dental, and vision coverage\n\nSubsidies to offset costs related to cycling to work\n\nBusiness travel insurance\n\nExpanded mental health counseling coverage from three to five visits per issue per year", "original_types": ["text"], "id": 710}
{"type": "section", "content": "Compensation and Financial Support\n\nAmazon provides competitive compensation to reward employees for their dedication and hard work. In 2023, we announced pay increases for U.S. frontline employees, with average pay for those in customer fulfillment and transportation increasing from $19 to $20.50 per hour—an investment of $1.3 billion. Our investments have had a positive influence on local communities across the U.S., helping to grow household incomes and boost local economies. Amazon also invested $600 million in international wage increases in 2023. In the UK, we committed to investing £170 million to increase the minimum starting hourly wage for our frontline operations employees between October 2023 and April 2024. By the end of 2023, we had increased the hourly wage in the UK to £12, representing a 14% increase in average hourly wages and a 26% increase since 2018. In Germany, we increased the entry hourly wage for logistics employees to €14, representing a 17% increase in average hourly wages and a 30% increase since 2019. Hourly wages automatically increase in Germany after 12 and 24 months, with full-time employees earning an average of €37,000 per year after two years.", "doc_id": "amazon2023", "page": 77, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Compensation and Financial Support\n\nAmazon provides competitive compensation to reward employees for their dedication and hard work. In 2023, we announced pay increases for U.S. frontline employees, with average pay for those in customer fulfillment and transportation increasing from $19 to $20.50 per hour—an investment of $1.3 billion. Our investments have had a positive influence on local communities across the U.S., helping to grow household incomes and boost local economies. Amazon also invested $600 million in international wage increases in 2023. In the UK, we committed to investing £170 million to increase the minimum starting hourly wage for our frontline operations employees between October 2023 and April 2024. By the end of 2023, we had increased the hourly wage in the UK to £12, representing a 14% increase in average hourly wages and a 26% increase since 2018. In Germany, we increased the entry hourly wage for logistics employees to €14, representing a 17% increase in average hourly wages and a 30% increase since 2019. Hourly wages automatically increase in Germany after 12 and 24 months, with full-time employees earning an average of €37,000 per year after two years.", "original_types": ["text", "header"], "id": 711}
{"type": "section", "content": "Supporting the Health of Amazon Employees and Their Families\n\nNavigating health care for family planning and fertility can be challenging, which is why Amazon proudly provides our U.S. employees—regardless of their gender, sexual orientation, or relationship status—with family-building benefits. This benefit offers support with in vitro fertilization (IVF), egg freezing, genetic testing, and more, and it supported nearly 30,400 full-time and hourly Amazon employees in 2023. Through this offering, our U.S. employees also receive access to approximately 1,070 fertility specialists. In 2023, Amazon received the National Infertility Association’s Award for Access, which recognizes companies whose work helps increase access to family-building options for people living with infertility.\n\nWorkforce Changes\n\nIn January 2023, Amazon announced the difficult decision to eliminate 18,000 roles, with an additional 9,000 roles eliminated in April 2023. Several of our businesses were affected by these decisions. We did not make these decisions lightly and were committed to providing support to ease the transition out of Amazon. Employees whose roles were eliminated received a separation payment, external job placement support, and region-specific transitional health insurance benefits, where applicable.\n\nEmployee Engagement in Sustainability\n\nIn 2023, Amazon launched the Sustainable Amazonian (SAM) program to engage our corporate employees worldwide in adopting more sustainable practices, both in their day-to-day jobs and outside of work. Approximately 7,540 employees signed the SAM Pledge, which included making a commitment to leave things better than they found them and completing a sustainability training.\n\nFlexible Scheduling\n\nOperations employees have provided us feedback that they want more flexibility in when and how they work. That’s why we offer various shift and schedule options, including permanent full- or part-time positions and seasonal work. Options for morning, day, night, and weekend shifts are provided, so employees can work during the times that best fit their circumstances.", "doc_id": "amazon2023", "page": 78, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Supporting the Health of Amazon Employees and Their Families\n\nNavigating health care for family planning and fertility can be challenging, which is why Amazon proudly provides our U.S. employees—regardless of their gender, sexual orientation, or relationship status—with family-building benefits. This benefit offers support with in vitro fertilization (IVF), egg freezing, genetic testing, and more, and it supported nearly 30,400 full-time and hourly Amazon employees in 2023. Through this offering, our U.S. employees also receive access to approximately 1,070 fertility specialists. In 2023, Amazon received the National Infertility Association’s Award for Access, which recognizes companies whose work helps increase access to family-building options for people living with infertility.\n\nWorkforce Changes\n\nIn January 2023, Amazon announced the difficult decision to eliminate 18,000 roles, with an additional 9,000 roles eliminated in April 2023. Several of our businesses were affected by these decisions. We did not make these decisions lightly and were committed to providing support to ease the transition out of Amazon. Employees whose roles were eliminated received a separation payment, external job placement support, and region-specific transitional health insurance benefits, where applicable.\n\nEmployee Engagement in Sustainability\n\nIn 2023, Amazon launched the Sustainable Amazonian (SAM) program to engage our corporate employees worldwide in adopting more sustainable practices, both in their day-to-day jobs and outside of work. Approximately 7,540 employees signed the SAM Pledge, which included making a commitment to leave things better than they found them and completing a sustainability training.\n\nFlexible Scheduling\n\nOperations employees have provided us feedback that they want more flexibility in when and how they work. That’s why we offer various shift and schedule options, including permanent full- or part-time positions and seasonal work. Options for morning, day, night, and weekend shifts are provided, so employees can work during the times that best fit their circumstances.", "original_types": ["text", "header"], "id": 712}
{"type": "section", "content": "Upskilling Our Employees\n\nWe are empowering our employees to build the skills they need to remain competitive, grow their careers, and move into higher-paying roles. In the process, we’re building a strong pipeline of talent to fill current and future in-demand jobs, both at Amazon and beyond.\n\nCentral to our upskilling efforts is our pledge to invest $1.2 billion to provide prepaid education and technical skills training to over 300,000 of our U.S. employees by 2025. We achieved this goal by the end of 2023—with approximately 358,180 U.S. employees participating in our upskilling programs. Career Choice, our longest-standing program, has provided job training to more than 175,000 employees worldwide since it launched in 2012, a 62% increase from 110,000 in 2022. In 2023, Surge2IT, a program to help entry-level IT employees across Amazon’s operations pursue careers in higher-paying technical roles, merged into Career Choice.\n\n2023 Upskilling Programs", "doc_id": "amazon2023", "page": 79, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Upskilling Our Employees\n\nWe are empowering our employees to build the skills they need to remain competitive, grow their careers, and move into higher-paying roles. In the process, we’re building a strong pipeline of talent to fill current and future in-demand jobs, both at Amazon and beyond.\n\nCentral to our upskilling efforts is our pledge to invest $1.2 billion to provide prepaid education and technical skills training to over 300,000 of our U.S. employees by 2025. We achieved this goal by the end of 2023—with approximately 358,180 U.S. employees participating in our upskilling programs. Career Choice, our longest-standing program, has provided job training to more than 175,000 employees worldwide since it launched in 2012, a 62% increase from 110,000 in 2022. In 2023, Surge2IT, a program to help entry-level IT employees across Amazon’s operations pursue careers in higher-paying technical roles, merged into Career Choice.\n\n2023 Upskilling Programs", "original_types": ["text", "header"], "id": 713}
{"type": "table", "content": "2023 Upskilling Programs\nAmazon Technical Academy | Amazon Technical Academy is a virtual, full-time, tuition-free training program designed to prepare employees—many of whom previously worked in non-technical roles—with the skills needed to become Amazon Software Development Engineers. In 2024, this program will integrate with other Amazon upskilling programs to provide greater accessibility to Amazon employees.\n\nCareer Choice | Career Choice offers prepaid tuition for associate and bachelor’s degrees, industry certifications in industries such as tech and transportation, English language learning, high school completion and GED programs, and career coaching. Receiving support from a global network of over 621 educational partners, annual participation in Career Choice has grown from 50,000 in 2021 to over 90,300 globally in 2023. Over 62% of Career Choice participants identify as Black, Indigenous, or a Person of Color.\n\nAmazon Technical Apprenticeship Program | The Amazon Technical Apprenticeship Program helps employees transition into technology careers by offering them paid opportunities to learn new skills and get on-the-job training. Since the program began in 2017, we have hired nearly 1,590 apprentices; nearly 1,000 have converted to full-time employment.\n\nAWS Grow Our Own Talent | AWS Grow Our Own Talent helps individuals with nontraditional experience and backgrounds develop skills needed for AWS data center roles through on-the-job training and placement opportunities. In 2023, we launched this program in new countries across Europe, the Middle East, Africa, and Asia-Pacific.\n\nMachine Learning University (MLU) | MLU trains Amazon employees in the theory and practical application of machine learning and artificial intelligence (AI) through hands-on education. In 2023, MLU expanded its course portfolio, offering 11 new courses, seven of which specifically focus on generative AI. In 2023, we delivered training through MLU to employees in over 100 different roles across 58 countries.\n\nAWS Intelligence Initiative | The AWS Intelligence Initiative is a 12- to 14-month rotational onboarding and technical upskilling program for engineers supporting Amazon Dedicated Cloud (ADC) regions. In 2023, there were 115 program participants in the U.S. and 193 outside the U.S., with 60 promoted to ADC engineers after completing the program. In the UK, 63% of our ADC workforce are graduates of this program.\n\nSustainability Certification | Sustainability Certification is a Career Choice initiative to upskill and retain our customer fulfillment and transportation employees through an education in sustainability. Through this initiative, participants enroll in a two-year program to pursue a Sustainability Certificate from UCLA Extension. In 2023, we piloted expanding this program in the UK, so that interested operations associates could pursue a UK National Diploma in Environmental Management.\n\nAWS Tech U | AWS Tech U is an accelerated workforce development program that empowers people to establish and hone the technical and professional skills needed to thrive at AWS. This program provides training for Amazon employees who want to pursue cloud-based technical careers and involves working on products that reflect real-world AWS solutions and interacting with customers. In 2023, approximately 1,670 employees enrolled as Tech U learners.\n\nUser Experience and Design Apprenticeship | The User Experience and Design Apprenticeship is a year-long program that helps employees build skills in research, information architecture, visual and interaction design, and validation techniques. Since the program’s launch in 2021, over 90% of graduates still work at Amazon. Participants get experience working on projects that directly affect the customer experience, such as finding ways for customers to discover new Alexa features or search for Amazon Prime Video content across devices.", "doc_id": "amazon2023", "page": 79, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "2023 Upskilling Programs\nAmazon Technical Academy | Amazon Technical Academy is a virtual, full-time, tuition-free training program designed to prepare employees—many of whom previously worked in non-technical roles—with the skills needed to become Amazon Software Development Engineers. In 2024, this program will integrate with other Amazon upskilling programs to provide greater accessibility to Amazon employees.\n\nCareer Choice | Career Choice offers prepaid tuition for associate and bachelor’s degrees, industry certifications in industries such as tech and transportation, English language learning, high school completion and GED programs, and career coaching. Receiving support from a global network of over 621 educational partners, annual participation in Career Choice has grown from 50,000 in 2021 to over 90,300 globally in 2023. Over 62% of Career Choice participants identify as Black, Indigenous, or a Person of Color.\n\nAmazon Technical Apprenticeship Program | The Amazon Technical Apprenticeship Program helps employees transition into technology careers by offering them paid opportunities to learn new skills and get on-the-job training. Since the program began in 2017, we have hired nearly 1,590 apprentices; nearly 1,000 have converted to full-time employment.\n\nAWS Grow Our Own Talent | AWS Grow Our Own Talent helps individuals with nontraditional experience and backgrounds develop skills needed for AWS data center roles through on-the-job training and placement opportunities. In 2023, we launched this program in new countries across Europe, the Middle East, Africa, and Asia-Pacific.\n\nMachine Learning University (MLU) | MLU trains Amazon employees in the theory and practical application of machine learning and artificial intelligence (AI) through hands-on education. In 2023, MLU expanded its course portfolio, offering 11 new courses, seven of which specifically focus on generative AI. In 2023, we delivered training through MLU to employees in over 100 different roles across 58 countries.\n\nAWS Intelligence Initiative | The AWS Intelligence Initiative is a 12- to 14-month rotational onboarding and technical upskilling program for engineers supporting Amazon Dedicated Cloud (ADC) regions. In 2023, there were 115 program participants in the U.S. and 193 outside the U.S., with 60 promoted to ADC engineers after completing the program. In the UK, 63% of our ADC workforce are graduates of this program.\n\nSustainability Certification | Sustainability Certification is a Career Choice initiative to upskill and retain our customer fulfillment and transportation employees through an education in sustainability. Through this initiative, participants enroll in a two-year program to pursue a Sustainability Certificate from UCLA Extension. In 2023, we piloted expanding this program in the UK, so that interested operations associates could pursue a UK National Diploma in Environmental Management.\n\nAWS Tech U | AWS Tech U is an accelerated workforce development program that empowers people to establish and hone the technical and professional skills needed to thrive at AWS. This program provides training for Amazon employees who want to pursue cloud-based technical careers and involves working on products that reflect real-world AWS solutions and interacting with customers. In 2023, approximately 1,670 employees enrolled as Tech U learners.\n\nUser Experience and Design Apprenticeship | The User Experience and Design Apprenticeship is a year-long program that helps employees build skills in research, information architecture, visual and interaction design, and validation techniques. Since the program’s launch in 2021, over 90% of graduates still work at Amazon. Participants get experience working on projects that directly affect the customer experience, such as finding ways for customers to discover new Alexa features or search for Amazon Prime Video content across devices.", "id": 714}
{"type": "section", "content": "Learn more about our community upskilling efforts", "doc_id": "amazon2023", "page": 79, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Learn more about our community upskilling efforts", "original_types": ["text"], "id": 715}
{"type": "section", "content": "Employee Communication Channels\n\nMany of our best ideas come from Amazon employees. We value their feedback—both positive sentiments and constructive comments that can help us improve the employee and customer experiences.\n\nAmazon uses various communication channels to maintain awareness of and responsiveness to changing dynamics, questions, concerns, and ideas across our large global workforce. This includes direct daily engagement, such as all-hands meetings with general managers, stand-up meetings with direct supervisors, and other one-on-one meetings.\n\nEmployees can also voice concerns, anonymously report potential violations of Amazon’s Code of Business Conduct and Ethics, and ask questions about potentially unethical conduct through Amazon’s Ethics Line. If needed, employees can access several other escalation mechanisms, including executive escalation and communication with their managers or human resources business partners. We actively encourage our employees to report any issues or concerns without fear of reprisal, intimidation, or harassment.\n\nMyVoice\n\nMyVoice, Amazon’s primary Voice of Associate platform, provides a two-way communication channel between our global associates and their site leadership. This online tool allows employees to express concerns, offer suggestions, and ask questions to leadership teams who will reply directly, enabling quicker, more collaborative issue remediation. Globally, employees provided over 600,000 comments through MyVoice in 2023.\n\nWe also completed a foundational re-architecture of the MyVoice tool and integrated it within our A to Z employee app, creating a mobile experience for employees that is simpler, is more user-friendly, and increases discoverability. Our enhancements to MyVoice also simplify the experience for leaders, centralizing feedback and associated follow-up actions and making it easier for leaders to review comments and track actions.\n\nAssociate Roundtables, Forums, and Safety Committees\n\nUsing the MyVoice mechanism, we continuously improve our policies and practices based on workers’ suggestions. For example, direct suggestions relating to time-tracking led to policy improvements that better served associate needs, provided increased flexibility, and allowed for better work/life balance.\n\nAt one fulfillment center in California, we received MyVoice feedback regarding difficulty in commuting to work. This feedback was taken into account, Amazon partnered with city and local officials to provide a better bus route and stop that would help alleviate the issue.\n\nAssociate Roundtables provide employees and managers with a meaningful opportunity to discuss issues, ask questions, and get immediate feedback in person. Amazon hosts these meetings around the globe, with their exact cadence varying by business line and site.\n\nAssociate Forums create opportunities for employees to connect with site leaders on decisions that affect the site or employee experience. In 2023, we held 146 Associate Forums.", "doc_id": "amazon2023", "page": 80, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employee Communication Channels\n\nMany of our best ideas come from Amazon employees. We value their feedback—both positive sentiments and constructive comments that can help us improve the employee and customer experiences.\n\nAmazon uses various communication channels to maintain awareness of and responsiveness to changing dynamics, questions, concerns, and ideas across our large global workforce. This includes direct daily engagement, such as all-hands meetings with general managers, stand-up meetings with direct supervisors, and other one-on-one meetings.\n\nEmployees can also voice concerns, anonymously report potential violations of Amazon’s Code of Business Conduct and Ethics, and ask questions about potentially unethical conduct through Amazon’s Ethics Line. If needed, employees can access several other escalation mechanisms, including executive escalation and communication with their managers or human resources business partners. We actively encourage our employees to report any issues or concerns without fear of reprisal, intimidation, or harassment.\n\nMyVoice\n\nMyVoice, Amazon’s primary Voice of Associate platform, provides a two-way communication channel between our global associates and their site leadership. This online tool allows employees to express concerns, offer suggestions, and ask questions to leadership teams who will reply directly, enabling quicker, more collaborative issue remediation. Globally, employees provided over 600,000 comments through MyVoice in 2023.\n\nWe also completed a foundational re-architecture of the MyVoice tool and integrated it within our A to Z employee app, creating a mobile experience for employees that is simpler, is more user-friendly, and increases discoverability. Our enhancements to MyVoice also simplify the experience for leaders, centralizing feedback and associated follow-up actions and making it easier for leaders to review comments and track actions.\n\nAssociate Roundtables, Forums, and Safety Committees\n\nUsing the MyVoice mechanism, we continuously improve our policies and practices based on workers’ suggestions. For example, direct suggestions relating to time-tracking led to policy improvements that better served associate needs, provided increased flexibility, and allowed for better work/life balance.\n\nAt one fulfillment center in California, we received MyVoice feedback regarding difficulty in commuting to work. This feedback was taken into account, Amazon partnered with city and local officials to provide a better bus route and stop that would help alleviate the issue.\n\nAssociate Roundtables provide employees and managers with a meaningful opportunity to discuss issues, ask questions, and get immediate feedback in person. Amazon hosts these meetings around the globe, with their exact cadence varying by business line and site.\n\nAssociate Forums create opportunities for employees to connect with site leaders on decisions that affect the site or employee experience. In 2023, we held 146 Associate Forums.", "original_types": ["text", "header"], "id": 716}
{"type": "section", "content": "Looking Forward\n\nAmazon is always exploring new ways to enhance the employee experience. We will continue focusing on upward mobility by offering technical and skills training for career advancement and encouraging participation in our Career Choice program, which provides employees the opportunity to get prepaid college, GED, or English as a second language (ESL) tuition. Finally, as we strive to be Earth’s best employer, we’ll seek to further integrate flexibility into how and when our employees work and will continue to offer competitive compensation, financial support, and benefits. We’re eager to keep improving our employee engagement efforts and build even more robust feedback mechanisms to ensure that our global workforce feels valued, heard, and appreciated.\n\nFreedom of Association\n\nWe respect freedom of association and our employees’ right to form, join, or not join labor unions or other lawful organizations of their choosing without fear of reprisal, intimidation, or harassment. These rights should be exercised in an informed and thoughtful manner.\n\nGlobally, Amazon applies or is party to dozens of collective bargaining agreements at national, regional, sectoral, and enterprise levels. In 2022, we established a European Works Council, holding our first meeting in April 2023. The European Works Council is composed of workers and employer representatives and meets regularly to discuss transnational company issues.\n\nLearn more about our respect for freedom of association in our Human Rights Commitment.", "doc_id": "amazon2023", "page": 80, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Looking Forward\n\nAmazon is always exploring new ways to enhance the employee experience. We will continue focusing on upward mobility by offering technical and skills training for career advancement and encouraging participation in our Career Choice program, which provides employees the opportunity to get prepaid college, GED, or English as a second language (ESL) tuition. Finally, as we strive to be Earth’s best employer, we’ll seek to further integrate flexibility into how and when our employees work and will continue to offer competitive compensation, financial support, and benefits. We’re eager to keep improving our employee engagement efforts and build even more robust feedback mechanisms to ensure that our global workforce feels valued, heard, and appreciated.\n\nFreedom of Association\n\nWe respect freedom of association and our employees’ right to form, join, or not join labor unions or other lawful organizations of their choosing without fear of reprisal, intimidation, or harassment. These rights should be exercised in an informed and thoughtful manner.\n\nGlobally, Amazon applies or is party to dozens of collective bargaining agreements at national, regional, sectoral, and enterprise levels. In 2022, we established a European Works Council, holding our first meeting in April 2023. The European Works Council is composed of workers and employer representatives and meets regularly to discuss transnational company issues.\n\nLearn more about our respect for freedom of association in our Human Rights Commitment.", "original_types": ["text", "header"], "id": 717}
{"type": "section", "content": "Health and Safety\n\nActions\n\n200K+ Employee safety observations successfully actioned to make our sites safer\n\n30% Improvement in global operations Recordable Incident Rate (RIR) over the past four years and 8% improvement from 2022. RIR includes any work-related injury that requires more than basic first aid treatment38, 39\n\n60% Improvement in global operations Lost Time Incident Rate (LTIR) over the past four years and 16% improvement from 2022. LTIR includes any work-related injury that requires someone to take time away from work (the most serious injuries)", "doc_id": "amazon2023", "page": 81, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Health and Safety\n\nActions\n\n200K+ Employee safety observations successfully actioned to make our sites safer\n\n30% Improvement in global operations Recordable Incident Rate (RIR) over the past four years and 8% improvement from 2022. RIR includes any work-related injury that requires more than basic first aid treatment38, 39\n\n60% Improvement in global operations Lost Time Incident Rate (LTIR) over the past four years and 16% improvement from 2022. LTIR includes any work-related injury that requires someone to take time away from work (the most serious injuries)", "original_types": ["text", "header"], "id": 718}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 81, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 719}
{"type": "section", "content": "Amazon’s people are the heart and soul of our operations and the reason that safety is integral to everything we do. We strive to be the safest place to work in the industries in which we operate, and we’re committed to making sure our employees’ health and well-being are prioritized. We are continuously working to enhance our safety processes, leveraging technology to reduce risk, investing in areas where we must improve, partnering with others, and listening to our employees. The meaningful progress we’ve made so far would not be possible without the combined efforts of our more than 9,000 dedicated safety professionals and every one of our employees around the world.", "doc_id": "amazon2023", "page": 81, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon’s people are the heart and soul of our operations and the reason that safety is integral to everything we do. We strive to be the safest place to work in the industries in which we operate, and we’re committed to making sure our employees’ health and well-being are prioritized. We are continuously working to enhance our safety processes, leveraging technology to reduce risk, investing in areas where we must improve, partnering with others, and listening to our employees. The meaningful progress we’ve made so far would not be possible without the combined efforts of our more than 9,000 dedicated safety professionals and every one of our employees around the world.", "original_types": ["text"], "id": 720}
{"type": "section", "content": "Our Approach\n\nOur Leadership Principle “Strive to Be Earth’s Best Employer” challenges us to create a safer, more productive, higher-performing, more diverse, and more just work environment. It reinforces that nothing is more important than the safety and well-being of our teams. We aim to be the safest workplace in the industries in which we operate. To drive continuous improvement, we measure progress against both lagging indicators, such as incident rates, and leading indicators, such as employee sentiment. We have safety audits, inspections, and feedback mechanisms in place at every Amazon facility. Together, these steps provide us with a full picture of our workplace safety. A risk management approach guides our prioritization and decision-making. It includes:\n\n- Engaging employees to continuously improve safety in our operations.\n- Assessing safety processes and adherence to standards through audits and inspections.\n- Measuring safety performance to assess program effectiveness and identify and remove hazards.\n- Working with our operations partners to enhance safety across our network.\n- Making targeted investments to enhance our safety performance.\n\nSupporting Our Operations and Workforce Safety\n\nCreating a culture where every employee feels engaged and empowered is essential to providing and maintaining a safe environment. We have established a series of feedback mechanisms to ensure we listen to our people on the front lines and incorporate their feedback on ways to improve safety across our operations.\n\nTraining Our Employees\n\nTraining is foundational to safety excellence. We are constantly updating and enhancing our suite of trainings to ensure they are effective and helpful for employees.\n\nSafety Observations\n\nOne method we use to take employee feedback and turn it into measurable action is Dragonfly. This tool—available on employees’ devices and at kiosks on-site—empowers employees to find and fix unsafe conditions or behaviors, assess safety processes and adherence to standards through audits and inspections, and suggest safety improvements during the course of their work. Dragonfly informs site managers so they can take appropriate action. In 2023, we successfully actioned over 200,000 Dragonfly observations to help make our sites safer.\n\nAnnual Training\n\nEvery employee and manager participates in annual safety training, which reinforces concepts such as emergency preparedness and response, hazards and controls of the specific jobs they are doing, safety coaching, identifying and reporting unsafe conditions and behaviors, and safety engagement. This goes beyond merely teaching someone the skills they need to complete a specific task. We incorporate real-world scenarios, with practical opportunities to practice what they have learned.\n\nSafety Committees", "doc_id": "amazon2023", "page": 82, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nOur Leadership Principle “Strive to Be Earth’s Best Employer” challenges us to create a safer, more productive, higher-performing, more diverse, and more just work environment. It reinforces that nothing is more important than the safety and well-being of our teams. We aim to be the safest workplace in the industries in which we operate. To drive continuous improvement, we measure progress against both lagging indicators, such as incident rates, and leading indicators, such as employee sentiment. We have safety audits, inspections, and feedback mechanisms in place at every Amazon facility. Together, these steps provide us with a full picture of our workplace safety. A risk management approach guides our prioritization and decision-making. It includes:\n\n- Engaging employees to continuously improve safety in our operations.\n- Assessing safety processes and adherence to standards through audits and inspections.\n- Measuring safety performance to assess program effectiveness and identify and remove hazards.\n- Working with our operations partners to enhance safety across our network.\n- Making targeted investments to enhance our safety performance.\n\nSupporting Our Operations and Workforce Safety\n\nCreating a culture where every employee feels engaged and empowered is essential to providing and maintaining a safe environment. We have established a series of feedback mechanisms to ensure we listen to our people on the front lines and incorporate their feedback on ways to improve safety across our operations.\n\nTraining Our Employees\n\nTraining is foundational to safety excellence. We are constantly updating and enhancing our suite of trainings to ensure they are effective and helpful for employees.\n\nSafety Observations\n\nOne method we use to take employee feedback and turn it into measurable action is Dragonfly. This tool—available on employees’ devices and at kiosks on-site—empowers employees to find and fix unsafe conditions or behaviors, assess safety processes and adherence to standards through audits and inspections, and suggest safety improvements during the course of their work. Dragonfly informs site managers so they can take appropriate action. In 2023, we successfully actioned over 200,000 Dragonfly observations to help make our sites safer.\n\nAnnual Training\n\nEvery employee and manager participates in annual safety training, which reinforces concepts such as emergency preparedness and response, hazards and controls of the specific jobs they are doing, safety coaching, identifying and reporting unsafe conditions and behaviors, and safety engagement. This goes beyond merely teaching someone the skills they need to complete a specific task. We incorporate real-world scenarios, with practical opportunities to practice what they have learned.\n\nSafety Committees", "original_types": ["text", "header"], "id": 721}
{"type": "section", "content": "Throughout 2023, more than 185,000 employees participated in over 25,000 safety meetings whose purpose was to gather feedback, describe new safety initiatives, and make sure that we are listening to employees’ feedback from their first day on the job, we ask whether their training has been prepared them to perform their jobs safely. Nearly 89% of those who responded to the survey told us that the training prepared them to do their jobs safely.\n\nMixed Reality Training\n\nBeyond routine training, we invest in new technologies to continuously build safety skills and competencies. We are piloting promising mixed reality training for some higher-risk jobs to simulate real-world scenarios in a controlled, safe environment. For example, we are testing a semi-immersive", "doc_id": "amazon2023", "page": 82, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Throughout 2023, more than 185,000 employees participated in over 25,000 safety meetings whose purpose was to gather feedback, describe new safety initiatives, and make sure that we are listening to employees’ feedback from their first day on the job, we ask whether their training has been prepared them to perform their jobs safely. Nearly 89% of those who responded to the survey told us that the training prepared them to do their jobs safely.\n\nMixed Reality Training\n\nBeyond routine training, we invest in new technologies to continuously build safety skills and competencies. We are piloting promising mixed reality training for some higher-risk jobs to simulate real-world scenarios in a controlled, safe environment. For example, we are testing a semi-immersive", "original_types": ["text", "header"], "id": 722}
{"type": "section", "content": "Our Partners\n\nOur delivery network is powered by thousands of small businesses and hundreds of thousands of drivers who leverage Amazon’s technology to improve on-road safety every day. While our partners each provide robust programs for their drivers, we offer additional safety resources and training for any delivery partner who wants to participate. For example, to help drivers practice how to navigate safely in a variety of weather conditions, we created a commercial driver simulator program that includes advanced simulations of potential ice, snow, heavy rain, wind, mechanical challenges, and tire failures.\n\nExamining Injuries at Amazon Facilities\n\nIn addition to looking at overall injury rates, we believe it is important to examine the types and frequency of injuries to help us prevent incidents from occurring in the first place. That is why we rigorously audit and inspect our sites to make sure our resources and protocols are helping us effectively identify, eliminate, or reduce safety hazards. In 2023, we conducted almost 6.3 million inspections within our operations facilities globally, a 152% increase from the 2.5 million conducted in 2020. We audited 240 sites across Amazon. These inspections and audits help us determine how to address the various types of injury. One of the most common types of injury at any warehousing or transportation company is a musculoskeletal disorder (MSD), more commonly known as a strain or sprain. Over the past four years, the rate of recordable MSD injuries at Amazon has improved by 27%, but they still make up about 57% of all recordable injuries at Amazon. The remaining 43% of our recordable injuries were due to slips, trips, and falls, or occasional objects that came loose and fell.\n\nOur Partners\n\nIn the Courier and Express Delivery Services industry, our RIR improved by 41% over the past four years. Amazon’s 2023 rate is 6.3, which is better than the latest BLS average of 11.5 for employers our size (250–999 employees).41 Our LTIR improved by 66% over the past four years. Amazon’s 2023 rate is 2.4, which is better than the latest BLS average of 4.7 for employers our size.", "doc_id": "amazon2023", "page": 83, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Partners\n\nOur delivery network is powered by thousands of small businesses and hundreds of thousands of drivers who leverage Amazon’s technology to improve on-road safety every day. While our partners each provide robust programs for their drivers, we offer additional safety resources and training for any delivery partner who wants to participate. For example, to help drivers practice how to navigate safely in a variety of weather conditions, we created a commercial driver simulator program that includes advanced simulations of potential ice, snow, heavy rain, wind, mechanical challenges, and tire failures.\n\nExamining Injuries at Amazon Facilities\n\nIn addition to looking at overall injury rates, we believe it is important to examine the types and frequency of injuries to help us prevent incidents from occurring in the first place. That is why we rigorously audit and inspect our sites to make sure our resources and protocols are helping us effectively identify, eliminate, or reduce safety hazards. In 2023, we conducted almost 6.3 million inspections within our operations facilities globally, a 152% increase from the 2.5 million conducted in 2020. We audited 240 sites across Amazon. These inspections and audits help us determine how to address the various types of injury. One of the most common types of injury at any warehousing or transportation company is a musculoskeletal disorder (MSD), more commonly known as a strain or sprain. Over the past four years, the rate of recordable MSD injuries at Amazon has improved by 27%, but they still make up about 57% of all recordable injuries at Amazon. The remaining 43% of our recordable injuries were due to slips, trips, and falls, or occasional objects that came loose and fell.\n\nOur Partners\n\nIn the Courier and Express Delivery Services industry, our RIR improved by 41% over the past four years. Amazon’s 2023 rate is 6.3, which is better than the latest BLS average of 11.5 for employers our size (250–999 employees).41 Our LTIR improved by 66% over the past four years. Amazon’s 2023 rate is 2.4, which is better than the latest BLS average of 4.7 for employers our size.", "original_types": ["text", "header"], "id": 723}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 83, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 724}
{"type": "section", "content": "While we still have work to do, we have made significant progress on safety since 2019. From the beginning of 2019 to the end of 2023, our global Recordable Incident Rate (RIR) improved by 30% and our Lost Time Incident Rate (LTIR) improved by 60%. These safety improvements came in the face of particularly tumultuous world events, including a pandemic that disrupted operations for almost every employer. Amazon’s safety performance continues to improve year over year—from 2022 to 2023, we improved our RIR by 8% and our LTIR by 16%. We are pleased with our progress but know we can do more. Each incident that occurs represents a person, and even one incident is too many. Learn more about how Amazon’s safety performance continues to improve year over year.\n\nSafety in the United States\n\nIn the U.S., we report our operations data to the Occupational Safety and Health Administration under two distinct industries: General Warehousing and Storage, and Courier and Express Delivery Services. The Bureau of Labor Statistics (BLS) determines industry averages for each industry based on the employers’ size and categorizes them accordingly. BLS publishes averages each November, meaning the 2022 averages published in November 2023 are the most recent. Similar to our global operations, we have made meaningful, measurable safety progress in the U.S. over the past four years. In the General Warehousing and Storage industry, our RIR improved by 24% over the past four years. Amazon’s 2023 rate is 6.5, which is better than the latest BLS average of 6.8 for employers our size (>1,000 employees).40 Our LTIR improved by 77% over the past four years. Amazon’s 2023 rate is 1.1, which is better than the latest BLS average of 2.6 for employers our size.", "doc_id": "amazon2023", "page": 83, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "While we still have work to do, we have made significant progress on safety since 2019. From the beginning of 2019 to the end of 2023, our global Recordable Incident Rate (RIR) improved by 30% and our Lost Time Incident Rate (LTIR) improved by 60%. These safety improvements came in the face of particularly tumultuous world events, including a pandemic that disrupted operations for almost every employer. Amazon’s safety performance continues to improve year over year—from 2022 to 2023, we improved our RIR by 8% and our LTIR by 16%. We are pleased with our progress but know we can do more. Each incident that occurs represents a person, and even one incident is too many. Learn more about how Amazon’s safety performance continues to improve year over year.\n\nSafety in the United States\n\nIn the U.S., we report our operations data to the Occupational Safety and Health Administration under two distinct industries: General Warehousing and Storage, and Courier and Express Delivery Services. The Bureau of Labor Statistics (BLS) determines industry averages for each industry based on the employers’ size and categorizes them accordingly. BLS publishes averages each November, meaning the 2022 averages published in November 2023 are the most recent. Similar to our global operations, we have made meaningful, measurable safety progress in the U.S. over the past four years. In the General Warehousing and Storage industry, our RIR improved by 24% over the past four years. Amazon’s 2023 rate is 6.5, which is better than the latest BLS average of 6.8 for employers our size (>1,000 employees).40 Our LTIR improved by 77% over the past four years. Amazon’s 2023 rate is 1.1, which is better than the latest BLS average of 2.6 for employers our size.", "original_types": ["text", "header"], "id": 725}
{"type": "figure", "content": "Figure 2: Title...", "doc_id": "amazon2023", "page": 83, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 2: Title...", "id": 726}
{"type": "section", "content": "Employees work together to load packages.\n\nComparative Safety Data\n\nWorldwide RIR: 30% improvement from 2019 to 2023", "doc_id": "amazon2023", "page": "83-84", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Employees work together to load packages.\n\nComparative Safety Data\n\nWorldwide RIR: 30% improvement from 2019 to 2023", "original_types": ["text", "header"], "id": 727}
{"type": "table", "content": "U.S. RIR Comparison Data for General Warehousing and Storage\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 84, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "U.S. RIR Comparison Data for General Warehousing and Storage\nMarkdown representation of the table", "id": 728}
{"type": "table", "content": "U.S. LTIR Comparison Data for Courier and Express Delivery Services\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 84, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "U.S. LTIR Comparison Data for Courier and Express Delivery Services\nMarkdown representation of the table", "id": 729}
{"type": "figure", "content": "Investing in Safety Improvements", "doc_id": "amazon2023", "page": 84, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Investing in Safety Improvements", "id": 730}
{"type": "section", "content": "Our progress is the result of investments of more than $1 billion in safety initiatives, technologies, and programs since 2019. We have also continued to invest in robotics that help make our operations safer by reducing employees' workloads. One example is Proteus, our first autonomous mobile robot, which helps move heavy objects and carts. Another is Cardinal, a robotic arm that can quickly select a single package, read its label, and sort it. Both innovations help reduce repetitive tasks that can cause injuries such as MSDs and create a safer, more comfortable work environment for our employees year-round. We also plan for factors that can impact our employees' safety such as extreme weather. Our policies and procedures for addressing extreme heat and cold are robust and often exceed industry standards and guidelines.", "doc_id": "amazon2023", "page": 84, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our progress is the result of investments of more than $1 billion in safety initiatives, technologies, and programs since 2019. We have also continued to invest in robotics that help make our operations safer by reducing employees' workloads. One example is Proteus, our first autonomous mobile robot, which helps move heavy objects and carts. Another is Cardinal, a robotic arm that can quickly select a single package, read its label, and sort it. Both innovations help reduce repetitive tasks that can cause injuries such as MSDs and create a safer, more comfortable work environment for our employees year-round. We also plan for factors that can impact our employees' safety such as extreme weather. Our policies and procedures for addressing extreme heat and cold are robust and often exceed industry standards and guidelines.", "original_types": ["text"], "id": 731}
{"type": "section", "content": "Delivery Network", "doc_id": "amazon2023", "page": 85, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Delivery Network", "original_types": ["header"], "id": 732}
{"type": "section", "content": "Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed", "doc_id": "amazon2023", "page": 85, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed. Based on initial success, we have expanded the test to deploy the devices to the drivers and the animals when needed", "id": 733}
{"type": "section", "content": "Inclusive Experiences\n\nAmazon creates inclusive technology and experiences that connect our diverse world. To guide our work, we set three companywide priorities. The first is to accelerate inclusive experiences globally, delivering initiatives for employees, customers, and communities around the world. The second is to build equity and inclusion into our talent strategies, with a greater focus on professional development, promotion, and retention. And the third is to advance diversity, equity, and inclusion (DEI) through technology.", "doc_id": "amazon2023", "page": 86, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Inclusive Experiences\n\nAmazon creates inclusive technology and experiences that connect our diverse world. To guide our work, we set three companywide priorities. The first is to accelerate inclusive experiences globally, delivering initiatives for employees, customers, and communities around the world. The second is to build equity and inclusion into our talent strategies, with a greater focus on professional development, promotion, and retention. And the third is to advance diversity, equity, and inclusion (DEI) through technology.", "original_types": ["text", "header"], "id": 734}
{"type": "section", "content": "Our Approach\n\nWe believe the best way to enable enduring inclusive experiences is through technology. We are evolving our approach to diversity, equity, and inclusion (DEI) work by shifting to a more scalable, long-term view through three new mental models. This new strategy enables us to accelerate progress on our priorities and create inclusive experiences at scale. The mental models that guide our work are:\n\n- Bolted-on DEI programs, which drive inclusive and equitable outcomes within our business operations. An example of a bolted-on program is diversity training, which teaches our employees to take more intentionally inclusive actions at work.\n\n- Built-in solutions, which are embedded into existing products to drive equitable experience and outcomes. For example, the captions and dialogue boosts in Prime Video are built-in solutions, which create a more accessible experience for customers who are hard of hearing.\n\n- Born-inclusive solutions, which we define as building equity and inclusion into the architecture of a product from the start. Examples of this include aspects of product design, such as accessibility.\n\nOur Progress\n\nAs we deliver on our goals and make progress on our priority commitments, we continue reaching across cultures to connect our diverse world. We use our size, speed, and innovation to unite us in new and exciting ways, incorporating inclusion into everything we do.", "doc_id": "amazon2023", "page": 87, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Our Approach\n\nWe believe the best way to enable enduring inclusive experiences is through technology. We are evolving our approach to diversity, equity, and inclusion (DEI) work by shifting to a more scalable, long-term view through three new mental models. This new strategy enables us to accelerate progress on our priorities and create inclusive experiences at scale. The mental models that guide our work are:\n\n- Bolted-on DEI programs, which drive inclusive and equitable outcomes within our business operations. An example of a bolted-on program is diversity training, which teaches our employees to take more intentionally inclusive actions at work.\n\n- Built-in solutions, which are embedded into existing products to drive equitable experience and outcomes. For example, the captions and dialogue boosts in Prime Video are built-in solutions, which create a more accessible experience for customers who are hard of hearing.\n\n- Born-inclusive solutions, which we define as building equity and inclusion into the architecture of a product from the start. Examples of this include aspects of product design, such as accessibility.\n\nOur Progress\n\nAs we deliver on our goals and make progress on our priority commitments, we continue reaching across cultures to connect our diverse world. We use our size, speed, and innovation to unite us in new and exciting ways, incorporating inclusion into everything we do.", "original_types": ["text", "header"], "id": 735}
{"type": "section", "content": "Refugees and Humanitarian-Based Immigrants\n\nAs part of our efforts to be Earth’s best employer, we welcome refugees and immigrants into our workforce and support their transition to their new communities. In 2022, we announced a goal to hire at least 5,000 refugees in the U.S. by the end of 2024—and we achieved this in 2023.\n\nTo support this hiring commitment, in 2023, we expanded our Welcome Door program, which offers resources and support for refugee and humanitarian-based immigrant employees in the U.S., to Germany, Poland, and Australia. Benefits vary by country (based on need) but may include financial reimbursement for immigration-related processes, free legal resources to help navigate immigration-related questions, the option to connect with immigration experts, access to upskilling opportunities, and customized mentorship.\n\nThrough the AWS program ITSkills4U", "doc_id": "amazon2023", "page": 88, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Refugees and Humanitarian-Based Immigrants\n\nAs part of our efforts to be Earth’s best employer, we welcome refugees and immigrants into our workforce and support their transition to their new communities. In 2022, we announced a goal to hire at least 5,000 refugees in the U.S. by the end of 2024—and we achieved this in 2023.\n\nTo support this hiring commitment, in 2023, we expanded our Welcome Door program, which offers resources and support for refugee and humanitarian-based immigrant employees in the U.S., to Germany, Poland, and Australia. Benefits vary by country (based on need) but may include financial reimbursement for immigration-related processes, free legal resources to help navigate immigration-related questions, the option to connect with immigration experts, access to upskilling opportunities, and customized mentorship.\n\nThrough the AWS program ITSkills4U", "original_types": ["text", "header"], "id": 736}
{"type": "section", "content": "Amazon Representation by the Numbers*†\n\nIn late 2021, we began allowing employees to identify as multiple specific races or ethnicities and updated our race/ethnicity reporting to begin counting multiracial employees under each separate group with which they identify. As such, category data may not add up to 100% exactly. We also count employees as they currently identify, both for race and ethnicity and for gender. Therefore, when an employee updates their identification, Amazon counts that employee according to their new identification at all times in their career at Amazon, which may change historic data and reporting. Data in this report reflects employee identification as of December 31, 2023.\n\nIn 2023, Amazon updated reporting to include directly employed seasonal and temporary Field and Customer Support workers as well as all employees in the Audible, Twitch, and MGM subsidiaries. Additionally, reporting has been updated to reflect the distinct Native Hawaiian or Other Pacific Islander (NHOPI) category. Moving forward this group is reported separately from Native American/Alaskan.", "doc_id": "amazon2023", "page": 89, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon Representation by the Numbers*†\n\nIn late 2021, we began allowing employees to identify as multiple specific races or ethnicities and updated our race/ethnicity reporting to begin counting multiracial employees under each separate group with which they identify. As such, category data may not add up to 100% exactly. We also count employees as they currently identify, both for race and ethnicity and for gender. Therefore, when an employee updates their identification, Amazon counts that employee according to their new identification at all times in their career at Amazon, which may change historic data and reporting. Data in this report reflects employee identification as of December 31, 2023.\n\nIn 2023, Amazon updated reporting to include directly employed seasonal and temporary Field and Customer Support workers as well as all employees in the Audible, Twitch, and MGM subsidiaries. Additionally, reporting has been updated to reflect the distinct Native Hawaiian or Other Pacific Islander (NHOPI) category. Moving forward this group is reported separately from Native American/Alaskan.", "original_types": ["text", "header"], "id": 737}
{"type": "table", "content": "Amazon Workforce (All Levels)\nTable 1: Amazon Workforce (All Levels)\n\n| Year | Men | Women | Other Gender | NHOP+ (2023) | Latino/e+ | Black+ | Asian+ | White+ | Other | Native American and NHOP+ (2021 and 2022) | Multiracial | Native American and Alaskan+ (2023) |\n|------|----|------|-------------|-------------|----------|-------|-------|-------|------|------------------------------------------|----------|------------------------------------------|\n| 2021 | 55.6% | 44.3% | 0.1% | 28.6% | 23.6% | 13.5% | 0.3% | 1.7% | 2.6% | 0.2% | 1.8% | 1.8% |\n| 2022 | 56.8% | 43.1% | 0.1% | 26.1% | 23.5% | 16.6% | 0.6% | 1.8% | 1.8% | 0.2% | 0.9% | 1.1% |\n| 2023 | 55.7% | 44.1% | 0.2% | 25.8% | 29.1% | 14.4% | 0.9% | 1.1% | 1.5% | 0.8% | 0.4% | 2.3% |\n", "doc_id": "amazon2023", "page": 89, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon Workforce (All Levels)\nTable 1: Amazon Workforce (All Levels)\n\n| Year | Men | Women | Other Gender | NHOP+ (2023) | Latino/e+ | Black+ | Asian+ | White+ | Other | Native American and NHOP+ (2021 and 2022) | Multiracial | Native American and Alaskan+ (2023) |\n|------|----|------|-------------|-------------|----------|-------|-------|-------|------|------------------------------------------|----------|------------------------------------------|\n| 2021 | 55.6% | 44.3% | 0.1% | 28.6% | 23.6% | 13.5% | 0.3% | 1.7% | 2.6% | 0.2% | 1.8% | 1.8% |\n| 2022 | 56.8% | 43.1% | 0.1% | 26.1% | 23.5% | 16.6% | 0.6% | 1.8% | 1.8% | 0.2% | 0.9% | 1.1% |\n| 2023 | 55.7% | 44.1% | 0.2% | 25.8% | 29.1% | 14.4% | 0.9% | 1.1% | 1.5% | 0.8% | 0.4% | 2.3% |\n", "id": 738}
{"type": "table", "content": "People Managers\nTable 2: People Managers\n\n| Year | Men | Women | Other Gender | NHOP+ (2023) | Latino/e+ | Black+ | Asian+ | White+ | Other | Native American and NHOP+ (2021 and 2022) | Multiracial | Native American and Alaskan+ (2023) |\n|------|----|------|-------------|-------------|----------|-------|-------|-------|------|------------------------------------------|----------|------------------------------------------|\n| 2021 | 68.8% | 31.1% | 0.1% | 30.8% | 12.1% | 11.1% | 1.0% | 3.0% | 20.0% | 0.1% | 1.0% | 3.0% |\n| 2022 | 68.3% | 31.7% | 0.0% | 22.8% | 11.9% | 11.4% | 1.0% | 2.6% | 0.1% | 0.3% | 1.0% | 2.6% |\n| 2023 | 68.3% | 31.6% | 0.1% | 23.0% | 12.3% | 12.3% | 0.4% | 2.3% | 0.7% | 0.4% | 0.4% | 2.3% |\n", "doc_id": "amazon2023", "page": 89, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "People Managers\nTable 2: People Managers\n\n| Year | Men | Women | Other Gender | NHOP+ (2023) | Latino/e+ | Black+ | Asian+ | White+ | Other | Native American and NHOP+ (2021 and 2022) | Multiracial | Native American and Alaskan+ (2023) |\n|------|----|------|-------------|-------------|----------|-------|-------|-------|------|------------------------------------------|----------|------------------------------------------|\n| 2021 | 68.8% | 31.1% | 0.1% | 30.8% | 12.1% | 11.1% | 1.0% | 3.0% | 20.0% | 0.1% | 1.0% | 3.0% |\n| 2022 | 68.3% | 31.7% | 0.0% | 22.8% | 11.9% | 11.4% | 1.0% | 2.6% | 0.1% | 0.3% | 1.0% | 2.6% |\n| 2023 | 68.3% | 31.6% | 0.1% | 23.0% | 12.3% | 12.3% | 0.4% | 2.3% | 0.7% | 0.4% | 0.4% | 2.3% |\n", "id": 739}
{"type": "table", "content": "Board Diversity\nTable 3: Board Diversity\n\n| Gender | Race/Ethnicity |\n|--------|----------------|\n| 8      | 1              |\n| 4      | 2              |\n| 1      | 9              |\n", "doc_id": "amazon2023", "page": 89, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Board Diversity\nTable 3: Board Diversity\n\n| Gender | Race/Ethnicity |\n|--------|----------------|\n| 8      | 1              |\n| 4      | 2              |\n| 1      | 9              |\n", "id": 740}
{"type": "figure", "content": "Figure 1: Amazon Representation by the Numbers", "doc_id": "amazon2023", "page": 89, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Amazon Representation by the Numbers", "id": 741}
{"type": "section", "content": "Amazon Representation by the Numbers\n\nThis section provides a detailed breakdown of Amazon's representation by the numbers, including gender and U.S. race/ethnicity across different employee levels.", "doc_id": "amazon2023", "page": 90, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Amazon Representation by the Numbers\n\nThis section provides a detailed breakdown of Amazon's representation by the numbers, including gender and U.S. race/ethnicity across different employee levels.", "original_types": ["text", "header"], "id": 742}
{"type": "table", "content": "Field and Customer Support Employees (L1–L3)\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 90, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Field and Customer Support Employees (L1–L3)\nMarkdown representation of the table", "id": 743}
{"type": "table", "content": "Corporate Employees (L4–L7)\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 90, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Corporate Employees (L4–L7)\nMarkdown representation of the table", "id": 744}
{"type": "table", "content": "Executives (L8+)\nMarkdown representation of the table", "doc_id": "amazon2023", "page": 90, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Executives (L8+)\nMarkdown representation of the table", "id": 745}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "amazon2023", "page": 90, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Figure 1: Title...", "id": 746}
{"type": "section", "content": "Awards and Recognitions\n\nIn the U.S., we signed our first memorandum of understanding (MOU) with a tribal nation in 2023, formalizing our collaboration with the Confederated Tribes of the Umatilla Indian Reservation (CTUIR). Under the MOU, Amazon will consult with the CTUIR, review their input, and implement risk mitigation processes for projects that should consider natural resources, areas of traditional or current use, and cultural or historic sites.\n\nA variety of organizations recognized Amazon as a great place to work in 2023. Some of the recognitions we received as a top employer include:\n\n• Human Rights Campaign’s Corporate Equality Index: 100/100 for the sixth year in a row\n\n• Disability Equality Index: 100/100 in 2022 and 2023 and a Best Place to Work for Disability Inclusion for the sixth year in a row\n\n• LinkedIn Top Companies U.S. Edition: No. 1 for the third year in a row\n\n• Business Disability Forum’s Disability Smart Technology Award\n\n• The Bell Seal for Workplace Mental Health: Platinum\n\n• Purple Certification from the Purple Method: Received top score of 100 on our corporate approach to preventing workplace harassment\n\nAccelerating Equitable Health Research\n\nMany communities are currently underrepresented in medical research. As a result, less is known about their health and ways to provide them with the best care. In September 2023, we began a partnership with the National Institutes of Health (NIH) to raise awareness of their All of Us Research Program, an effort to collect and study data from people living in the U.S. to build one of the most diverse health databases in history. This initiative focuses on accelerating precision medicine research, increasing health equity, and providing DNA results to participants. In support of this work, All of Us works closely with organizations with deep ties to communities that have been historically underrepresented in biomedical research.\n\nWe were NIH’s first corporate health equity partner. In 2023, more than 1,300 Amazon employees have either visited the All of Us website or interacted directly with All of Us staff to learn more about the program, and more than 200 Amazon employees have participated in this landmark study. Of the participants who joined a kickoff event at our second corporate headquarters in Arlington, Virginia, 79% are from communities that have been historically underrepresented in biomedical research.", "doc_id": "amazon2023", "page": 91, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Awards and Recognitions\n\nIn the U.S., we signed our first memorandum of understanding (MOU) with a tribal nation in 2023, formalizing our collaboration with the Confederated Tribes of the Umatilla Indian Reservation (CTUIR). Under the MOU, Amazon will consult with the CTUIR, review their input, and implement risk mitigation processes for projects that should consider natural resources, areas of traditional or current use, and cultural or historic sites.\n\nA variety of organizations recognized Amazon as a great place to work in 2023. Some of the recognitions we received as a top employer include:\n\n• Human Rights Campaign’s Corporate Equality Index: 100/100 for the sixth year in a row\n\n• Disability Equality Index: 100/100 in 2022 and 2023 and a Best Place to Work for Disability Inclusion for the sixth year in a row\n\n• LinkedIn Top Companies U.S. Edition: No. 1 for the third year in a row\n\n• Business Disability Forum’s Disability Smart Technology Award\n\n• The Bell Seal for Workplace Mental Health: Platinum\n\n• Purple Certification from the Purple Method: Received top score of 100 on our corporate approach to preventing workplace harassment\n\nAccelerating Equitable Health Research\n\nMany communities are currently underrepresented in medical research. As a result, less is known about their health and ways to provide them with the best care. In September 2023, we began a partnership with the National Institutes of Health (NIH) to raise awareness of their All of Us Research Program, an effort to collect and study data from people living in the U.S. to build one of the most diverse health databases in history. This initiative focuses on accelerating precision medicine research, increasing health equity, and providing DNA results to participants. In support of this work, All of Us works closely with organizations with deep ties to communities that have been historically underrepresented in biomedical research.\n\nWe were NIH’s first corporate health equity partner. In 2023, more than 1,300 Amazon employees have either visited the All of Us website or interacted directly with All of Us staff to learn more about the program, and more than 200 Amazon employees have participated in this landmark study. Of the participants who joined a kickoff event at our second corporate headquarters in Arlington, Virginia, 79% are from communities that have been historically underrepresented in biomedical research.", "original_types": ["text", "header"], "id": 747}
{"type": "section", "content": "Looking Forward\n\nThe opportunity ahead of us to incorporate inclusion into everything we do at Amazon is greater than ever. The industry is at a unique inflection point where technology is rapidly progressing and the landscape for how we work is changing. In order to accelerate our DEI priorities, we are implementing born-inclusive standards across everything we do—equity and inclusion are being built into the architecture of our products, talent strategies, and initiatives for employees, customers, and communities around the globe. Amazon has changed the world through our innovations, and we’re committed to delivering inclusive experiences through technology in the same way: by disrupting the status quo and uniting us in new and exciting ways.", "doc_id": "amazon2023", "page": 92, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Looking Forward\n\nThe opportunity ahead of us to incorporate inclusion into everything we do at Amazon is greater than ever. The industry is at a unique inflection point where technology is rapidly progressing and the landscape for how we work is changing. In order to accelerate our DEI priorities, we are implementing born-inclusive standards across everything we do—equity and inclusion are being built into the architecture of our products, talent strategies, and initiatives for employees, customers, and communities around the globe. Amazon has changed the world through our innovations, and we’re committed to delivering inclusive experiences through technology in the same way: by disrupting the status quo and uniting us in new and exciting ways.", "original_types": ["text", "header"], "id": 748}
{"type": "section", "content": "Appendix\n\nIn This Section\n\n94 Sustainability Reporting Topic Assessment\n\n95 Endnotes\n\n96 Assurance Statements\n\n97 Disclaimer and Forward-Looking Statements\n\nSustainability Reporting Topic Assessment\n\nAmazon’s business spans many industries, including but not limited to e-commerce, cloud computing, consumer goods, food and beverage, and logistics. This broad scope means we identify environmental, social, and governance topics and focus our efforts by assessing our business holistically. In 2022, Amazon conducted a comprehensive analysis to inform the sustainability topics that are relevant to our reporting.", "doc_id": "amazon2023", "page": "93-94", "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Appendix\n\nIn This Section\n\n94 Sustainability Reporting Topic Assessment\n\n95 Endnotes\n\n96 Assurance Statements\n\n97 Disclaimer and Forward-Looking Statements\n\nSustainability Reporting Topic Assessment\n\nAmazon’s business spans many industries, including but not limited to e-commerce, cloud computing, consumer goods, food and beverage, and logistics. This broad scope means we identify environmental, social, and governance topics and focus our efforts by assessing our business holistically. In 2022, Amazon conducted a comprehensive analysis to inform the sustainability topics that are relevant to our reporting.", "original_types": ["text", "header"], "id": 749}
{"type": "section", "content": "Environment\n\nSupply chains focus on sourcing materials and delivering goods to customers. Value chains include upstream supply chain, as well as downstream delivery to customers, customer use of products, and end-of-life of products.\n\nAmazon's carbon footprint is reported in metric tons of carbon dioxide equivalent (MTCO2e), where each metric ton of CO2 emissions represents the same global warming potential as one metric ton of another greenhouse gas. Learn more in our Carbon Methodology.\n\nCarbon intensity at Amazon is measured as grams of CO2e per dollar of gross merchandise sales (g CO2e/$GMS).\n\nIn 2023, we included new programs in the scope for this metric. With inclusion of the new programs, the amount of packaging we avoided has increased. We previously reported saving more than 2 million metric tons from 2015 through 2022, but with the inclusion of the additional programs, actual savings was more than 3 million metric tons in 2022 and more than 4 million in 2023.\n\nBOTTLE: Bio-Optimized Technologies to keep Thermoplastics out of Landfills and the Environment.\n\nCircularity Gap Report 2024.\n\nGoal scope covers food that is considered inventory. It is measured with a food waste intensity metric that calculates the amount of food waste generated as a percentage of total food handled within Amazon.\n\nUL's Zero Waste to Landfill methodology defines Silver level sites as those diverting 90%-94% and Gold level sites as those diverting 95%-99% of waste.\n\n33 A diverse supplier is a business at least 51% owned and operated by an individual or group that is part of a traditionally underrepresented or underserved group. Certified means the supplier holds a valid certificate from one of five major U.S. supplier diversity agencies: National Minority Supplier Development Council or regional affiliate, Women's Business Enterprise National Council or regional affiliate, National LGBT Chamber of Commerce, National Veteran Business Development Council, or Disability:IN. Certified Tier 1 diverse suppliers are companies that Amazon pays directly for goods and services and are certified by an Amazon-recognized agency.\n\n34 The total spend comprises the direct, indirect, and induced spend. Direct spend denotes spend at small and diverse suppliers. Indirect impact denotes spend to the businesses that suppliers (and their suppliers) purchase goods and services from. Induced spend denotes spend generated in communities of suppliers' employees. This measures the purchases through these employees and jobs supported through these purchases.\n\n35 Certified Tier 2 diverse businesses are businesses that provide goods and services to Amazon's Tier 1 suppliers.\n\n36 Amazon utilized a third-party provider to evaluate and report our 2023 economic impact, which included estimates of our engagement with diverse-owned businesses.\n\n37 Affordable multifamily housing data is calculated through March 2024, as this analysis is not done on an annual basis.\n\nValue Chain", "doc_id": "amazon2023", "page": 95, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Environment\n\nSupply chains focus on sourcing materials and delivering goods to customers. Value chains include upstream supply chain, as well as downstream delivery to customers, customer use of products, and end-of-life of products.\n\nAmazon's carbon footprint is reported in metric tons of carbon dioxide equivalent (MTCO2e), where each metric ton of CO2 emissions represents the same global warming potential as one metric ton of another greenhouse gas. Learn more in our Carbon Methodology.\n\nCarbon intensity at Amazon is measured as grams of CO2e per dollar of gross merchandise sales (g CO2e/$GMS).\n\nIn 2023, we included new programs in the scope for this metric. With inclusion of the new programs, the amount of packaging we avoided has increased. We previously reported saving more than 2 million metric tons from 2015 through 2022, but with the inclusion of the additional programs, actual savings was more than 3 million metric tons in 2022 and more than 4 million in 2023.\n\nBOTTLE: Bio-Optimized Technologies to keep Thermoplastics out of Landfills and the Environment.\n\nCircularity Gap Report 2024.\n\nGoal scope covers food that is considered inventory. It is measured with a food waste intensity metric that calculates the amount of food waste generated as a percentage of total food handled within Amazon.\n\nUL's Zero Waste to Landfill methodology defines Silver level sites as those diverting 90%-94% and Gold level sites as those diverting 95%-99% of waste.\n\n33 A diverse supplier is a business at least 51% owned and operated by an individual or group that is part of a traditionally underrepresented or underserved group. Certified means the supplier holds a valid certificate from one of five major U.S. supplier diversity agencies: National Minority Supplier Development Council or regional affiliate, Women's Business Enterprise National Council or regional affiliate, National LGBT Chamber of Commerce, National Veteran Business Development Council, or Disability:IN. Certified Tier 1 diverse suppliers are companies that Amazon pays directly for goods and services and are certified by an Amazon-recognized agency.\n\n34 The total spend comprises the direct, indirect, and induced spend. Direct spend denotes spend at small and diverse suppliers. Indirect impact denotes spend to the businesses that suppliers (and their suppliers) purchase goods and services from. Induced spend denotes spend generated in communities of suppliers' employees. This measures the purchases through these employees and jobs supported through these purchases.\n\n35 Certified Tier 2 diverse businesses are businesses that provide goods and services to Amazon's Tier 1 suppliers.\n\n36 Amazon utilized a third-party provider to evaluate and report our 2023 economic impact, which included estimates of our engagement with diverse-owned businesses.\n\n37 Affordable multifamily housing data is calculated through March 2024, as this analysis is not done on an annual basis.\n\nValue Chain", "original_types": ["text", "header"], "id": 750}
{"type": "section", "content": "The concept of salience uses the lens of risk to people, not to the business, as the starting point, while recognizing that where risks to people's human rights are greatest, there is often strong convergence with risks to the business. UN Guiding Principles Reporting Framework.\n\nAs detailed in our Renewable Energy Methodology, to calculate the percentage of renewable energy powering Amazon's operations, we evaluate both the amount of renewable energy from Amazon's projects and the renewable energy in the grid. This total renewable energy is then compared to Amazon's total energy use.\n\nTwitch is an Amazon subsidiary that provides interactive livestreaming services for content spanning gaming and entertainment. To learn more about the human rights assessment conducted, please refer to our 2022 Sustainability Report.\n\nThe 17 U.S. states in question are Arizona, Arkansas, California, Georgia, Illinois, Indiana, Kentucky, Maryland, Michigan, Mississippi, Missouri, Ohio, Oklahoma, Oregon, Pennsylvania, Texas, and Virginia.\n\nIn 2023, we included new programs in the scope for this metric. With inclusion of the new programs, the amount of packaging we avoided has increased. We previously reported saving more than 2 million metric tons from 2015 through 2022, but with the inclusion of the additional programs, actual savings was more than 3 million metric tons in 2022 and more than 4 million in 2023.\n\nCarbonCure is a commercialized portfolio of carbon removal technologies that consume carbon dioxide (CO2) in concrete during production, permanently sequestering CO2 and enabling the reduction of cement content in mixes without impacting concrete performance.\n\nCarbon credits are permits that are purchased to offset the emissions of a certain amount of CO2 or other GHGs.\n\nRenewable natural gas (RNG) is created by decomposing organic waste materials anaerobically (without oxygen).\n\nElectric vehicles include vans, four-wheel vehicles, three-wheel vehicles, two-wheel e-bikes, and e-mopeds.\n\nCarbon credits are permits that are purchased to offset the emissions of a certain amount of CO2 or other GHGs.\n\nRenewable natural gas (RNG) is created by decomposing organic waste materials anaerobically (without oxygen).\n\nElectric vehicles include vans, four-wheel vehicles, three-wheel vehicles, two-wheel e-bikes, and e-mopeds.\n\nPeople\n\nAll these numbers and other comparisons are based on the rates Amazon has reported to applicable regulators or are otherwise derived from the same tracking systems used for that reporting.\n\nGlobal operations in reference to health and safety rates means fulfillment (Amazon Robotics sortable, traditional non-sort, in-bound cross dock), transportation (sort center, delivery station, and air), and Amazon Robotics operations facilities.\n\nThe 17 U.S. states in question are Arizona, Arkansas, California, Georgia, Illinois, Indiana, Kentucky, Maryland, Michigan, Mississippi, Missouri, Ohio, Oklahoma, Oregon, Pennsylvania, Texas, and Virginia.", "doc_id": "amazon2023", "page": 95, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "The concept of salience uses the lens of risk to people, not to the business, as the starting point, while recognizing that where risks to people's human rights are greatest, there is often strong convergence with risks to the business. UN Guiding Principles Reporting Framework.\n\nAs detailed in our Renewable Energy Methodology, to calculate the percentage of renewable energy powering Amazon's operations, we evaluate both the amount of renewable energy from Amazon's projects and the renewable energy in the grid. This total renewable energy is then compared to Amazon's total energy use.\n\nTwitch is an Amazon subsidiary that provides interactive livestreaming services for content spanning gaming and entertainment. To learn more about the human rights assessment conducted, please refer to our 2022 Sustainability Report.\n\nThe 17 U.S. states in question are Arizona, Arkansas, California, Georgia, Illinois, Indiana, Kentucky, Maryland, Michigan, Mississippi, Missouri, Ohio, Oklahoma, Oregon, Pennsylvania, Texas, and Virginia.\n\nIn 2023, we included new programs in the scope for this metric. With inclusion of the new programs, the amount of packaging we avoided has increased. We previously reported saving more than 2 million metric tons from 2015 through 2022, but with the inclusion of the additional programs, actual savings was more than 3 million metric tons in 2022 and more than 4 million in 2023.\n\nCarbonCure is a commercialized portfolio of carbon removal technologies that consume carbon dioxide (CO2) in concrete during production, permanently sequestering CO2 and enabling the reduction of cement content in mixes without impacting concrete performance.\n\nCarbon credits are permits that are purchased to offset the emissions of a certain amount of CO2 or other GHGs.\n\nRenewable natural gas (RNG) is created by decomposing organic waste materials anaerobically (without oxygen).\n\nElectric vehicles include vans, four-wheel vehicles, three-wheel vehicles, two-wheel e-bikes, and e-mopeds.\n\nCarbon credits are permits that are purchased to offset the emissions of a certain amount of CO2 or other GHGs.\n\nRenewable natural gas (RNG) is created by decomposing organic waste materials anaerobically (without oxygen).\n\nElectric vehicles include vans, four-wheel vehicles, three-wheel vehicles, two-wheel e-bikes, and e-mopeds.\n\nPeople\n\nAll these numbers and other comparisons are based on the rates Amazon has reported to applicable regulators or are otherwise derived from the same tracking systems used for that reporting.\n\nGlobal operations in reference to health and safety rates means fulfillment (Amazon Robotics sortable, traditional non-sort, in-bound cross dock), transportation (sort center, delivery station, and air), and Amazon Robotics operations facilities.\n\nThe 17 U.S. states in question are Arizona, Arkansas, California, Georgia, Illinois, Indiana, Kentucky, Maryland, Michigan, Mississippi, Missouri, Ohio, Oklahoma, Oregon, Pennsylvania, Texas, and Virginia.", "original_types": ["text", "header"], "id": 751}
{"type": "section", "content": "In the General Warehousing and Storage industry, Amazon benchmarks itself against the industry average for employers with >1,000 employees because the average number of employees at sites reporting into that code is more than 1,400.\n\nIn the Courier and Express Delivery Services industry, Amazon benchmarks itself against the industry average for employers with 250-999 employees because the average number of employees at sites reporting into that code is more than 290.\n\n38 A switch is defined as a customer who purchases a product recognized by certifications in the Climate Pledge Friendly program and has purchased only products not recognized by Climate Pledge Friendly within the past two years in the same product category.\n\n39 Ships in Product Packaging was formerly called Ships in Own Container.\n\n31 Diverse-owned businesses are those whose majority owners (51% or more) are ethnic minorities, women, individuals living with disabilities, veterans, or those who identify as LGBTQIA+.\n\n32 Whole Foods Market refers to Whole Foods Market in the U.S., unless stated otherwise.", "doc_id": "amazon2023", "page": 95, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "In the General Warehousing and Storage industry, Amazon benchmarks itself against the industry average for employers with >1,000 employees because the average number of employees at sites reporting into that code is more than 1,400.\n\nIn the Courier and Express Delivery Services industry, Amazon benchmarks itself against the industry average for employers with 250-999 employees because the average number of employees at sites reporting into that code is more than 290.\n\n38 A switch is defined as a customer who purchases a product recognized by certifications in the Climate Pledge Friendly program and has purchased only products not recognized by Climate Pledge Friendly within the past two years in the same product category.\n\n39 Ships in Product Packaging was formerly called Ships in Own Container.\n\n31 Diverse-owned businesses are those whose majority owners (51% or more) are ethnic minorities, women, individuals living with disabilities, veterans, or those who identify as LGBTQIA+.\n\n32 Whole Foods Market refers to Whole Foods Market in the U.S., unless stated otherwise.", "original_types": ["text"], "id": 752}
{"type": "section", "content": "Assurance Statements\n\nAmazon assures carbon and renewable energy data. Please see our 2023 assurance statements at the links below:\n\n• {'type': 'text', 'content': 'Amazon Renewable Energy Assurance'}\n• {'type': 'text', 'content': 'Devices Renewable Energy Assurance'}\n• {'type': 'text', 'content': 'Amazon Scopes 1 and 2 Assurance'}\n• {'type': 'text', 'content': 'Amazon Scope 3 Assurance'}", "doc_id": "amazon2023", "page": 96, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Assurance Statements\n\nAmazon assures carbon and renewable energy data. Please see our 2023 assurance statements at the links below:\n\n• {'type': 'text', 'content': 'Amazon Renewable Energy Assurance'}\n• {'type': 'text', 'content': 'Devices Renewable Energy Assurance'}\n• {'type': 'text', 'content': 'Amazon Scopes 1 and 2 Assurance'}\n• {'type': 'text', 'content': 'Amazon Scope 3 Assurance'}", "original_types": ["text", "header", "list"], "id": 753}
{"type": "section", "content": "Disclaimer and Forward-Looking Statements\n\nThe information and opinions contained in this report are provided as of the date of this report and are subject to change without notice. Amazon does not undertake to update or revise any such statements. This report represents current Amazon policy and intent and is not intended to create legal rights or obligations. This report may contain, or incorporate by reference, public information not separately reviewed, approved, or endorsed by Amazon, and no representation, warranty, or undertaking is made by Amazon as to the accuracy, reasonableness, or completeness of such information. Inclusion of information in this report may be based on a variety of standards, frameworks, and considerations and is not an indication that the subject or information is material to Amazon’s business, strategy, outlook, operating results, or financial condition or material as it relates to Amazon’s impact on other parties or sustainability matters. This report was originally drafted in English and then translated into other languages. The English version is the authoritative version.", "doc_id": "amazon2023", "page": 97, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Disclaimer and Forward-Looking Statements\n\nThe information and opinions contained in this report are provided as of the date of this report and are subject to change without notice. Amazon does not undertake to update or revise any such statements. This report represents current Amazon policy and intent and is not intended to create legal rights or obligations. This report may contain, or incorporate by reference, public information not separately reviewed, approved, or endorsed by Amazon, and no representation, warranty, or undertaking is made by Amazon as to the accuracy, reasonableness, or completeness of such information. Inclusion of information in this report may be based on a variety of standards, frameworks, and considerations and is not an indication that the subject or information is material to Amazon’s business, strategy, outlook, operating results, or financial condition or material as it relates to Amazon’s impact on other parties or sustainability matters. This report was originally drafted in English and then translated into other languages. The English version is the authoritative version.", "original_types": ["text", "header"], "id": 754}
{"type": "section", "content": "Published July 2024\n\nsustainability.aboutamazon.com", "doc_id": "amazon2023", "page": 98, "url": "https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf", "embedded_text": "Published July 2024\n\nsustainability.aboutamazon.com", "original_types": ["header"], "id": 755}
{"type": "section", "content": "Abstract\n\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny. Nonetheless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under the radar. For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond to the global water challenges, AI can, and also must, take social responsibility and lead by example by addressing its own water footprint. In this paper, we provide a principled methodology to estimate the water footprint of AI, and also discuss the unique spatial-temporal diversities of AI’s runtime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to enable truly sustainable AI.", "doc_id": "li2025b", "page": 1, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Abstract\n\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny. Nonetheless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under the radar. For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond to the global water challenges, AI can, and also must, take social responsibility and lead by example by addressing its own water footprint. In this paper, we provide a principled methodology to estimate the water footprint of AI, and also discuss the unique spatial-temporal diversities of AI’s runtime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to enable truly sustainable AI.", "original_types": ["text", "header"], "id": 756}
{"type": "section", "content": "Background\n\n2.1 Water Withdrawal vs. Water Consumption\n\nThere are two related but different concepts — water withdrawal and water consumption, both of which are important for understanding the impacts on water stress and availability [12, 13].\n\n- Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12]. As water is a finite shared resource, water withdrawal indicates the level of competition as well as dependence on water resources among different sectors.\n\n- Water consumption: It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13]. Water consumption reflects the impact on downstream water availability and is crucial for assessing watershed-level scarcity [12].\n\nThese two types of water usage correspond to two different water footprints, i.e., water withdrawal foot-", "doc_id": "li2025b", "page": 2, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Background\n\n2.1 Water Withdrawal vs. Water Consumption\n\nThere are two related but different concepts — water withdrawal and water consumption, both of which are important for understanding the impacts on water stress and availability [12, 13].\n\n- Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12]. As water is a finite shared resource, water withdrawal indicates the level of competition as well as dependence on water resources among different sectors.\n\n- Water consumption: It is defined as “water withdrawal minus water discharge”, and means the amount of water “evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment” [13]. Water consumption reflects the impact on downstream water availability and is crucial for assessing watershed-level scarcity [12].\n\nThese two types of water usage correspond to two different water footprints, i.e., water withdrawal foot-", "original_types": ["text", "header"], "id": 757}
{"type": "section", "content": "2.2 How Does AI Use Water?\n\nAI’s water usage spans three scopes: on-site water for data center cooling (scope 1), off-site water for electricity generation (scope 2), and supply-chain water for server manufacturing (scope 3).\n\n2.2.1 Scope-1 Water Usage\n\nNearly all the server energy is converted into heat, which must then be removed from the data center server room to avoid overheating. This process involves two sequential stages: server-level cooling followed by facility-level cooling.\n\nIn the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods (e.g., direct-to-chip cooling or immersion cooling), which do not evaporate or consume water. In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.\n\nIn the facility-level cooling stage, heat is rejected from the data center facility to the outside environment. While there are various cooling methods, water-intensive cooling towers and water evaporation-assisted air cooling are two common approaches used in many data centers, including those operated by major technology companies [1,4].", "doc_id": "li2025b", "page": 3, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "2.2 How Does AI Use Water?\n\nAI’s water usage spans three scopes: on-site water for data center cooling (scope 1), off-site water for electricity generation (scope 2), and supply-chain water for server manufacturing (scope 3).\n\n2.2.1 Scope-1 Water Usage\n\nNearly all the server energy is converted into heat, which must then be removed from the data center server room to avoid overheating. This process involves two sequential stages: server-level cooling followed by facility-level cooling.\n\nIn the server-level cooling stage, heat is transferred from the servers to the facility or a heat exchanger, typically using either air or liquid cooling methods (e.g., direct-to-chip cooling or immersion cooling), which do not evaporate or consume water. In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.\n\nIn the facility-level cooling stage, heat is rejected from the data center facility to the outside environment. While there are various cooling methods, water-intensive cooling towers and water evaporation-assisted air cooling are two common approaches used in many data centers, including those operated by major technology companies [1,4].", "original_types": ["text", "header"], "id": 758}
{"type": "figure", "content": "Figure 1: An example of data center’s operational water usage: on-site scope-1 water usage for data center cooling (via cooling towers in the example), and off-site scope-2 water usage for electricity generation. The icons for AI models are only for illustration purposes.", "doc_id": "li2025b", "page": 3, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Figure 1: An example of data center’s operational water usage: on-site scope-1 water usage for data center cooling (via cooling towers in the example), and off-site scope-2 water usage for electricity generation. The icons for AI models are only for illustration purposes.", "id": 759}
{"type": "section", "content": "2.2.2 Scope-2 Water Usage\n\nIn many countries, thermoelectric power is among the top sectors in terms of water withdrawal and water consumption [8]. Thus, similarly to scope-2 carbon emissions, data centers are accountable for off-site scope-2 water usage associated with electricity consumption, which forms part of the “true water cost of data centers,” as highlighted by the recent U.S. data center energy report [1].\n\nDifferent power plants use different amounts of water for each kWh generation, depending on the cooling techniques. Typically, water withdrawal due to hydropower generation is excluded, but water consumption due to increased water evaporation rates from hydropower generation is included [1]. For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively. Meta’s self-reported scope-2 water consumption for its global data center fleet was 3.7 L/kWh (i.e., 55,475 megaliters divided by 14,975,435 MWh) in 2023 [18].\n\n2.2.3 Scope-3 Water Usage\n\nAI chip and server manufacturing uses a huge amount of water [21,22]. For example, ultrapure water is needed for wafer fabrication and water is also needed for keeping semiconductor plants cool. Importantly, the discharged water may contain toxic chemicals and/or hazardous wastes. While water recycling at semiconductor plants can effectively reduce water withdrawal, the recycling rate in many cases remains low, e.g., the average recycling rate for wafer plants and semiconductor plants in Singapore are 45% and 23%, respectively [22]. Although largely obscure, scope-3 water usage is likely significant [21]. For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].\n\nIt is important to recognize that, unlike agriculture whose water footprint is mostly green (i.e., water stored in soil and used by plants), the majority of AI’s water footprint is blue water extracted from rivers, lakes, or groundwater, which is directly accessible for human use but often more limited in availability.", "doc_id": "li2025b", "page": 4, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "2.2.2 Scope-2 Water Usage\n\nIn many countries, thermoelectric power is among the top sectors in terms of water withdrawal and water consumption [8]. Thus, similarly to scope-2 carbon emissions, data centers are accountable for off-site scope-2 water usage associated with electricity consumption, which forms part of the “true water cost of data centers,” as highlighted by the recent U.S. data center energy report [1].\n\nDifferent power plants use different amounts of water for each kWh generation, depending on the cooling techniques. Typically, water withdrawal due to hydropower generation is excluded, but water consumption due to increased water evaporation rates from hydropower generation is included [1]. For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively. Meta’s self-reported scope-2 water consumption for its global data center fleet was 3.7 L/kWh (i.e., 55,475 megaliters divided by 14,975,435 MWh) in 2023 [18].\n\n2.2.3 Scope-3 Water Usage\n\nAI chip and server manufacturing uses a huge amount of water [21,22]. For example, ultrapure water is needed for wafer fabrication and water is also needed for keeping semiconductor plants cool. Importantly, the discharged water may contain toxic chemicals and/or hazardous wastes. While water recycling at semiconductor plants can effectively reduce water withdrawal, the recycling rate in many cases remains low, e.g., the average recycling rate for wafer plants and semiconductor plants in Singapore are 45% and 23%, respectively [22]. Although largely obscure, scope-3 water usage is likely significant [21]. For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].\n\nIt is important to recognize that, unlike agriculture whose water footprint is mostly green (i.e., water stored in soil and used by plants), the majority of AI’s water footprint is blue water extracted from rivers, lakes, or groundwater, which is directly accessible for human use but often more limited in availability.", "original_types": ["text", "header"], "id": 760}
{"type": "section", "content": "Table 1: Estimate of GPT-3's operational water consumption footprint. '*' denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.", "doc_id": "li2025b", "page": 5, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Table 1: Estimate of GPT-3's operational water consumption footprint. '*' denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.", "original_types": ["header"], "id": 761}
{"type": "table", "content": "Table 1: Estimate of GPT-3's operational water consumption footprint. '*' denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.\n```markdown\n| Location | PUE | On-site WUE (L/kWh) | Off-site EWIF (L/kWh) | Water for Training (million L) | Water for Each Request (mL) | # of Requests for 500ml Water |\n|----------|-----|---------------------|-----------------------|-----------------------------|-----------------------------|---------------------------------\n| U.S. Average | 1.170 | 0.550 | 3.142 | 0.708 | 4.731 | 5.439 |\n| Arizona | 1.180 | 1.630 | 4.959 | 2.098 | 7.531 | 9.629 |\n| Georgia* | 1.120 | 0.060 | 2.309 | 0.077 | 3.328 | 3.406 |\n| Illinois | 1.350 | 0.740 | 2.233 | 0.952 | 3.880 | 4.833 |\n| Iowa | 1.160 | 0.140 | 3.104 | 0.180 | 4.634 | 4.814 |\n| Texas | 1.280 | 0.250 | 1.287 | 0.322 | 2.120 | 2.442 |\n| Virginia | 1.140 | 0.140 | 2.385 | 0.180 | 3.499 | 3.679 |\n| Washington | 1.150 | 0.950 | 9.501 | 1.223 | 14.063 | 15.285 |\n| Wyoming | 1.110 | 0.130 | 2.574 | 0.167 | 3.677 | 3.845 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.54", "doc_id": "li2025b", "page": 5, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Table 1: Estimate of GPT-3's operational water consumption footprint. '*' denotes data centers under construction as of July 2023, whose PUE and WUE are projected by Microsoft.\n```markdown\n| Location | PUE | On-site WUE (L/kWh) | Off-site EWIF (L/kWh) | Water for Training (million L) | Water for Each Request (mL) | # of Requests for 500ml Water |\n|----------|-----|---------------------|-----------------------|-----------------------------|-----------------------------|---------------------------------\n| U.S. Average | 1.170 | 0.550 | 3.142 | 0.708 | 4.731 | 5.439 |\n| Arizona | 1.180 | 1.630 | 4.959 | 2.098 | 7.531 | 9.629 |\n| Georgia* | 1.120 | 0.060 | 2.309 | 0.077 | 3.328 | 3.406 |\n| Illinois | 1.350 | 0.740 | 2.233 | 0.952 | 3.880 | 4.833 |\n| Iowa | 1.160 | 0.140 | 3.104 | 0.180 | 4.634 | 4.814 |\n| Texas | 1.280 | 0.250 | 1.287 | 0.322 | 2.120 | 2.442 |\n| Virginia | 1.140 | 0.140 | 2.385 | 0.180 | 3.499 | 3.679 |\n| Washington | 1.150 | 0.950 | 9.501 | 1.223 | 14.063 | 15.285 |\n| Wyoming | 1.110 | 0.130 | 2.574 | 0.167 | 3.677 | 3.845 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.542 | 0.013 | 6.548 | 6.561 |\n| India* | 1.430 | 0.000 | 3.445 | 0.000 | 6.340 | 6.340 |\n| Indonesia* | 1.320 | 1.900 | 2.271 | 2.445 | 3.858 | 6.304 |\n| Ireland | 1.190 | 0.020 | 1.476 | 0.026 | 2.261 | 2.287 |\n| Mexico* | 1.120 | 0.056 | 5.300 | 0.072 | 7.639 | 7.711 |\n| Netherlands | 1.140 | 0.060 | 3.445 | 0.077 | 5.054 | 5.131 |\n| Sweden | 1.160 | 0.090 | 6.019 | 0.116 | 8.986 | 9.101 |\n\n| Australia* | 1.120 | 0.012 | 4.259 | 0.015 | 6.138 | 6.154 |\n| Denmark* | 1.160 | 0.010 | 3.180 | 0.013 | 4.747 | 4.760 |\n| Finland* | 1.120 | 0.010 | 4.54", "id": 762}
{"type": "figure", "content": "Figure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33]. The dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water consumption efficiencies are not aligned. (b) A 5-day snapshot of scope-2 carbon emission rate and water consumption intensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission rate and water consumption intensity for each fuel type [8, 20, 33]. The scope-2 carbon and water efficiencies only have a weak Pearson correlation coefficient of 0.06 in Virginia. (c) A 5-day snapshot of energy fuel mixes serving Virginia, starting from April 4, 2022 [20].", "doc_id": "li2025b", "page": 6, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Figure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33]. The dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water consumption efficiencies are not aligned. (b) A 5-day snapshot of scope-2 carbon emission rate and water consumption intensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission rate and water consumption intensity for each fuel type [8, 20, 33]. The scope-2 carbon and water efficiencies only have a weak Pearson correlation coefficient of 0.06 in Virginia. (c) A 5-day snapshot of energy fuel mixes serving Virginia, starting from April 4, 2022 [20].", "id": 763}
{"type": "section", "content": "per-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF are the same as those used for estimating the training water consumption. Our estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-grade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a much smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request when using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For the Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches approximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft’s data centers already have some of the lowest on-site WUE in the industry. If the same model is deployed in a third-party colocation data center, the scope-1 direct water consumption is expected to be several times higher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the 4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like GPT-4 could consume substantially more energy and water than GPT-3 for processing the same request [31, 32]. With continued efforts to reduce AI’s computational demand and improve the overall water efficiency, the water consumption per request may decrease in the future. However, the total water consumption is likely to continue rising due to the growing demand for AI services and the increasing scale of AI applications [1].\n\n4. Our Recommendations\n\nWe provide our recommendations to address AI’s water footprint from the scheduling and policy perspectives, making future AI more environmentally sustainable.", "doc_id": "li2025b", "page": 6, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "per-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF are the same as those used for estimating the training water consumption. Our estimate of inference water consumption for GPT-3 is on the conservative side, and the actual water consumption could be several times higher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-grade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a much smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request when using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For the Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches approximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft’s data centers already have some of the lowest on-site WUE in the industry. If the same model is deployed in a third-party colocation data center, the scope-1 direct water consumption is expected to be several times higher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the 4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like GPT-4 could consume substantially more energy and water than GPT-3 for processing the same request [31, 32]. With continued efforts to reduce AI’s computational demand and improve the overall water efficiency, the water consumption per request may decrease in the future. However, the total water consumption is likely to continue rising due to the growing demand for AI services and the increasing scale of AI applications [1].\n\n4. Our Recommendations\n\nWe provide our recommendations to address AI’s water footprint from the scheduling and policy perspectives, making future AI more environmentally sustainable.", "original_types": ["text", "header"], "id": 764}
{"type": "section", "content": "4.2  \"When\" and \"Where\" Matter\n\nJudiciously deciding \"when\" and \"where\" to train a large AI model can significantly affect the water footprint. The water efficiency exhibits a spatial-temporal diversity — on-site water efficiency changes due to variations of outside weather conditions, and off-site water efficiency changes due to variations of the grid’s energy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI training and inference in a water-wise manner to cut the water footprint. For example, we may schedule AI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the real-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient hours and/or in water-efficient data centers, which can reduce AI’s water footprint by enabling demand-side flexibility.\n\n4.3  \"Follow the Sun\" or \"Unfollow the Sun\"\n\nTo cut the carbon footprint, it is preferable to \"follow the sun\" when solar energy is more abundant. Nonetheless, to cut the water footprint, it may be more appealing to \"unfollow the sun\" to avoid high-temperature hours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we see misalignment between the scope-2 water consumption intensity factor and carbon emission rate: minimizing one footprint might increase the other footprint. This observation further corroborates the previous finding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore, to judiciously achieve a balance between \"follow the sun\" for carbon efficiency and \"unfollow the sun\" for water efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that are both carbon-efficient and water-wise.\n\n5  Conclusion\n\nIn this paper, we uncover AI’s water usage as a critical concern for socially responsible and environmentally sustainable AI. We present a principled methodology to estimate AI’s water footprint. Then, using GPT-3 as an example, we show that a large AI model can consume millions of liters of water for training. We also discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally — judiciously deciding \"when\" and \"where\" to run a large AI model can significantly cut the water footprint. In addition, we recommend increased transparency and comprehensive reporting of AI’s water footprint, and highlight the necessity of holistically addressing the water footprint along with the carbon footprint to build truly sustainable AI.\nAI’s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective efforts to combat global water challenges.", "doc_id": "li2025b", "page": 7, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "4.2  \"When\" and \"Where\" Matter\n\nJudiciously deciding \"when\" and \"where\" to train a large AI model can significantly affect the water footprint. The water efficiency exhibits a spatial-temporal diversity — on-site water efficiency changes due to variations of outside weather conditions, and off-site water efficiency changes due to variations of the grid’s energy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI training and inference in a water-wise manner to cut the water footprint. For example, we may schedule AI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the real-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient hours and/or in water-efficient data centers, which can reduce AI’s water footprint by enabling demand-side flexibility.\n\n4.3  \"Follow the Sun\" or \"Unfollow the Sun\"\n\nTo cut the carbon footprint, it is preferable to \"follow the sun\" when solar energy is more abundant. Nonetheless, to cut the water footprint, it may be more appealing to \"unfollow the sun\" to avoid high-temperature hours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we see misalignment between the scope-2 water consumption intensity factor and carbon emission rate: minimizing one footprint might increase the other footprint. This observation further corroborates the previous finding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore, to judiciously achieve a balance between \"follow the sun\" for carbon efficiency and \"unfollow the sun\" for water efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that are both carbon-efficient and water-wise.\n\n5  Conclusion\n\nIn this paper, we uncover AI’s water usage as a critical concern for socially responsible and environmentally sustainable AI. We present a principled methodology to estimate AI’s water footprint. Then, using GPT-3 as an example, we show that a large AI model can consume millions of liters of water for training. We also discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally — judiciously deciding \"when\" and \"where\" to run a large AI model can significantly cut the water footprint. In addition, we recommend increased transparency and comprehensive reporting of AI’s water footprint, and highlight the necessity of holistically addressing the water footprint along with the carbon footprint to build truly sustainable AI.\nAI’s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective efforts to combat global water challenges.", "original_types": ["text", "header"], "id": 765}
{"type": "section", "content": "[2] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. Commun. ACM, 63(12):54–63, nov 2020.\n\n[3] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy, July 2019. Association for Computational Linguistics.\n\n[4] Google. Environmental report. https://sustainability.google/reports/, 2024.\n\n[5] PepsiCo. ESG - water. https://www.pepsico.com/our-impact/esg-topics-a-z/water, 2023.\n\n[6] Microsoft. Environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, 2024.\n\n[7] Alex de Vries. The growing energy footprint of artificial intelligence. Joule, October 2023.\n\n[8] Paul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water use embedded in purchased electricity. World Resources Institute, 2020.\n\n[9] Mohammad A. Islam, Kishwar Ahmed, Hong Xu, Nguyen H. Tran, Gang Quan, and Shaolei Ren. Exploiting spatio-temporal diversity for water saving in geo-distributed data centers. IEEE Transactions on Cloud Computing, 6(3):734–746, 2018.\n\n[10] ISO/IEC JTC for AI (SC42). ISO/IEC TR 20226 sustainability: Harnessing the power of AI. https://etech.iec.ch/issue/2023-06/sustainability-harnessing-the-power-of-ai, 2023.\n\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.\n\n[12] Paul Reig. What’s the difference between water use and water consumption? World Resources Institute Commentary, 2013.\n\n[13] Jordan Macknick, Robin Newmark, Garvin Heath, and KC Hallett. A review of operational water consumption and withdrawal factors for electricity generating technologies. NREL Tech. Report: NREL/TP-6A20-50900, 2011.\n\n[14] Elliot Cohen and Anu Ramaswami. The water withdrawal footprint of energy supply to cities. Journal of Industrial Ecology, 18(1):26–39, 2014.\n\n[15] Md Abu Bakar Siddik, Arman Shehabi, and Landon Marston. The environmental footprint of data centers in the United States. Environmental Research Letters, 16(6):064017, 2021.", "doc_id": "li2025b", "page": 8, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "[2] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. Commun. ACM, 63(12):54–63, nov 2020.\n\n[3] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy, July 2019. Association for Computational Linguistics.\n\n[4] Google. Environmental report. https://sustainability.google/reports/, 2024.\n\n[5] PepsiCo. ESG - water. https://www.pepsico.com/our-impact/esg-topics-a-z/water, 2023.\n\n[6] Microsoft. Environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, 2024.\n\n[7] Alex de Vries. The growing energy footprint of artificial intelligence. Joule, October 2023.\n\n[8] Paul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water use embedded in purchased electricity. World Resources Institute, 2020.\n\n[9] Mohammad A. Islam, Kishwar Ahmed, Hong Xu, Nguyen H. Tran, Gang Quan, and Shaolei Ren. Exploiting spatio-temporal diversity for water saving in geo-distributed data centers. IEEE Transactions on Cloud Computing, 6(3):734–746, 2018.\n\n[10] ISO/IEC JTC for AI (SC42). ISO/IEC TR 20226 sustainability: Harnessing the power of AI. https://etech.iec.ch/issue/2023-06/sustainability-harnessing-the-power-of-ai, 2023.\n\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.\n\n[12] Paul Reig. What’s the difference between water use and water consumption? World Resources Institute Commentary, 2013.\n\n[13] Jordan Macknick, Robin Newmark, Garvin Heath, and KC Hallett. A review of operational water consumption and withdrawal factors for electricity generating technologies. NREL Tech. Report: NREL/TP-6A20-50900, 2011.\n\n[14] Elliot Cohen and Anu Ramaswami. The water withdrawal footprint of energy supply to cities. Journal of Industrial Ecology, 18(1):26–39, 2014.\n\n[15] Md Abu Bakar Siddik, Arman Shehabi, and Landon Marston. The environmental footprint of data centers in the United States. Environmental Research Letters, 16(6):064017, 2021.", "original_types": ["text"], "id": 766}
{"type": "section", "content": "[16] Leila Karimi, Leeann Yacuel, Joseph Degraft-Johnson, Jamie Ashby, Michael Green, Matt Renner, Aryn Bergman, Robert Norwood, and Kerri L. Hickenbottom. Water-energy tradeoffs in data centers: A case study in hot-arid climates. Resources, Conservation and Recycling, 181:106194, 2022.\n\n[17] Meta. Sustainability — water. https://sustainability.fb.com/water/, 2023.\n\n[18] Meta. Sustainability report. https://sustainability.atmeta.com/2024-sustainability-report/, 2024.\n\n[19] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for cooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/, 2024.", "doc_id": "li2025b", "page": 8, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "[16] Leila Karimi, Leeann Yacuel, Joseph Degraft-Johnson, Jamie Ashby, Michael Green, Matt Renner, Aryn Bergman, Robert Norwood, and Kerri L. Hickenbottom. Water-energy tradeoffs in data centers: A case study in hot-arid climates. Resources, Conservation and Recycling, 181:106194, 2022.\n\n[17] Meta. Sustainability — water. https://sustainability.fb.com/water/, 2023.\n\n[18] Meta. Sustainability report. https://sustainability.atmeta.com/2024-sustainability-report/, 2024.\n\n[19] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for cooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/, 2024.", "original_types": ["text"], "id": 767}
{"type": "section", "content": "[20] U.S. Energy Information Administration. Open data. https://www.eia.gov/opendata/.\n\n[21] Kali Frost and Inez Hua. Quantifying spatiotemporal impacts of the interaction of water scarcity and water use by the global semiconductor manufacturing industry. Water Resources and Industry, 22:100115, 2019.\n\n[22] Singapore Public Utilities Board. Wafer fabrication and semiconductor plants benchmarks. https://www.pub.gov.sg/Documents/WaterEfficiencyBenchmark_WaferFab.pdf.\n\n[23] Apple. Environmental responsibility report. https://www.apple.com/environment/, 2024.\n\n[24] Kishwar Ahmed, Mohammad A. Islam, Shaolei Ren, and Gang Quan. Exploiting temporal diversity of water efficiency to make data center less “thirsty”. In ICAC, 2014.\n\n[25] Peter Xiang Gao, Andrew R. Curtis, Bernard Wong, and Srinivasan Keshav. It’s not easy being green. SIGCOMM Comput. Commun. Rev., 2012.\n\n[26] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of BLOOM, a 176B parameter language model. J. Mach. Learn. Res., 24(1), mar 2024.\n\n[27] Microsoft. Microsoft in your community. https://local.microsoft.com/.\n\n[28] Microsoft. Microsoft’s sustainability targets. https://datacenters.microsoft.com/sustainability/efficiency/, 2023.\n\n[29] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training, 2021.\n\n[30] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing LLM inference clusters for performance and energy efficiency. In IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2025.\n\n[31] Noah Shumba, Opelo Tshekiso, Pengfei Li, Giulia Fanti, and Shaolei Ren. A water efficiency dataset for african data centers. In NeurIPS Workshop on Tackling Climate Change with Machine Learning, 2024.\n\n[32] Adrien Banse Samuel Rincé and Valentin Defour. Ecologits calculator. https://huggingface.co/spaces/genai-impact/ecologits-calculator, 2024.\n\n[33] U.S. EPA. eGRID data explorer. https://www.epa.gov/egrid/data-explorer.\n\n[34] Equinix. Sustainability report. https://sustainability.equinix.com/wp-content/uploads/2024/07/Equinix-Inc_2023-Sustainability-Report.pdf, 2024.\n\n[35] U.S. Energy Information Administration. U.S. electric power sector continues water efficiency gains. https://www.eia.gov/todayinenergy/detail.php?id=56820, 2022.\n\n[36] U.S. Central Intelligence Agency. The world fact book — total water withdrawal. https://www.cia.gov/the-world-factbook/field/total-water-withdrawal/, 2020.", "doc_id": "li2025b", "page": 9, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "[20] U.S. Energy Information Administration. Open data. https://www.eia.gov/opendata/.\n\n[21] Kali Frost and Inez Hua. Quantifying spatiotemporal impacts of the interaction of water scarcity and water use by the global semiconductor manufacturing industry. Water Resources and Industry, 22:100115, 2019.\n\n[22] Singapore Public Utilities Board. Wafer fabrication and semiconductor plants benchmarks. https://www.pub.gov.sg/Documents/WaterEfficiencyBenchmark_WaferFab.pdf.\n\n[23] Apple. Environmental responsibility report. https://www.apple.com/environment/, 2024.\n\n[24] Kishwar Ahmed, Mohammad A. Islam, Shaolei Ren, and Gang Quan. Exploiting temporal diversity of water efficiency to make data center less “thirsty”. In ICAC, 2014.\n\n[25] Peter Xiang Gao, Andrew R. Curtis, Bernard Wong, and Srinivasan Keshav. It’s not easy being green. SIGCOMM Comput. Commun. Rev., 2012.\n\n[26] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of BLOOM, a 176B parameter language model. J. Mach. Learn. Res., 24(1), mar 2024.\n\n[27] Microsoft. Microsoft in your community. https://local.microsoft.com/.\n\n[28] Microsoft. Microsoft’s sustainability targets. https://datacenters.microsoft.com/sustainability/efficiency/, 2023.\n\n[29] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training, 2021.\n\n[30] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing LLM inference clusters for performance and energy efficiency. In IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2025.\n\n[31] Noah Shumba, Opelo Tshekiso, Pengfei Li, Giulia Fanti, and Shaolei Ren. A water efficiency dataset for african data centers. In NeurIPS Workshop on Tackling Climate Change with Machine Learning, 2024.\n\n[32] Adrien Banse Samuel Rincé and Valentin Defour. Ecologits calculator. https://huggingface.co/spaces/genai-impact/ecologits-calculator, 2024.\n\n[33] U.S. EPA. eGRID data explorer. https://www.epa.gov/egrid/data-explorer.\n\n[34] Equinix. Sustainability report. https://sustainability.equinix.com/wp-content/uploads/2024/07/Equinix-Inc_2023-Sustainability-Report.pdf, 2024.\n\n[35] U.S. Energy Information Administration. U.S. electric power sector continues water efficiency gains. https://www.eia.gov/todayinenergy/detail.php?id=56820, 2022.\n\n[36] U.S. Central Intelligence Agency. The world fact book — total water withdrawal. https://www.cia.gov/the-world-factbook/field/total-water-withdrawal/, 2020.", "original_types": ["text"], "id": 768}
{"type": "section", "content": "Appendix: Operational Water for Global AI in 2027\n\nA recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7], whereas a more aggressive estimate by the U.S. data center energy report projects that AI servers’ electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1]. Based on the former and more conservative projection, we estimate the potential water usage for global AI in 2027, while noting that our global estimates will be exceeded by the water usage attributed to AI in the U.S. alone in 2028 if the projection in [1] comes to fruition.\n\nScope-1 water usage. The scope-1 water efficiency depends on a variety of factors, including the cooling system designs, climate conditions, and operational settings. To set the global scope-1 water efficiency, we utilize the annualized water efficiencies reported by two leading data center operators, Google and Equinix, in their latest sustainability reports [4,34]. Specifically, for on-site scope-1 water withdrawal, we assume 1.2 L/kWh, which results in a total scope-1 water withdrawal of 0.11 – 0.16 billion cubic meters. Similarly, assuming 1.0 L/kWh for global scope-1 water consumption efficiency, we obtain a total on-site scope-1 water consumption of 0.09 – 0.14 billion cubic meters. Note that Google and Equinix both operate data centers globally, but represent two distinct categories of data centers: hyperscale data centers (Google) and multi-tenant colocation data centers (Equinix). According to the recent U.S. data center energy report [1], these two types of data centers collectively account for the vast majority of data center energy consumption in the U.S., with colocation data centers consuming slightly more energy than hyperscalers.", "doc_id": "li2025b", "page": 10, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Appendix: Operational Water for Global AI in 2027\n\nA recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7], whereas a more aggressive estimate by the U.S. data center energy report projects that AI servers’ electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1]. Based on the former and more conservative projection, we estimate the potential water usage for global AI in 2027, while noting that our global estimates will be exceeded by the water usage attributed to AI in the U.S. alone in 2028 if the projection in [1] comes to fruition.\n\nScope-1 water usage. The scope-1 water efficiency depends on a variety of factors, including the cooling system designs, climate conditions, and operational settings. To set the global scope-1 water efficiency, we utilize the annualized water efficiencies reported by two leading data center operators, Google and Equinix, in their latest sustainability reports [4,34]. Specifically, for on-site scope-1 water withdrawal, we assume 1.2 L/kWh, which results in a total scope-1 water withdrawal of 0.11 – 0.16 billion cubic meters. Similarly, assuming 1.0 L/kWh for global scope-1 water consumption efficiency, we obtain a total on-site scope-1 water consumption of 0.09 – 0.14 billion cubic meters. Note that Google and Equinix both operate data centers globally, but represent two distinct categories of data centers: hyperscale data centers (Google) and multi-tenant colocation data centers (Equinix). According to the recent U.S. data center energy report [1], these two types of data centers collectively account for the vast majority of data center energy consumption in the U.S., with colocation data centers consuming slightly more energy than hyperscalers.", "original_types": ["text", "header"], "id": 769}
{"type": "section", "content": "Scope-2 water usage. As noted by the recent U.S. data center energy report [1], scope-2 water usage is part of the true water cost of data centers. The U.S. average electricity water withdrawal and consumption intensity factors are both lower than the global averages [8]. Thus, in our estimate, we use the U.S. average electricity water withdrawal intensity factor 43.83 L/kWh [35], and electricity water consumption intensity factor 3.14 L/kWh [8], respectively. Note that, since [8] includes hydropower in the calculation, it has a higher electricity water withdrawal factor than the U.S. Energy Information Administration’s calculation (i.e., 386.07 L/kWh vs. 43.83 L/kWh for the U.S.). Moreover, our value of 3.14 L/kWh for the U.S. average water consumption factor is lower than 4.35 L/kWh reported by [1], as well as lower than Meta’s global electricity water consumption intensity factor of 3.70 L/kWh in 2024 (i.e., 55,475 megaliters divided by 14,975,435 MWh) [18]. Therefore, our choices of 43.83 L/kWh and 3.14 L/kWh for electricity water withdrawal and consumption intensity factors are both on the conservative side, which can partly absorb potential over-estimates of global AI’s energy demand in 2027 provided by [7].\n\nTo account for the data center non-IT energy overheads, we conservatively assume a power usage effectiveness (PUE) of 1.1, which is a fairly low value even for state-of-the-art data center facilities [4]. Thus, AI’s total electricity consumption becomes 93.5 – 147.4 TWh. Thus, after multiplying 43.83 L/kWh and 3.14 L/kWh by 93.5 – 147.4 TWh, we obtain the total scope-2 water withdrawal of 4.10 – 6.46 billion cubic meters and water consumption of 0.29 – 0.46 billion cubic meters, respectively.\n\nTotal water usage. By adding up scope-1 and scope-2 water usage together, the total water withdrawal and water consumption of global AI may reach 4.2 – 6.6 billion cubic meters and 0.38 – 0.60 billion cubic meters, respectively. According to the U.S. Central Intelligence Agency [36], the estimated U.S. annual water withdrawals in Denmark and the United Kingdom in 2020 (the latest year available as of January, 2025) were 0.98 billion cubic meters and 8.42 billion cubic meters, respectively. Thus, assuming that the 2027 water withdrawals in these two countries remain similar to their 2020 levels, the total water withdrawal attributed to global AI in 2027 is projected to surpass the equivalent of the total annual water withdrawal of 4 – 6 Denmark or approximately half of the United Kingdom. The U.S. Central Intelligence Agency [36] does not provide the country-wide annual water consumption information, and hence we do not contextualize the total water consumption of global AI in 2027.", "doc_id": "li2025b", "page": 10, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "Scope-2 water usage. As noted by the recent U.S. data center energy report [1], scope-2 water usage is part of the true water cost of data centers. The U.S. average electricity water withdrawal and consumption intensity factors are both lower than the global averages [8]. Thus, in our estimate, we use the U.S. average electricity water withdrawal intensity factor 43.83 L/kWh [35], and electricity water consumption intensity factor 3.14 L/kWh [8], respectively. Note that, since [8] includes hydropower in the calculation, it has a higher electricity water withdrawal factor than the U.S. Energy Information Administration’s calculation (i.e., 386.07 L/kWh vs. 43.83 L/kWh for the U.S.). Moreover, our value of 3.14 L/kWh for the U.S. average water consumption factor is lower than 4.35 L/kWh reported by [1], as well as lower than Meta’s global electricity water consumption intensity factor of 3.70 L/kWh in 2024 (i.e., 55,475 megaliters divided by 14,975,435 MWh) [18]. Therefore, our choices of 43.83 L/kWh and 3.14 L/kWh for electricity water withdrawal and consumption intensity factors are both on the conservative side, which can partly absorb potential over-estimates of global AI’s energy demand in 2027 provided by [7].\n\nTo account for the data center non-IT energy overheads, we conservatively assume a power usage effectiveness (PUE) of 1.1, which is a fairly low value even for state-of-the-art data center facilities [4]. Thus, AI’s total electricity consumption becomes 93.5 – 147.4 TWh. Thus, after multiplying 43.83 L/kWh and 3.14 L/kWh by 93.5 – 147.4 TWh, we obtain the total scope-2 water withdrawal of 4.10 – 6.46 billion cubic meters and water consumption of 0.29 – 0.46 billion cubic meters, respectively.\n\nTotal water usage. By adding up scope-1 and scope-2 water usage together, the total water withdrawal and water consumption of global AI may reach 4.2 – 6.6 billion cubic meters and 0.38 – 0.60 billion cubic meters, respectively. According to the U.S. Central Intelligence Agency [36], the estimated U.S. annual water withdrawals in Denmark and the United Kingdom in 2020 (the latest year available as of January, 2025) were 0.98 billion cubic meters and 8.42 billion cubic meters, respectively. Thus, assuming that the 2027 water withdrawals in these two countries remain similar to their 2020 levels, the total water withdrawal attributed to global AI in 2027 is projected to surpass the equivalent of the total annual water withdrawal of 4 – 6 Denmark or approximately half of the United Kingdom. The U.S. Central Intelligence Agency [36] does not provide the country-wide annual water consumption information, and hence we do not contextualize the total water consumption of global AI in 2027.", "original_types": ["text"], "id": 770}
{"type": "section", "content": "The estimates of global AI’s water usage in 2027 are naturally subject to uncertainties, e.g., the future water efficiency may differ from the current value we use. Nonetheless, we emphasize that our estimates are on the conservative side. For example, according to the U.S. data center energy report, the scope-1 water consumption attributed to AI in the U.S. alone could exceed 0.2 billion cubic meters in 2028 [1]. Moreover, based on the reported scope-2 water consumption efficiency, the combined scope-1 and scope-2 water consumption attributed to AI in the U.S. alone is projected to reach up to about 2 billion cubic meters in 2028 [1], which is significantly higher than our estimate of global AI’s total water consumption in 2027.", "doc_id": "li2025b", "page": 10, "url": "https://arxiv.org/pdf/2304.03271", "embedded_text": "The estimates of global AI’s water usage in 2027 are naturally subject to uncertainties, e.g., the future water efficiency may differ from the current value we use. Nonetheless, we emphasize that our estimates are on the conservative side. For example, according to the U.S. data center energy report, the scope-1 water consumption attributed to AI in the U.S. alone could exceed 0.2 billion cubic meters in 2028 [1]. Moreover, based on the reported scope-2 water consumption efficiency, the combined scope-1 and scope-2 water consumption attributed to AI in the U.S. alone is projected to reach up to about 2 billion cubic meters in 2028 [1], which is significantly higher than our estimate of global AI’s total water consumption in 2027.", "original_types": ["text"], "id": 771}
{"type": "section", "content": "Abstract\n\nThe surging demand for AI has led to a rapid expansion of energy-intensive data centers, impacting the environment through escalating carbon emissions and water consumption. While significant attention has been paid to data centers' growing environmental footprint, the public health burden, a hidden toll of data centers, has been largely overlooked. Specifically, data centers' lifecycle, from chip manufacturing to operation, can significantly degrade air quality through emissions of criteria air pollutants such as fine particulate matter, substantially impacting public health. This paper introduces a principled methodology to model lifecycle pollutant emissions for data centers and computing tasks, quantifying the public health impacts. Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California. Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities. Finally, we propose a health-informed computing framework that explicitly incorporates public health risk as a key metric for scheduling data center workloads across space and time, which can effectively mitigate adverse health impacts while advancing environmental sustainability. More broadly, we also recommend adopting a standard reporting protocol for the public health impacts of data centers and paying attention to all impacted communities.", "doc_id": "han2024", "page": 1, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Abstract\n\nThe surging demand for AI has led to a rapid expansion of energy-intensive data centers, impacting the environment through escalating carbon emissions and water consumption. While significant attention has been paid to data centers' growing environmental footprint, the public health burden, a hidden toll of data centers, has been largely overlooked. Specifically, data centers' lifecycle, from chip manufacturing to operation, can significantly degrade air quality through emissions of criteria air pollutants such as fine particulate matter, substantially impacting public health. This paper introduces a principled methodology to model lifecycle pollutant emissions for data centers and computing tasks, quantifying the public health impacts. Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The growing demand for AI is projected to push the total annual public health burden of U.S. data centers up to more than $20 billion in 2028, rivaling that of on-road emissions of California. Further, the public health costs are more felt in disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities. Finally, we propose a health-informed computing framework that explicitly incorporates public health risk as a key metric for scheduling data center workloads across space and time, which can effectively mitigate adverse health impacts while advancing environmental sustainability. More broadly, we also recommend adopting a standard reporting protocol for the public health impacts of data centers and paying attention to all impacted communities.", "original_types": ["text", "header"], "id": 772}
{"type": "section", "content": "deep into lungs and cause serious health effects), sulfur dioxide (SO2), and nitrogen dioxide (NO2). Concretely, the server manufacturing process [18], electricity generation from fossil fuels to power data centers, and the maintenance and usage of diesel backup generators to ensure continuous data center operation all produce significant amounts of criteria air pollutants. Moreover, the distinct spatial-temporal heterogeneities of emission sources suggest that focusing solely on reducing data centers’ carbon footprints may not minimize its emissions of criteria air pollutants or the resulting public health impacts (Section 6.2).\n\nExposure to criteria air pollutants is directly and causally linked to various adverse health outcomes,2 including premature mortality, lung cancer, asthma, heart attacks, cardiovascular diseases, and even cognitive decline, especially for the elderly and vulnerable individuals with pre-existing conditions [20–22]. Moreover, even short-term (hours to days) PM2.5 exposure is harmful and deadly, accounting for approximately 1 million premature deaths per year from 2000 to 2019 and representing 2% of total global deaths [23].\n\nCriteria air pollutants are not confined to the immediate vicinity of their emission sources; they can travel hundreds of miles through a dispersion process (i.e., cross-state air pollution) [24,25], impacting public health across vast regions. Further, PM2.5 is considered “non-threshold,” i.e., there is no absolutely safe exposure level [26]. Thus, compliance with the national/regional air quality standards does not necessarily ensure the air is healthy.\n\nGlobally, 4.2 million deaths were attributed to ambient (i.e., outdoor) air pollution in 2019 [27]. Air pollution has become the second highest risk factor for noncommunicable diseases [28]. Notably, according to the latest Global Burden of Disease report [29], along with high blood pressure and high blood sugar, ambient particulate matter is placed among the leading risk factors for disease burden globally in every socio-demographic group.\n\nImportantly, along with transportation and industrial activities, electricity generation is a key contributor to ambient air pollution with substantial public health impacts [28,30,31]. For example, a recent study [32] shows that, between 1999 and 2020, a total of 460,000 excess deaths were attributed to PM2.5 generated by coal-fired power plants alone in the U.S. As highlighted by the U.S. EPA [30], despite years of progress, power plants “remain a leading source of air, water, and land pollution that affects communities nationwide.” In Europe, the public health cost of air pollution from power plants is valued at approximately 1% of the gross domestic product (GDP), according to the European Environment Agency’s study in 2024 [33].", "doc_id": "han2024", "page": 2, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "deep into lungs and cause serious health effects), sulfur dioxide (SO2), and nitrogen dioxide (NO2). Concretely, the server manufacturing process [18], electricity generation from fossil fuels to power data centers, and the maintenance and usage of diesel backup generators to ensure continuous data center operation all produce significant amounts of criteria air pollutants. Moreover, the distinct spatial-temporal heterogeneities of emission sources suggest that focusing solely on reducing data centers’ carbon footprints may not minimize its emissions of criteria air pollutants or the resulting public health impacts (Section 6.2).\n\nExposure to criteria air pollutants is directly and causally linked to various adverse health outcomes,2 including premature mortality, lung cancer, asthma, heart attacks, cardiovascular diseases, and even cognitive decline, especially for the elderly and vulnerable individuals with pre-existing conditions [20–22]. Moreover, even short-term (hours to days) PM2.5 exposure is harmful and deadly, accounting for approximately 1 million premature deaths per year from 2000 to 2019 and representing 2% of total global deaths [23].\n\nCriteria air pollutants are not confined to the immediate vicinity of their emission sources; they can travel hundreds of miles through a dispersion process (i.e., cross-state air pollution) [24,25], impacting public health across vast regions. Further, PM2.5 is considered “non-threshold,” i.e., there is no absolutely safe exposure level [26]. Thus, compliance with the national/regional air quality standards does not necessarily ensure the air is healthy.\n\nGlobally, 4.2 million deaths were attributed to ambient (i.e., outdoor) air pollution in 2019 [27]. Air pollution has become the second highest risk factor for noncommunicable diseases [28]. Notably, according to the latest Global Burden of Disease report [29], along with high blood pressure and high blood sugar, ambient particulate matter is placed among the leading risk factors for disease burden globally in every socio-demographic group.\n\nImportantly, along with transportation and industrial activities, electricity generation is a key contributor to ambient air pollution with substantial public health impacts [28,30,31]. For example, a recent study [32] shows that, between 1999 and 2020, a total of 460,000 excess deaths were attributed to PM2.5 generated by coal-fired power plants alone in the U.S. As highlighted by the U.S. EPA [30], despite years of progress, power plants “remain a leading source of air, water, and land pollution that affects communities nationwide.” In Europe, the public health cost of air pollution from power plants is valued at approximately 1% of the gross domestic product (GDP), according to the European Environment Agency’s study in 2024 [33].", "original_types": ["text"], "id": 773}
{"type": "section", "content": "The public health outcomes of data centers due to their emission of criteria air pollutants lead to various losses, such as hospitalizations, medication usage, emergency room visits, school loss days, and lost workdays. Nonetheless, despite recent policy efforts [34,35], the tangible and growing public health impacts of data centers have remained under the radar, almost entirely omitted from today’s risk assessments and sustainability reports [10,36,37].\n\nQuantifying and addressing the public health impacts of data centers. In this paper, we uncover and quantify the hidden public health impacts of data centers. We introduce a principled methodology to model the emission of criteria air pollutants associated with a computing task and data center across three distinct scopes: emissions from the maintenance and operation of backup generators (Scope 1), emissions from fossil fuel combustion for electricity generation (Scope 2), and emissions resulting from the manufacturing of server hardware (Scope 3). Then, we analyze the dispersion of criteria air pollutants and the resulting public health impacts.", "doc_id": "han2024", "page": 2, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "The public health outcomes of data centers due to their emission of criteria air pollutants lead to various losses, such as hospitalizations, medication usage, emergency room visits, school loss days, and lost workdays. Nonetheless, despite recent policy efforts [34,35], the tangible and growing public health impacts of data centers have remained under the radar, almost entirely omitted from today’s risk assessments and sustainability reports [10,36,37].\n\nQuantifying and addressing the public health impacts of data centers. In this paper, we uncover and quantify the hidden public health impacts of data centers. We introduce a principled methodology to model the emission of criteria air pollutants associated with a computing task and data center across three distinct scopes: emissions from the maintenance and operation of backup generators (Scope 1), emissions from fossil fuel combustion for electricity generation (Scope 2), and emissions resulting from the manufacturing of server hardware (Scope 3). Then, we analyze the dispersion of criteria air pollutants and the resulting public health impacts.", "original_types": ["text"], "id": 774}
{"type": "section", "content": "Background on Air Pollutants\n\nThis section provides background on criteria air pollutants and U.S. air quality policies. Other countries have similar policies in place to safeguard public health, although their levels of enforcement strictness often differ [33].\n\nCriteria air pollutants, including PM2.5, SO2 and NO2, are a group of airborne contaminants that are emitted from various sources such as industrial activities and vehicle emissions. The direct emission of PM2.5 is called primary PM2.5, while precursor pollutants such as SO2, NOx, and VOCs, can form secondary PM2.5 and/or ozones [45]. These air pollutants can travel a long distance (a.k.a. cross-state air pollution), posing direct and significant risks to public health over large areas, particularly for vulnerable populations including the elderly and individuals with respiratory conditions [24,25].\n\nLong-term exposure to PM2.5, even at a low level, are directly linked to numerous health outcomes, including premature mortality, heart attacks, asthma, stroke, lung cancer, and even cognitive decline [21,22]. These health effects result in various losses, such as hospitalizations, medication usage, emergency room visits, school loss days, and lost workdays, which can be further quantified in economic costs based on public health research for various health endpoints [46]. In addition, short-term (hours to days) PM2.5 exposure is also dangerous, contributing to approximately 1 million premature deaths per year globally from 2000 to 2019 [23].", "doc_id": "han2024", "page": 3, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Background on Air Pollutants\n\nThis section provides background on criteria air pollutants and U.S. air quality policies. Other countries have similar policies in place to safeguard public health, although their levels of enforcement strictness often differ [33].\n\nCriteria air pollutants, including PM2.5, SO2 and NO2, are a group of airborne contaminants that are emitted from various sources such as industrial activities and vehicle emissions. The direct emission of PM2.5 is called primary PM2.5, while precursor pollutants such as SO2, NOx, and VOCs, can form secondary PM2.5 and/or ozones [45]. These air pollutants can travel a long distance (a.k.a. cross-state air pollution), posing direct and significant risks to public health over large areas, particularly for vulnerable populations including the elderly and individuals with respiratory conditions [24,25].\n\nLong-term exposure to PM2.5, even at a low level, are directly linked to numerous health outcomes, including premature mortality, heart attacks, asthma, stroke, lung cancer, and even cognitive decline [21,22]. These health effects result in various losses, such as hospitalizations, medication usage, emergency room visits, school loss days, and lost workdays, which can be further quantified in economic costs based on public health research for various health endpoints [46]. In addition, short-term (hours to days) PM2.5 exposure is also dangerous, contributing to approximately 1 million premature deaths per year globally from 2000 to 2019 [23].", "original_types": ["text", "header"], "id": 775}
{"type": "section", "content": "Figure 1: The overview of data centers' contribution to air pollutants and public health impacts. Scope-1 and scope-2 impacts occur during the operation of data centers (“operational”), whereas scope-3 impacts arise from activities across the supply chain (“embodied”).\n\nUnder the Clean Air Act, the U.S. EPA is authorized to regulate the emission levels of criteria air pollutants, reducing concentrations to comply with the National Ambient Air Quality Standards (NAAQS) [47]. For example, the NAAQS primary standards set the annual average PM2.5 concentration at 9μg/m3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48]. In addition, state and local governments may set additional regulations on criteria air pollutants to strengthen or reinforce national standards [49]. While the U.S. has generally better air quality than many other countries, 4 in 10 people in the U.S. still live with unhealthy levels of air pollution, according to the “State of the Air 2024” report published by the American Lung Association [50]. In 2019 (the latest year of data provided by the World Health Organization, or WHO, as of November 2024), an estimate of 93,886 deaths in the U.S. were attributed to ambient air pollution [51]. In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9 μg/m3, considerably higher than the WHO’s recommended level of 5 μg/m3 [48,52]. Moreover, the EPA projects that 53 U.S. counties, including 23 in the already most populous state of California, would fail to meet the revised national annual PM2.5 standard in 2032 [53]. Although CO2 is broadly classified by the EPA as an air pollutant following the U.S. Supreme Court ruling in 2007 [54] and contributes to long-term climate change, it often does not cause the same immediate health impacts as criteria pollutants [55]. In the U.S., CO2 and other greenhouse gases are subject to different EPA regulations from those for criteria air pollutants. Thus, for the sake of presentation in this paper, we use “air pollutants” to solely refer to criteria air pollutants wherever applicable.\n\nData Centers' Contribution to Air Pollutants\n\nThis section presents an overview of data centers' impact on air quality and contribution to criteria air pollutants throughout its lifecycle across three scopes (Fig. 1). The scoping definition in this paper parallels the well-established greenhouse gas protocol [56]. Specifically, scope-1 and scope-2 air pollutants primarily originate from onsite generators and power plants, collectively referred to as operational emissions, while scope-3 pollutants arise from the supply chain and are referred to as embodied emissions.\n\nScope 1: Onsite Generator", "doc_id": "han2024", "page": 4, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 1: The overview of data centers' contribution to air pollutants and public health impacts. Scope-1 and scope-2 impacts occur during the operation of data centers (“operational”), whereas scope-3 impacts arise from activities across the supply chain (“embodied”).\n\nUnder the Clean Air Act, the U.S. EPA is authorized to regulate the emission levels of criteria air pollutants, reducing concentrations to comply with the National Ambient Air Quality Standards (NAAQS) [47]. For example, the NAAQS primary standards set the annual average PM2.5 concentration at 9μg/m3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48]. In addition, state and local governments may set additional regulations on criteria air pollutants to strengthen or reinforce national standards [49]. While the U.S. has generally better air quality than many other countries, 4 in 10 people in the U.S. still live with unhealthy levels of air pollution, according to the “State of the Air 2024” report published by the American Lung Association [50]. In 2019 (the latest year of data provided by the World Health Organization, or WHO, as of November 2024), an estimate of 93,886 deaths in the U.S. were attributed to ambient air pollution [51]. In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9 μg/m3, considerably higher than the WHO’s recommended level of 5 μg/m3 [48,52]. Moreover, the EPA projects that 53 U.S. counties, including 23 in the already most populous state of California, would fail to meet the revised national annual PM2.5 standard in 2032 [53]. Although CO2 is broadly classified by the EPA as an air pollutant following the U.S. Supreme Court ruling in 2007 [54] and contributes to long-term climate change, it often does not cause the same immediate health impacts as criteria pollutants [55]. In the U.S., CO2 and other greenhouse gases are subject to different EPA regulations from those for criteria air pollutants. Thus, for the sake of presentation in this paper, we use “air pollutants” to solely refer to criteria air pollutants wherever applicable.\n\nData Centers' Contribution to Air Pollutants\n\nThis section presents an overview of data centers' impact on air quality and contribution to criteria air pollutants throughout its lifecycle across three scopes (Fig. 1). The scoping definition in this paper parallels the well-established greenhouse gas protocol [56]. Specifically, scope-1 and scope-2 air pollutants primarily originate from onsite generators and power plants, collectively referred to as operational emissions, while scope-3 pollutants arise from the supply chain and are referred to as embodied emissions.\n\nScope 1: Onsite Generator", "original_types": ["text", "header"], "id": 776}
{"type": "section", "content": "While the construction phase of a data center directly increases air pollutant emissions, its amortized health impact over a typical 15–20 year lifespan is negligible. Therefore, we focus on onsite backup generators as the primary source of scope-1 direct air pollutants. Data centers are mission-critical facilities that are designed to operate with high availability and uptime guarantees. As a result, to maintain operation during emergencies such as grid outages, data centers require highly reliable backup power sources [10,37]. Diesel generators are known to emit significant amounts of air pollutants and even hazardous emissions during operation [57]. For example, they emit 200-600 times more NOx than new or controlled existing natural gas-fired power plants for each unit of electricity produced [58]. Moreover, capacity redundancy is typically followed for diesel generator installations to ensure high availability [59]. Nonetheless, there is limited practical experience with cleaner backup alternatives that", "doc_id": "han2024", "page": 4, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "While the construction phase of a data center directly increases air pollutant emissions, its amortized health impact over a typical 15–20 year lifespan is negligible. Therefore, we focus on onsite backup generators as the primary source of scope-1 direct air pollutants. Data centers are mission-critical facilities that are designed to operate with high availability and uptime guarantees. As a result, to maintain operation during emergencies such as grid outages, data centers require highly reliable backup power sources [10,37]. Diesel generators are known to emit significant amounts of air pollutants and even hazardous emissions during operation [57]. For example, they emit 200-600 times more NOx than new or controlled existing natural gas-fired power plants for each unit of electricity produced [58]. Moreover, capacity redundancy is typically followed for diesel generator installations to ensure high availability [59]. Nonetheless, there is limited practical experience with cleaner backup alternatives that", "original_types": ["text"], "id": 777}
{"type": "figure", "content": "Figure 2: The county-level total scope-1 health cost of data center backup generators operated in Virginia (mostly in Loudoun County, Fairfax County, and Prince William County) [62]. The backup generators are assumed to emit air pollutants at 10% of the permitted levels per year. The total annual public health cost is $220-300 million, including $190-260 million incurred in Virginia, West Virginia, Maryland, Pennsylvania, New York, New Jersey, Delaware, and Washington D.C. (a) County-level health cost in Virginia, West Virginia, Maryland, Pennsylvania, New York, New Jersey, Delaware, and Washington D.C. Counties with data centers are marked in orange, except for Loudoun County (marked in yellow). (b) CDF of the county-level cost. (c) Top-10 counties by the total health cost.", "doc_id": "han2024", "page": 5, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 2: The county-level total scope-1 health cost of data center backup generators operated in Virginia (mostly in Loudoun County, Fairfax County, and Prince William County) [62]. The backup generators are assumed to emit air pollutants at 10% of the permitted levels per year. The total annual public health cost is $220-300 million, including $190-260 million incurred in Virginia, West Virginia, Maryland, Pennsylvania, New York, New Jersey, Delaware, and Washington D.C. (a) County-level health cost in Virginia, West Virginia, Maryland, Pennsylvania, New York, New Jersey, Delaware, and Washington D.C. Counties with data centers are marked in orange, except for Loudoun County (marked in yellow). (b) CDF of the county-level cost. (c) Top-10 counties by the total health cost.", "id": 778}
{"type": "section", "content": "can provide comparable reliability at scale in real-world settings in the near term, as highlighted by the U.S. Department of Energy in its recent recommendations regarding AI data center infrastructures [6]. Consequently, the vast majority of data centers, even including those newly built by major technology companies, depend on onsite diesel generators for backup power [6,60]. For example, in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax), the number of permits for data center diesel generators has increased by more than 70% since 2023 compared to the total number of permits issued between 2000 and 2022 [60]. Importantly, nearly all the diesel generators are Tier 2, which have significantly higher emission rates than Tier 4 units [60,61]. The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. While diesel generators need to comply with air quality regulations and typically do not operate over extended periods of time, regular maintenance and testing are essential to ensure their operational reliability. A recent report by the state of Virginia [42] found that the actual air pollutant emissions from backup generators at Virginia’s data centers reached approximately 7% of the total permitted amounts in 2023, primarily for maintenance. Likewise, the actual emissions took up 3% to 12% of the permitted levels for some data centers in Quincy, Washington [43]. Moreover, the U.S. EPA recently issued a clarification that would allow data centers to run backup generators for up to 50 hours a year (or roughly 10% of the permits that typically allow 500 hours per year) to participate in demand response—a program designed to reduce grid demand during peak hours, which is increasingly activated as surging data center demand strains grid capacity [44]. This trend may necessitate extended reliance on backup generators [6]. What further adds to the public health impact is that many data center generators in a region may operate simultaneously for demand response during grid capacity shortages, potentially resulting in a short-term spike in PM2.5 and NOx emissions that can be particularly harmful [6,23,48]. For example, from June 23 to 25, 2025, some data centers in Loudoun County, Virginia, were instructed to run their on-site diesel generators for demand response, releasing large amounts of air pollutants and “black smoke” [61]. The high emission rate from onsite generators, combined with extended operation for maintenance and demand response beyond grid outages, could pose serious health risks, especially in regions with a concentration of large data centers. To illustrate this point, we consider the data centers’ onsite generators in Virginia. Assuming that the actual emissions are 10% of the permitted level as a reference case that reflects both the historical reports and future demand response projections [42–44], \\footnote{If the actual percentage is \\(x\\%\\), our value will be approximately scaled by \\(\\frac{x}{10}\\) [22].} the backup generators could already cause 14,000 asthma symptom cases and 13-19 deaths each year among other health implications.", "doc_id": "han2024", "page": 5, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "can provide comparable reliability at scale in real-world settings in the near term, as highlighted by the U.S. Department of Energy in its recent recommendations regarding AI data center infrastructures [6]. Consequently, the vast majority of data centers, even including those newly built by major technology companies, depend on onsite diesel generators for backup power [6,60]. For example, in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax), the number of permits for data center diesel generators has increased by more than 70% since 2023 compared to the total number of permits issued between 2000 and 2022 [60]. Importantly, nearly all the diesel generators are Tier 2, which have significantly higher emission rates than Tier 4 units [60,61]. The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. While diesel generators need to comply with air quality regulations and typically do not operate over extended periods of time, regular maintenance and testing are essential to ensure their operational reliability. A recent report by the state of Virginia [42] found that the actual air pollutant emissions from backup generators at Virginia’s data centers reached approximately 7% of the total permitted amounts in 2023, primarily for maintenance. Likewise, the actual emissions took up 3% to 12% of the permitted levels for some data centers in Quincy, Washington [43]. Moreover, the U.S. EPA recently issued a clarification that would allow data centers to run backup generators for up to 50 hours a year (or roughly 10% of the permits that typically allow 500 hours per year) to participate in demand response—a program designed to reduce grid demand during peak hours, which is increasingly activated as surging data center demand strains grid capacity [44]. This trend may necessitate extended reliance on backup generators [6]. What further adds to the public health impact is that many data center generators in a region may operate simultaneously for demand response during grid capacity shortages, potentially resulting in a short-term spike in PM2.5 and NOx emissions that can be particularly harmful [6,23,48]. For example, from June 23 to 25, 2025, some data centers in Loudoun County, Virginia, were instructed to run their on-site diesel generators for demand response, releasing large amounts of air pollutants and “black smoke” [61]. The high emission rate from onsite generators, combined with extended operation for maintenance and demand response beyond grid outages, could pose serious health risks, especially in regions with a concentration of large data centers. To illustrate this point, we consider the data centers’ onsite generators in Virginia. Assuming that the actual emissions are 10% of the permitted level as a reference case that reflects both the historical reports and future demand response projections [42–44], \\footnote{If the actual percentage is \\(x\\%\\), our value will be approximately scaled by \\(\\frac{x}{10}\\) [22].} the backup generators could already cause 14,000 asthma symptom cases and 13-19 deaths each year among other health implications.", "id": 779}
{"type": "section", "content": "Scope 2: Power Plants\n\nJust as data centers are accountable for scope-2 carbon emissions, they also contribute to scope-2 air pollution through their electricity usage.\n\nAlong with transportation and manufacturing, the combustion of fossil fuels for electricity production is a leading anthropogenic source of criteria air pollutants, releasing large amounts of PM2.5, SO2, NOx, VOCs, and others [30].5 More alarmingly, the growing energy demands of AI data centers are already delaying the decommissioning of coal-fired power plants and driving the expansion of fossil-fuel-based plants in the U.S. and globally [6,65,66]. For example, in addition to keeping 2,099 MW coal generation capacity until 2039 (more than 80% of the 2024 level), Virginia Electric and Power Company plans to install 5,934 MW gas-fired plants to meet the growing energy demand driven by AI data centers [66].", "doc_id": "han2024", "page": 6, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Scope 2: Power Plants\n\nJust as data centers are accountable for scope-2 carbon emissions, they also contribute to scope-2 air pollution through their electricity usage.\n\nAlong with transportation and manufacturing, the combustion of fossil fuels for electricity production is a leading anthropogenic source of criteria air pollutants, releasing large amounts of PM2.5, SO2, NOx, VOCs, and others [30].5 More alarmingly, the growing energy demands of AI data centers are already delaying the decommissioning of coal-fired power plants and driving the expansion of fossil-fuel-based plants in the U.S. and globally [6,65,66]. For example, in addition to keeping 2,099 MW coal generation capacity until 2039 (more than 80% of the 2024 level), Virginia Electric and Power Company plans to install 5,934 MW gas-fired plants to meet the growing energy demand driven by AI data centers [66].", "original_types": ["text", "header"], "id": 780}
{"type": "figure", "content": "Figure 3: Public health costs of electricity generation and on-road emissions in the contiguous U.S. in 2023 and 2028 [39]. The error bars represent high and low estimates returned by COBRA using two different exposure-response functions.", "doc_id": "han2024", "page": 6, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 3: Public health costs of electricity generation and on-road emissions in the contiguous U.S. in 2023 and 2028 [39]. The error bars represent high and low estimates returned by COBRA using two different exposure-response functions.", "id": 781}
{"type": "section", "content": "Based on the emission data projected by the U.S. EPA's COBRA modeling tool [39], we show in Fig. 3 that the electric power sector's total public health cost in the contiguous U.S. is on track to rise, rivaling that of on-road vehicle emissions by all the registered vehicles (including tailpipe exhausts and brakes) in 2028.", "doc_id": "han2024", "page": 6, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Based on the emission data projected by the U.S. EPA's COBRA modeling tool [39], we show in Fig. 3 that the electric power sector's total public health cost in the contiguous U.S. is on track to rise, rivaling that of on-road vehicle emissions by all the registered vehicles (including tailpipe exhausts and brakes) in 2028.", "original_types": ["text"], "id": 782}
{"type": "section", "content": "4 Quantifying Task-Specific Public Health Impact\n\nTo quantify the public health impact of a specific computing task, we present a principled end-to-end methodology illustrated in Fig. 1. Specifically, the process includes: (1) Quantifying the task’s criteria air pollutants at the emission source; (2) Modeling the dispersed air pollutants at different receptors (i.e., destination regions); (3) Calculating the public health impact and assigning a corresponding monetary value at each receptor.", "doc_id": "han2024", "page": 7, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "4 Quantifying Task-Specific Public Health Impact\n\nTo quantify the public health impact of a specific computing task, we present a principled end-to-end methodology illustrated in Fig. 1. Specifically, the process includes: (1) Quantifying the task’s criteria air pollutants at the emission source; (2) Modeling the dispersed air pollutants at different receptors (i.e., destination regions); (3) Calculating the public health impact and assigning a corresponding monetary value at each receptor.", "original_types": ["text", "header"], "id": 783}
{"type": "section", "content": "the emission rate and electricity generation of the power plant \\( k \\) or the emission rate of the marginal power plant (i.e., the power plant dispatched in response to the next electricity demand increment), which are referred to as average emission rate or marginal emission rate, respectively. The average emission represents a proportional share of the overall air pollutant emission by an electricity consumer, while the marginal emission is useful for quantifying the additionality of air pollutants due to a consumer’s electricity usage. Suppose that the electricity consumption by the computing task is \\( e \\), including the data center overhead captured by the power usage effectiveness. Then, we write the scope-2 air pollutants as\n\np^s = e \\cdot \\gamma\n\nwhich is either based on either average or marginal accounting. To evaluate the public health impacts of U.S. data centers, we consider the average attribution method unless otherwise noted, which is also the standard methodology of carbon emission accounting used by technology companies in their sustainability reports [10,37,78]. We can also refine the calculation of scope-2 air pollutants in (2) by considering the summation of air pollutants over multiple time slots over the task’s duration. Location-based emission. There are two types of scope-2 carbon emission accounting associated with electricity consumption: location-based and market-based [10]. Specifically, location-based carbon emissions refer to the physical carbon emissions attributed to an electricity consumer connected to the power grid, while market-based carbon emissions are net emissions after applying reductions due to contractual arrangements and other credits (e.g., renewable energy credits). As noted by a recent study on carbon accounting [79], location-based accounting is considered essential, whereas market-based accounting is valuable. Moreover, market-based accounting relies on market instruments whose detailed information is often not publicly disclosed. Thus, in this paper, we follow the literature [8] and focus on location-based accounting for scope-2 criteria air pollutants without considering market-based pollution reduction mechanisms. We also note that market-based emission reduction is likely less effective to mitigate the public health impact. The reason is that, unlike carbon emissions that have a similar effect on climate change regardless of the emission locations, the public health impact of criteria air pollutants heavily depends on the location of the emission source. For example, the public health impact of using pollutant-intensive electricity generated from a populated region may not be effectively mitigated by the clean energy credits generated elsewhere.", "doc_id": "han2024", "page": 8, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "the emission rate and electricity generation of the power plant \\( k \\) or the emission rate of the marginal power plant (i.e., the power plant dispatched in response to the next electricity demand increment), which are referred to as average emission rate or marginal emission rate, respectively. The average emission represents a proportional share of the overall air pollutant emission by an electricity consumer, while the marginal emission is useful for quantifying the additionality of air pollutants due to a consumer’s electricity usage. Suppose that the electricity consumption by the computing task is \\( e \\), including the data center overhead captured by the power usage effectiveness. Then, we write the scope-2 air pollutants as\n\np^s = e \\cdot \\gamma\n\nwhich is either based on either average or marginal accounting. To evaluate the public health impacts of U.S. data centers, we consider the average attribution method unless otherwise noted, which is also the standard methodology of carbon emission accounting used by technology companies in their sustainability reports [10,37,78]. We can also refine the calculation of scope-2 air pollutants in (2) by considering the summation of air pollutants over multiple time slots over the task’s duration. Location-based emission. There are two types of scope-2 carbon emission accounting associated with electricity consumption: location-based and market-based [10]. Specifically, location-based carbon emissions refer to the physical carbon emissions attributed to an electricity consumer connected to the power grid, while market-based carbon emissions are net emissions after applying reductions due to contractual arrangements and other credits (e.g., renewable energy credits). As noted by a recent study on carbon accounting [79], location-based accounting is considered essential, whereas market-based accounting is valuable. Moreover, market-based accounting relies on market instruments whose detailed information is often not publicly disclosed. Thus, in this paper, we follow the literature [8] and focus on location-based accounting for scope-2 criteria air pollutants without considering market-based pollution reduction mechanisms. We also note that market-based emission reduction is likely less effective to mitigate the public health impact. The reason is that, unlike carbon emissions that have a similar effect on climate change regardless of the emission locations, the public health impact of criteria air pollutants heavily depends on the location of the emission source. For example, the public health impact of using pollutant-intensive electricity generated from a populated region may not be effectively mitigated by the clean energy credits generated elsewhere.", "original_types": ["text", "equation"], "id": 784}
{"type": "section", "content": "4.3 Converting Health Outcomes to Economic Costs\n\nBy assessing pollutant levels \\( p_i^r = (p_{i,1}^r, \\cdots, p_{i,M}^r) \\) and population size at each receptor region \\( i \\), we can estimate the incidences of health outcomes \\( h_i = (h_{i,1}, \\cdots, h_{i,H}) \\) and the corresponding public health cost \\( c_i = (c_{i,1}, \\cdots, c_{i,H}) \\). The relations between \\( p_i^r \\) and \\( h_i \\) and between \\( h_i \\) and \\( c_i \\) is captured by an exposure-response function and can be established based on epidemiology research [22]. For example, the premature mortality rate can be modeled as a log-linear function in terms of the PM2.5 level [84].\n\n4.4 End-to-End Modeling\n\nFollowing the end-to-end process shown in Fig. 1, we now briefly describe our modeling methodology to study the public health impact of U.S. data centers and AI training. The details are available in Appendix A. To quantify data centers' scope-1 and scope-2 air pollutant emissions, we use air quality permit data for onsite generators [60] and electricity consumption data, including both historical records and 2028 projections, from the Lawrence Berkeley National Laboratory report [4]. To model the air pollutant dispersion and quantify health impacts, we use the latest COBRA (Desktop v5.1, as of October 2024) provided by the U.S. EPA [39]. COBRA integrates reduced-complexity air dispersion modeling (including both primarily emitted PM2.5 and secondly formed PM2.5 and ozone [83]) with various concentration-response functions [22], offering a quantitative screening analysis particularly suitable for large-scale health impacts. The same or similar reduced-complexity modeling tools have been commonly used in the literature to examine the health impacts of various industries over a large area [82,85], including electric vehicles [86], bitcoin mining [87], and inter-region electricity imports [88], among others. While each health impact model used by COBRA considers 95% confidence intervals, the high-end and low-end estimates provided by COBRA are based on different models instead of the 95% confidence interval of a single model [22].", "doc_id": "han2024", "page": 9, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "4.3 Converting Health Outcomes to Economic Costs\n\nBy assessing pollutant levels \\( p_i^r = (p_{i,1}^r, \\cdots, p_{i,M}^r) \\) and population size at each receptor region \\( i \\), we can estimate the incidences of health outcomes \\( h_i = (h_{i,1}, \\cdots, h_{i,H}) \\) and the corresponding public health cost \\( c_i = (c_{i,1}, \\cdots, c_{i,H}) \\). The relations between \\( p_i^r \\) and \\( h_i \\) and between \\( h_i \\) and \\( c_i \\) is captured by an exposure-response function and can be established based on epidemiology research [22]. For example, the premature mortality rate can be modeled as a log-linear function in terms of the PM2.5 level [84].\n\n4.4 End-to-End Modeling\n\nFollowing the end-to-end process shown in Fig. 1, we now briefly describe our modeling methodology to study the public health impact of U.S. data centers and AI training. The details are available in Appendix A. To quantify data centers' scope-1 and scope-2 air pollutant emissions, we use air quality permit data for onsite generators [60] and electricity consumption data, including both historical records and 2028 projections, from the Lawrence Berkeley National Laboratory report [4]. To model the air pollutant dispersion and quantify health impacts, we use the latest COBRA (Desktop v5.1, as of October 2024) provided by the U.S. EPA [39]. COBRA integrates reduced-complexity air dispersion modeling (including both primarily emitted PM2.5 and secondly formed PM2.5 and ozone [83]) with various concentration-response functions [22], offering a quantitative screening analysis particularly suitable for large-scale health impacts. The same or similar reduced-complexity modeling tools have been commonly used in the literature to examine the health impacts of various industries over a large area [82,85], including electric vehicles [86], bitcoin mining [87], and inter-region electricity imports [88], among others. While each health impact model used by COBRA considers 95% confidence intervals, the high-end and low-end estimates provided by COBRA are based on different models instead of the 95% confidence interval of a single model [22].", "original_types": ["text", "header"], "id": 785}
{"type": "section", "content": "Public Health Impact of U.S. Data Centers\n\nWe now present our public health impact analysis for U.S. data centers in aggregate, beginning with the historical analysis from 2019 to 2023 and highlighting the uneven distribution of health impacts, followed by a 2028 projection. The overall trend is shown in Fig. 4, which demonstrates a significant increase in the public health impact of U.S. data centers from 2023 to 2028. Specifically, the surging demand for AI data centers in the U.S. has outweighed the power plant emission efficiency improvement, potentially tripling the public health cost from 2023 to 2028.", "doc_id": "han2024", "page": 10, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Public Health Impact of U.S. Data Centers\n\nWe now present our public health impact analysis for U.S. data centers in aggregate, beginning with the historical analysis from 2019 to 2023 and highlighting the uneven distribution of health impacts, followed by a 2028 projection. The overall trend is shown in Fig. 4, which demonstrates a significant increase in the public health impact of U.S. data centers from 2023 to 2028. Specifically, the surging demand for AI data centers in the U.S. has outweighed the power plant emission efficiency improvement, potentially tripling the public health cost from 2023 to 2028.", "original_types": ["text", "header"], "id": 786}
{"type": "table", "content": "Table 1: Title...", "doc_id": "han2024", "page": 10, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 1: Title...", "id": 787}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "han2024", "page": 10, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 1: Title...", "id": 788}
{"type": "figure", "content": "Figure 4: The public health costs of U.S. data centers and top-3 state on-road emissions from 2019 to 2023 and the 2028 projection based on the Lawrence Berkeley National Lab's report [4]. The cost for U.S. data centers includes scope-1 and scope-2 impacts. The “High” and “Low” represent the high and low growth rates considered in [4].", "doc_id": "han2024", "page": 11, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 4: The public health costs of U.S. data centers and top-3 state on-road emissions from 2019 to 2023 and the 2028 projection based on the Lawrence Berkeley National Lab's report [4]. The cost for U.S. data centers includes scope-1 and scope-2 impacts. The “High” and “Low” represent the high and low growth rates considered in [4].", "id": 789}
{"type": "table", "content": "Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028", "doc_id": "han2024", "page": 11, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028", "id": 790}
{"type": "table", "content": "Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028", "doc_id": "han2024", "page": 11, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028", "id": 791}
{"type": "figure", "content": "Figure 5: The county-level total health cost of U.S. data centers from 2019 to 2023. (a) Health cost map; (b) CDF of county-level health cost; (c) Top-10 counties by total health cost.", "doc_id": "han2024", "page": 11, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 5: The county-level total health cost of U.S. data centers from 2019 to 2023. (a) Health cost map; (b) CDF of county-level health cost; (c) Top-10 counties by total health cost.", "id": 792}
{"type": "figure", "content": "Figure 6: The county-level per-household health cost of U.S. data centers from 2019 to 2023. (a) Per-household health cost map; (b) CDF of county-level per-household health cost; (c) Top-10 counties by per-household health cost. IR represents “County-to-Nation Per-Household Median Income Ratio.”", "doc_id": "han2024", "page": 12, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 6: The county-level per-household health cost of U.S. data centers from 2019 to 2023. (a) Per-household health cost map; (b) CDF of county-level per-household health cost; (c) Top-10 counties by per-household health cost. IR represents “County-to-Nation Per-Household Median Income Ratio.”", "id": 793}
{"type": "section", "content": "different communities in terms of the public health cost suggests that we need to carefully examine the local and regional health impacts of data centers and improve public health equity to enable truly responsible computing. Furthermore, many of the hardest-hit communities neither host large data centers nor directly benefit economically from AI data centers, such as through tax revenues. For example, several counties in West Virginia are among the most affected, because many coal-fired power plants in West Virginia are supplying electricity to data centers in the neighboring state of Virginia [4,5]. By contrast, despite hosting a large number of data centers and incurring high total health costs due to its large population, California’s clean power grid results in some of the lowest per-household health impacts in the country.", "doc_id": "han2024", "page": 12, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "different communities in terms of the public health cost suggests that we need to carefully examine the local and regional health impacts of data centers and improve public health equity to enable truly responsible computing. Furthermore, many of the hardest-hit communities neither host large data centers nor directly benefit economically from AI data centers, such as through tax revenues. For example, several counties in West Virginia are among the most affected, because many coal-fired power plants in West Virginia are supplying electricity to data centers in the neighboring state of Virginia [4,5]. By contrast, despite hosting a large number of data centers and incurring high total health costs due to its large population, California’s clean power grid results in some of the lowest per-household health impacts in the country.", "original_types": ["text"], "id": 794}
{"type": "section", "content": "5.2 Public Health Impact of Generative AI Training\n\nWe now study the health impact of a specific computing task. Specifically, we consider the training of an LLM and assume that the electricity consumption is the same as training Llama-3.1 recently released by Meta [92]. As the scope-2 impact is dominant and the power allocated to train Llama-3.1 is unknown to determine scope-1 impacts, we focus on scope-2 health costs associated with the electricity consumption. While we use Meta’s Llama-3.1 training electricity consumption and U.S. data center locations as an example, our results should be interpreted as the estimated public health impact of training a general LLM with a comparable scale of Llama-3.1.\n\nWe show the results in Table 2. It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76]. Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.\n\nThe results highlight that the public health impact of AI model training is highly location-dependent. Combined with the spatial flexibility of model training, they suggest that AI model developers should take into account potential health impacts when choosing data center locations for training.\n\n5.3 Location-Dependent Public Health Impacts of Two Technology Companies\n\nWe further highlight locational dependency of public health impacts by considering two major technology companies’ U.S. data center locations in 2023, excluding their leased colocation data centers whose locations are proprietary. We name these two companies A and B, respectively. These two companies do not have same data center locations. While company B discloses its per-location electricity usage [37], company A does not. Thus, we uniformly distribute company B’s North America electricity consumption over its U.S. data center locations based on its latest sustainability report [10]. We consider location-based emission accounting without taking into account renewable energy credits these two companies apply to offset their grid electricity consumption (see “Location-based emission” in Section 4.1.2).", "doc_id": "han2024", "page": 13, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "5.2 Public Health Impact of Generative AI Training\n\nWe now study the health impact of a specific computing task. Specifically, we consider the training of an LLM and assume that the electricity consumption is the same as training Llama-3.1 recently released by Meta [92]. As the scope-2 impact is dominant and the power allocated to train Llama-3.1 is unknown to determine scope-1 impacts, we focus on scope-2 health costs associated with the electricity consumption. While we use Meta’s Llama-3.1 training electricity consumption and U.S. data center locations as an example, our results should be interpreted as the estimated public health impact of training a general LLM with a comparable scale of Llama-3.1.\n\nWe show the results in Table 2. It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76]. Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.\n\nThe results highlight that the public health impact of AI model training is highly location-dependent. Combined with the spatial flexibility of model training, they suggest that AI model developers should take into account potential health impacts when choosing data center locations for training.\n\n5.3 Location-Dependent Public Health Impacts of Two Technology Companies\n\nWe further highlight locational dependency of public health impacts by considering two major technology companies’ U.S. data center locations in 2023, excluding their leased colocation data centers whose locations are proprietary. We name these two companies A and B, respectively. These two companies do not have same data center locations. While company B discloses its per-location electricity usage [37], company A does not. Thus, we uniformly distribute company B’s North America electricity consumption over its U.S. data center locations based on its latest sustainability report [10]. We consider location-based emission accounting without taking into account renewable energy credits these two companies apply to offset their grid electricity consumption (see “Location-based emission” in Section 4.1.2).", "original_types": ["text", "header"], "id": 795}
{"type": "section", "content": "We see from Fig. 7 that the two companies have significant differences in terms of the per-household health cost distribution and most-affected counties. This is primarily due to the two companies’ different data center locations, and highlights the locational dependency of public health impacts. That is, unlike carbon emissions that have a similar effect regardless of the emission source locations, the public health impact of criteria air pollutants heavily depends on the location of the emission source. Thus, technology companies should account for public health impacts when deciding where they build data centers, where they get electricity for their data centers, and where they install onsite renewables in order to best mitigate the adverse health effects.", "doc_id": "han2024", "page": 13, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "We see from Fig. 7 that the two companies have significant differences in terms of the per-household health cost distribution and most-affected counties. This is primarily due to the two companies’ different data center locations, and highlights the locational dependency of public health impacts. That is, unlike carbon emissions that have a similar effect regardless of the emission source locations, the public health impact of criteria air pollutants heavily depends on the location of the emission source. Thus, technology companies should account for public health impacts when deciding where they build data centers, where they get electricity for their data centers, and where they install onsite renewables in order to best mitigate the adverse health effects.", "original_types": ["text"], "id": 796}
{"type": "section", "content": "6 Health-Informed Computing: Addressing Data Centers' Public Health Impact\n\nIn this section, we present Health-Informed Computing, a framework that explicitly incorporates public health impacts as a key optimization objective and strategically manages data center workloads to minimize adverse health outcomes while supporting broader sustainability goals.\n\nTo mitigate the public health impact of computing, one straightforward approach is to focus solely on reducing the energy consumption (e.g., reducing AI model sizes [93,94]). While reducing energy consumption is beneficial, overlooking the downstream public health impact of where and when energy is produced does not necessarily lead to minimized health burdens. For example, Table 2 demonstrates a 10x difference in health costs for training the same AI model across different locations. This highlights that health-informed and energy-aware computing, when combined, offer complementary benefits, leading to better public health outcomes.\n\n6.1 Opportunities for Health-Informed Computing\n\nData centers, including those operated by major technology companies [10,37], predominantly rely on grid electricity due to the practical challenges of installing on-site low-pollutant and low-carbon energy sources at scale. However, the spatial-temporal variations of scope-2 health costs (Fig. 8) open up new opportunities to reduce the public health impact by exploiting the high scheduling flexibilities of computing workloads (e.g., AI training). For example, as further supported by EPRI's recent initiative on maximizing data center flexibility for demand response [11], AI training can be scheduled in more than one data center, while multiple AI models with different sizes are often available to serve AI inference requests, offering flexible resource-performance tradeoffs.\n\nTo date, the existing data centers have mostly exploited such scheduling flexibilities for reducing electricity costs [95], carbon emissions [15], water consumption [96], and/or environmental inequity [97]. Nonetheless, the public health impact of AI significantly differs from these environmental costs or metrics.\n\nConcretely, despite sharing some common sources (e.g., fossil fuels) with carbon emissions, the public health impact resulting from the dispersion of criteria air pollutants is highly dependent on the emission source location and only exhibits a weak correlation with carbon emissions. For example, the same quantity of carbon emissions generally results in the same climate change impacts regardless of the emission source; in contrast, criteria air pollutants have substantially greater public health impacts if emitted in densely populated regions compared to sparsely populated or unpopulated regions, emphasizing the importance of considering spatial variability.", "doc_id": "han2024", "page": 14, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "6 Health-Informed Computing: Addressing Data Centers' Public Health Impact\n\nIn this section, we present Health-Informed Computing, a framework that explicitly incorporates public health impacts as a key optimization objective and strategically manages data center workloads to minimize adverse health outcomes while supporting broader sustainability goals.\n\nTo mitigate the public health impact of computing, one straightforward approach is to focus solely on reducing the energy consumption (e.g., reducing AI model sizes [93,94]). While reducing energy consumption is beneficial, overlooking the downstream public health impact of where and when energy is produced does not necessarily lead to minimized health burdens. For example, Table 2 demonstrates a 10x difference in health costs for training the same AI model across different locations. This highlights that health-informed and energy-aware computing, when combined, offer complementary benefits, leading to better public health outcomes.\n\n6.1 Opportunities for Health-Informed Computing\n\nData centers, including those operated by major technology companies [10,37], predominantly rely on grid electricity due to the practical challenges of installing on-site low-pollutant and low-carbon energy sources at scale. However, the spatial-temporal variations of scope-2 health costs (Fig. 8) open up new opportunities to reduce the public health impact by exploiting the high scheduling flexibilities of computing workloads (e.g., AI training). For example, as further supported by EPRI's recent initiative on maximizing data center flexibility for demand response [11], AI training can be scheduled in more than one data center, while multiple AI models with different sizes are often available to serve AI inference requests, offering flexible resource-performance tradeoffs.\n\nTo date, the existing data centers have mostly exploited such scheduling flexibilities for reducing electricity costs [95], carbon emissions [15], water consumption [96], and/or environmental inequity [97]. Nonetheless, the public health impact of AI significantly differs from these environmental costs or metrics.\n\nConcretely, despite sharing some common sources (e.g., fossil fuels) with carbon emissions, the public health impact resulting from the dispersion of criteria air pollutants is highly dependent on the emission source location and only exhibits a weak correlation with carbon emissions. For example, the same quantity of carbon emissions generally results in the same climate change impacts regardless of the emission source; in contrast, criteria air pollutants have substantially greater public health impacts if emitted in densely populated regions compared to sparsely populated or unpopulated regions, emphasizing the importance of considering spatial variability.", "original_types": ["text", "header"], "id": 797}
{"type": "section", "content": "To further confirm this point and highlight the potential of health-informed data center load shifting, we analyze the scope-2 marginal carbon intensity and public health cost for each unit of electricity generation across all the 114 U.S. regions between October 1, 2023, and September 30, 2024, provided by WattTime [77].7 The time granularity for data collection is 5 minutes.\n\nHere, we focus on marginal health impacts and carbon emissions for two main reasons: first, WattTime provides only real-time marginal health impact estimates [98]; and second, marginal signals are generally considered more useful for guiding energy load adjustments [99], which also explain why the EPA reports marginal health benefits per kWh (i.e., marginal health price) to inform energy demand changes [100].\n\nWe show in Fig. 8a the region-wise normalized interquartile ranges (IQR divided by the yearly average) for both public health costs and carbon emissions. The normalized IQR measures the spread of the time-varying health and carbon signals. Specifically, in 110 out of the 114 U.S. regions (96%), the normalized IQR of health cost is higher than that of the carbon intensity for each unit of electricity consumption. Moreover, the normalized IQR for carbon emissions is less than 0.2 in most of the regions. This implies that health costs exhibit a greater temporal variation than carbon emissions in 110 out of the 114 U.S. regions. Likewise, in Fig. 8b, the greater temporal variation of health costs is also supported by its greater normalized standard deviation (STD divided by the yearly average) in 90 out of the 114 U.S. regions (79%). Next, we show in Fig. 8c the weak spatial correlation (Pearson correlation coefficient: 0.292) between the yearly average health cost and carbon intensity across the 114 regions. Furthermore, the normalized IQR of the health cost spatial", "doc_id": "han2024", "page": 14, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "To further confirm this point and highlight the potential of health-informed data center load shifting, we analyze the scope-2 marginal carbon intensity and public health cost for each unit of electricity generation across all the 114 U.S. regions between October 1, 2023, and September 30, 2024, provided by WattTime [77].7 The time granularity for data collection is 5 minutes.\n\nHere, we focus on marginal health impacts and carbon emissions for two main reasons: first, WattTime provides only real-time marginal health impact estimates [98]; and second, marginal signals are generally considered more useful for guiding energy load adjustments [99], which also explain why the EPA reports marginal health benefits per kWh (i.e., marginal health price) to inform energy demand changes [100].\n\nWe show in Fig. 8a the region-wise normalized interquartile ranges (IQR divided by the yearly average) for both public health costs and carbon emissions. The normalized IQR measures the spread of the time-varying health and carbon signals. Specifically, in 110 out of the 114 U.S. regions (96%), the normalized IQR of health cost is higher than that of the carbon intensity for each unit of electricity consumption. Moreover, the normalized IQR for carbon emissions is less than 0.2 in most of the regions. This implies that health costs exhibit a greater temporal variation than carbon emissions in 110 out of the 114 U.S. regions. Likewise, in Fig. 8b, the greater temporal variation of health costs is also supported by its greater normalized standard deviation (STD divided by the yearly average) in 90 out of the 114 U.S. regions (79%). Next, we show in Fig. 8c the weak spatial correlation (Pearson correlation coefficient: 0.292) between the yearly average health cost and carbon intensity across the 114 regions. Furthermore, the normalized IQR of the health cost spatial", "original_types": ["text"], "id": 798}
{"type": "figure", "content": "Figure 8: Analysis of marginal scope-2 carbon emission rates and public health costs over 114 U.S. regions between October 1, 2023 and September 30, 2024 [77]. (a) In 110 out of the 114 U.S. regions (96%), the normalized IQR of marginal health cost is higher than that of marginal carbon intensity. (b) In 90 out of the 114 U.S. regions (79%), the normalized standard deviation of marginal health cost is higher than that of marginal carbon intensity. (c) The Pearson correlation between the per-region yearly average marginal health cost and carbon intensity is 0.292.", "doc_id": "han2024", "page": 15, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 8: Analysis of marginal scope-2 carbon emission rates and public health costs over 114 U.S. regions between October 1, 2023 and September 30, 2024 [77]. (a) In 110 out of the 114 U.S. regions (96%), the normalized IQR of marginal health cost is higher than that of marginal carbon intensity. (b) In 90 out of the 114 U.S. regions (79%), the normalized standard deviation of marginal health cost is higher than that of marginal carbon intensity. (c) The Pearson correlation between the per-region yearly average marginal health cost and carbon intensity is 0.292.", "id": 799}
{"type": "table", "content": "Table 3: Correlation analysis of marginal carbon emissions and health impacts for a technology company’s U.S. data center locations between October 1, 2023, and September 30, 2024 [77]. According to the region classification of WattTime [98], the two data centers in Storey County, NV, and Henderson, NV, belong to the same power grid region, and so do those in Jackson County, AL, and Montgomery County, TN.", "doc_id": "han2024", "page": 15, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 3: Correlation analysis of marginal carbon emissions and health impacts for a technology company’s U.S. data center locations between October 1, 2023, and September 30, 2024 [77]. According to the region classification of WattTime [98], the two data centers in Storey County, NV, and Henderson, NV, belong to the same power grid region, and so do those in Jackson County, AL, and Montgomery County, TN.", "id": 800}
{"type": "section", "content": "distribution is 3.62x that of carbon emission spatial distribution (1.05 vs. 0.29), while the health-to-carbon ratio in terms of the spatial distribution’s normalized STD is 3.37 (0.64 vs. 0.19). In other words, the health cost could have a greater spatial spread than the carbon emission.", "doc_id": "han2024", "page": 15, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "distribution is 3.62x that of carbon emission spatial distribution (1.05 vs. 0.29), while the health-to-carbon ratio in terms of the spatial distribution’s normalized STD is 3.37 (0.64 vs. 0.19). In other words, the health cost could have a greater spatial spread than the carbon emission.", "original_types": ["text"], "id": 801}
{"type": "section", "content": "6.2 Benefits of Health-Informed Computing\n\nTo improve system performance and reliability, technology companies typically operate data centers over a variety of geographically distributed regions and dynamically distribute computing workloads through a process known as geographical load balancing (GLB). Here, we leverage the unique spatial load flexibility of geographically distributed data centers to demonstrate the benefits of health-informed computing as a proof of concept.", "doc_id": "han2024", "page": 16, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "6.2 Benefits of Health-Informed Computing\n\nTo improve system performance and reliability, technology companies typically operate data centers over a variety of geographically distributed regions and dynamically distribute computing workloads through a process known as geographical load balancing (GLB). Here, we leverage the unique spatial load flexibility of geographically distributed data centers to demonstrate the benefits of health-informed computing as a proof of concept.", "original_types": ["text", "header"], "id": 802}
{"type": "table", "content": "Table 4: Comparison between health-informed and carbon-aware GLB (λ = 1.5)\n```markdown\n| Metric | Baseline | Carbon-Aware GLB | Health-Informed GLB |\n|---|---|---|---|\n| Health (Million $) | 393.23 | 416.29 (5.86%) | 383.32 (-2.52%) | 404.26 (2.80%) | 289.67 (-26.34%) | 291.94 (-25.76%) |\n| Energy (Million $) | 756.50 | 714.99 (-5.49%) | 714.99 (-5.49%) | 734.77 (-2.87%) | 765.68 (1.21%) | 741.49 (-1.98%) | 733.66 (-3.02%) |\n| Carbon (Million Ton) | 6.60 | 6.67 (1.02%) | 6.67 (1.02%) | 6.20 (-6.01%) | 6.12 (-7.23%) | 6.54 (-0.89%) | 6.51 (-1.38%) |\n|---|---|---|---|\n```", "doc_id": "han2024", "page": 17, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 4: Comparison between health-informed and carbon-aware GLB (λ = 1.5)\n```markdown\n| Metric | Baseline | Carbon-Aware GLB | Health-Informed GLB |\n|---|---|---|---|\n| Health (Million $) | 393.23 | 416.29 (5.86%) | 383.32 (-2.52%) | 404.26 (2.80%) | 289.67 (-26.34%) | 291.94 (-25.76%) |\n| Energy (Million $) | 756.50 | 714.99 (-5.49%) | 714.99 (-5.49%) | 734.77 (-2.87%) | 765.68 (1.21%) | 741.49 (-1.98%) | 733.66 (-3.02%) |\n| Carbon (Million Ton) | 6.60 | 6.67 (1.02%) | 6.67 (1.02%) | 6.20 (-6.01%) | 6.12 (-7.23%) | 6.54 (-0.89%) | 6.51 (-1.38%) |\n|---|---|---|---|\n```", "id": 803}
{"type": "figure", "content": "Figure 9: Correlation analysis. (a) CDF of correlation coefficients between hourly health prices and marginal carbon emission rates for all the U.S. regions; (b) Scatter plot of health price and marginal carbon emission rate (annual average in 2023) across Meta's U.S. data center locations.", "doc_id": "han2024", "page": 17, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Figure 9: Correlation analysis. (a) CDF of correlation coefficients between hourly health prices and marginal carbon emission rates for all the U.S. regions; (b) Scatter plot of health price and marginal carbon emission rate (annual average in 2023) across Meta's U.S. data center locations.", "id": 804}
{"type": "section", "content": "Spatial correlation analysis\n\nTo further explain the results, we analyze the correlation between health costs and carbon emissions across the 13 regions where Meta operates its U.S. data centers. Figure 9b presents a scatter plot illustrating the correlation between the annual average carbon emission rate and health price across various regions. The Pearson correlation coefficient between different locations' health prices and carbon emissions is approximately -0.35, indicating a negative relationship. This suggests a potential conflict between efforts to optimize carbon emissions and those aimed at improving health outcomes via GLB. Furthermore, as highlighted by the spatial patterns in Figure 9b, the health prices exhibit a significantly higher degree of spatial variability compared to carbon emissions across regions. This discrepancy further reinforces the point that focusing solely on optimizing environmental factors, such as carbon emissions, may not effectively reduce health costs, underscoring the importance of integrating health-informed optimization strategies to achieve more comprehensive benefits.", "doc_id": "han2024", "page": 18, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Spatial correlation analysis\n\nTo further explain the results, we analyze the correlation between health costs and carbon emissions across the 13 regions where Meta operates its U.S. data centers. Figure 9b presents a scatter plot illustrating the correlation between the annual average carbon emission rate and health price across various regions. The Pearson correlation coefficient between different locations' health prices and carbon emissions is approximately -0.35, indicating a negative relationship. This suggests a potential conflict between efforts to optimize carbon emissions and those aimed at improving health outcomes via GLB. Furthermore, as highlighted by the spatial patterns in Figure 9b, the health prices exhibit a significantly higher degree of spatial variability compared to carbon emissions across regions. This discrepancy further reinforces the point that focusing solely on optimizing environmental factors, such as carbon emissions, may not effectively reduce health costs, underscoring the importance of integrating health-informed optimization strategies to achieve more comprehensive benefits.", "original_types": ["text", "header"], "id": 805}
{"type": "section", "content": "Recommendation 3: Promoting Public Health Equity\n\nThe public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31,103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. Therefore, it is imperative to address the substantial health impact disparities across communities.\n\n8 Conclusion\n\nIn this paper, we quantify and address the overlooked public health impact of data centers. We introduce a principled methodology to model these lifecycle pollutant emissions and quantify their associated public health impacts. Our findings suggest that the total annual public health burden of U.S. data centers could exceed $20 billion by 2028, approaching or even surpassing the impacts of on-road vehicle emissions in California. Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas. This highlights the need for targeted mitigation strategies. To this end, we propose health-informed computing, a framework that explicitly incorporates public health risk as a key metric when scheduling data center workloads, enabling more informed and equitable operational decisions.\n\nMore broadly, we recommend the adoption of standardized reporting protocols for the public health costs of data centers, alongside policies that ensure attention to all impacted communities, thereby supporting responsible, sustainable, and inclusive deployment of AI infrastructure.\n\nAcknowledgement\n\nThe authors would like to thank Susan Wierman for providing comments on an initial draft of this paper.\n\nReferences", "doc_id": "han2024", "page": 19, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Recommendation 3: Promoting Public Health Equity\n\nThe public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31,103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. Therefore, it is imperative to address the substantial health impact disparities across communities.\n\n8 Conclusion\n\nIn this paper, we quantify and address the overlooked public health impact of data centers. We introduce a principled methodology to model these lifecycle pollutant emissions and quantify their associated public health impacts. Our findings suggest that the total annual public health burden of U.S. data centers could exceed $20 billion by 2028, approaching or even surpassing the impacts of on-road vehicle emissions in California. Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas. This highlights the need for targeted mitigation strategies. To this end, we propose health-informed computing, a framework that explicitly incorporates public health risk as a key metric when scheduling data center workloads, enabling more informed and equitable operational decisions.\n\nMore broadly, we recommend the adoption of standardized reporting protocols for the public health costs of data centers, alongside policies that ensure attention to all impacted communities, thereby supporting responsible, sustainable, and inclusive deployment of AI infrastructure.\n\nAcknowledgement\n\nThe authors would like to thank Susan Wierman for providing comments on an initial draft of this paper.\n\nReferences", "original_types": ["text", "header"], "id": 806}
{"type": "section", "content": "[1] U.S. Centers for Disease Control and Prevention. Artificial intelligence and machine learning: Applying advanced tools for public health. https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html.\n\n[2] Mary Tran. U.S. Department of State DipNote: New air quality dashboard uses AI to forecast pollution levels. https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/, May 2024.\n\n[3] Mihaela Van der Schaar, Ahmed M Alaa, Andres Floto, Alexander Gimson, Stefan Scholtes, Angela Wood, Eoin McKinney, Daniel Jarrett, Pietro Lio, and Ari Ercole. How artificial intelligence and machine learning can help healthcare systems respond to COVID-19. Machine Learning, 110:1–14, 2021.\n\n[4] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage report. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n\n[5] EPRI. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. White Paper on Technology Innovation Report, 2024. https://www.epri.com/research/products/3002028905.\n\n[6] U.S. Department of Energy. Recommendations on powering artificial intelligence and data center infrastructure, Jul. 2024.\n\n[7] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and opportunities. In Proceedings of Machine Learning and Systems, volume 4, pages 795–813, 2022.", "doc_id": "han2024", "page": 19, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[1] U.S. Centers for Disease Control and Prevention. Artificial intelligence and machine learning: Applying advanced tools for public health. https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html.\n\n[2] Mary Tran. U.S. Department of State DipNote: New air quality dashboard uses AI to forecast pollution levels. https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/, May 2024.\n\n[3] Mihaela Van der Schaar, Ahmed M Alaa, Andres Floto, Alexander Gimson, Stefan Scholtes, Angela Wood, Eoin McKinney, Daniel Jarrett, Pietro Lio, and Ari Ercole. How artificial intelligence and machine learning can help healthcare systems respond to COVID-19. Machine Learning, 110:1–14, 2021.\n\n[4] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage report. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n\n[5] EPRI. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. White Paper on Technology Innovation Report, 2024. https://www.epri.com/research/products/3002028905.\n\n[6] U.S. Department of Energy. Recommendations on powering artificial intelligence and data center infrastructure, Jul. 2024.\n\n[7] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and opportunities. In Proceedings of Machine Learning and Systems, volume 4, pages 795–813, 2022.", "original_types": ["text"], "id": 807}
{"type": "section", "content": "[8] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S. Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. Chasing carbon: The elusive environmental footprint of computing. IEEE Micro, 42(4):37–47, jul 2022.\n\n[9] Pengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making AI less 'thirsty'. Commun. ACM, 68(7):54–61, June 2025.\n\n[10] Google. Environmental report. https://sustainability.google/reports/google-2024-environmental-report/, 2024.\n\n[11] EPRI. DCFlex initiative. https://msites.epri.com/dcflex, 2024.\n\n[12] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-MOE: Advancing mixture-of-experts inference and training to power next-generation AI scale. In ICML, 2022.\n\n[13] Jennifer Switzer, Gabriel Marcano, Ryan Kastner, and Pat Pannuto. Junkyard computing: Repurposing discarded smartphones to minimize carbon. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 2023, page 400–412, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[14] Jaylen Wang, Daniel S. Berger, Fiodar Kazhamiaka, Celine Irvene, Chaojie Zhang, Esha Choukse, Kali Frost, Rodrigo Fonseca, Brijesh Warrier, Chetan Bansal, Jonathan Stern, Ricardo Bianchini, and Akshitha Sriraman. Designing cloud servers for lower carbon. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 452–470, 2024.\n\n[15] Ana Radovanović, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne. Carbon-aware computing for datacenters. IEEE Transactions on Power Systems, 38(2):1270–1280, 2023.\n\n[16] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. Reducing the carbon impact of generative AI inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems, HotCarbon '23, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[17] Walid A. Hanafy, Qianlin Liang, Noman Bashir, Abel Souza, David Irwin, and Prashant Shenoy. Going green for less green: Optimizing the cost of reducing cloud carbon emissions. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS '24, page 479–496, New York, NY, USA, 2024. Association for Computing Machinery.\n\n[18] Intel. 2024 H1 semi-annual monitoring report (Intel Ocotillo facility). https://www.exploreintel.com/ocotillo, 2024.\n\n[19] UC Davis Air Quality Research Center. Interagency monitoring of protected visual environments. https://airquality.ucdavis.edu/improve.\n\n[20] U.S. EPA. Research on health effects from air pollution. https://www.epa.gov/air-research/research-health-effects-air-pollution.", "doc_id": "han2024", "page": 20, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[8] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S. Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. Chasing carbon: The elusive environmental footprint of computing. IEEE Micro, 42(4):37–47, jul 2022.\n\n[9] Pengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making AI less 'thirsty'. Commun. ACM, 68(7):54–61, June 2025.\n\n[10] Google. Environmental report. https://sustainability.google/reports/google-2024-environmental-report/, 2024.\n\n[11] EPRI. DCFlex initiative. https://msites.epri.com/dcflex, 2024.\n\n[12] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-MOE: Advancing mixture-of-experts inference and training to power next-generation AI scale. In ICML, 2022.\n\n[13] Jennifer Switzer, Gabriel Marcano, Ryan Kastner, and Pat Pannuto. Junkyard computing: Repurposing discarded smartphones to minimize carbon. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 2023, page 400–412, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[14] Jaylen Wang, Daniel S. Berger, Fiodar Kazhamiaka, Celine Irvene, Chaojie Zhang, Esha Choukse, Kali Frost, Rodrigo Fonseca, Brijesh Warrier, Chetan Bansal, Jonathan Stern, Ricardo Bianchini, and Akshitha Sriraman. Designing cloud servers for lower carbon. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 452–470, 2024.\n\n[15] Ana Radovanović, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne. Carbon-aware computing for datacenters. IEEE Transactions on Power Systems, 38(2):1270–1280, 2023.\n\n[16] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. Reducing the carbon impact of generative AI inference (today and in 2035). In Proceedings of the 2nd Workshop on Sustainable Computer Systems, HotCarbon '23, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[17] Walid A. Hanafy, Qianlin Liang, Noman Bashir, Abel Souza, David Irwin, and Prashant Shenoy. Going green for less green: Optimizing the cost of reducing cloud carbon emissions. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS '24, page 479–496, New York, NY, USA, 2024. Association for Computing Machinery.\n\n[18] Intel. 2024 H1 semi-annual monitoring report (Intel Ocotillo facility). https://www.exploreintel.com/ocotillo, 2024.\n\n[19] UC Davis Air Quality Research Center. Interagency monitoring of protected visual environments. https://airquality.ucdavis.edu/improve.\n\n[20] U.S. EPA. Research on health effects from air pollution. https://www.epa.gov/air-research/research-health-effects-air-pollution.", "original_types": ["text"], "id": 808}
{"type": "section", "content": "[21] Giulia Grande, Jing Wu, Petter LS Ljungman, Massimo Stafoggia, Tom Bellander, and Debora Rizzuto. Long-term exposure to PM 2.5 and cognitive decline: A longitudinal population-based study. Journal of Alzheimer's Disease, 80(2):591–599, 2021.\n\n[22] U.S. EPA. User's manual for the co-benefits risk assessment (COBRA) screening model. https://www.epa.gov/cobra/users-manual-co-benefits-risk-assessment-cobra-screening-model.", "doc_id": "han2024", "page": 20, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[21] Giulia Grande, Jing Wu, Petter LS Ljungman, Massimo Stafoggia, Tom Bellander, and Debora Rizzuto. Long-term exposure to PM 2.5 and cognitive decline: A longitudinal population-based study. Journal of Alzheimer's Disease, 80(2):591–599, 2021.\n\n[22] U.S. EPA. User's manual for the co-benefits risk assessment (COBRA) screening model. https://www.epa.gov/cobra/users-manual-co-benefits-risk-assessment-cobra-screening-model.", "original_types": ["text"], "id": 809}
{"type": "section", "content": "[23] Wenhua Yu, Rongbin Xu, Tingting Ye, Michael J Abramson, Lidia Morawska, Bin Jalaludin, Fay H Johnston, Sarah B Henderson, Luke D Knibbs, Geoffrey G Morgan, et al. Estimates of global mortality burden associated with short-term exposure to fine particulate matter (PM2.5). The Lancet Planetary Health, 8(3):e146–e155, 2024.\n\n[24] U.S. EPA. What is cross-state air pollution? https://www.epa.gov/Cross-State-Air-Pollution/what-cross-state-air-pollution.\n\n[25] Jian Zhang and S. Trivikrama Rao. The role of vertical mixing in the temporal evolution of ground-level ozone concentrations. Journal of Applied Meteorology, 38(12):1674–1691, 1999.\n\n[26] Health Canada. Guidance for evaluating human health impacts in environmental assessment: Air quality. https://iaac-aeic.gc.ca/050/documents/p80054/119376E.pdf, 2016.\n\n[27] World Health Organization. Air pollution is responsible for 6.7 million premature deaths every year. https://www.who.int/teams/environment-climate-change-and-health/air-quality-and-health/health-impacts/types-of-pollutants.\n\n[28] World Health Organization. Ambient (outdoor) air pollution. https://www.who.int/news-room/fact-sheets/detail/ambient-(outdoor)-air-quality-and-health, 2024.\n\n[29] Institute for Health Metrics and Evaluation (IHME). Global burden of disease 2021: Findings from the GBD 2021 study. https://www.healthdata.org/research-analysis/library/global-burden-disease-2021-findings-gbd-2021-study, May 2024.\n\n[30] U.S. EPA. Human health and environmental impacts of the electric power sector. https://www.epa.gov/power-sector/human-health-environmental-impacts-electric-power-sector.\n\n[31] U.S. EPA. Power plants and neighboring communities. https://www.epa.gov/power-sector/power-plants-and-neighboring-communities.\n\n[32] Lucas Henneman, Christine Choirat, Irene Dedoussi, Francesca Dominici, Jessica Roberts, and Corwin Zigler. Mortality risk from united states coal electricity generation. Science, 382(6673):941–946, 2023.\n\n[33] European Environment Agency. The costs to health and the environment from industrial air pollution in Europe — 2024 update. https://www.eea.europa.eu/publications/the-cost-to-health-and-the, 2025.\n\n[34] Washington Department of Ecology. Diesel pollution from data centers. https://ecology.wa.gov/air-climate/air-quality/data-centers.\n\n[35] The U.S. White House. Executive order on advancing United States leadership in artificial intelligence infrastructure. https://www.federalregister.gov/documents/2025/01/17/2025-01395/advancing-united-states-leadership-in-artificial-intelligence-infrastructure, January 2025.\n\n[36] Meta Llama. Model information. https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\n\n[37] Meta. Sustainability report. https://sustainability.atmeta.com/2024-sustainability-report/, 2024.\n\n[38] McKinsey. Investing in the rising data center economy. White Paper, January 2023.\n\n[39] U.S. EPA. Co-benefits risk assessment health impacts screening and mapping tool (COBRA). https://cobra.epa.gov/.", "doc_id": "han2024", "page": 21, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[23] Wenhua Yu, Rongbin Xu, Tingting Ye, Michael J Abramson, Lidia Morawska, Bin Jalaludin, Fay H Johnston, Sarah B Henderson, Luke D Knibbs, Geoffrey G Morgan, et al. Estimates of global mortality burden associated with short-term exposure to fine particulate matter (PM2.5). The Lancet Planetary Health, 8(3):e146–e155, 2024.\n\n[24] U.S. EPA. What is cross-state air pollution? https://www.epa.gov/Cross-State-Air-Pollution/what-cross-state-air-pollution.\n\n[25] Jian Zhang and S. Trivikrama Rao. The role of vertical mixing in the temporal evolution of ground-level ozone concentrations. Journal of Applied Meteorology, 38(12):1674–1691, 1999.\n\n[26] Health Canada. Guidance for evaluating human health impacts in environmental assessment: Air quality. https://iaac-aeic.gc.ca/050/documents/p80054/119376E.pdf, 2016.\n\n[27] World Health Organization. Air pollution is responsible for 6.7 million premature deaths every year. https://www.who.int/teams/environment-climate-change-and-health/air-quality-and-health/health-impacts/types-of-pollutants.\n\n[28] World Health Organization. Ambient (outdoor) air pollution. https://www.who.int/news-room/fact-sheets/detail/ambient-(outdoor)-air-quality-and-health, 2024.\n\n[29] Institute for Health Metrics and Evaluation (IHME). Global burden of disease 2021: Findings from the GBD 2021 study. https://www.healthdata.org/research-analysis/library/global-burden-disease-2021-findings-gbd-2021-study, May 2024.\n\n[30] U.S. EPA. Human health and environmental impacts of the electric power sector. https://www.epa.gov/power-sector/human-health-environmental-impacts-electric-power-sector.\n\n[31] U.S. EPA. Power plants and neighboring communities. https://www.epa.gov/power-sector/power-plants-and-neighboring-communities.\n\n[32] Lucas Henneman, Christine Choirat, Irene Dedoussi, Francesca Dominici, Jessica Roberts, and Corwin Zigler. Mortality risk from united states coal electricity generation. Science, 382(6673):941–946, 2023.\n\n[33] European Environment Agency. The costs to health and the environment from industrial air pollution in Europe — 2024 update. https://www.eea.europa.eu/publications/the-cost-to-health-and-the, 2025.\n\n[34] Washington Department of Ecology. Diesel pollution from data centers. https://ecology.wa.gov/air-climate/air-quality/data-centers.\n\n[35] The U.S. White House. Executive order on advancing United States leadership in artificial intelligence infrastructure. https://www.federalregister.gov/documents/2025/01/17/2025-01395/advancing-united-states-leadership-in-artificial-intelligence-infrastructure, January 2025.\n\n[36] Meta Llama. Model information. https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md.\n\n[37] Meta. Sustainability report. https://sustainability.atmeta.com/2024-sustainability-report/, 2024.\n\n[38] McKinsey. Investing in the rising data center economy. White Paper, January 2023.\n\n[39] U.S. EPA. Co-benefits risk assessment health impacts screening and mapping tool (COBRA). https://cobra.epa.gov/.", "original_types": ["text"], "id": 810}
{"type": "section", "content": "[40] U.S. National Center for Health Statistics. Factstats: Asthma. https://www.cdc.gov/nchs/fastats/asthma.htm.\n\n[41] California DMV. Vehicles registered by county. https://www.dmv.ca.gov/portal/dmv-research-reports/research-development-data-dashboards/vehicles-registered-by-county/.\n\n[42] Virginia Joint Legislative Audit and Review Commission. Report to the Governor and the General Assembly of Virginia: Data centers in Virginia (JLARC report 158), December 2024.\n\n[43] Washington Department of Ecology. Health risks from diesel emissions in the Quincy area. Air Quality Program Report (Publication 20-02-019), August 2020.\n\n[44] U.S. EPA. Use of backup generators to maintain the reliability of the electric grid. https://www.epa.gov/system/files/documents/2025-05/rice-memo-on-duke-energy-regulatory-interpretation-04_17_25.pdf, May 2025.\n\n[45] Neil M. Donahue, Allen L. Robinson, and Spyros N. Pandis. Atmospheric organic particulate matter: From smoke to secondary organic aerosol. Atmospheric Environment, 43(1):94–106, 2009.\n\n[46] U.S. EPA. Benefits and costs of the Clean Air Act 1990-2020, the second prospective study. https://www.epa.gov/clean-air-act-overview/benefits-and-costs-clean-air-act-1990-2020-second-prospective-study, March 2011.\n\n[47] U.S. EPA. Summary of the Clean Air Act. https://www.epa.gov/laws-regulations/summary-clean-air-act.\n\n[48] U.S. EPA. National ambient air quality standards (NAAQS) table. https://www.epa.gov/criteria-air-pollutants/naaqs-table.\n\n[49] California Air Resources Board. Laws and regulations. https://ww2.arb.ca.gov/resources/documents/laws-and-regulations.\n\n[50] American Lung Association. State of the air. Report, 2024.\n\n[51] World Health Organization. Ambient air pollution attributable deaths. https://www.who.int/data/gho/data/indicators/indicator-details/GHO/ambient-air-pollution-attributable-deaths, 2024.\n\n[52] World Health Organization. WHO global air quality guidelines. https://www.who.int/publications/i/item/9789240034228.\n\n[53] U.S. EPA. Projection of counties with monitors that would not meet in 2032. https://www.epa.gov/system/files/documents/2024-02/2024-pm-naaqs-final-2032-projections-map.pdf, February 2024.\n\n[54] John P. Stevens and Supreme Court of The United States. U.S. reports: Massachusetts v. EPA, 549 U.S. 497. The Library of Congress, 2007.\n\n[55] U.S. EPA. Climate change and human health. https://www.epa.gov/climateimpacts/climate-change-and-human-health.\n\n[56] Greenhouse Gas Protocol. Standards and guidance. https://ghgprotocol.org/.\n\n[57] U.S. EPA. Controlling air pollution from stationary engines. https://www.epa.gov/stationary-engines.\n\n[58] California Air Resources Board. Fact sheet on emergency backup generators. https://www.aqmd.gov/home/permits/emergency-generators.\n\n[59] Uptime Institute. Explaining the Uptime Institute’s tier classification system (April 2021 update). https://journal.uptimeinstitute.com/explaining-uptime-institutes-tier-classification-system/", "doc_id": "han2024", "page": "21-22", "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[40] U.S. National Center for Health Statistics. Factstats: Asthma. https://www.cdc.gov/nchs/fastats/asthma.htm.\n\n[41] California DMV. Vehicles registered by county. https://www.dmv.ca.gov/portal/dmv-research-reports/research-development-data-dashboards/vehicles-registered-by-county/.\n\n[42] Virginia Joint Legislative Audit and Review Commission. Report to the Governor and the General Assembly of Virginia: Data centers in Virginia (JLARC report 158), December 2024.\n\n[43] Washington Department of Ecology. Health risks from diesel emissions in the Quincy area. Air Quality Program Report (Publication 20-02-019), August 2020.\n\n[44] U.S. EPA. Use of backup generators to maintain the reliability of the electric grid. https://www.epa.gov/system/files/documents/2025-05/rice-memo-on-duke-energy-regulatory-interpretation-04_17_25.pdf, May 2025.\n\n[45] Neil M. Donahue, Allen L. Robinson, and Spyros N. Pandis. Atmospheric organic particulate matter: From smoke to secondary organic aerosol. Atmospheric Environment, 43(1):94–106, 2009.\n\n[46] U.S. EPA. Benefits and costs of the Clean Air Act 1990-2020, the second prospective study. https://www.epa.gov/clean-air-act-overview/benefits-and-costs-clean-air-act-1990-2020-second-prospective-study, March 2011.\n\n[47] U.S. EPA. Summary of the Clean Air Act. https://www.epa.gov/laws-regulations/summary-clean-air-act.\n\n[48] U.S. EPA. National ambient air quality standards (NAAQS) table. https://www.epa.gov/criteria-air-pollutants/naaqs-table.\n\n[49] California Air Resources Board. Laws and regulations. https://ww2.arb.ca.gov/resources/documents/laws-and-regulations.\n\n[50] American Lung Association. State of the air. Report, 2024.\n\n[51] World Health Organization. Ambient air pollution attributable deaths. https://www.who.int/data/gho/data/indicators/indicator-details/GHO/ambient-air-pollution-attributable-deaths, 2024.\n\n[52] World Health Organization. WHO global air quality guidelines. https://www.who.int/publications/i/item/9789240034228.\n\n[53] U.S. EPA. Projection of counties with monitors that would not meet in 2032. https://www.epa.gov/system/files/documents/2024-02/2024-pm-naaqs-final-2032-projections-map.pdf, February 2024.\n\n[54] John P. Stevens and Supreme Court of The United States. U.S. reports: Massachusetts v. EPA, 549 U.S. 497. The Library of Congress, 2007.\n\n[55] U.S. EPA. Climate change and human health. https://www.epa.gov/climateimpacts/climate-change-and-human-health.\n\n[56] Greenhouse Gas Protocol. Standards and guidance. https://ghgprotocol.org/.\n\n[57] U.S. EPA. Controlling air pollution from stationary engines. https://www.epa.gov/stationary-engines.\n\n[58] California Air Resources Board. Fact sheet on emergency backup generators. https://www.aqmd.gov/home/permits/emergency-generators.\n\n[59] Uptime Institute. Explaining the Uptime Institute’s tier classification system (April 2021 update). https://journal.uptimeinstitute.com/explaining-uptime-institutes-tier-classification-system/", "original_types": ["text"], "id": 811}
{"type": "section", "content": "[60] Virginia Department of Environmental Quality. Issued air permits for data centers. https://www.deq.virginia.gov/permits/air/issued-air-permits-for-data-centers.\n\n[61] Hanna Pampaloni. Heat wave prompts increased data center generator use; turner pushes for tier 4 upgrades. https://www.loudounnow.com/news/heat-wave-prompts-increased-data-center-generator-use-turner-pushes-for-tier-4-upgrades/article_60a48bda-dc50-4d1a-8b16-399cd4340350.html, July 2025.\n\n[62] Piedmont Environmental Council. Data centers, diesel generators and air quality – PEC web map. https://www.pecva.org/uncategorized/data-centers-diesel-generators-and-air-quality-pec-web-map/\n\n[63] Government of Canada. Wet cooling towers: Guide to reporting. https://www.canada.ca/en/environment-climate-change/services/national-pollutant-release-inventory/report/sector-specific-tools-calculate-emissions/wet-cooling-tower-particulate-guide.html.\n\n[64] Anthony S. Wexler, Chris D. Wallis, Patrick Chuang, and Mason Leandro. Assessing particulate emissions from power plant cooling towers. California Energy Commission Final Project Report (CEC-500-2023-048), July 2013.\n\n[65] Reuters. Data center reliance on fossil fuels may delay clean-energy transition. https://www.reuters.com/technology/artificial-intelligence/how-ai-cloud-computing-may-delay-transition-clean-energy-2024-11-21/, November 2024.\n\n[66] Virginia Electric and Power Company. Integrated resource plan. https://www.dominionenergy.com/-/media/pdfs/global/company/IRP/2024-IRP-w_o-Appendices.pdf, October 2024.\n\n[67] U.S. Energy Information Administration. Annual energy outlook 2025. https://www.eia.gov/outlooks/aeo.\n\n[68] Hannah Ritchie and Pablo Rosado. Electricity mix. Our World in Data, 2024.\n\n[69] Google. New nuclear clean energy agreement with Kairos Power. https://blog.google/outreach-initiatives/sustainability/google-kairos-power-nuclear-energy-agreement/, October 2024.\n\n[70] CNBC. To land Meta’s massive $10 billion data center, Louisiana pulled out all the stops. will it be worth it? https://www.cnbc.com/2025/06/25/meta-massive-data-center-louisiana-cost-jobs-energy-use.html, June 2025.\n\n[71] The Associated Press. OpenAI shows off Stargate AI data center in Texas and plans 5 more elsewhere with Oracle, Softbank. https://apnews.com/article/openai-stargate-oracle-data-center-0b3f4fa6e8d8141b4c143e3e7f41aba1, September 2025.\n\n[72] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing LLM inference clusters for performance and energy efficiency. In IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2025.\n\n[73] Peng Wang, Ling-Yu Zhang, Asaf Tzachor, and Wei-Qiang Chen. E-waste challenges of generative artificial intelligence. Nature Computational Science, 4:818–823, October 2024.\n\n[74] U.S. EPA. Semiconductor industry. https://www.epa.gov/eps-partnership/semiconductor-industry.\n\n[75] McKinsey. Generative AI: The next S-curve for the semiconductor industry? White Paper, March 2024.", "doc_id": "han2024", "page": 23, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[60] Virginia Department of Environmental Quality. Issued air permits for data centers. https://www.deq.virginia.gov/permits/air/issued-air-permits-for-data-centers.\n\n[61] Hanna Pampaloni. Heat wave prompts increased data center generator use; turner pushes for tier 4 upgrades. https://www.loudounnow.com/news/heat-wave-prompts-increased-data-center-generator-use-turner-pushes-for-tier-4-upgrades/article_60a48bda-dc50-4d1a-8b16-399cd4340350.html, July 2025.\n\n[62] Piedmont Environmental Council. Data centers, diesel generators and air quality – PEC web map. https://www.pecva.org/uncategorized/data-centers-diesel-generators-and-air-quality-pec-web-map/\n\n[63] Government of Canada. Wet cooling towers: Guide to reporting. https://www.canada.ca/en/environment-climate-change/services/national-pollutant-release-inventory/report/sector-specific-tools-calculate-emissions/wet-cooling-tower-particulate-guide.html.\n\n[64] Anthony S. Wexler, Chris D. Wallis, Patrick Chuang, and Mason Leandro. Assessing particulate emissions from power plant cooling towers. California Energy Commission Final Project Report (CEC-500-2023-048), July 2013.\n\n[65] Reuters. Data center reliance on fossil fuels may delay clean-energy transition. https://www.reuters.com/technology/artificial-intelligence/how-ai-cloud-computing-may-delay-transition-clean-energy-2024-11-21/, November 2024.\n\n[66] Virginia Electric and Power Company. Integrated resource plan. https://www.dominionenergy.com/-/media/pdfs/global/company/IRP/2024-IRP-w_o-Appendices.pdf, October 2024.\n\n[67] U.S. Energy Information Administration. Annual energy outlook 2025. https://www.eia.gov/outlooks/aeo.\n\n[68] Hannah Ritchie and Pablo Rosado. Electricity mix. Our World in Data, 2024.\n\n[69] Google. New nuclear clean energy agreement with Kairos Power. https://blog.google/outreach-initiatives/sustainability/google-kairos-power-nuclear-energy-agreement/, October 2024.\n\n[70] CNBC. To land Meta’s massive $10 billion data center, Louisiana pulled out all the stops. will it be worth it? https://www.cnbc.com/2025/06/25/meta-massive-data-center-louisiana-cost-jobs-energy-use.html, June 2025.\n\n[71] The Associated Press. OpenAI shows off Stargate AI data center in Texas and plans 5 more elsewhere with Oracle, Softbank. https://apnews.com/article/openai-stargate-oracle-data-center-0b3f4fa6e8d8141b4c143e3e7f41aba1, September 2025.\n\n[72] Jovan Stojkovic, Chaojie Zhang, Inigo Goiri, Josep Torrellas, and Esha Choukse. DynamoLLM: Designing LLM inference clusters for performance and energy efficiency. In IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2025.\n\n[73] Peng Wang, Ling-Yu Zhang, Asaf Tzachor, and Wei-Qiang Chen. E-waste challenges of generative artificial intelligence. Nature Computational Science, 4:818–823, October 2024.\n\n[74] U.S. EPA. Semiconductor industry. https://www.epa.gov/eps-partnership/semiconductor-industry.\n\n[75] McKinsey. Generative AI: The next S-curve for the semiconductor industry? White Paper, March 2024.", "original_types": ["text"], "id": 812}
{"type": "section", "content": "[76] U.S. EPA. Avoided emissions and generation tool (AVERT). https://www.epa.gov/avert.\n\n[77] WattTime. https://watttime.org/", "doc_id": "han2024", "page": 23, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[76] U.S. EPA. Avoided emissions and generation tool (AVERT). https://www.epa.gov/avert.\n\n[77] WattTime. https://watttime.org/", "original_types": ["text"], "id": 813}
{"type": "section", "content": "[78] Microsoft. Environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, 2024.\n\n[79] Ian Schneider, Hui Xu, Stephan Benecke, David Patterson, Keguo Huang, Parthasarathy Ranganathan, and Cooper Elsworth. Life-cycle emissions of AI hardware: A cradle-to-grave approach and generational trends, 2025.\n\n[80] U.S. EPA. Air quality dispersion modeling. https://www.epa.gov/scram/air-quality-dispersion-modeling.\n\n[81] Richard T. McNider and Arastoo Pour-Biazar. Meteorological modeling relevant to mesoscale and regional air quality applications: A review. Journal of the Air & Waste Management Association, 70(1):2–43, 2020.\n\n[82] Christopher W. Tessum, Jason D. Hill, and Julian D. Marshall. InMAP: A model for air pollution interventions. PloS ONE, 12(4):e0176131, 2017.\n\n[83] Kirk R. Baker, Heather Simon, Barron Henderson, Colby Tucker, David Cooley, and Emma Zinsmeister. Source–receptor relationships between precursor emissions and O3 and PM2.5 air pollution impacts. Environmental Science & Technology, 57(39):14626–14637, 2023.\n\n[84] Qian Di, Yan Wang, Antonella Zanobetti, Yun Wang, Petros Koutrakis, Christine Choirat, Francesca Dominici, and Joel D. Schwartz. Air pollution and mortality in the medicare population. New England Journal of Medicine, 376(26):2513–2522, 2017.\n\n[85] U.S. EPA. Publications that cite COBRA. https://www.epa.gov/cobra/publications-cite-cobra.\n\n[86] Jean Schmitt, Marianne Hatzopoulou, Amir FN Abdul-Manan, Heather L MacLean, and I Daniel Posen. Health benefits of US light-duty vehicle electrification: Roles of fleet dynamics, clean electricity, and policy timing. Proceedings of the National Academy of Sciences, 121(43):e2320858121, 2024.\n\n[87] Gianluca Guidi, Francesca Dominici, Nat Steinsultz, Gabriel Dance, Lucas Henneman, Henry Richardson, Edgar Castro, Falco J Bargagli-Stoffi, and Scott Delaney. The environmental burden of the United States' bitcoin mining boom. https://pubmed.ncbi.nlm.nih.gov/39502776/, 2024.\n\n[88] Eleanor M. Hennessy, Jacques A. de Chalendar, Sally M. Benson, and Inês ML Azevedo. Distributional health impacts of electricity imports in the United States. Environmental Research Letters, 17(6):064011, 2022.\n\n[89] U.S. National Park Service. Where does air pollution come from? https://www.nps.gov/subjects/air/sources.htm.\n\n[90] U.S. EPA. Clean Air Act vehicle and engine enforcement case resolutions. https://www.epa.gov/enforcement/clean-air-act-vehicle-and-engine-enforcement-case-resolutions.\n\n[91] U.S. EPA. Final rule: Multi-pollutant emissions standards for model years 2027 and later light-duty and medium-duty vehicles. https://www.epa.gov/regulations-emissions-vehicles-and-engines/final-rule-multi-pollutant-emissions-standards-model, April 2024.\n\n[92] Meta. Introducing Llama 3.1: Our most capable models to date. https://ai.meta.com/blog/meta-llama-3-1/.\n\n[93] DeepSeek-AI. Deepseek-v3 technical report. arXiv 2412.19437, 2024.", "doc_id": "han2024", "page": 24, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[78] Microsoft. Environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, 2024.\n\n[79] Ian Schneider, Hui Xu, Stephan Benecke, David Patterson, Keguo Huang, Parthasarathy Ranganathan, and Cooper Elsworth. Life-cycle emissions of AI hardware: A cradle-to-grave approach and generational trends, 2025.\n\n[80] U.S. EPA. Air quality dispersion modeling. https://www.epa.gov/scram/air-quality-dispersion-modeling.\n\n[81] Richard T. McNider and Arastoo Pour-Biazar. Meteorological modeling relevant to mesoscale and regional air quality applications: A review. Journal of the Air & Waste Management Association, 70(1):2–43, 2020.\n\n[82] Christopher W. Tessum, Jason D. Hill, and Julian D. Marshall. InMAP: A model for air pollution interventions. PloS ONE, 12(4):e0176131, 2017.\n\n[83] Kirk R. Baker, Heather Simon, Barron Henderson, Colby Tucker, David Cooley, and Emma Zinsmeister. Source–receptor relationships between precursor emissions and O3 and PM2.5 air pollution impacts. Environmental Science & Technology, 57(39):14626–14637, 2023.\n\n[84] Qian Di, Yan Wang, Antonella Zanobetti, Yun Wang, Petros Koutrakis, Christine Choirat, Francesca Dominici, and Joel D. Schwartz. Air pollution and mortality in the medicare population. New England Journal of Medicine, 376(26):2513–2522, 2017.\n\n[85] U.S. EPA. Publications that cite COBRA. https://www.epa.gov/cobra/publications-cite-cobra.\n\n[86] Jean Schmitt, Marianne Hatzopoulou, Amir FN Abdul-Manan, Heather L MacLean, and I Daniel Posen. Health benefits of US light-duty vehicle electrification: Roles of fleet dynamics, clean electricity, and policy timing. Proceedings of the National Academy of Sciences, 121(43):e2320858121, 2024.\n\n[87] Gianluca Guidi, Francesca Dominici, Nat Steinsultz, Gabriel Dance, Lucas Henneman, Henry Richardson, Edgar Castro, Falco J Bargagli-Stoffi, and Scott Delaney. The environmental burden of the United States' bitcoin mining boom. https://pubmed.ncbi.nlm.nih.gov/39502776/, 2024.\n\n[88] Eleanor M. Hennessy, Jacques A. de Chalendar, Sally M. Benson, and Inês ML Azevedo. Distributional health impacts of electricity imports in the United States. Environmental Research Letters, 17(6):064011, 2022.\n\n[89] U.S. National Park Service. Where does air pollution come from? https://www.nps.gov/subjects/air/sources.htm.\n\n[90] U.S. EPA. Clean Air Act vehicle and engine enforcement case resolutions. https://www.epa.gov/enforcement/clean-air-act-vehicle-and-engine-enforcement-case-resolutions.\n\n[91] U.S. EPA. Final rule: Multi-pollutant emissions standards for model years 2027 and later light-duty and medium-duty vehicles. https://www.epa.gov/regulations-emissions-vehicles-and-engines/final-rule-multi-pollutant-emissions-standards-model, April 2024.\n\n[92] Meta. Introducing Llama 3.1: Our most capable models to date. https://ai.meta.com/blog/meta-llama-3-1/.\n\n[93] DeepSeek-AI. Deepseek-v3 technical report. arXiv 2412.19437, 2024.", "original_types": ["text"], "id": 814}
{"type": "section", "content": "[94] Han Cai, Chuang Gan, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In ICLR, 2019.\n\n[95] Asfandyar Qureshi, Rick Weber, Hari Balakrishnan, John Guttag, and Bruce Maggs. Cutting the electric bill for internet-scale systems. In Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM '09, page 123–134, New York, NY, USA, 2009. Association for Computing Machinery.\n\n[96] Mohammad A. Islam, Kishwar Ahmed, Hong Xu, Nguyen H. Tran, Gang Quan, and Shaolei Ren. Exploiting spatio-temporal diversity for water saving in geo-distributed data centers. IEEE Transactions on Cloud Computing, 6(3):734–746, 2018.\n\n[97] Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. Towards environmentally equitable AI via geographical load balancing. In e-Energy, 2024.\n\n[98] WattTime. Signal: Health damage. https://watttime.org/data-science/data-signals/health-damage/\n\n[99] Joe Gorka, Noah Rhodes, and Line Roald. ElectricityEmissions.jl: A framework for the comparison of carbon intensity signals. arXiv 2411.06560, 2024.\n\n[100] U.S. EPA. Estimating the health benefits per kilowatt-hour of energy efficiency and renewable energy. https://www.epa.gov/statelocalenergy/estimating-health-benefits-kilowatt-hour-energy-efficiency-and-renewable-energy, 2024.\n\n[101] Institute for Energy Research. EPA proposes exorbitant estimate for the social cost of carbon. https://www.instituteforenergyresearch.org/regulation/epa-proposes-exorbitant-estimate-for-the-social-cost-of-carbon/, 2022.\n\n[102] U.S. EIA. Electric power plants, capacity, generation, fuel consumption, sales, prices and customers. https://www.eia.gov/electricity/data.php, 2023.\n\n[103] U.S. EPA. About the U.S. electricity system and its impact on the environment. https://www.epa.gov/energy/about-us-electricity-system-and-its-impact-environment.\n\n[104] U.S. Department of Transportation. Estimated U.S. average vehicle emissions rates per vehicle by vehicle type using gasoline and diesel. National Transportation Statistics Table 4-43, June 2024.\n\n[105] U.S. Energy Information Administration. Annual energy outlook 2023. https://www.eia.gov/outlooks/aeo.\n\n[106] U.S. EPA. Arizona nonattainment/maintenance status for each county by year for all criteria pollutants. https://www3.epa.gov/airquality/greenbook/anayo_az.html, 2024.\n\n[107] Intel. Ocotillo campus. https://www.exploreintel.com/ocotillo, 2024.\n\n[108] Intel. 2023-24 corporate responsibility report, 2024.\n\n[109] U.S. Energy Information Administration (EIA). EIA open data. https://www.eia.gov/opendata/", "doc_id": "han2024", "page": "24-25", "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "[94] Han Cai, Chuang Gan, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In ICLR, 2019.\n\n[95] Asfandyar Qureshi, Rick Weber, Hari Balakrishnan, John Guttag, and Bruce Maggs. Cutting the electric bill for internet-scale systems. In Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM '09, page 123–134, New York, NY, USA, 2009. Association for Computing Machinery.\n\n[96] Mohammad A. Islam, Kishwar Ahmed, Hong Xu, Nguyen H. Tran, Gang Quan, and Shaolei Ren. Exploiting spatio-temporal diversity for water saving in geo-distributed data centers. IEEE Transactions on Cloud Computing, 6(3):734–746, 2018.\n\n[97] Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. Towards environmentally equitable AI via geographical load balancing. In e-Energy, 2024.\n\n[98] WattTime. Signal: Health damage. https://watttime.org/data-science/data-signals/health-damage/\n\n[99] Joe Gorka, Noah Rhodes, and Line Roald. ElectricityEmissions.jl: A framework for the comparison of carbon intensity signals. arXiv 2411.06560, 2024.\n\n[100] U.S. EPA. Estimating the health benefits per kilowatt-hour of energy efficiency and renewable energy. https://www.epa.gov/statelocalenergy/estimating-health-benefits-kilowatt-hour-energy-efficiency-and-renewable-energy, 2024.\n\n[101] Institute for Energy Research. EPA proposes exorbitant estimate for the social cost of carbon. https://www.instituteforenergyresearch.org/regulation/epa-proposes-exorbitant-estimate-for-the-social-cost-of-carbon/, 2022.\n\n[102] U.S. EIA. Electric power plants, capacity, generation, fuel consumption, sales, prices and customers. https://www.eia.gov/electricity/data.php, 2023.\n\n[103] U.S. EPA. About the U.S. electricity system and its impact on the environment. https://www.epa.gov/energy/about-us-electricity-system-and-its-impact-environment.\n\n[104] U.S. Department of Transportation. Estimated U.S. average vehicle emissions rates per vehicle by vehicle type using gasoline and diesel. National Transportation Statistics Table 4-43, June 2024.\n\n[105] U.S. Energy Information Administration. Annual energy outlook 2023. https://www.eia.gov/outlooks/aeo.\n\n[106] U.S. EPA. Arizona nonattainment/maintenance status for each county by year for all criteria pollutants. https://www3.epa.gov/airquality/greenbook/anayo_az.html, 2024.\n\n[107] Intel. Ocotillo campus. https://www.exploreintel.com/ocotillo, 2024.\n\n[108] Intel. 2023-24 corporate responsibility report, 2024.\n\n[109] U.S. Energy Information Administration (EIA). EIA open data. https://www.eia.gov/opendata/", "original_types": ["text"], "id": 815}
{"type": "section", "content": "Appendix\n\nA Modeling Details\n\nWe describe the evaluation methodology used for our empirical analysis. We use the latest COBRA (Desktop v5.1, as of October 2024) provided by the U.S. EPA [39] to study the public health impact of U.S. data centers in both 2019-2023 and 2028. While COBRA uses a reduced-complexity air quality dispersion model based on a source-receptor matrix for rapid evaluation, its accuracy has been validated and the same or similar model has been commonly adopted in the literature for large-area air quality and health impact analysis [82,85,87,88]. We consider county-level air pollutant dispersion throughout the contiguous U.S., which is the area currently supported by COBRA [39]. Note that cities considered county-equivalents for census purposes are also referred to as “counties” in COBRA. Throughout the paper, we use “county” without further specification.\n\nAll the monetary values are presented in the 2023 U.S. dollars unless otherwise stated. We set the discount rate as 2% in COBRA as recommended by the EPA based on the U.S. Office of Management and Budget Circular No. A-4 guidance [39]. When presenting a single value or a ratio (e.g., health-to-electricity cost ratio) if applicable, we use the midrange of the low and high estimates provided by COBRA.\n\nCOBRA provides data for county-level population, health incidence, valuation, and baseline emissions for 2016, 2023, and 2028 [39]. For the data from 2019 to 2022, we use linear interpolation as recommended by the EPA’s COBRA team.\n\nWe show in Table 5 and Table 6 the total baseline emissions of air pollutants for electricity generation and on-road traffic provided by COBRA [39]. By reducing a state’s on-road emissions to zero in COBRA, we obtain the corresponding public health cost in that state.", "doc_id": "han2024", "page": 26, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Appendix\n\nA Modeling Details\n\nWe describe the evaluation methodology used for our empirical analysis. We use the latest COBRA (Desktop v5.1, as of October 2024) provided by the U.S. EPA [39] to study the public health impact of U.S. data centers in both 2019-2023 and 2028. While COBRA uses a reduced-complexity air quality dispersion model based on a source-receptor matrix for rapid evaluation, its accuracy has been validated and the same or similar model has been commonly adopted in the literature for large-area air quality and health impact analysis [82,85,87,88]. We consider county-level air pollutant dispersion throughout the contiguous U.S., which is the area currently supported by COBRA [39]. Note that cities considered county-equivalents for census purposes are also referred to as “counties” in COBRA. Throughout the paper, we use “county” without further specification.\n\nAll the monetary values are presented in the 2023 U.S. dollars unless otherwise stated. We set the discount rate as 2% in COBRA as recommended by the EPA based on the U.S. Office of Management and Budget Circular No. A-4 guidance [39]. When presenting a single value or a ratio (e.g., health-to-electricity cost ratio) if applicable, we use the midrange of the low and high estimates provided by COBRA.\n\nCOBRA provides data for county-level population, health incidence, valuation, and baseline emissions for 2016, 2023, and 2028 [39]. For the data from 2019 to 2022, we use linear interpolation as recommended by the EPA’s COBRA team.\n\nWe show in Table 5 and Table 6 the total baseline emissions of air pollutants for electricity generation and on-road traffic provided by COBRA [39]. By reducing a state’s on-road emissions to zero in COBRA, we obtain the corresponding public health cost in that state.", "original_types": ["text", "header"], "id": 816}
{"type": "table", "content": "Table 5: U.S. electricity generation baseline emissions from 2016 to 2028\n```markdown\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 1100575.41                                 |\n| SO2  | 1369417.44                                 |\n| PM2.5| 111604.62                                  |\n| VOC  | 30250.76                                    |\n\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 711746.94                                  |\n| SO2  | 717409.25                                  |\n| PM2.5| 110878.22                                  |\n| VOC  | 34311.54                                    |\n\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 695495.34                                  |\n| SO2  | 733437.11                                  |\n| PM2.5| 110279.40                                  |\n| VOC  | 34446.71                                    |\n```", "doc_id": "han2024", "page": 26, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 5: U.S. electricity generation baseline emissions from 2016 to 2028\n```markdown\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 1100575.41                                 |\n| SO2  | 1369417.44                                 |\n| PM2.5| 111604.62                                  |\n| VOC  | 30250.76                                    |\n\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 711746.94                                  |\n| SO2  | 717409.25                                  |\n| PM2.5| 110878.22                                  |\n| VOC  | 34311.54                                    |\n\n| Year | Electricity Generation Emission (Metric Ton) |\n|------|---------------------------------------------|\n| NOx  | 695495.34                                  |\n| SO2  | 733437.11                                  |\n| PM2.5| 110279.40                                  |\n| VOC  | 34446.71                                    |\n```", "id": 817}
{"type": "table", "content": "Table 6: U.S. and California on-road baseline emissions from 2016 to 2028\n```markdown\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 3293579.05                       | 202427.66                               |\n| SO2  | 25001.53                         | 1438.07                                 |\n| PM2.5| 106828.36                        | 10197.26                                |\n| VOC  | 1680342.17                       | 89087.60                                |\n\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 1588423.83                       | 98095.76                                |\n| SO2  | 11325.07                         | 1280.27                                 |\n| PM2.5| 65742.16                         | 8144.83                                 |\n| VOC  | 996965.92                        | 54141.57                                |\n\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 1130369.84                       | 86573.30                                |\n| SO2  | 10616.37                         | 1154.27                                 |\n| PM2.5| 53455.43                         | 8276.27                                 |\n| VOC  | 758508.40                        | 44586.45                                |\n```", "doc_id": "han2024", "page": 26, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 6: U.S. and California on-road baseline emissions from 2016 to 2028\n```markdown\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 3293579.05                       | 202427.66                               |\n| SO2  | 25001.53                         | 1438.07                                 |\n| PM2.5| 106828.36                        | 10197.26                                |\n| VOC  | 1680342.17                       | 89087.60                                |\n\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 1588423.83                       | 98095.76                                |\n| SO2  | 11325.07                         | 1280.27                                 |\n| PM2.5| 65742.16                         | 8144.83                                 |\n| VOC  | 996965.92                        | 54141.57                                |\n\n| Year | U.S. On-road Emission (Metric Ton) | California On-road Emission (Metric Ton) |\n|------|------------------------------------|-----------------------------------------|\n| NOx  | 1130369.84                       | 86573.30                                |\n| SO2  | 10616.37                         | 1154.27                                 |\n| PM2.5| 53455.43                         | 8276.27                                 |\n| VOC  | 758508.40                        | 44586.45                                |\n```", "id": 818}
{"type": "section", "content": "On-road emissions are categorized as the “Highway Vehicles” sector in COBRA and include both tailpipe exhaust and tire and brake wear. Thus, following the EPA and U.S. Department of Transportation classification [22,104], PM2.5 resulting from road dust is not counted as emissions of highway vehicles in our study. If the PM2.5 from paved road dust (categorized as “Miscellaneous → Other Fugitive Dust → Paved Roads” in COBRA) is considered, California is still projected to have the highest state-wide public health cost of on-road vehicles among all the U.S. states.\n\nElectricity price. When estimating the electricity cost for data centers in 2023 and 2038, we use the state-level average price for industrial users in [102]. The projected U.S. nominal electricity price for industrial users remains nearly the same from 2023 to 2030 (24.96 $/MMBtu in 2023 vs. 23.04 $/MMBTu in 2030) in the baseline case per the EIA’s Energy Outlook 2023 [105]. Thus, our estimated health-to-electricity cost ratio will be even higher if we further adjust inflation.\n\nA.1 Public Health Impact of Backup Generators in Virginia\n\nVirginia has issued a total of 174 air quality permits for data center backup generators as of December 1, 2024 [60]. More than half of the data center sites are within Loudoun County. We collect a dataset of the air quality permits: permits issued before January 1, 2023, from [62], and permits issued between January", "doc_id": "han2024", "page": 26, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "On-road emissions are categorized as the “Highway Vehicles” sector in COBRA and include both tailpipe exhaust and tire and brake wear. Thus, following the EPA and U.S. Department of Transportation classification [22,104], PM2.5 resulting from road dust is not counted as emissions of highway vehicles in our study. If the PM2.5 from paved road dust (categorized as “Miscellaneous → Other Fugitive Dust → Paved Roads” in COBRA) is considered, California is still projected to have the highest state-wide public health cost of on-road vehicles among all the U.S. states.\n\nElectricity price. When estimating the electricity cost for data centers in 2023 and 2038, we use the state-level average price for industrial users in [102]. The projected U.S. nominal electricity price for industrial users remains nearly the same from 2023 to 2030 (24.96 $/MMBtu in 2023 vs. 23.04 $/MMBTu in 2030) in the baseline case per the EIA’s Energy Outlook 2023 [105]. Thus, our estimated health-to-electricity cost ratio will be even higher if we further adjust inflation.\n\nA.1 Public Health Impact of Backup Generators in Virginia\n\nVirginia has issued a total of 174 air quality permits for data center backup generators as of December 1, 2024 [60]. More than half of the data center sites are within Loudoun County. We collect a dataset of the air quality permits: permits issued before January 1, 2023, from [62], and permits issued between January", "original_types": ["text", "header"], "id": 819}
{"type": "section", "content": "A.2 Data Centers' Scope-2 Public Health Impact\n\nThe locations of emission sources depend on the power plants supplying electricity to data centers. To evaluate the public health impacts of U.S. data centers, we focus on average attribution method, which is also the standard methodology of carbon emission accounting used by technology companies in their sustainability reports [10,37,78].\n\nWe first calculate the total data center electricity consumption \\( e_{DC} \\) and the overall electricity consumption (including non-data center loads) \\( e_{Total} \\) within each electricity region. The U.S. electricity grid is divided into 14 regions following the AVoided Emissions and geneRation Tool (AVERT, the latest version v4.3 as of October 2024) provided by the EPA [76]. We use the state-level data center electricity consumption distribution for 2023 provided by EPRI [5], scale it by the U.S. total data center electricity consumption in 2019-2023 and for the 2028 projection based on data provided by [4], and then distribute state-level electricity consumption to relevant electricity regions following the state-to-region electricity apportionment used by AVERT.\n\nWe calculate the percentage \\( x\\% = \\frac{e_{DC}}{e_{Total}} \\) of the data center electricity consumption with respect to the overall electricity consumption for each electricity region. The relationship between the health impact and emission reduction in COBRA is approximately linear. Thus, we apply a reduction by \\( x\\% \\) to the baseline emissions of all the power plants within the respective electricity region in COBRA and estimate the corresponding county-level health impacts, including health outcomes and costs.\n\nWhen assessing the health impact of generative AI training, we follow the same approach, except for changing the total data center electricity consumption to the AI model training electricity consumption.", "doc_id": "han2024", "page": 27, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "A.2 Data Centers' Scope-2 Public Health Impact\n\nThe locations of emission sources depend on the power plants supplying electricity to data centers. To evaluate the public health impacts of U.S. data centers, we focus on average attribution method, which is also the standard methodology of carbon emission accounting used by technology companies in their sustainability reports [10,37,78].\n\nWe first calculate the total data center electricity consumption \\( e_{DC} \\) and the overall electricity consumption (including non-data center loads) \\( e_{Total} \\) within each electricity region. The U.S. electricity grid is divided into 14 regions following the AVoided Emissions and geneRation Tool (AVERT, the latest version v4.3 as of October 2024) provided by the EPA [76]. We use the state-level data center electricity consumption distribution for 2023 provided by EPRI [5], scale it by the U.S. total data center electricity consumption in 2019-2023 and for the 2028 projection based on data provided by [4], and then distribute state-level electricity consumption to relevant electricity regions following the state-to-region electricity apportionment used by AVERT.\n\nWe calculate the percentage \\( x\\% = \\frac{e_{DC}}{e_{Total}} \\) of the data center electricity consumption with respect to the overall electricity consumption for each electricity region. The relationship between the health impact and emission reduction in COBRA is approximately linear. Thus, we apply a reduction by \\( x\\% \\) to the baseline emissions of all the power plants within the respective electricity region in COBRA and estimate the corresponding county-level health impacts, including health outcomes and costs.\n\nWhen assessing the health impact of generative AI training, we follow the same approach, except for changing the total data center electricity consumption to the AI model training electricity consumption.", "original_types": ["text", "header"], "id": 820}
{"type": "section", "content": "A.4 Energy Consumption for Training a Generative AI Model\n\nWe consider Llama-3.1 as an example generative AI model. According to the model card [36], the training process of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of computation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts. Considering Meta’s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total training energy consumption as approximately 30 GWh. Our estimation method follows Meta’s guideline [36] and is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with server operations.\n\nA.5 Average Emission for Each LA-NYC Round Trip by Car\n\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S. Department of Transportation [104]. The emission rate accounts for tailpipe exhaust, tire wear and brake wear. Specifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust, 0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate is 0.199 grams/mile for exhaust. We see that half of PM2.5 for light-duty vehicles comes from brake and tire wear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles. The distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580 miles. Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5 and 1110.42 grams of NOx.\n\nA.6 State-wide Electricity Consumption by U.S. Data Centers in 2023\n\nWe show in Fig. 10 the state-wide data center electricity consumption in 2023 [5]. It can be seen that Virginia, Texas and California have the highest data center electricity consumption in 2023. The total national electricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly in our calculations to ensure consistency.", "doc_id": "han2024", "page": 28, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "A.4 Energy Consumption for Training a Generative AI Model\n\nWe consider Llama-3.1 as an example generative AI model. According to the model card [36], the training process of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of computation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts. Considering Meta’s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total training energy consumption as approximately 30 GWh. Our estimation method follows Meta’s guideline [36] and is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with server operations.\n\nA.5 Average Emission for Each LA-NYC Round Trip by Car\n\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S. Department of Transportation [104]. The emission rate accounts for tailpipe exhaust, tire wear and brake wear. Specifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust, 0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate is 0.199 grams/mile for exhaust. We see that half of PM2.5 for light-duty vehicles comes from brake and tire wear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles. The distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580 miles. Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5 and 1110.42 grams of NOx.\n\nA.6 State-wide Electricity Consumption by U.S. Data Centers in 2023\n\nWe show in Fig. 10 the state-wide data center electricity consumption in 2023 [5]. It can be seen that Virginia, Texas and California have the highest data center electricity consumption in 2023. The total national electricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly in our calculations to ensure consistency.", "original_types": ["text", "header"], "id": 821}
{"type": "section", "content": "Table 7: Information about Meta's U.S. data center locations in 2023", "doc_id": "han2024", "page": 29, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 7: Information about Meta's U.S. data center locations in 2023", "original_types": ["header"], "id": 822}
{"type": "table", "content": "Table 7: Information about Meta's U.S. data center locations in 2023\nMarkdown representation of the table", "doc_id": "han2024", "page": 29, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "Table 7: Information about Meta's U.S. data center locations in 2023\nMarkdown representation of the table", "id": 823}
{"type": "section", "content": "In our study, we use the WattTime data from 0:00 on January 1, 2023, to 23:55 on December 31, 2023. The actual time interval for our experiment is 1 hour, where the 5-minute data points are averaged hourly to compute $p_{i,t}^h$ and $r_{i,t}^c$, $\\forall t = \\{1, 2, \\ldots, T\\}$, with $T = 8760$.\n\nB.2 Sensitivity analysis of capacity slackness $\\lambda$\n\nThe parameter $\\lambda \\geq 1$ represents the capacity slackness to accept additional loads: The greater $\\lambda$, the more spatial flexibility there is. Here, we vary $\\lambda$ and report the results for $\\lambda = 1.2$ and $\\lambda = 2.0$ in Tables 8 and 9, respectively. Similar to Table 4, the potential reduction in health costs is significant. An increase in $\\lambda$ corresponds to an expansion in the spatial flexibility, leading to a larger reduction in health cost. Specifically, when $\\lambda = 2.0$, the health-informed algorithm achieves a reduction in health cost of over 40% compared to the baseline. In contrast, under $\\lambda = 2.0$, the pure carbon-aware algorithm, which focuses exclusively on optimizing carbon emissions with an infinite carbon price $p^c = \\infty$, can achieve a reduction in carbon emissions of approximately 12% relative to the baseline. This comparison highlights the great potential for health cost reduction and reinforces the importance of designing HI-GLB.", "doc_id": "han2024", "page": 29, "url": "https://arxiv.org/pdf/2412.06288", "embedded_text": "In our study, we use the WattTime data from 0:00 on January 1, 2023, to 23:55 on December 31, 2023. The actual time interval for our experiment is 1 hour, where the 5-minute data points are averaged hourly to compute $p_{i,t}^h$ and $r_{i,t}^c$, $\\forall t = \\{1, 2, \\ldots, T\\}$, with $T = 8760$.\n\nB.2 Sensitivity analysis of capacity slackness $\\lambda$\n\nThe parameter $\\lambda \\geq 1$ represents the capacity slackness to accept additional loads: The greater $\\lambda$, the more spatial flexibility there is. Here, we vary $\\lambda$ and report the results for $\\lambda = 1.2$ and $\\lambda = 2.0$ in Tables 8 and 9, respectively. Similar to Table 4, the potential reduction in health costs is significant. An increase in $\\lambda$ corresponds to an expansion in the spatial flexibility, leading to a larger reduction in health cost. Specifically, when $\\lambda = 2.0$, the health-informed algorithm achieves a reduction in health cost of over 40% compared to the baseline. In contrast, under $\\lambda = 2.0$, the pure carbon-aware algorithm, which focuses exclusively on optimizing carbon emissions with an infinite carbon price $p^c = \\infty$, can achieve a reduction in carbon emissions of approximately 12% relative to the baseline. This comparison highlights the great potential for health cost reduction and reinforces the importance of designing HI-GLB.", "original_types": ["text", "header"], "id": 824}
{"type": "section", "content": "Abstract\n\nDue to the cost-prohibitive nature of training Large Language Models (LLMs), fine-tuning has emerged as an attractive alternative for specializing LLMs for specific tasks using limited compute resources in a cost-effective manner. In this paper, we characterize sparse Mixture of Experts (MoE) based LLM fine-tuning to understand their accuracy and runtime performance on a single GPU. Our evaluation provides unique insights into the training efficacy of sparse and dense versions of MoE models, as well as their runtime characteristics, including maximum batch size, execution time breakdown, end-to-end throughput, GPU hardware utilization, and load distribution. Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning. Using our profiling results, we also develop and validate an analytical model to estimate the cost of LLM fine-tuning on the cloud. This model, based on parameters of the model and GPU architecture, estimates LLM throughput and the cost of training, aiding practitioners in industry and academia to budget the cost of fine-tuning a specific model.", "doc_id": "xia2024", "page": 1, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Abstract\n\nDue to the cost-prohibitive nature of training Large Language Models (LLMs), fine-tuning has emerged as an attractive alternative for specializing LLMs for specific tasks using limited compute resources in a cost-effective manner. In this paper, we characterize sparse Mixture of Experts (MoE) based LLM fine-tuning to understand their accuracy and runtime performance on a single GPU. Our evaluation provides unique insights into the training efficacy of sparse and dense versions of MoE models, as well as their runtime characteristics, including maximum batch size, execution time breakdown, end-to-end throughput, GPU hardware utilization, and load distribution. Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning. Using our profiling results, we also develop and validate an analytical model to estimate the cost of LLM fine-tuning on the cloud. This model, based on parameters of the model and GPU architecture, estimates LLM throughput and the cost of training, aiding practitioners in industry and academia to budget the cost of fine-tuning a specific model.", "original_types": ["text", "header"], "id": 825}
{"type": "section", "content": "Background\n\nA detailed accuracy and runtime performance analysis to understand the LLM fine-tuning workload behavior.\n\nDesign and validation of an analytical model to estimate the cost of LLM fine-tuning in the cloud.\n\nA. LLM and Finetuning\n\nThe decoder-only Transformer is designed to handle tasks where the output generation depends solely on the preceding tokens, making it particularly suited for auto-regressive tasks such as language modeling and text generation [9]. In the classic decoder-only Transformer design, multiple decoder layers are connected in sequence. Each decoder layer consists of a self-attention block followed by a feed-forward network (FFN). Fig. 1 presents an overview of the decoder-only Transformer model with a Mixture-of-Experts (MoE) design. In this model, the FFN layers are divided into several smaller FFNs, referred to as experts, which are sparsely activated by a gating mechanism. The self-attention block can also be replaced with a Mamba layer to improve performance in sequence modeling (a model known as state-space model). LLMs like GPT [10], [11], LLaMA [3], Claude [12], Mistral [13] have demonstrated their ability to excel in many natural language processing (NLP) tasks Training an LLM model from scratch requires a large amount of hardware resources and budget.", "doc_id": "xia2024", "page": 2, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Background\n\nA detailed accuracy and runtime performance analysis to understand the LLM fine-tuning workload behavior.\n\nDesign and validation of an analytical model to estimate the cost of LLM fine-tuning in the cloud.\n\nA. LLM and Finetuning\n\nThe decoder-only Transformer is designed to handle tasks where the output generation depends solely on the preceding tokens, making it particularly suited for auto-regressive tasks such as language modeling and text generation [9]. In the classic decoder-only Transformer design, multiple decoder layers are connected in sequence. Each decoder layer consists of a self-attention block followed by a feed-forward network (FFN). Fig. 1 presents an overview of the decoder-only Transformer model with a Mixture-of-Experts (MoE) design. In this model, the FFN layers are divided into several smaller FFNs, referred to as experts, which are sparsely activated by a gating mechanism. The self-attention block can also be replaced with a Mamba layer to improve performance in sequence modeling (a model known as state-space model). LLMs like GPT [10], [11], LLaMA [3], Claude [12], Mistral [13] have demonstrated their ability to excel in many natural language processing (NLP) tasks Training an LLM model from scratch requires a large amount of hardware resources and budget.", "original_types": ["text", "header"], "id": 826}
{"type": "figure", "content": "Figure 1: LLM model overview. We evaluate accuracy, throughput, runtime, and GPU characterization for different models, input datasets, and fine-tuning sparsity. The different colored expert boxes in MoE layer means different sets of experts are activated according to the input token.", "doc_id": "xia2024", "page": 2, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Figure 1: LLM model overview. We evaluate accuracy, throughput, runtime, and GPU characterization for different models, input datasets, and fine-tuning sparsity. The different colored expert boxes in MoE layer means different sets of experts are activated according to the input token.", "id": 827}
{"type": "section", "content": "Fine-tuning LLMs allows organizations to harness the full potential of advanced AI systems by tailoring them to specific tasks and domains. This customization involves training the model on domain-specific data, enabling it to understand and generate content that aligns closely with the unique needs of the users. For instance, in the healthcare sector, a fine-tuned LLM can assist in diagnosing conditions by interpreting patient data and medical literature with high precision. Another attractive feature of fine-tuning LLMs is that it can be achieved at a cost-efficient manner. While pre-training LLMs require thousands of GPU hours, fine-tuning can be achieved using a handful of GPUs in a relatively short amount of time [6]. This work uses case study of mathematics and common-sense question-answer datasets to demonstrate the fine-tuning process of LLMs.\n\nB. LoRA\n\nLow-Rank Adaption (LoRA) is a technique that freezes the pre-trained model weights and injects trainable rank decomposition into layers of the transformer architecture [14]. LoRA significantly reduces the number of parameters, thereby decreasing the GPU memory footprint. LoRA can be used independently of the aforementioned fine-tuning techniques. In this work, we apply QLoRA [15] to the Mixtral-8x7B model [4]; more details are provided in §III.\n\nC. Mixture of Experts (MoE)\n\nThe quality of an LLM is highly related to its scale. Given a fixed computation budget, it is often desirable to train a model with more parameters to achieve higher accuracy. Mixture-of-Experts (MoE) is a technique that, instead of using one large model for all tasks, combines multiple expert sub-networks into a single, large model. As shown in Fig. 1, with MoE, different sets of experts are selectively activated for different tokens. This approach can significantly reduce the amount of computation required for both training and inference, enabling the scaling up of model size and achieving better model accuracy [16].", "doc_id": "xia2024", "page": 2, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fine-tuning LLMs allows organizations to harness the full potential of advanced AI systems by tailoring them to specific tasks and domains. This customization involves training the model on domain-specific data, enabling it to understand and generate content that aligns closely with the unique needs of the users. For instance, in the healthcare sector, a fine-tuned LLM can assist in diagnosing conditions by interpreting patient data and medical literature with high precision. Another attractive feature of fine-tuning LLMs is that it can be achieved at a cost-efficient manner. While pre-training LLMs require thousands of GPU hours, fine-tuning can be achieved using a handful of GPUs in a relatively short amount of time [6]. This work uses case study of mathematics and common-sense question-answer datasets to demonstrate the fine-tuning process of LLMs.\n\nB. LoRA\n\nLow-Rank Adaption (LoRA) is a technique that freezes the pre-trained model weights and injects trainable rank decomposition into layers of the transformer architecture [14]. LoRA significantly reduces the number of parameters, thereby decreasing the GPU memory footprint. LoRA can be used independently of the aforementioned fine-tuning techniques. In this work, we apply QLoRA [15] to the Mixtral-8x7B model [4]; more details are provided in §III.\n\nC. Mixture of Experts (MoE)\n\nThe quality of an LLM is highly related to its scale. Given a fixed computation budget, it is often desirable to train a model with more parameters to achieve higher accuracy. Mixture-of-Experts (MoE) is a technique that, instead of using one large model for all tasks, combines multiple expert sub-networks into a single, large model. As shown in Fig. 1, with MoE, different sets of experts are selectively activated for different tokens. This approach can significantly reduce the amount of computation required for both training and inference, enabling the scaling up of model size and achieving better model accuracy [16].", "original_types": ["text", "header"], "id": 828}
{"type": "section", "content": "Datasets\n\nOur fine-tuning process is implemented in PyTorch using the LLaMA-Factory framework [19], with a learning rate of 5e-5 and 10 epochs. Both models were fine-tuned on two datasets focused on different tasks: commonsense_15k (CS) and Math_14k (MATH), which address commonsense reasoning and arithmetic reasoning respectively (provided by LLM-adapters [20]). The details of datasets are used in Table II. For evaluation, we tested the models on GSM8K [21] for arithmetic reasoning and HE [22] for commonsense reasoning. Each dataset consists of thousands of queries. We define a query as the concatenation of a prompt and its ground-truth answer, which is feed to LLMs for fine-tuning.\n\nProfiling experiments\n\nWe evaluate the fine-tuning process from both software and hardware perspectives. The software evaluation includes an end-to-end assessment of the fine-tuning process and measures the performance of the two models on various tasks post-fine-tuning. Using PyTorch, we provide essential algorithm-level information such as test accuracy, training throughput, and layer-level latency breakdown. The hardware evaluation offers a detailed analysis of GPU performance. Utilizing NVIDIA Nsight Compute [23], we gather kernel-level information, including SM utilization, memory utilization, and kernel latency. These metrics collectively offer a comprehensive overview of the models' performance, capturing both high-level algorithmic efficiency and detailed hardware utilization. Software evaluation is dataset-dependent, and we will show the test accuracy and fine-tuning throughput by utilizing both datasets. In contrast, hardware evaluation is dataset-independent as these workload characteristics do not depend on runtime data. Because profiling is time-consuming (approximately 10,000× costlier compared to a native run without the profiler enabled), we manually set the batch size and sequence length to facilitate a more direct and efficient profiling process.", "doc_id": "xia2024", "page": 3, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Datasets\n\nOur fine-tuning process is implemented in PyTorch using the LLaMA-Factory framework [19], with a learning rate of 5e-5 and 10 epochs. Both models were fine-tuned on two datasets focused on different tasks: commonsense_15k (CS) and Math_14k (MATH), which address commonsense reasoning and arithmetic reasoning respectively (provided by LLM-adapters [20]). The details of datasets are used in Table II. For evaluation, we tested the models on GSM8K [21] for arithmetic reasoning and HE [22] for commonsense reasoning. Each dataset consists of thousands of queries. We define a query as the concatenation of a prompt and its ground-truth answer, which is feed to LLMs for fine-tuning.\n\nProfiling experiments\n\nWe evaluate the fine-tuning process from both software and hardware perspectives. The software evaluation includes an end-to-end assessment of the fine-tuning process and measures the performance of the two models on various tasks post-fine-tuning. Using PyTorch, we provide essential algorithm-level information such as test accuracy, training throughput, and layer-level latency breakdown. The hardware evaluation offers a detailed analysis of GPU performance. Utilizing NVIDIA Nsight Compute [23], we gather kernel-level information, including SM utilization, memory utilization, and kernel latency. These metrics collectively offer a comprehensive overview of the models' performance, capturing both high-level algorithmic efficiency and detailed hardware utilization. Software evaluation is dataset-dependent, and we will show the test accuracy and fine-tuning throughput by utilizing both datasets. In contrast, hardware evaluation is dataset-independent as these workload characteristics do not depend on runtime data. Because profiling is time-consuming (approximately 10,000× costlier compared to a native run without the profiler enabled), we manually set the batch size and sequence length to facilitate a more direct and efficient profiling process.", "original_types": ["text", "header"], "id": 829}
{"type": "figure", "content": "Fig. 2. Sequence length distribution for evaluated datasets.", "doc_id": "xia2024", "page": 3, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 2. Sequence length distribution for evaluated datasets.", "id": 830}
{"type": "section", "content": "We present the sequence length distribution for the CS and MATH datasets in Fig. 2. The median sequence length is 79 for CS and 174 for MATH. Therefore, we select a sequence length of 128 for the hardware evaluation section to achieve an approximate profiling effect. We also show a sensitivity study by varying sequence length to demonstrate its effect on performance.\n\nGPU platform\n\nOur study is focused on characterizing the LLM fine-tuning process on a resource-constrained environment. Therefore, we focus on fine-tuning these models on a single GPU. Specifically, we conduct our experiments using NVIDIA A40 GPU with Ampere architecture. The GPU has 48GB memory. While our profiling study is based on this particular GPU, we show the versatility of our analytical model by validating our model against three other GPU with different sizes of compute and memory resources: (1) A100 GPU with 40GB memory, (2) A100 GPU with 80GB memory, and (3) H100 GPU with 80GB memory. We use Python v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\n\nIV. Characterization Study\n\nUsing the experimental setup discussed above, next, we conduct an in-depth characterization of LLM fine-tuning to understand both accuracy and runtime behaviors.\n\nA. Analysis of Model Trainability\n\nWe first evaluate if fine-tuning sparse LLM models can achieve the desired accuracy levels. Pre-trained models show low accuracy: HE and GS have under 25% on Mixtral and", "doc_id": "xia2024", "page": 3, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "We present the sequence length distribution for the CS and MATH datasets in Fig. 2. The median sequence length is 79 for CS and 174 for MATH. Therefore, we select a sequence length of 128 for the hardware evaluation section to achieve an approximate profiling effect. We also show a sensitivity study by varying sequence length to demonstrate its effect on performance.\n\nGPU platform\n\nOur study is focused on characterizing the LLM fine-tuning process on a resource-constrained environment. Therefore, we focus on fine-tuning these models on a single GPU. Specifically, we conduct our experiments using NVIDIA A40 GPU with Ampere architecture. The GPU has 48GB memory. While our profiling study is based on this particular GPU, we show the versatility of our analytical model by validating our model against three other GPU with different sizes of compute and memory resources: (1) A100 GPU with 40GB memory, (2) A100 GPU with 80GB memory, and (3) H100 GPU with 80GB memory. We use Python v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\n\nIV. Characterization Study\n\nUsing the experimental setup discussed above, next, we conduct an in-depth characterization of LLM fine-tuning to understand both accuracy and runtime behaviors.\n\nA. Analysis of Model Trainability\n\nWe first evaluate if fine-tuning sparse LLM models can achieve the desired accuracy levels. Pre-trained models show low accuracy: HE and GS have under 25% on Mixtral and", "original_types": ["text", "header"], "id": 831}
{"type": "section", "content": "Analysis of Runtime Performance\n\nAfter confirming that both Mixtral and BlackMamba can be fine-tuned to achieve acceptable accuracy, we examine their performance in a resource-constrained environment using a single GPU. This setup highlights unique runtime characteristics such as execution time breakdown, throughput, maximum batch size, compute and memory utilization, load imbalance, and sensitivity analysis. We also compare sparse and dense models. Insights from this study will help develop a robust analytical model for estimating fine-tuning costs.", "doc_id": "xia2024", "page": 4, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Analysis of Runtime Performance\n\nAfter confirming that both Mixtral and BlackMamba can be fine-tuned to achieve acceptable accuracy, we examine their performance in a resource-constrained environment using a single GPU. This setup highlights unique runtime characteristics such as execution time breakdown, throughput, maximum batch size, compute and memory utilization, load imbalance, and sensitivity analysis. We also compare sparse and dense models. Insights from this study will help develop a robust analytical model for estimating fine-tuning costs.", "original_types": ["text", "header"], "id": 832}
{"type": "table", "content": "Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse.\nCS | Mixtral-D | Mixtral-S | BlackMamba-D | BlackMamba-S\n\nCS | 2 | 8 | 6 | 20\nMATH | 1 | 3 | 2 | 8\n\n", "doc_id": "xia2024", "page": 4, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Maximum batch size supported by LLM fine-tuning; D: dense and S:sparse.\nCS | Mixtral-D | Mixtral-S | BlackMamba-D | BlackMamba-S\n\nCS | 2 | 8 | 6 | 20\nMATH | 1 | 3 | 2 | 8\n\n", "id": 833}
{"type": "figure", "content": "Execution time breakdown.", "doc_id": "xia2024", "page": 4, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Execution time breakdown.", "id": 834}
{"type": "figure", "content": "Fig. 5. Execution time breakdown in terms of different model layers.", "doc_id": "xia2024", "page": 5, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 5. Execution time breakdown in terms of different model layers.", "id": 835}
{"type": "figure", "content": "Fig. 6. Execution breakdown of the MoE layer for different kernels.", "doc_id": "xia2024", "page": 5, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 6. Execution breakdown of the MoE layer for different kernels.", "id": 836}
{"type": "figure", "content": "Fig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).", "doc_id": "xia2024", "page": 5, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).", "id": 837}
{"type": "figure", "content": "Fig. 8. Query throughput of Mixtral and BlackMamba.", "doc_id": "xia2024", "page": 6, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 8. Query throughput of Mixtral and BlackMamba.", "id": 838}
{"type": "figure", "content": "Fig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.", "doc_id": "xia2024", "page": 6, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.", "id": 839}
{"type": "section", "content": "Takeaway 4. Sparse model significantly improves throughput, reducing end-to-end cost of fine-tuning.\n\n4) Hardware characterization: As shown in Fig. 4, the execution time of LLM fine-tuning is dominated by the MoE layer. To offer further insights, we use detailed microarchitecture hardware metrics on the GPU to further understand execution bottlenecks in the MoE layer. The goal of this study is to identify whether various kernels in the MoE layers are bound by compute or memory resources, and how future GPU designs can further scale performance.\n\nCompute resource utilization study. Fig. 9 shows the kernel-level breakdown of GPU Streaming Multi-processor (SM) utilization for the MoE layer. This utilization is", "doc_id": "xia2024", "page": 6, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Takeaway 4. Sparse model significantly improves throughput, reducing end-to-end cost of fine-tuning.\n\n4) Hardware characterization: As shown in Fig. 4, the execution time of LLM fine-tuning is dominated by the MoE layer. To offer further insights, we use detailed microarchitecture hardware metrics on the GPU to further understand execution bottlenecks in the MoE layer. The goal of this study is to identify whether various kernels in the MoE layers are bound by compute or memory resources, and how future GPU designs can further scale performance.\n\nCompute resource utilization study. Fig. 9 shows the kernel-level breakdown of GPU Streaming Multi-processor (SM) utilization for the MoE layer. This utilization is", "original_types": ["text"], "id": 840}
{"type": "section", "content": "GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.", "doc_id": "xia2024", "page": 7, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.", "original_types": ["header"], "id": 841}
{"type": "figure", "content": "Fig. 10.", "doc_id": "xia2024", "page": 7, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 10.", "id": 842}
{"type": "section", "content": "Fig. 12. Pseudo code for MoE layers.\n\n6) Sensitivity Study on Sequence Length: To further analyze the effect of sequence length on the fine-tuning process, we chose the batch size that would maximize the memory for each sequence length (64, 128, 256, 512, and 1024) and compared the latency, SM utilization, and DRAM utilization. Our evaluation (the figure is omitted from the paper due to page limitation) shows that the latency for Mixtral remains almost constant across different sequence lengths, while BlackMamba fine-tuning exhibited a slight reduction in latency as sequence length increased, with approximately 19% and 25% decreases for sparse and dense fine-tuning, respectively. This is due to the varying maximum batch sizes supported by each sequence length, resulting in a similar number of tokens in each batch. Because latency remains consistent with increasing sequence length and we can use larger batch sizes, throughput is higher for shorter sequences.\n\nV. Analytical Model to Estimate the Cost of Fine-Tuning LLMs", "doc_id": "xia2024", "page": 8, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 12. Pseudo code for MoE layers.\n\n6) Sensitivity Study on Sequence Length: To further analyze the effect of sequence length on the fine-tuning process, we chose the batch size that would maximize the memory for each sequence length (64, 128, 256, 512, and 1024) and compared the latency, SM utilization, and DRAM utilization. Our evaluation (the figure is omitted from the paper due to page limitation) shows that the latency for Mixtral remains almost constant across different sequence lengths, while BlackMamba fine-tuning exhibited a slight reduction in latency as sequence length increased, with approximately 19% and 25% decreases for sparse and dense fine-tuning, respectively. This is due to the varying maximum batch sizes supported by each sequence length, resulting in a similar number of tokens in each batch. Because latency remains consistent with increasing sequence length and we can use larger batch sizes, throughput is higher for shorter sequences.\n\nV. Analytical Model to Estimate the Cost of Fine-Tuning LLMs", "original_types": ["text"], "id": 843}
{"type": "figure", "content": "Fig. 13. Projected maximum batch size of Mixtral for different GPUs.", "doc_id": "xia2024", "page": 9, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 13. Projected maximum batch size of Mixtral for different GPUs.", "id": 844}
{"type": "section", "content": "Moreover, the sequence length and sparsity also affect the maximum batch size. Because the sparsity only affects the MoE part of the LLM, we multiply its influence by C1, which we call MoE coefficient. We apply the sequence length and the sparsity in the denominator as they are inversely related to batch size. Then, we multiply the result by C0, the scaling coefficient, which scales the batch size by a constant. The scaling coefficient is different across LLM models, because different models have different architecture (§III), and generate different amounts of intermediate data for each query. The scaling coefficient for BlackMamba is higher than that of Mixtral because it is a smaller model. Finally, we use floor to round it to the maximum integer.\n\nThe MoE coefficient and scaling coefficient vary across models. These coefficients are independent of GPU microarchitectural parameters. We find the maximum batch size for both LLM models on NVIDIA A40 (48GB), A100 (40GB), A100 (80GB), and H100 (80GB), and apply our model to find the optimal coefficients. For Mixtral, C0 = 82 and C1 = 0.95, and for BlackMamba, C0 = 83 and C1 = 0.88. While we showcase these parameters for the models evaluated, §V-D discusses how to generalize this approach for other models.\n\nUsing our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively. Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth. While the maximum memory capacity available in NVIDIA GPUs today is 80GB, we use our analytical model to project the maximum batch size that future GPUs might support. For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.", "doc_id": "xia2024", "page": 9, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Moreover, the sequence length and sparsity also affect the maximum batch size. Because the sparsity only affects the MoE part of the LLM, we multiply its influence by C1, which we call MoE coefficient. We apply the sequence length and the sparsity in the denominator as they are inversely related to batch size. Then, we multiply the result by C0, the scaling coefficient, which scales the batch size by a constant. The scaling coefficient is different across LLM models, because different models have different architecture (§III), and generate different amounts of intermediate data for each query. The scaling coefficient for BlackMamba is higher than that of Mixtral because it is a smaller model. Finally, we use floor to round it to the maximum integer.\n\nThe MoE coefficient and scaling coefficient vary across models. These coefficients are independent of GPU microarchitectural parameters. We find the maximum batch size for both LLM models on NVIDIA A40 (48GB), A100 (40GB), A100 (80GB), and H100 (80GB), and apply our model to find the optimal coefficients. For Mixtral, C0 = 82 and C1 = 0.95, and for BlackMamba, C0 = 83 and C1 = 0.88. While we showcase these parameters for the models evaluated, §V-D discusses how to generalize this approach for other models.\n\nUsing our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively. Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth. While the maximum memory capacity available in NVIDIA GPUs today is 80GB, we use our analytical model to project the maximum batch size that future GPUs might support. For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.", "original_types": ["text"], "id": 845}
{"type": "figure", "content": "Fig. 14. Estimation and validation of LLM fine-tuning throughput for different models, datasets for A40 GPU. Dots represent ground truth and lines present the estimation.", "doc_id": "xia2024", "page": 10, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 14. Estimation and validation of LLM fine-tuning throughput for different models, datasets for A40 GPU. Dots represent ground truth and lines present the estimation.", "id": 846}
{"type": "figure", "content": "Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.", "doc_id": "xia2024", "page": 10, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.", "id": 847}
{"type": "section", "content": "resource renting per hour is calculated based on CUDO compute [33], as other popular cloud providers do not offer cost/hour rates for the NVIDIA A40 GPU. However, one can easily adjust the GPU renting cost per hour to estimate the cost on other clouds such as Amazon AWS [34] or Lambda [35]. Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. Enterprises may use larger datasets for fine-tuning, such as, OpenOrca [36] and LaMini-instruction [37] containing more than 2M queries. For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.\n\nD. Generalization of the Analytical Model\n\nThe analytical models for estimating maximum batch size and throughput can be generalized to various LLM models and datasets. These models consider the characteristics of the LLM, dataset, and GPU. Specifically, the maximum batch size model combines GPU memory and LLM model size to determine available memory for input data, while dataset sequence length and LLM sparsity determine space needed per batch. In throughput estimation, based on the observation we made (§IV-B4 Takeaway 5), GPU shifts from memory-bound to compute-bound as batch size increases. This characteristic generally applies to all GPUs due to the resource constraint, so the logarithmic relation between batch size and throughput persists. The sparsity in (2) is model dependent, the influence of GPU, LLM model, and dataset are embedded in the coefficients C2, C3, and C4 in (2).\n\nThe coefficients in (1) and (2) are dependent on GPU, LLM model, and dataset; however, the underlying models are generalizable to unseen GPU, LLM model, and datasets. Although it takes some effort to sweep batch sizes and collect throughput data points to fit our models, the benefits greatly outweigh the cost. Once the models are fit, our model can help choose the most cost-efficient GPU for fine-tuning LLM models, greatly saving resources and money.\n\nVI. RELATED WORKS\n\nParameter-Efficient Fine-Tuning (PEFT) has been widely adopted to fine-tune LLM model for specialized tasks [15], [38]–[43]. MoE additioally train specialized experts for different areas and the dynamic selection of experts makes it possible to scale the fine-tuning workload to different experts in parallel. [44]–[47] show that MoE models can improve the ability to process knowledge for specific tasks, while maintaining the world knowledge in LLM. Kim et al. [48] construct an analytical model to estimate GPU memory consumption for distributed fine-tuning. The model also provides insights into optimizing memory usage through tensor, model, and pipeline parallelism.\n\nVII. CONCLUSIONS", "doc_id": "xia2024", "page": 10, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "resource renting per hour is calculated based on CUDO compute [33], as other popular cloud providers do not offer cost/hour rates for the NVIDIA A40 GPU. However, one can easily adjust the GPU renting cost per hour to estimate the cost on other clouds such as Amazon AWS [34] or Lambda [35]. Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. Enterprises may use larger datasets for fine-tuning, such as, OpenOrca [36] and LaMini-instruction [37] containing more than 2M queries. For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.\n\nD. Generalization of the Analytical Model\n\nThe analytical models for estimating maximum batch size and throughput can be generalized to various LLM models and datasets. These models consider the characteristics of the LLM, dataset, and GPU. Specifically, the maximum batch size model combines GPU memory and LLM model size to determine available memory for input data, while dataset sequence length and LLM sparsity determine space needed per batch. In throughput estimation, based on the observation we made (§IV-B4 Takeaway 5), GPU shifts from memory-bound to compute-bound as batch size increases. This characteristic generally applies to all GPUs due to the resource constraint, so the logarithmic relation between batch size and throughput persists. The sparsity in (2) is model dependent, the influence of GPU, LLM model, and dataset are embedded in the coefficients C2, C3, and C4 in (2).\n\nThe coefficients in (1) and (2) are dependent on GPU, LLM model, and dataset; however, the underlying models are generalizable to unseen GPU, LLM model, and datasets. Although it takes some effort to sweep batch sizes and collect throughput data points to fit our models, the benefits greatly outweigh the cost. Once the models are fit, our model can help choose the most cost-efficient GPU for fine-tuning LLM models, greatly saving resources and money.\n\nVI. RELATED WORKS\n\nParameter-Efficient Fine-Tuning (PEFT) has been widely adopted to fine-tune LLM model for specialized tasks [15], [38]–[43]. MoE additioally train specialized experts for different areas and the dynamic selection of experts makes it possible to scale the fine-tuning workload to different experts in parallel. [44]–[47] show that MoE models can improve the ability to process knowledge for specific tasks, while maintaining the world knowledge in LLM. Kim et al. [48] construct an analytical model to estimate GPU memory consumption for distributed fine-tuning. The model also provides insights into optimizing memory usage through tensor, model, and pipeline parallelism.\n\nVII. CONCLUSIONS", "original_types": ["text", "header"], "id": 848}
{"type": "section", "content": "Fine-tuning LLMs is an attractive technique for tailoring modern language models using domain-specific knowledge in a cost-effective manner. This paper delved into understanding the performance of fine-tuning MoE LLM models on a single GPU. Our profiling demonstrated that sparse MoE layers offer the best bang-for-buck trade-off. Using our profiling results, we developed and validated an accurate analytical model to estimate the cost of LLM fine-tuning. Using this model, we showed the dollar amount that needs to be budgeted for fine-tuning LLMs, which is much lower than pre-training. For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers. While we showcase our study on fine-tuning LLMs using a single GPU, extending this model to multi-GPU systems is left for future exploration.", "doc_id": "xia2024", "page": 10, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Fine-tuning LLMs is an attractive technique for tailoring modern language models using domain-specific knowledge in a cost-effective manner. This paper delved into understanding the performance of fine-tuning MoE LLM models on a single GPU. Our profiling demonstrated that sparse MoE layers offer the best bang-for-buck trade-off. Using our profiling results, we developed and validated an accurate analytical model to estimate the cost of LLM fine-tuning. Using this model, we showed the dollar amount that needs to be budgeted for fine-tuning LLMs, which is much lower than pre-training. For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers. While we showcase our study on fine-tuning LLMs using a single GPU, extending this model to multi-GPU systems is left for future exploration.", "original_types": ["text"], "id": 849}
{"type": "section", "content": "References\n\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.\n\n[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen Chu. Dissecting the runtime performance of the training, fine-tuning, and inference of large language models, 2023.\n\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n\n[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.\n\n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.\n\n[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning, 2023.\n\n[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning design spaces, 2023.\n\n[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024.\n\n[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n\n[10] Introducing chatgpt. https://openai.com/index/chatgpt.\n\n[11] Josh Achiam et.al. Gpt-4 technical report, 2024.\n\n[12] Introducing the next generation of claude. https://www.anthropic.com/news/claude-3-family.\n\n[13] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.", "doc_id": "xia2024", "page": 11, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "References\n\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.\n\n[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen Chu. Dissecting the runtime performance of the training, fine-tuning, and inference of large language models, 2023.\n\n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n\n[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.\n\n[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.\n\n[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning, 2023.\n\n[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning design spaces, 2023.\n\n[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024.\n\n[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n\n[10] Introducing chatgpt. https://openai.com/index/chatgpt.\n\n[11] Josh Achiam et.al. Gpt-4 technical report, 2024.\n\n[12] Introducing the next generation of claude. https://www.anthropic.com/news/claude-3-family.\n\n[13] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.", "original_types": ["text", "header"], "id": 850}
{"type": "section", "content": "[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n\n[16] Amin Vahdat. Societal infrastructure in the age of artificial general intelligence. ASPLOS 2024 Keynote, 2024.\n\n[17] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\n\n[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.\n\n[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024.\n\n[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023.\n\n[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.\n\n[23] Nvidia nsight compute. https://developer.nvidia.com/nsight-compute.\n\n[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges, 2024.\n\n[25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024.\n\n[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. One student knows all experts know: From sparse to dense, 2022.\n\n[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. CoRR, abs/2206.03382, June 2022.\n\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n\n[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n\n[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R. Gao. Dynamic load balancing on single- and multi-gpu systems. In 2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS), pages 1–12, 2010.", "doc_id": "xia2024", "page": 11, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\n[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n\n[16] Amin Vahdat. Societal infrastructure in the age of artificial general intelligence. ASPLOS 2024 Keynote, 2024.\n\n[17] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\n\n[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.\n\n[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024.\n\n[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023.\n\n[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.\n\n[23] Nvidia nsight compute. https://developer.nvidia.com/nsight-compute.\n\n[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges, 2024.\n\n[25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024.\n\n[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. One student knows all experts know: From sparse to dense, 2022.\n\n[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. CoRR, abs/2206.03382, June 2022.\n\n[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n\n[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n\n[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R. Gao. Dynamic load balancing on single- and multi-gpu systems. In 2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS), pages 1–12, 2010.", "original_types": ["text"], "id": 851}
{"type": "section", "content": "[31] Mohamed Wahib, Muhammet Abdullah Soytürk, and Didem Unat. Elastic load balancing for dynamic LLMs, 2024.\n\n[32] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020.\n\n[33] CUDO compute: https://www.cudocompute.com.\n\n[34] Amazon AWS: https://aws.amazon.com.\n\n[35] Lambda: https://www.gpus.com.\n\n[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.\n\n[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions, 2024.\n\n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\n[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.\n\n[40] Shuai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n\n[41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.", "doc_id": "xia2024", "page": 11, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "[31] Mohamed Wahib, Muhammet Abdullah Soytürk, and Didem Unat. Elastic load balancing for dynamic LLMs, 2024.\n\n[32] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020.\n\n[33] CUDO compute: https://www.cudocompute.com.\n\n[34] Amazon AWS: https://aws.amazon.com.\n\n[35] Lambda: https://www.gpus.com.\n\n[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.\n\n[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions, 2024.\n\n[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\n[39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.\n\n[40] Shuai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n\n[41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.", "original_types": ["text"], "id": 852}
{"type": "section", "content": "[42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.\n\n[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora: High-rank updating for parameter-efficient fine-tuning, 2024.\n\n[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models. arXiv preprint arXiv:2404.05567, 2024.\n\n[45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe: Alleviate world knowledge forgetting in large language models via moe-style plugin, 2024.\n\n[46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\n\n[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.\n\n[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta, Seyeon Kim, Yongin Kwon, and Sangtae Ha. Llmem: Estimating gpu memory usage for fine-tuning pre-trained llms, 2024.", "doc_id": "xia2024", "page": 12, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "[42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.\n\n[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora: High-rank updating for parameter-efficient fine-tuning, 2024.\n\n[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models. arXiv preprint arXiv:2404.05567, 2024.\n\n[45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe: Alleviate world knowledge forgetting in large language models via moe-style plugin, 2024.\n\n[46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\n\n[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.\n\n[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta, Seyeon Kim, Yongin Kwon, and Sangtae Ha. Llmem: Estimating gpu memory usage for fine-tuning pre-trained llms, 2024.", "original_types": ["text"], "id": 853}
{"type": "section", "content": "Abstract\n\nThis artifact reproduces the results presented in the Characterization Study. It includes a detailed three-level runtime breakdown, analysis of SM and MEM utilization, and a comprehensive study of throughput.", "doc_id": "xia2024", "page": 13, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "Abstract\n\nThis artifact reproduces the results presented in the Characterization Study. It includes a detailed three-level runtime breakdown, analysis of SM and MEM utilization, and a comprehensive study of throughput.", "original_types": ["text", "header"], "id": 854}
{"type": "section", "content": "High-level and layer-level latency breakdown results shown in Fig. 4 and 5 can be obtained by running:\n\n./mixtral_lt.sh\n\npython3 mixtral_latency.py ./profile_data/mixtral/latency > mixtral_latency_breakdown.txt\n\n./mamba_lt.sh\n\npython3 mamba_latency.py ./profile_data/blackmamba/latency > mamba_latency_breakdown.txt\n\nYou can also use Nsight Compute to profile and generate kernel-level latency breakdown, SM and MEM utilization results shown in Fig. 6, 9 and 10 by running:\n\n./mixtral_pf.sh\n\npython3 sm_mixtral.py ./profile_data/mixtral/ncu > mixtral_sm.txt\n\npython3 mem_mixtral.py ./profile_data/mixtral/ncu > mixtral_mem.txt\n\n./mamba_pf.sh\n\npython3 sm_mamba.py ./profile_data/blackmamba/ncu > mamba_sm.txt\n\npython3 mem_mamba.py ./profile_data/blackmamba/ncu > mamba_mem.txt\n\npython3 sm_mamba_back.py ./profile_data/blackmamba/ncu_back > mamba_sm_backward.txt\n\npython3 mem_mamba_back.py ./profile_data/blackmamba/ncu_back > mamba_mem_backward.txt\n\nF. Evaluation and expected results\n\nThe generated results are stored in specific text files as indicated in the commands above, such as mixtral_sm.txt for SM utilization data of the Mixtral model.\n\nG. Experiment customization\n\nCustomized experiments can be conducted with varying batch sizes and query sequence lengths, both of which can be adjusted in each bash script.\n\nH. Methodology\n\nSubmission, reviewing and badging methodology:", "doc_id": "xia2024", "page": 14, "url": "https://arxiv.org/pdf/2408.04693", "embedded_text": "High-level and layer-level latency breakdown results shown in Fig. 4 and 5 can be obtained by running:\n\n./mixtral_lt.sh\n\npython3 mixtral_latency.py ./profile_data/mixtral/latency > mixtral_latency_breakdown.txt\n\n./mamba_lt.sh\n\npython3 mamba_latency.py ./profile_data/blackmamba/latency > mamba_latency_breakdown.txt\n\nYou can also use Nsight Compute to profile and generate kernel-level latency breakdown, SM and MEM utilization results shown in Fig. 6, 9 and 10 by running:\n\n./mixtral_pf.sh\n\npython3 sm_mixtral.py ./profile_data/mixtral/ncu > mixtral_sm.txt\n\npython3 mem_mixtral.py ./profile_data/mixtral/ncu > mixtral_mem.txt\n\n./mamba_pf.sh\n\npython3 sm_mamba.py ./profile_data/blackmamba/ncu > mamba_sm.txt\n\npython3 mem_mamba.py ./profile_data/blackmamba/ncu > mamba_mem.txt\n\npython3 sm_mamba_back.py ./profile_data/blackmamba/ncu_back > mamba_sm_backward.txt\n\npython3 mem_mamba_back.py ./profile_data/blackmamba/ncu_back > mamba_mem_backward.txt\n\nF. Evaluation and expected results\n\nThe generated results are stored in specific text files as indicated in the commands above, such as mixtral_sm.txt for SM utilization data of the Mixtral model.\n\nG. Experiment customization\n\nCustomized experiments can be conducted with varying batch sizes and query sequence lengths, both of which can be adjusted in each bash script.\n\nH. Methodology\n\nSubmission, reviewing and badging methodology:", "original_types": ["text", "header"], "id": 855}
{"type": "section", "content": "Measuring the Carbon Intensity of AI in Cloud Instances\n\nJESSE DODGE, Allen Institute for AI, USA\n\nTAYLOR PREWITT, University of Washington, USA\n\nREMI TACHET DES COMBES, Microsoft Research Montreal, USA\n\nERIKA ODMARK, Microsoft, USA\n\nROY SCHWARTZ, Hebrew University of Jerusalem, Israel\n\nEMMA STRUBELL, Carnegie Mellon University, USA\n\nALEXANDRA SASHA LUCCIONI, Hugging Face, USA\n\nNOAH A. SMITH, Allen Institute for AI and University of Washington, USA\n\nNICOLE DECARIO, Allen Institute for AI, USA\n\nWILL BUCHANAN, Microsoft, USA\n\nThe advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions.\n\nIn this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.\n\nAdditional Key Words and Phrases: CO2, emissions, cloud, carbon intensity, carbon awareness, grid\n\nACM Reference Format:", "doc_id": "dodge2022", "page": 1, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Measuring the Carbon Intensity of AI in Cloud Instances\n\nJESSE DODGE, Allen Institute for AI, USA\n\nTAYLOR PREWITT, University of Washington, USA\n\nREMI TACHET DES COMBES, Microsoft Research Montreal, USA\n\nERIKA ODMARK, Microsoft, USA\n\nROY SCHWARTZ, Hebrew University of Jerusalem, Israel\n\nEMMA STRUBELL, Carnegie Mellon University, USA\n\nALEXANDRA SASHA LUCCIONI, Hugging Face, USA\n\nNOAH A. SMITH, Allen Institute for AI and University of Washington, USA\n\nNICOLE DECARIO, Allen Institute for AI, USA\n\nWILL BUCHANAN, Microsoft, USA\n\nThe advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions.\n\nIn this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.\n\nAdditional Key Words and Phrases: CO2, emissions, cloud, carbon intensity, carbon awareness, grid\n\nACM Reference Format:", "original_types": ["text", "header"], "id": 856}
{"type": "section", "content": "Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the Carbon Intensity of AI in Cloud Instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21–24, 2022, Seoul, Republic of Korea. ACM, New York, NY, USA, 25 pages. https://doi.org/10.1145/3531146.3533234\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n© 2022 Copyright held by the owner/author(s).\n\nManuscript submitted to ACM", "doc_id": "dodge2022", "page": 1, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the Carbon Intensity of AI in Cloud Instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21–24, 2022, Seoul, Republic of Korea. ACM, New York, NY, USA, 25 pages. https://doi.org/10.1145/3531146.3533234\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n\n© 2022 Copyright held by the owner/author(s).\n\nManuscript submitted to ACM", "original_types": ["text"], "id": 857}
{"type": "section", "content": "1. INTRODUCTION", "doc_id": "dodge2022", "page": 2, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "1. INTRODUCTION", "original_types": ["header"], "id": 858}
{"type": "section", "content": "Climate change is an increasing threat to life on our planet, which disproportionately impacts the most disadvantaged communities and fragile ecosystems [28]. One of the main drivers of climate change is carbon dioxide, or CO2, which contributes to the greenhouse effect by trapping the heat from the sun within the atmosphere without letting it dissipate. CO2 (and other types of greenhouse gases, such as methane and ozone) are emitted by many sources, some natural but most man-made, such as the burning of oil and gas for transportation and heating or for industrial processes such as smelting. In 2018, it was estimated that global data center energy use represented close to 1% of global energy usage [27]. While it is not yet known what proportion of data center use is for training artificial intelligence (AI) models, it is undeniable that AI and its sub-fields have grown dramatically in recent years, with no sign of slowing down [39, 43]. While a number of papers have addressed the CO2 emissions produced by AI (e.g., [23, 26, 33, 34]), the extent and provenance of CO2 emissions in the field is still under-explored. Nonetheless, a common theme of previous work is that it aims to estimate the emissions produced by training AI models, or carrying out the accompanying neural architecture search (NAS) process, based on coarse measures such as CO2 emissions of electricity used in the region where the computations were carried out (e.g., [42]), or post-hoc analyses using information that is not publicly available (e.g., [34]).\n\nWith an increasing amount of AI model training being done on cloud compute instances, reducing the emissions generated by these workloads will be key to reducing our carbon footprint as a field. However, to reduce greenhouse gas emissions from cloud computing, we need consider the role of two types of actors: the cloud provider (such as Microsoft Azure, Google’s GCP, or Amazon’s AWS) and the user who reserves and uses cloud resources (e.g., an AI researcher training a model on a cloud instance, or a company hosting a website). Typically, the provider’s motivation is to build a system where users can access the computing power and storage that best meets their needs. The user, on the other hand, is motivated by some end task which requires computing power, such as running a set of experiments or putting a model into production. Often the user will first consider the minimal computational requirements to achieve their goals, then later ease-of-use features relating to transfer speed or extra storage depending on available budget. Driven by these motivations, providers and users can each take actions to meet their goals: providers can build data centers and set up APIs to enable users’ access and accounting, while users can choose their cloud provider, which region to use, and the number and type of cloud instances required for their end task at a given point in time. Based on these stakeholders and motivations, in this work we address the following research questions: 1) how should we measure and report operational carbon costs of AI workloads? And 2) can we shift computation spatially and temporally to mitigate emissions?\n\nIn this article, we introduce the first tool to estimate the real-time CO2 emissions impact of instances on a cloud computing platform. The tool calculates operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. Using the tool, we explore several case studies on the Microsoft Azure cloud compute platform spanning the areas of natural language processing (NLP) and computer vision, estimating the carbon intensity of training a variety of commonly used machine learning models. We also explore two avenues for users of cloud instances to reduce their CO2 using this tool by: (1) Changing the region of compute and (2) changing the time of day during which the model is run. While the former has been recognized by prior work [13, 23], we are the first to address the latter to the best of our knowledge. Further, our tool makes it possible to automatically schedule jobs in order to reduce their carbon footprint by leveraging these differences in carbon intensity due to time and geographic location. Finally, we provide guidance regarding what should be measured and how, following the Green Software", "doc_id": "dodge2022", "page": 2, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Climate change is an increasing threat to life on our planet, which disproportionately impacts the most disadvantaged communities and fragile ecosystems [28]. One of the main drivers of climate change is carbon dioxide, or CO2, which contributes to the greenhouse effect by trapping the heat from the sun within the atmosphere without letting it dissipate. CO2 (and other types of greenhouse gases, such as methane and ozone) are emitted by many sources, some natural but most man-made, such as the burning of oil and gas for transportation and heating or for industrial processes such as smelting. In 2018, it was estimated that global data center energy use represented close to 1% of global energy usage [27]. While it is not yet known what proportion of data center use is for training artificial intelligence (AI) models, it is undeniable that AI and its sub-fields have grown dramatically in recent years, with no sign of slowing down [39, 43]. While a number of papers have addressed the CO2 emissions produced by AI (e.g., [23, 26, 33, 34]), the extent and provenance of CO2 emissions in the field is still under-explored. Nonetheless, a common theme of previous work is that it aims to estimate the emissions produced by training AI models, or carrying out the accompanying neural architecture search (NAS) process, based on coarse measures such as CO2 emissions of electricity used in the region where the computations were carried out (e.g., [42]), or post-hoc analyses using information that is not publicly available (e.g., [34]).\n\nWith an increasing amount of AI model training being done on cloud compute instances, reducing the emissions generated by these workloads will be key to reducing our carbon footprint as a field. However, to reduce greenhouse gas emissions from cloud computing, we need consider the role of two types of actors: the cloud provider (such as Microsoft Azure, Google’s GCP, or Amazon’s AWS) and the user who reserves and uses cloud resources (e.g., an AI researcher training a model on a cloud instance, or a company hosting a website). Typically, the provider’s motivation is to build a system where users can access the computing power and storage that best meets their needs. The user, on the other hand, is motivated by some end task which requires computing power, such as running a set of experiments or putting a model into production. Often the user will first consider the minimal computational requirements to achieve their goals, then later ease-of-use features relating to transfer speed or extra storage depending on available budget. Driven by these motivations, providers and users can each take actions to meet their goals: providers can build data centers and set up APIs to enable users’ access and accounting, while users can choose their cloud provider, which region to use, and the number and type of cloud instances required for their end task at a given point in time. Based on these stakeholders and motivations, in this work we address the following research questions: 1) how should we measure and report operational carbon costs of AI workloads? And 2) can we shift computation spatially and temporally to mitigate emissions?\n\nIn this article, we introduce the first tool to estimate the real-time CO2 emissions impact of instances on a cloud computing platform. The tool calculates operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. Using the tool, we explore several case studies on the Microsoft Azure cloud compute platform spanning the areas of natural language processing (NLP) and computer vision, estimating the carbon intensity of training a variety of commonly used machine learning models. We also explore two avenues for users of cloud instances to reduce their CO2 using this tool by: (1) Changing the region of compute and (2) changing the time of day during which the model is run. While the former has been recognized by prior work [13, 23], we are the first to address the latter to the best of our knowledge. Further, our tool makes it possible to automatically schedule jobs in order to reduce their carbon footprint by leveraging these differences in carbon intensity due to time and geographic location. Finally, we provide guidance regarding what should be measured and how, following the Green Software", "id": 859}
{"type": "section", "content": "2. RELATED WORK\n\nAttention was first drawn to the environmental impact of AI research by the seminal work of Strubell et al. [42], which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars. Patterson et al. [34] presented some updated analyses of similar experiments, including popular architectures like T5 [35] and BERT [8], analyzing CO2 emissions as a factor of their energy consumption, carbon intensity of training servers, etc. Other work such as Green AI [39] delved further into inequality of access to computational resources within the research community, and advocated for the inclusion of efficiency evaluation alongside accuracy as a primary evaluation criterion. Much existing and ongoing work on quantifying the environmental footprint of ML has been focused on estimating the CO2 emissions of model training. This is a more straightforward endeavor compared to other stages both upstream and downstream from the training process, given that it is well-defined in time and its emissions can be measured in real-time with tools like Code Carbon [38] and Carbon Tracker [1] or estimated post-hoc using tools such as ML CO2 Impact Tracker [23]. Our tool builds upon this work by making carbon tracking on cloud instances possible, enabling a larger portion of ML model training work to profit from fine-grained carbon estimation. However, recent work has found that their results vary significantly and are not fully representative of the true emissions incurred by training [3]. Perhaps most similar to our work, EnergyVis [41] is an interactive tool for visualizing and comparing energy consumption of ML models as a function of hardware and physical location (U.S. state), given metadata about a model’s energy use per epoch. Other studies have gone beyond simply tracking the emissions from training models, aiming to quantify the emissions resulting from manufacturing computing hardware [15], the broader impacts of sustainable AI [49], and the methodologies used to assess those impacts [21, 26]. Building upon this research, efforts have also been made to certify systems as being socially- and environmentally-conscious [14], working towards comparing both the environmental costs and potential benefits of AI models in order to paint a more holistic picture of AI.", "doc_id": "dodge2022", "page": 3, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "2. RELATED WORK\n\nAttention was first drawn to the environmental impact of AI research by the seminal work of Strubell et al. [42], which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars. Patterson et al. [34] presented some updated analyses of similar experiments, including popular architectures like T5 [35] and BERT [8], analyzing CO2 emissions as a factor of their energy consumption, carbon intensity of training servers, etc. Other work such as Green AI [39] delved further into inequality of access to computational resources within the research community, and advocated for the inclusion of efficiency evaluation alongside accuracy as a primary evaluation criterion. Much existing and ongoing work on quantifying the environmental footprint of ML has been focused on estimating the CO2 emissions of model training. This is a more straightforward endeavor compared to other stages both upstream and downstream from the training process, given that it is well-defined in time and its emissions can be measured in real-time with tools like Code Carbon [38] and Carbon Tracker [1] or estimated post-hoc using tools such as ML CO2 Impact Tracker [23]. Our tool builds upon this work by making carbon tracking on cloud instances possible, enabling a larger portion of ML model training work to profit from fine-grained carbon estimation. However, recent work has found that their results vary significantly and are not fully representative of the true emissions incurred by training [3]. Perhaps most similar to our work, EnergyVis [41] is an interactive tool for visualizing and comparing energy consumption of ML models as a function of hardware and physical location (U.S. state), given metadata about a model’s energy use per epoch. Other studies have gone beyond simply tracking the emissions from training models, aiming to quantify the emissions resulting from manufacturing computing hardware [15], the broader impacts of sustainable AI [49], and the methodologies used to assess those impacts [21, 26]. Building upon this research, efforts have also been made to certify systems as being socially- and environmentally-conscious [14], working towards comparing both the environmental costs and potential benefits of AI models in order to paint a more holistic picture of AI.", "original_types": ["text", "header"], "id": 860}
{"type": "section", "content": "Major technology companies have also been increasingly committed to reducing their emissions, largely via the purchase of Renewable Energy Credits (RECs), which involves directly buying quantities of energy produced by renewable sources, translating into carbon reductions under the assumption that the clean energy is displacing an equivalent amount of electricity produced by non-renewable methods [11]. Many cloud providers, from Google Cloud Platform to Microsoft Azure, therefore claim that they are now “carbon-neutral,” given that they offset the entirety of the emissions of their cloud centers, though we must be wary of the precise provenance of RECs, and the details of how each organization defines “zero” net emissions [36]. This is complemented by efforts to mitigate the actual CO2 emissions of the compute regions themselves, with several server locations partially powered by renewable energy sources such as solar and wind [12, 29, 40] and giving users the necessary tools to pick compute regions with a smaller carbon footprint [13, 17], which are often tied to the amount of low-carbon energy that is being purchased, and not the grid emissions intensity. It is important to note that the decision on when and where to deploy a workload should be based on a grid emissions signal, not the amount of emissions offset through market-based measures (e.g., green power purchase agreements (PPAs), renewable energy certificates (RECs), or other carbon offset mechanisms): purchasing clean energy is not the same as consuming clean energy.", "doc_id": "dodge2022", "page": 3, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Major technology companies have also been increasingly committed to reducing their emissions, largely via the purchase of Renewable Energy Credits (RECs), which involves directly buying quantities of energy produced by renewable sources, translating into carbon reductions under the assumption that the clean energy is displacing an equivalent amount of electricity produced by non-renewable methods [11]. Many cloud providers, from Google Cloud Platform to Microsoft Azure, therefore claim that they are now “carbon-neutral,” given that they offset the entirety of the emissions of their cloud centers, though we must be wary of the precise provenance of RECs, and the details of how each organization defines “zero” net emissions [36]. This is complemented by efforts to mitigate the actual CO2 emissions of the compute regions themselves, with several server locations partially powered by renewable energy sources such as solar and wind [12, 29, 40] and giving users the necessary tools to pick compute regions with a smaller carbon footprint [13, 17], which are often tied to the amount of low-carbon energy that is being purchased, and not the grid emissions intensity. It is important to note that the decision on when and where to deploy a workload should be based on a grid emissions signal, not the amount of emissions offset through market-based measures (e.g., green power purchase agreements (PPAs), renewable energy certificates (RECs), or other carbon offset mechanisms): purchasing clean energy is not the same as consuming clean energy.", "original_types": ["text"], "id": 861}
{"type": "section", "content": "REPORTING AI CARBON INTENSITY\n\nCarbon accounting and reporting is becoming increasingly common in ML, with conferences such as NeurIPS requesting that submissions report their emissions [31] and recent work reporting the emissions incurred [37, 44]. However, it has yet to become the norm in our field, and we are still lacking systematic information regarding the environmental footprint of training ML models and how we can reduce it. In this paper, we argue that if members of the ML community had access to information about the CO2 emissions of their actions, they could adapt their decisions to reduce these emissions while still meeting the computational needs for their end tasks. In addition, providers building tools that enable users to track their CO2 emissions directly aligns with providers’ goals, as it will inform users’ decisions without being overly burdensome. Any cloud provider that discloses this information to users will, in fact, be improving those customers’ experiences, and likely increase usage of the platform. More specifically, we propose that, for a cloud user who wants to estimate their carbon footprint, the most salient information providers can report is the CO2 emissions generated by their cloud instances. Arguably the single most important contribution of this paper is the simplest: a presentation of the software carbon intensity (SCI) as a proxy for carbon emissions for a given cloud instance as it is running.\n\nMethodology: Computing CO2 Intensity\n\nIn this section we describe a method for estimating carbon intensity for cloud instances. At a high level, this involves tracking electricity consumption of hardware related to a single cloud instance, and mapping that electricity usage to CO2 emissions by using a grid-based carbon intensity. \n\nAs developed by the Green Software Foundation, the Software Carbon Intensity (SCI) is a rate, carbon emissions per one functional unit, or R. The equation used to calculate the SCI value of a software system is therefore:", "doc_id": "dodge2022", "page": 4, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "REPORTING AI CARBON INTENSITY\n\nCarbon accounting and reporting is becoming increasingly common in ML, with conferences such as NeurIPS requesting that submissions report their emissions [31] and recent work reporting the emissions incurred [37, 44]. However, it has yet to become the norm in our field, and we are still lacking systematic information regarding the environmental footprint of training ML models and how we can reduce it. In this paper, we argue that if members of the ML community had access to information about the CO2 emissions of their actions, they could adapt their decisions to reduce these emissions while still meeting the computational needs for their end tasks. In addition, providers building tools that enable users to track their CO2 emissions directly aligns with providers’ goals, as it will inform users’ decisions without being overly burdensome. Any cloud provider that discloses this information to users will, in fact, be improving those customers’ experiences, and likely increase usage of the platform. More specifically, we propose that, for a cloud user who wants to estimate their carbon footprint, the most salient information providers can report is the CO2 emissions generated by their cloud instances. Arguably the single most important contribution of this paper is the simplest: a presentation of the software carbon intensity (SCI) as a proxy for carbon emissions for a given cloud instance as it is running.\n\nMethodology: Computing CO2 Intensity\n\nIn this section we describe a method for estimating carbon intensity for cloud instances. At a high level, this involves tracking electricity consumption of hardware related to a single cloud instance, and mapping that electricity usage to CO2 emissions by using a grid-based carbon intensity. \n\nAs developed by the Green Software Foundation, the Software Carbon Intensity (SCI) is a rate, carbon emissions per one functional unit, or R. The equation used to calculate the SCI value of a software system is therefore:", "original_types": ["text", "header"], "id": 862}
{"type": "section", "content": "3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\n\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs, and networking components. We can break down the electricity usage for data centers into: 1) electricity that is used for a single cloud instance, and 2) electricity that is used for the benefit of the whole data center. In this work we focus on the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity consumption and emissions are underestimates.1\n\nElectricity Consumption from a Single Cloud Instance. The most accurate and popular AI models today are typically (deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive hardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide significant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming 250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at the core of neural network computations and a high rate of parallelization, GPUs can perform many more of these types of computations in the same amount of time as a CPU, but this increased computation throughput comes at an increased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [5, 45]. While this result is fairly uncontroversial, we ran an experiment to confirm it. To do so, we trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption of different components. We trained the model using the original code provided by Devlin et al. [8] on the language model pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM for each socket throughout that period, then averaging to get average power draw per component in watts. GPU energy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel’s RAPL. Our measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity consumption.\n\nFocus on GPUs. In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple instances; while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it", "doc_id": "dodge2022", "page": 5, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\n\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs, and networking components. We can break down the electricity usage for data centers into: 1) electricity that is used for a single cloud instance, and 2) electricity that is used for the benefit of the whole data center. In this work we focus on the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity consumption and emissions are underestimates.1\n\nElectricity Consumption from a Single Cloud Instance. The most accurate and popular AI models today are typically (deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive hardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide significant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming 250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at the core of neural network computations and a high rate of parallelization, GPUs can perform many more of these types of computations in the same amount of time as a CPU, but this increased computation throughput comes at an increased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [5, 45]. While this result is fairly uncontroversial, we ran an experiment to confirm it. To do so, we trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption of different components. We trained the model using the original code provided by Devlin et al. [8] on the language model pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM for each socket throughout that period, then averaging to get average power draw per component in watts. GPU energy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel’s RAPL. Our measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity consumption.\n\nFocus on GPUs. In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple instances; while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it", "original_types": ["text", "header"], "id": 863}
{"type": "table", "content": "Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.\n```markdown\n| Hardwa. | GPU | CPU0 | CPU1 | DRAM0 | DRAM1 | Total |\n|---------|----|------|------|-------|-------|------|\n| Watts   | 187.1 | 22.9 | 9.3  | 23.0  | 9.3   | 251.6 |\n| Fraction | 74% | 9%   | 4%   | 9%    | 4%    | 100% |\n```", "doc_id": "dodge2022", "page": 6, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.\n```markdown\n| Hardwa. | GPU | CPU0 | CPU1 | DRAM0 | DRAM1 | Total |\n|---------|----|------|------|-------|-------|------|\n| Watts   | 187.1 | 22.9 | 9.3  | 23.0  | 9.3   | 251.6 |\n| Fraction | 74% | 9%   | 4%   | 9%    | 4%    | 100% |\n```", "id": 864}
{"type": "section", "content": "CO2 Grams Emitted, BERT Language Modeling", "doc_id": "dodge2022", "page": 8, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "CO2 Grams Emitted, BERT Language Modeling", "original_types": ["header"], "id": 865}
{"type": "figure", "content": "Fig. 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different regions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).", "doc_id": "dodge2022", "page": 8, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different regions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).", "id": 866}
{"type": "section", "content": "emitted from training BERT (see §4 for more details) on 8 V100 GPUs for 36 hours in 16 different regions (one region per line) at different times throughout the year. What do emissions look like across the 11 experiments described in §4? In Figure 2 we show results for all 11 experiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion parameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by a vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions. The top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom is the emissions from running that experiment in the region with the least emissions, the black line represents the average, and the light blue regions are the top and bottom quartiles. In Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental Protection Agency [46]. One phone charge is estimated to emit 8.22 × 10−6 metric tons (using US national weighted average CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 × 10−4 metric tons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed is estimated to emit 8.887 × 10−3 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons, one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.", "doc_id": "dodge2022", "page": 8, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "emitted from training BERT (see §4 for more details) on 8 V100 GPUs for 36 hours in 16 different regions (one region per line) at different times throughout the year. What do emissions look like across the 11 experiments described in §4? In Figure 2 we show results for all 11 experiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion parameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by a vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions. The top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom is the emissions from running that experiment in the region with the least emissions, the black line represents the average, and the light blue regions are the top and bottom quartiles. In Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental Protection Agency [46]. One phone charge is estimated to emit 8.22 × 10−6 metric tons (using US national weighted average CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 × 10−4 metric tons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed is estimated to emit 8.887 × 10−3 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons, one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.", "original_types": ["text"], "id": 867}
{"type": "figure", "content": "CO2 Relative Size Comparison", "doc_id": "dodge2022", "page": 9, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "CO2 Relative Size Comparison", "id": 868}
{"type": "section", "content": "5.2 Time of Day\n\nWhile the choice of region is a major source of variation in CO2 emissions, diurnal variations also play a significant role. During the day, a region may have a higher mix of renewable energy or fossil-fuel based source [6]. As one can see in Table 3, depending on the day, starting the BERT finetuning at, e.g., midnight instead of 6:00 can result in carbon emissions increasing by up to 8%. The amount of variation varies by region and time of year as well.", "doc_id": "dodge2022", "page": 10, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "5.2 Time of Day\n\nWhile the choice of region is a major source of variation in CO2 emissions, diurnal variations also play a significant role. During the day, a region may have a higher mix of renewable energy or fossil-fuel based source [6]. As one can see in Table 3, depending on the day, starting the BERT finetuning at, e.g., midnight instead of 6:00 can result in carbon emissions increasing by up to 8%. The amount of variation varies by region and time of year as well.", "original_types": ["text", "header"], "id": 869}
{"type": "figure", "content": "Figure 3. What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region. Note: we confirmed with WattTime that emissions estimates for West US were correct, that region has large variance.", "doc_id": "dodge2022", "page": 11, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Figure 3. What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US; for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region. Note: we confirmed with WattTime that emissions estimates for West US were correct, that region has large variance.", "id": 870}
{"type": "section", "content": "during those intervals and compute the corresponding emissions. We explored two sets of values for N: one absolute, corresponding to increasing the total duration of the job by at most {6, 12, 18, 24} hours; and a second one relative, where we allow the job to increase in duration by at most {25%, 50%, 75%, 100%}. In other words, for the second set, we allow the workload to last for at most twice its duration had it not been stopped. While arbitrary, we motivate the choice of those two sets by the extreme range of possible job duration (from minutes to weeks). Note that we assume pausing and restarting the job is immediate and does not consume additional energy: this is similar in spirit (for carbon emissions) to Spot Instances on existing cloud platforms which automatically pause an instance if its price rises above a threshold set by the user.", "doc_id": "dodge2022", "page": 11, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "during those intervals and compute the corresponding emissions. We explored two sets of values for N: one absolute, corresponding to increasing the total duration of the job by at most {6, 12, 18, 24} hours; and a second one relative, where we allow the job to increase in duration by at most {25%, 50%, 75%, 100%}. In other words, for the second set, we allow the workload to last for at most twice its duration had it not been stopped. While arbitrary, we motivate the choice of those two sets by the extreme range of possible job duration (from minutes to weeks). Note that we assume pausing and restarting the job is immediate and does not consume additional energy: this is similar in spirit (for carbon emissions) to Spot Instances on existing cloud platforms which automatically pause an instance if its price rises above a threshold set by the user.", "original_types": ["text"], "id": 871}
{"type": "section", "content": "that are longer than a day; this aligns with our expectations, as short jobs can be run when emissions are lowest throughout a day, but long jobs naturally average across multiple days. See Figure 3, with results for all experiments in the appendix. This analysis is designed to highlight a use case where an AI workload needs to run regularly, but the practitioner has some flexibility on when it runs (so it could, e.g., run over night, if that is when carbon intensity is lowest). This is in fact a common use case in production ML systems deployed at companies, where models are re-trained on a regular schedule to incorporate new data over time [16].\n\nPause and Resume. When evaluating the Pause and Resume algorithm for durations up to 100% of the duration of the original experiment, we find the opposite of the Flexible Start result: short experiments like DenseNet 201 only see emissions reductions smaller than 10%, while the 6 billion transformer training run (our experiment with the largest carbon intensity) actually sees the largest decrease in emissions. See Figure 4 for two examples, with results for all 11 experiments in the appendix. This analysis is designed to highlight a use case where an AI workload can be increased in duration by some proportion of the original run time.", "doc_id": "dodge2022", "page": 12, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "that are longer than a day; this aligns with our expectations, as short jobs can be run when emissions are lowest throughout a day, but long jobs naturally average across multiple days. See Figure 3, with results for all experiments in the appendix. This analysis is designed to highlight a use case where an AI workload needs to run regularly, but the practitioner has some flexibility on when it runs (so it could, e.g., run over night, if that is when carbon intensity is lowest). This is in fact a common use case in production ML systems deployed at companies, where models are re-trained on a regular schedule to incorporate new data over time [16].\n\nPause and Resume. When evaluating the Pause and Resume algorithm for durations up to 100% of the duration of the original experiment, we find the opposite of the Flexible Start result: short experiments like DenseNet 201 only see emissions reductions smaller than 10%, while the 6 billion transformer training run (our experiment with the largest carbon intensity) actually sees the largest decrease in emissions. See Figure 4 for two examples, with results for all 11 experiments in the appendix. This analysis is designed to highlight a use case where an AI workload can be increased in duration by some proportion of the original run time.", "original_types": ["text"], "id": 872}
{"type": "table", "content": "Table 4. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 14.5% | 3.4% | 0.5% | 26.8% | 26.4% | 25.9% | 5.6% | 5.3% | 4.2% | 1.3% | 0.5%\n\nP&R | 19.0% | 8.5% | 2.5% | 27.7% | 27.3% | 27.1% | 12.5% | 12.3% | 11.7% | 4.7% | 2.4%\n\nPauses / hr | 0.23 | 0.3 | 0.15 | 0.06 | 0.07 | 0.08 | 0.3 | 0.3 | 0.3 | 0.23 | 0.14", "doc_id": "dodge2022", "page": 13, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 4. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 14.5% | 3.4% | 0.5% | 26.8% | 26.4% | 25.9% | 5.6% | 5.3% | 4.2% | 1.3% | 0.5%\n\nP&R | 19.0% | 8.5% | 2.5% | 27.7% | 27.3% | 27.1% | 12.5% | 12.3% | 11.7% | 4.7% | 2.4%\n\nPauses / hr | 0.23 | 0.3 | 0.15 | 0.06 | 0.07 | 0.08 | 0.3 | 0.3 | 0.3 | 0.23 | 0.14", "id": 873}
{"type": "table", "content": "Table 5. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 7.0% | 4.1% | 2.6% | 1.8% | 2.5% | 2.7% | 5.0% | 4.8% | 3.9% | 3.3% | 3.0%\n\nP&R | 9.5% | 11.0% | 11.4% | 2.0% | 2.8% | 3.1% | 11.0% | 11.0% | 10.8% | 11.4% | 11.3%\n\nPauses / hr | 0.42 | 0.29 | 0.27 | 1.5 | 1.88 | 2.0 | 0.31 | 0.32 | 0.31 | 0.27 | 0.26", "doc_id": "dodge2022", "page": 13, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 5. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 7.0% | 4.1% | 2.6% | 1.8% | 2.5% | 2.7% | 5.0% | 4.8% | 3.9% | 3.3% | 3.0%\n\nP&R | 9.5% | 11.0% | 11.4% | 2.0% | 2.8% | 3.1% | 11.0% | 11.0% | 10.8% | 11.4% | 11.3%\n\nPauses / hr | 0.42 | 0.29 | 0.27 | 1.5 | 1.88 | 2.0 | 0.31 | 0.32 | 0.31 | 0.27 | 0.26", "id": 874}
{"type": "section", "content": "7. CONSIDERATIONS FOR MODEL DEVELOPMENT AND DEPLOYMENT\n\nGenerally speaking, we advocate that researchers and practitioners record and report the amount of emissions incurred by ML projects, starting with the initial exploratory training phases all the way through hyperparameter tuning and deployment for the final model. This can inform an Operational Lifecycle Analysis (OLCA) for a machine learning model, which would account for all phases of the ML lifecycle. In the subsections below, we outline some ways in which the proposed tool can be used at different stages of the model development and deployment process, and describe some environmental impacts due to ML modeling that are outside the scope of measurement of this tool.", "doc_id": "dodge2022", "page": 13, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "7. CONSIDERATIONS FOR MODEL DEVELOPMENT AND DEPLOYMENT\n\nGenerally speaking, we advocate that researchers and practitioners record and report the amount of emissions incurred by ML projects, starting with the initial exploratory training phases all the way through hyperparameter tuning and deployment for the final model. This can inform an Operational Lifecycle Analysis (OLCA) for a machine learning model, which would account for all phases of the ML lifecycle. In the subsections below, we outline some ways in which the proposed tool can be used at different stages of the model development and deployment process, and describe some environmental impacts due to ML modeling that are outside the scope of measurement of this tool.", "original_types": ["text", "header"], "id": 875}
{"type": "section", "content": "We see various ways in which our tool can help guide model training, for instance via carbon-informed optimization (similarly to what [22] proposed for energy efficiency in federated learning), or for cloud-based recommendations that enable users to opt-in for carbon-aware configurations (in terms of region, time, etc.) to reduce the carbon intensity of their training workloads. We believe that tracking and reducing greenhouse gas emissions can be a very important feature for users deciding on how they will set up their cloud usage, but we recognize that there are natural trade-offs that must be considered. We therefore recommend that the measurements provided by our tool be used to guide informed decisions alongside other considerations as part of a holistic approach, and not as a single gold standard to optimize against. For example, even just within the scope of ML model development, it often takes engineering time to optimize a workload to be more efficient (i.e., use less computational resources), and a user should consider whether that time would be better spent elsewhere (e.g., transferring the workload to another region with lower average emissions). Furthermore, some projects have strict time constraints, and so scheduling jobs to only run at night would significantly delay progress, potentially leading to more emissions in other parts of the project. Thus, our suggestions are not meant as a one-size-fits-all solution which will eliminate carbon emissions, but instead as a set of options which can be referenced by users and decided upon on a case-by-case basis. Finally, there are also many additional upstream and downstream emissions considerations due to the ML model lifecycle, due to, e.g., hardware manufacturing and downstream uses or misuses of the model, that could eclipse the direct emissions due to model training alone. See §2 for further discussion of this crucial point.\n\nAnother important consideration is operating cost; it could be the case that Region A is lower emissions but higher cost than Region B for a particular workload, and thus a user could run their workload in Region B and have some budget left over that could be used for other reductions in emissions. A final consideration is cost of data transfer; it could be the case that Region A is lower emissions and monetary cost than Region B for a particular workload, but the energetic, environmental, or monetary cost of moving the data could exceed the benefits gained.\n\nIf we see broad adoption of such reporting tools, we may see increases in cloud use in regions which have low emissions. In such a scenario, providers could be incentivized to build new data centers, and providers should consider the local impact of such construction.\n\n8 FUTURE DIRECTIONS", "doc_id": "dodge2022", "page": 14, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "We see various ways in which our tool can help guide model training, for instance via carbon-informed optimization (similarly to what [22] proposed for energy efficiency in federated learning), or for cloud-based recommendations that enable users to opt-in for carbon-aware configurations (in terms of region, time, etc.) to reduce the carbon intensity of their training workloads. We believe that tracking and reducing greenhouse gas emissions can be a very important feature for users deciding on how they will set up their cloud usage, but we recognize that there are natural trade-offs that must be considered. We therefore recommend that the measurements provided by our tool be used to guide informed decisions alongside other considerations as part of a holistic approach, and not as a single gold standard to optimize against. For example, even just within the scope of ML model development, it often takes engineering time to optimize a workload to be more efficient (i.e., use less computational resources), and a user should consider whether that time would be better spent elsewhere (e.g., transferring the workload to another region with lower average emissions). Furthermore, some projects have strict time constraints, and so scheduling jobs to only run at night would significantly delay progress, potentially leading to more emissions in other parts of the project. Thus, our suggestions are not meant as a one-size-fits-all solution which will eliminate carbon emissions, but instead as a set of options which can be referenced by users and decided upon on a case-by-case basis. Finally, there are also many additional upstream and downstream emissions considerations due to the ML model lifecycle, due to, e.g., hardware manufacturing and downstream uses or misuses of the model, that could eclipse the direct emissions due to model training alone. See §2 for further discussion of this crucial point.\n\nAnother important consideration is operating cost; it could be the case that Region A is lower emissions but higher cost than Region B for a particular workload, and thus a user could run their workload in Region B and have some budget left over that could be used for other reductions in emissions. A final consideration is cost of data transfer; it could be the case that Region A is lower emissions and monetary cost than Region B for a particular workload, but the energetic, environmental, or monetary cost of moving the data could exceed the benefits gained.\n\nIf we see broad adoption of such reporting tools, we may see increases in cloud use in regions which have low emissions. In such a scenario, providers could be incentivized to build new data centers, and providers should consider the local impact of such construction.\n\n8 FUTURE DIRECTIONS", "original_types": ["text", "header"], "id": 876}
{"type": "section", "content": "As mentioned in §7, single-instance emissions are a well-defined starting place for quantifying, mitigating, and reducing the environmental impact due to ML, but do not present a complete picture of the total emissions that should be accounted for when considering the overall carbon emissions of the ML life cycle. Here are some aspects that are yet to be accounted for (and in some cases, yet to be defined) in terms of the overall OLCA of machine learning:", "doc_id": "dodge2022", "page": 14, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "As mentioned in §7, single-instance emissions are a well-defined starting place for quantifying, mitigating, and reducing the environmental impact due to ML, but do not present a complete picture of the total emissions that should be accounted for when considering the overall carbon emissions of the ML life cycle. Here are some aspects that are yet to be accounted for (and in some cases, yet to be defined) in terms of the overall OLCA of machine learning:", "original_types": ["text"], "id": 877}
{"type": "section", "content": "that the GHGP Scope 2 guidance is incompatible with the proposed method; this paper illustrates the need to revisit the Scope 2 guidance to better align with consequential accounting methods.\n\nWe do not cover the Scope 1 emissions (e.g. emissions that directly result from business activities, such as stationary combustion of fuels for backup power generation in cloud datacenters), for a more detailed discussion see e.g. Gupta et al. [15], nor the Scope 3 emissions (e.g. emissions that indirectly result from all other business activities, such as those associated with the upstream raw materials extraction, manufacturing, and delivery of cloud-based IT asset infrastructure such as servers from suppliers to be used in a cloud provider’s datacenters). Both of these types of emissions warrant discussion and debate by the AI community— and indeed some work has begun on the subject, e.g., [21, 26]—but we are missing a more concrete structure for categorizing, quantifying and mitigating the different scopes of emissions in our field. This would involve the active participation of specific stakeholders to establish the tooling and reporting required to better estimate these aspects, which is a challenge in itself.\n\nDeveloping certification systems for “Green AI”. While initiatives like the Green Software Foundation are making important progress towards measuring and mitigating the carbon footprint of software in general, the decentralized and data-driven nature of ML will call for specific approaches and guidelines to ensure its efficiency. We anticipate that AI-specific initiatives, spanning both research and academia, will help establish certification systems (or badge systems) that will allow both model developers and users make more informed choices with regards to sustainability. The current framing of Scopes 1, 2, and 3 may not encompass all the emissions reasonably associated with an AI program.\n\nImproving the carbon transparency of research and practice. Despite the existence of tools such as Code Carbon [38] and EvergyVis [41], both carbon estimation and reporting in ML publications and technical reports remain a relatively rare phenomenon. Conferences such as NeurIPS and NAACL have recently added emissions reporting as an optional part of the submission process; however, more encouragement will be necessary for this to become commonplace. Gathering more data about the environmental impact of our field is a crucial step towards identifying room for improvement and, eventually, reducing our emissions.\n\nSupporting improved estimates of emissions rates. The estimates of emissions rates providers would benefit from more and better data being provided by electric system operators. This is particularly true in areas of the world where it is currently not possible to produce hourly marginal estimates.", "doc_id": "dodge2022", "page": 15, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "that the GHGP Scope 2 guidance is incompatible with the proposed method; this paper illustrates the need to revisit the Scope 2 guidance to better align with consequential accounting methods.\n\nWe do not cover the Scope 1 emissions (e.g. emissions that directly result from business activities, such as stationary combustion of fuels for backup power generation in cloud datacenters), for a more detailed discussion see e.g. Gupta et al. [15], nor the Scope 3 emissions (e.g. emissions that indirectly result from all other business activities, such as those associated with the upstream raw materials extraction, manufacturing, and delivery of cloud-based IT asset infrastructure such as servers from suppliers to be used in a cloud provider’s datacenters). Both of these types of emissions warrant discussion and debate by the AI community— and indeed some work has begun on the subject, e.g., [21, 26]—but we are missing a more concrete structure for categorizing, quantifying and mitigating the different scopes of emissions in our field. This would involve the active participation of specific stakeholders to establish the tooling and reporting required to better estimate these aspects, which is a challenge in itself.\n\nDeveloping certification systems for “Green AI”. While initiatives like the Green Software Foundation are making important progress towards measuring and mitigating the carbon footprint of software in general, the decentralized and data-driven nature of ML will call for specific approaches and guidelines to ensure its efficiency. We anticipate that AI-specific initiatives, spanning both research and academia, will help establish certification systems (or badge systems) that will allow both model developers and users make more informed choices with regards to sustainability. The current framing of Scopes 1, 2, and 3 may not encompass all the emissions reasonably associated with an AI program.\n\nImproving the carbon transparency of research and practice. Despite the existence of tools such as Code Carbon [38] and EvergyVis [41], both carbon estimation and reporting in ML publications and technical reports remain a relatively rare phenomenon. Conferences such as NeurIPS and NAACL have recently added emissions reporting as an optional part of the submission process; however, more encouragement will be necessary for this to become commonplace. Gathering more data about the environmental impact of our field is a crucial step towards identifying room for improvement and, eventually, reducing our emissions.\n\nSupporting improved estimates of emissions rates. The estimates of emissions rates providers would benefit from more and better data being provided by electric system operators. This is particularly true in areas of the world where it is currently not possible to produce hourly marginal estimates.", "original_types": ["text"], "id": 878}
{"type": "section", "content": "Reducing AI’s scope-enabled emissions. Responsible development and application of AI must account not only for the hidden costs of development, as discussed in this paper, but for the positive or negative carbon impact the application enables. AI models continue to be used for oil exploration [32], deforestation [30], and mining [18], among other environmentally-detrimental practices. When considering the net impacts of an AI application, it is imperative to determine the extent to which AI is incentivizing practices that have a negative impact on the environment, or the extent to which applications are directly reducing emissions or otherwise incentivizing practices that are beneficial to the climate, and take these downstream direct and indirect effects into account in the overall environmental impact assessment of our field [4, 21].\n\nACKNOWLEDGMENTS", "doc_id": "dodge2022", "page": 15, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Reducing AI’s scope-enabled emissions. Responsible development and application of AI must account not only for the hidden costs of development, as discussed in this paper, but for the positive or negative carbon impact the application enables. AI models continue to be used for oil exploration [32], deforestation [30], and mining [18], among other environmentally-detrimental practices. When considering the net impacts of an AI application, it is imperative to determine the extent to which AI is incentivizing practices that have a negative impact on the environment, or the extent to which applications are directly reducing emissions or otherwise incentivizing practices that are beneficial to the climate, and take these downstream direct and indirect effects into account in the overall environmental impact assessment of our field [4, 21].\n\nACKNOWLEDGMENTS", "original_types": ["text", "header"], "id": 879}
{"type": "section", "content": "measurements included in Table 1. We also thank Alessandro Sordoni, Payal Bajaj, and Vibhav Vineet for sharing their training and inference jobs, and Jon Borchardt for help with plotting.\n\nREFERENCES\n\n[1] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models. arXiv:2007.03051 [cs.CY]\n\n[2] Rhonda Ascierto and A Lawrence. 2020. Uptime institute global data center survey 2020. Uptime Institute 2 (2020).\n\n[3] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[4] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The values encoded in machine learning research. arXiv preprint arXiv:2106.15590 (2021).\n\n[5] Buildcomputers.net. 2021. Power Consumption of PC Components in Watts. https://www.buildcomputers.net/power-consumption-of-pc-components.html\n\n[6] Jacques A de Chalendar and Sally M Benson. 2019. Why 100% renewable energy is not enough. Joule 3, 6 (2019), 1389–1393.\n\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL]\n\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\n\n[11] Michael Gillenwater. 2008. Redefining RECs—Part 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109–2119.\n\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-google-cloud-region-with-the-lowest-co2\n\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint arXiv:2006.06217 (2020).\n\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 854–867.", "doc_id": "dodge2022", "page": 16, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "measurements included in Table 1. We also thank Alessandro Sordoni, Payal Bajaj, and Vibhav Vineet for sharing their training and inference jobs, and Jon Borchardt for help with plotting.\n\nREFERENCES\n\n[1] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models. arXiv:2007.03051 [cs.CY]\n\n[2] Rhonda Ascierto and A Lawrence. 2020. Uptime institute global data center survey 2020. Uptime Institute 2 (2020).\n\n[3] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.\n\n[4] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The values encoded in machine learning research. arXiv preprint arXiv:2106.15590 (2021).\n\n[5] Buildcomputers.net. 2021. Power Consumption of PC Components in Watts. https://www.buildcomputers.net/power-consumption-of-pc-components.html\n\n[6] Jacques A de Chalendar and Sally M Benson. 2019. Why 100% renewable energy is not enough. Joule 3, 6 (2019), 1389–1393.\n\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL]\n\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\n\n[11] Michael Gillenwater. 2008. Redefining RECs—Part 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109–2119.\n\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\n\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-google-cloud-region-with-the-lowest-co2\n\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint arXiv:2006.06217 (2020).\n\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 854–867.", "original_types": ["text", "header"], "id": 880}
{"type": "section", "content": "[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang. 2018. Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). 620–629. https://doi.org/10.1109/HPCA.2018.00059\n\n[17] Kees Hertogh. 2021. Empowering cloud sustainability with the Microsoft Emissions Impact Dashboard. https://azure.microsoft.com/en-us/blog/empowering-cloud-sustainability-with-the-microsoft-emissions-impact-dashboard/\n\n[18] Zeshan Hyder, Keng Siau, and Fiona Nah. 2019. Artificial intelligence, machine learning, and autonomous technologies in mining industry. Journal of Database Management (JDM) 30, 2 (2019), 67–79.\n\n[19] Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer. 2014. Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869 (2014).\n\n[20] International Energy Authority (IEA). 2020. Energy Technology Perspectives 2020. https://www.iea.org/reports/energy-technology-perspectives-2020\n\n[21] Lynn Kaack, Priya Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2021. Aligning artificial intelligence with climate change mitigation. (2021).\n\n[22] Young Geun Kim and Carole-Jean Wu. 2021. AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. 183–198.\n\n[23] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).\n\n[24] Nevena Lazic, Tyler Lu, Craig Boutilier, MK Ryu, Eehern Jay Wong, Binz Roy, and Greg Imwalle. 2018. Data center cooling using model-predictive control. (2018).\n\n[25] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.\n\n[26] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).", "doc_id": "dodge2022", "page": 16, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang. 2018. Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). 620–629. https://doi.org/10.1109/HPCA.2018.00059\n\n[17] Kees Hertogh. 2021. Empowering cloud sustainability with the Microsoft Emissions Impact Dashboard. https://azure.microsoft.com/en-us/blog/empowering-cloud-sustainability-with-the-microsoft-emissions-impact-dashboard/\n\n[18] Zeshan Hyder, Keng Siau, and Fiona Nah. 2019. Artificial intelligence, machine learning, and autonomous technologies in mining industry. Journal of Database Management (JDM) 30, 2 (2019), 67–79.\n\n[19] Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer. 2014. Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869 (2014).\n\n[20] International Energy Authority (IEA). 2020. Energy Technology Perspectives 2020. https://www.iea.org/reports/energy-technology-perspectives-2020\n\n[21] Lynn Kaack, Priya Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2021. Aligning artificial intelligence with climate change mitigation. (2021).\n\n[22] Young Geun Kim and Carole-Jean Wu. 2021. AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. 183–198.\n\n[23] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).\n\n[24] Nevena Lazic, Tyler Lu, Craig Boutilier, MK Ryu, Eehern Jay Wong, Binz Roy, and Greg Imwalle. 2018. Data center cooling using model-predictive control. (2018).\n\n[25] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.\n\n[26] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).", "original_types": ["text"], "id": 881}
{"type": "section", "content": "[27] Eric Masanet, Arman Shehabi, Nuo Lei, Sarah Smith, and Jonathan Koomey. 2020. Recalibrating global data center energy-use estimates. Science 367, 6481 (2020), 984–986.\n\n[28] Valérie Masson-Delmotte, Panmao Zhai, Hans-Otto Pörtner, Debra Roberts, Jim Skea, Priyadarshi R Shukla, Anna Pirani, Wilfran Moufouma-Okia, Clotilde Péan, Roz Pidcock, et al. 2018. Global warming of 1.5 C. An IPCC Special Report on the impacts of global warming of 1, 5 (2018).\n\n[29] Microsoft Azure. 2021. Azure sustainability. https://azure.microsoft.com/en-us/global-infrastructure/sustainability/#overview\n\n[30] Vasilii Mosin, Roberto Aguilar, Alexander Platonov, Albert Vasiliev, Alexander Kedrov, and Anton Ivanov. 2019. Remote sensing and machine learning for tree detection and classification in forestry applications. In Image and Signal Processing for Remote Sensing XXV, Vol. 11155. International Society for Optics and Photonics, 11155F.\n\n[31] NeurIPS 2021 Conference. 2021. NeurIPS 2021 Paper Checklist Guidelines. https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist\n\n[32] Vito Alexander Nordloh, Anna Roubíčková, and Nick Brown. 2020. Machine Learning for Gas and Oil Exploration. arXiv preprint arXiv:2010.04186 (2020).\n\n[33] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. TexRxiv (2022).\n\n[34] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).\n\n[36] Joeri Rogelj, Oliver Geden, Annette Cowie, and Andy Reisinger. 2021. Net-zero emissions targets are vague: three ways to fix. Nature 591 (2021), 365–368.\n\n[37] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021).\n\n[38] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n\n[39] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.\n\n[40] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud", "doc_id": "dodge2022", "page": 17, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "[27] Eric Masanet, Arman Shehabi, Nuo Lei, Sarah Smith, and Jonathan Koomey. 2020. Recalibrating global data center energy-use estimates. Science 367, 6481 (2020), 984–986.\n\n[28] Valérie Masson-Delmotte, Panmao Zhai, Hans-Otto Pörtner, Debra Roberts, Jim Skea, Priyadarshi R Shukla, Anna Pirani, Wilfran Moufouma-Okia, Clotilde Péan, Roz Pidcock, et al. 2018. Global warming of 1.5 C. An IPCC Special Report on the impacts of global warming of 1, 5 (2018).\n\n[29] Microsoft Azure. 2021. Azure sustainability. https://azure.microsoft.com/en-us/global-infrastructure/sustainability/#overview\n\n[30] Vasilii Mosin, Roberto Aguilar, Alexander Platonov, Albert Vasiliev, Alexander Kedrov, and Anton Ivanov. 2019. Remote sensing and machine learning for tree detection and classification in forestry applications. In Image and Signal Processing for Remote Sensing XXV, Vol. 11155. International Society for Optics and Photonics, 11155F.\n\n[31] NeurIPS 2021 Conference. 2021. NeurIPS 2021 Paper Checklist Guidelines. https://neurips.cc/Conferences/2021/PaperInformation/PaperChecklist\n\n[32] Vito Alexander Nordloh, Anna Roubíčková, and Nick Brown. 2020. Machine Learning for Gas and Oil Exploration. arXiv preprint arXiv:2010.04186 (2020).\n\n[33] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. TexRxiv (2022).\n\n[34] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).\n\n[36] Joeri Rogelj, Oliver Geden, Annette Cowie, and Andy Reisinger. 2021. Net-zero emissions targets are vague: three ways to fix. Nature 591 (2021), 365–368.\n\n[37] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021).\n\n[38] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.\n\n[39] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.\n\n[40] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud", "original_types": ["text"], "id": 882}
{"type": "section", "content": "[41] Omar Shaikh, Jon Saad-Falcon, Austin P Wright, Nilaksh Das, Scott Freitas, Omar Asensio, and Duen Horng Chau. 2021. EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411763.3451780\n\n[42] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[43] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint arXiv:2007.05558 (2020).\n\n[44] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications. arXiv:2201.08239 [cs.CL]\n\n[45] Georgina Torbet. 2019. How Much Energy Does Your PC Use? (And 8 Ways to Cut It Down). https://www.makeuseof.com/tag/much-energy-pc-use-8-ways-cut/\n\n[46] United States Environmental Protection Agency. 2021. Greenhouse Gas Equivalencies Calculator. https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n\n[47] US Department of Energy. 2021. Energy-Efficient Cooling Control Systems for Data Centers. https://www.energy.gov/eere/amo/energy-efficient-cooling-control-systems-data-centers\n\n[48] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426 (2017).\n\n[49] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable ai: Environmental implications, challenges and opportunities. arXiv preprint arXiv:2111.00364 (2021).", "doc_id": "dodge2022", "page": 17, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "[41] Omar Shaikh, Jon Saad-Falcon, Austin P Wright, Nilaksh Das, Scott Freitas, Omar Asensio, and Duen Horng Chau. 2021. EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3411763.3451780\n\n[42] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[43] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint arXiv:2007.05558 (2020).\n\n[44] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications. arXiv:2201.08239 [cs.CL]\n\n[45] Georgina Torbet. 2019. How Much Energy Does Your PC Use? (And 8 Ways to Cut It Down). https://www.makeuseof.com/tag/much-energy-pc-use-8-ways-cut/\n\n[46] United States Environmental Protection Agency. 2021. Greenhouse Gas Equivalencies Calculator. https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n\n[47] US Department of Energy. 2021. Energy-Efficient Cooling Control Systems for Data Centers. https://www.energy.gov/eere/amo/energy-efficient-cooling-control-systems-data-centers\n\n[48] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426 (2017).\n\n[49] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable ai: Environmental implications, challenges and opportunities. arXiv preprint arXiv:2111.00364 (2021).", "original_types": ["text"], "id": 883}
{"type": "section", "content": "ADDITIONAL PLOTS", "doc_id": "dodge2022", "page": 18, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "ADDITIONAL PLOTS", "original_types": ["header"], "id": 884}
{"type": "figure", "content": "Fig. 5. Optimization results for the training of BERT small on 8 V100s. Without optimization, the job ran for approximately 36 hours and consumed 37.3 kWh.", "doc_id": "dodge2022", "page": 18, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 5. Optimization results for the training of BERT small on 8 V100s. Without optimization, the job ran for approximately 36 hours and consumed 37.3 kWh.", "id": 885}
{"type": "figure", "content": "Fig. 6. Optimization results for the finetuning of BERT small on the MNLI dataset, using 4 V100s. Without optimization, the job ran for approximately 6 hours and consumed 3.1 kWh.", "doc_id": "dodge2022", "page": 18, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 6. Optimization results for the finetuning of BERT small on the MNLI dataset, using 4 V100s. Without optimization, the job ran for approximately 6 hours and consumed 3.1 kWh.", "id": 886}
{"type": "figure", "content": "Fig. 7. Optimization results for the training of a 6B Parameter Transformer on 256 A100s. Without optimization, the job ran for approximately 8 days and consumed 13,812 kWh.", "doc_id": "dodge2022", "page": 19, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 7. Optimization results for the training of a 6B Parameter Transformer on 256 A100s. Without optimization, the job ran for approximately 8 days and consumed 13,812 kWh.", "id": 887}
{"type": "figure", "content": "Fig. 8. Optimization results for a DenseNet 121 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 20 minutes and consumed 20 WH.", "doc_id": "dodge2022", "page": 19, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 8. Optimization results for a DenseNet 121 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 20 minutes and consumed 20 WH.", "id": 888}
{"type": "figure", "content": "Fig. 9. Optimization results for a DenseNet 169 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 20 minutes and consumed 28 WH.", "doc_id": "dodge2022", "page": 20, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 9. Optimization results for a DenseNet 169 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 20 minutes and consumed 28 WH.", "id": 889}
{"type": "figure", "content": "Fig. 10. Optimization results for a DenseNet 201 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 25 minutes and consumed 37 WH.", "doc_id": "dodge2022", "page": 20, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 10. Optimization results for a DenseNet 201 trained on MNIST on 1 P40. Without optimization, the job ran for approximately 25 minutes and consumed 37 WH.", "id": 890}
{"type": "figure", "content": "Fig. 11. Optimization results for a Tiny ViT trained on 1 V100. Without optimization, the job ran for approximately 19 hours and consumed 1.7 kWh.", "doc_id": "dodge2022", "page": 21, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 11. Optimization results for a Tiny ViT trained on 1 V100. Without optimization, the job ran for approximately 19 hours and consumed 1.7 kWh.", "id": 891}
{"type": "figure", "content": "Fig. 12. Optimization results for a Small ViT trained on 1 V100. Without optimization, the job ran for approximately 19 hours and consumed 2.2 kWh.", "doc_id": "dodge2022", "page": 21, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 12. Optimization results for a Small ViT trained on 1 V100. Without optimization, the job ran for approximately 19 hours and consumed 2.2 kWh.", "id": 892}
{"type": "figure", "content": "Fig. 13. Optimization results for a Base ViT trained on 1 V100. Without optimization, the job ran for approximately 21 hours and consumed 4.7 kWh.", "doc_id": "dodge2022", "page": 22, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 13. Optimization results for a Base ViT trained on 1 V100. Without optimization, the job ran for approximately 21 hours and consumed 4.7 kWh.", "id": 893}
{"type": "figure", "content": "Fig. 14. Optimization results for a Large ViT trained on 4 V100. Without optimization, the job ran for approximately 90 hours and consumed 93.3 kWh.", "doc_id": "dodge2022", "page": 22, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 14. Optimization results for a Large ViT trained on 4 V100. Without optimization, the job ran for approximately 90 hours and consumed 93.3 kWh.", "id": 894}
{"type": "figure", "content": "Fig. 15. Optimization results for a Huge ViT trained on 4 V100. Without optimization, the job ran for approximately 9 days and consumed 237.6 kWh.", "doc_id": "dodge2022", "page": 23, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Fig. 15. Optimization results for a Huge ViT trained on 4 V100. Without optimization, the job ran for approximately 9 days and consumed 237.6 kWh.", "id": 895}
{"type": "section", "content": "ADDITIONAL TABLES\n\nIn Tables 6, 7, 8, 9, 10, 11, 12 and 13, we report the decrease in CO2 emissions (in percent) obtained when performing the two optimizations introduced in the main text for all 11 models, averaged across the 16 regions we consider and over the year, for various values of the N denoting the increase in job duration stemming from the optimization.", "doc_id": "dodge2022", "page": 24, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "ADDITIONAL TABLES\n\nIn Tables 6, 7, 8, 9, 10, 11, 12 and 13, we report the decrease in CO2 emissions (in percent) obtained when performing the two optimizations introduced in the main text for all 11 models, averaged across the 16 regions we consider and over the year, for various values of the N denoting the increase in job duration stemming from the optimization.", "original_types": ["text", "header"], "id": 896}
{"type": "table", "content": "Table 6. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 25% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "doc_id": "dodge2022", "page": 24, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 6. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 25% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "id": 897}
{"type": "table", "content": "Table 7. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 50% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "doc_id": "dodge2022", "page": 24, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 7. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 50% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "id": 898}
{"type": "table", "content": "Table 8. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 75% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "doc_id": "dodge2022", "page": 24, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 8. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 75% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "id": 899}
{"type": "table", "content": "Table 9. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "doc_id": "dodge2022", "page": 24, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 9. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 100% increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.", "id": 900}
{"type": "table", "content": "Table 10. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 6h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 6.9% | 1.2% | 0.2% | 15.3% | 14.9% | 14.5% | 2.3% | 2.2% | 1.7% | 0.5% | 0.2%\n\nP&R | 9.4% | 2.9% | 0.8% | 15.8% | 15.5% | 15.3% | 5.5% | 5.3% | 4.8% | 1.5% | 0.7%\n\nPauses / hr | 0.41 | 0.21 | 0.06 | 0.22 | 0.27 | 0.28 | 0.29 | 0.3 | 0.29 | 0.11 | 0.06", "doc_id": "dodge2022", "page": 25, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 10. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 6h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 6.9% | 1.2% | 0.2% | 15.3% | 14.9% | 14.5% | 2.3% | 2.2% | 1.7% | 0.5% | 0.2%\n\nP&R | 9.4% | 2.9% | 0.8% | 15.8% | 15.5% | 15.3% | 5.5% | 5.3% | 4.8% | 1.5% | 0.7%\n\nPauses / hr | 0.41 | 0.21 | 0.06 | 0.22 | 0.27 | 0.28 | 0.29 | 0.3 | 0.29 | 0.11 | 0.06", "id": 901}
{"type": "table", "content": "Table 11. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 12h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 10.1% | 2.3% | 0.3% | 21.6% | 21.1% | 20.6% | 3.8% | 3.6% | 2.7% | 0.8% | 0.3%\n\nP&R | 13.8% | 5.3% | 1.4% | 22.2% | 21.7% | 21.5% | 8.3% | 8.1% | 7.7% | 2.7% | 1.3%\n\nPauses / hr | 0.33 | 0.27 | 0.1 | 0.12 | 0.15 | 0.15 | 0.32 | 0.33 | 0.32 | 0.17 | 0.09", "doc_id": "dodge2022", "page": 25, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 11. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 12h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 10.1% | 2.3% | 0.3% | 21.6% | 21.1% | 20.6% | 3.8% | 3.6% | 2.7% | 0.8% | 0.3%\n\nP&R | 13.8% | 5.3% | 1.4% | 22.2% | 21.7% | 21.5% | 8.3% | 8.1% | 7.7% | 2.7% | 1.3%\n\nPauses / hr | 0.33 | 0.27 | 0.1 | 0.12 | 0.15 | 0.15 | 0.32 | 0.33 | 0.32 | 0.17 | 0.09", "id": 902}
{"type": "table", "content": "Table 12. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 18h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 13.3% | 2.9% | 0.4% | 24.1% | 23.7% | 23.2% | 4.9% | 4.6% | 3.6% | 1.1% | 0.4%\n\nP&R | 17.4% | 7.0% | 2.0% | 24.9% | 24.5% | 24.2% | 10.8% | 10.5% | 9.9% | 3.8% | 1.9%\n\nPauses / hr | 0.26 | 0.29 | 0.13 | 0.08 | 0.09 | 0.1 | 0.31 | 0.32 | 0.32 | 0.2 | 0.12", "doc_id": "dodge2022", "page": 25, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 12. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 18h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 13.3% | 2.9% | 0.4% | 24.1% | 23.7% | 23.2% | 4.9% | 4.6% | 3.6% | 1.1% | 0.4%\n\nP&R | 17.4% | 7.0% | 2.0% | 24.9% | 24.5% | 24.2% | 10.8% | 10.5% | 9.9% | 3.8% | 1.9%\n\nPauses / hr | 0.26 | 0.29 | 0.13 | 0.08 | 0.09 | 0.1 | 0.31 | 0.32 | 0.32 | 0.2 | 0.12", "id": 903}
{"type": "table", "content": "Table 13. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 14.5% | 3.4% | 0.5% | 26.8% | 26.4% | 25.9% | 5.6% | 5.3% | 4.2% | 1.3% | 0.5%\n\nP&R | 19.0% | 8.5% | 2.5% | 27.7% | 27.3% | 27.1% | 12.5% | 12.3% | 11.7% | 4.7% | 2.4%\n\nPauses / hr | 0.23 | 0.3 | 0.15 | 0.06 | 0.07 | 0.08 | 0.3 | 0.3 | 0.3 | 0.23 | 0.14", "doc_id": "dodge2022", "page": 25, "url": "https://arxiv.org/pdf/2206.05229", "embedded_text": "Table 13. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The last line represents the average number of pauses per hour performed by the P&R optimization.\nModel | BERT finetune | BERT LM | 6B Transf. | Dense 121 | Dense 169 | Dense 201 | ViT Tiny | ViT Small | ViT Base | ViT Large | ViT Huge\n\nFS | 14.5% | 3.4% | 0.5% | 26.8% | 26.4% | 25.9% | 5.6% | 5.3% | 4.2% | 1.3% | 0.5%\n\nP&R | 19.0% | 8.5% | 2.5% | 27.7% | 27.3% | 27.1% | 12.5% | 12.3% | 11.7% | 4.7% | 2.4%\n\nPauses / hr | 0.23 | 0.3 | 0.15 | 0.06 | 0.07 | 0.08 | 0.3 | 0.3 | 0.3 | 0.23 | 0.14", "id": 904}
{"type": "section", "content": "Abstract\n\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.", "doc_id": "chen2024", "page": 1, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Abstract\n\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.", "original_types": ["text", "header"], "id": 905}
{"type": "table", "content": "Table 1: H100, H20, and TPU v6e specifications.\n| H100 | H20 | TPU v6e [7] \n\n|---|---|---|\n| BF16 TFLOPs | 989 | 148 | 918 \n| Memory capacity | 80 GB | 96 GB | 32 GB \n| Memory bandwidth | 3.35 TB/s | 4.0 TB/s | 1.64 TB/s \n| Power rating | 700 W | 400 W | unlisted \n| Inter-chip bandwidth | 450 GB/s | 450 GB/s | 448 GB/s \n| Network bandwidth | 400 Gbps | 400 Gbps | 200 Gbps \n| Price per chip [2] | $11.06/hr | $4.63/hr * | $2.70/hr", "doc_id": "chen2024", "page": 2, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Table 1: H100, H20, and TPU v6e specifications.\n| H100 | H20 | TPU v6e [7] \n\n|---|---|---|\n| BF16 TFLOPs | 989 | 148 | 918 \n| Memory capacity | 80 GB | 96 GB | 32 GB \n| Memory bandwidth | 3.35 TB/s | 4.0 TB/s | 1.64 TB/s \n| Power rating | 700 W | 400 W | unlisted \n| Inter-chip bandwidth | 450 GB/s | 450 GB/s | 448 GB/s \n| Network bandwidth | 400 Gbps | 400 Gbps | 200 Gbps \n| Price per chip [2] | $11.06/hr | $4.63/hr * | $2.70/hr", "id": 906}
{"type": "section", "content": "*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.\n\n2.1 Preliminaries\n\nModern large language models (LLMs) primarily rely on the transformer architecture [49]. In a transformer-based LLM, each input token is first mapped to a word embedding of dimension d. These embeddings then pass through a series of transformer blocks. The final output embeddings are multiplied by a sampling matrix to generate the predicted likelihoods for the next token.\n\nWithin each transformer block, the input embeddings are projected into three distinct vectors: query (qi), key (ki), and value (vi), all of which have the same dimension d as hidden states. These vectors are processed through an attention operator to compute attention scores. The attention scores are then weighted by a matrix Wout to produce the output embeddings yi of the attention layer.\n\nq_i = W_q x_i, k_i = W_k x_i, v_i = W_v x_i,\n\na_i = ∑_j=1^n softmax (q_i^T k_j) v_j, *\n\ny_i = W_out a_i.\n\nThe output yi is then passed through a feedforward network that scales it into an intermediate vector space, followed by another matrix multiplication to scale it back:\n\nx_i' = W_proj · f_act (W_fc · y_i) .\n\n2.2 Hardware Underutilization\n\n2.2.1 The Underutilization in Non-Attention Operators\n\nTo improve GPU utilization in LLM decoding, continuous batching is widely adopted [16, 20, 46]. By processing multiple inputs concurrently, the model parameters in GPU memory can be reused, making the workload more computation-intensive. For a batch of B requests, the non-attention operator requires approximately 2NB floating-point operations. Additionally, these operators involve loading model parameters eN and reading/writing a total of 2eBd input and output data from", "doc_id": "chen2024", "page": "2-3", "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.\n\n2.1 Preliminaries\n\nModern large language models (LLMs) primarily rely on the transformer architecture [49]. In a transformer-based LLM, each input token is first mapped to a word embedding of dimension d. These embeddings then pass through a series of transformer blocks. The final output embeddings are multiplied by a sampling matrix to generate the predicted likelihoods for the next token.\n\nWithin each transformer block, the input embeddings are projected into three distinct vectors: query (qi), key (ki), and value (vi), all of which have the same dimension d as hidden states. These vectors are processed through an attention operator to compute attention scores. The attention scores are then weighted by a matrix Wout to produce the output embeddings yi of the attention layer.\n\nq_i = W_q x_i, k_i = W_k x_i, v_i = W_v x_i,\n\na_i = ∑_j=1^n softmax (q_i^T k_j) v_j, *\n\ny_i = W_out a_i.\n\nThe output yi is then passed through a feedforward network that scales it into an intermediate vector space, followed by another matrix multiplication to scale it back:\n\nx_i' = W_proj · f_act (W_fc · y_i) .\n\n2.2 Hardware Underutilization\n\n2.2.1 The Underutilization in Non-Attention Operators\n\nTo improve GPU utilization in LLM decoding, continuous batching is widely adopted [16, 20, 46]. By processing multiple inputs concurrently, the model parameters in GPU memory can be reused, making the workload more computation-intensive. For a batch of B requests, the non-attention operator requires approximately 2NB floating-point operations. Additionally, these operators involve loading model parameters eN and reading/writing a total of 2eBd input and output data from", "original_types": ["text", "header"], "id": 907}
{"type": "section", "content": "2.2.2 The Underutilization in Attention Operators\n\nDifferent from the weight matrix projection operators, the attention operator, when processing a batch of requestss still performs a batched matrix-vector multiplication, where each query accesses and processes its own KV cache. As a result, the arithmetic intensity of the attention operator remains constant, irrespective of the batch size. This behavior makes attention operations memory-bound, and increasing the batch size does not improve resource utilization. More recent models have adopt grouped-query attention (GQA), which splits qi into a group of G independent queries and reduce the size of ki and vi by a factor of G. Each query goes through the attention computation with the same ki and vi and the outputs are simply concatenated. With GQA, the arithmetic intensity of attention operators is increased G times, but is still quite low compared with other operators. As shown in Figure 3, the bandwidth utilization of attention operators remains above 70% even for small batch sizes, such as 20. This holds true even on memory-specialized accelerators like H20, which delivers only 15% of the TFLOPs of the H100. However, the batch size achievable for attention operations is constrained by GPU memory capacity, particularly due to the high memory demand of KV caches for longer context lengths. For example, with a context length of 8192, the full memory of an H100 can only hold KV caches for about 30 requests, with the actual number being lower due to memory used by model weights. Consequently, the limited batch size for attention operations becomes a key bottleneck, preventing efficient utilization of computational resources for non-attention operations during the decoding phase.", "doc_id": "chen2024", "page": 4, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "2.2.2 The Underutilization in Attention Operators\n\nDifferent from the weight matrix projection operators, the attention operator, when processing a batch of requestss still performs a batched matrix-vector multiplication, where each query accesses and processes its own KV cache. As a result, the arithmetic intensity of the attention operator remains constant, irrespective of the batch size. This behavior makes attention operations memory-bound, and increasing the batch size does not improve resource utilization. More recent models have adopt grouped-query attention (GQA), which splits qi into a group of G independent queries and reduce the size of ki and vi by a factor of G. Each query goes through the attention computation with the same ki and vi and the outputs are simply concatenated. With GQA, the arithmetic intensity of attention operators is increased G times, but is still quite low compared with other operators. As shown in Figure 3, the bandwidth utilization of attention operators remains above 70% even for small batch sizes, such as 20. This holds true even on memory-specialized accelerators like H20, which delivers only 15% of the TFLOPs of the H100. However, the batch size achievable for attention operations is constrained by GPU memory capacity, particularly due to the high memory demand of KV caches for longer context lengths. For example, with a context length of 8192, the full memory of an H100 can only hold KV caches for about 30 requests, with the actual number being lower due to memory used by model weights. Consequently, the limited batch size for attention operations becomes a key bottleneck, preventing efficient utilization of computational resources for non-attention operations during the decoding phase.", "original_types": ["text", "header"], "id": 908}
{"type": "section", "content": "the disparity between memory bandwidth and computing power in modern high-performance accelerators, which favor high arithmetic intensity for efficient resource utilization, these operators tend to underutilize the computation resources of advanced GPUs.\n\nFor non-attention operators, while increasing the batch size could potentially enhance hardware utilization, this also results in a corresponding increase in the KV cache, which may exceed the available memory capacity. Consequently, to prevent memory overflow, the batch size is often kept small, which also leads to inefficient hardware utilization because of low arithmetic intensity.\n\nTo address the above limitations of homogeneous decoding solutions, we propose the model-attention disaggregation architecture, which uses memory-specialized accelerators to store KV caches and compute the attention operators; the non-attention operators are still executed on original accelerators. A model-attention disaggregation system can use multiple devices of each kind to provide different degrees of parallelism (DOPs). If we use a GPUs for non-attention operators and b memory-optimized GPUs for attention operators, we denote the DOP as (a, b).\n\nBy leveraging the cheaper memory-optimized devices, we can make larger batch sizes due to the extended memory capacities to store the KV caches, hence increasing the arithmetic intensity and promoting the hardware utilization of non-attention operators. Moreover, as the attention computation are moved to memory-optimized devices, we avoid wasting precious computation resources of high-end GPUs.\n\nOne potential obstacle in implementing attention offloading lies in the necessity of data transmission between heterogeneous accelerators for each layer of the model, which could encounter the communication wall problem and increase the end-to-end decoding latency. We conduct a quantitative analysis to determine the required interconnect bandwidth for such transfers. Say we run one iteration with batch size B, and we can afford α× more latency for the networking overhead, the minimum interconnect bandwidth required can thus be calculated as\n\nminimum bandwidth = size of data to transmit α·computation time (2 + 2/G)edBL = α[MTIME(B) + ATIME(B, l)]\n\nwhere MTIME(B) and ATIME(B, l) is running time of non-attention and attention operators at batch size B and sequence length l, respectively, and they can be measured experimentally. The estimated minimum bandwidths required for different batch sizes, when α = 0.2, are calculated and presented in Figure 4.\n\nAs evident from the data presented, the required interconnect bandwidth does not exceed 30 GB/s, even when dealing with batch sizes as high as 300. This bandwidth demand can be easily met by networking technologies like 400Gbps Ethernet. Indeed, contemporary data centers already fulfill this requirement, where each GPU is typically equipped with an exclusive 400Gbps NIC to provide sufficient networking bandwidth for LLM training.", "doc_id": "chen2024", "page": 5, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "the disparity between memory bandwidth and computing power in modern high-performance accelerators, which favor high arithmetic intensity for efficient resource utilization, these operators tend to underutilize the computation resources of advanced GPUs.\n\nFor non-attention operators, while increasing the batch size could potentially enhance hardware utilization, this also results in a corresponding increase in the KV cache, which may exceed the available memory capacity. Consequently, to prevent memory overflow, the batch size is often kept small, which also leads to inefficient hardware utilization because of low arithmetic intensity.\n\nTo address the above limitations of homogeneous decoding solutions, we propose the model-attention disaggregation architecture, which uses memory-specialized accelerators to store KV caches and compute the attention operators; the non-attention operators are still executed on original accelerators. A model-attention disaggregation system can use multiple devices of each kind to provide different degrees of parallelism (DOPs). If we use a GPUs for non-attention operators and b memory-optimized GPUs for attention operators, we denote the DOP as (a, b).\n\nBy leveraging the cheaper memory-optimized devices, we can make larger batch sizes due to the extended memory capacities to store the KV caches, hence increasing the arithmetic intensity and promoting the hardware utilization of non-attention operators. Moreover, as the attention computation are moved to memory-optimized devices, we avoid wasting precious computation resources of high-end GPUs.\n\nOne potential obstacle in implementing attention offloading lies in the necessity of data transmission between heterogeneous accelerators for each layer of the model, which could encounter the communication wall problem and increase the end-to-end decoding latency. We conduct a quantitative analysis to determine the required interconnect bandwidth for such transfers. Say we run one iteration with batch size B, and we can afford α× more latency for the networking overhead, the minimum interconnect bandwidth required can thus be calculated as\n\nminimum bandwidth = size of data to transmit α·computation time (2 + 2/G)edBL = α[MTIME(B) + ATIME(B, l)]\n\nwhere MTIME(B) and ATIME(B, l) is running time of non-attention and attention operators at batch size B and sequence length l, respectively, and they can be measured experimentally. The estimated minimum bandwidths required for different batch sizes, when α = 0.2, are calculated and presented in Figure 4.\n\nAs evident from the data presented, the required interconnect bandwidth does not exceed 30 GB/s, even when dealing with batch sizes as high as 300. This bandwidth demand can be easily met by networking technologies like 400Gbps Ethernet. Indeed, contemporary data centers already fulfill this requirement, where each GPU is typically equipped with an exclusive 400Gbps NIC to provide sufficient networking bandwidth for LLM training.", "original_types": ["text"], "id": 909}
{"type": "section", "content": "For memory devices, the identical interconnection bandwidth is also necessary to communicate with computational devices. Since we employ a collection of more economical yet less powerful memory devices to collaboratively compute attention, the communication bandwidth needed for each individual device is significantly smaller. Consequently, we can choose to either equip each device with a less powerful NIC or install a single shared 400Gbps NIC to serve multiple memory devices.", "doc_id": "chen2024", "page": 5, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "For memory devices, the identical interconnection bandwidth is also necessary to communicate with computational devices. Since we employ a collection of more economical yet less powerful memory devices to collaboratively compute attention, the communication bandwidth needed for each individual device is significantly smaller. Consequently, we can choose to either equip each device with a less powerful NIC or install a single shared 400Gbps NIC to serve multiple memory devices.", "original_types": ["text"], "id": 910}
{"type": "section", "content": "Difficult execution overlapping.\n\nIn a heterogeneous disaggregated system, various devices such as compute-optimized GPUs, memory-optimized GPUs, and NICs can be utilized simultaneously. Hence, we might achieve significant execution time reduction if the execution of operations occupying different devices could be overlapped. However, in the transformers architectures of current LLMs, attention operators and model operators are tightly interleaved in a sequential manner, with the output of one operator being transmitted over the network for the input of the other. Consequently, operations that depend on distinct hardware resources cannot be effectively overlapped in time, leading to considerable resource underutilization. Therefore, careful orchestration of operations on various devices and efficient design of task pipelines are required to promote execution overlapping and increase resource utilization.\n\nSystem Design\n\nWe build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation and solves the related challenges. Lamina employs two kinds of acceleration devices: memory devices are used for storing KV cache and computing the attention operator, and computation devices are used for storing model parameters and computing other parts of the model. These two kinds of devices are interconnected with high-speed DCN, e.g., Infiniband or Ethernet.\n\n4.1 Fully Host-Bypassed Network Stack\n\nThe communication between GPUs across different nodes, often utilizing RDMA technologies, is a complex process that requires the coordination of multiple system agents, including the CPU, GPU, and NIC. To reduce GPU-aware networking overhead, GPUDirect RDMA (GDR) [3] is developed to allow the RDMA-capable NIC (RNIC) to directly access GPU memory. This eliminates the need for host memory as an intermediate buffer, thereby enhancing both network latency and bandwidth. However, the control path still requires CPU involvement and includes several steps, all of which lie on the critical path and contribute to network latency. Specifically, when transferring data using GPUDirect RDMA, the following steps are performed:", "doc_id": "chen2024", "page": 6, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Difficult execution overlapping.\n\nIn a heterogeneous disaggregated system, various devices such as compute-optimized GPUs, memory-optimized GPUs, and NICs can be utilized simultaneously. Hence, we might achieve significant execution time reduction if the execution of operations occupying different devices could be overlapped. However, in the transformers architectures of current LLMs, attention operators and model operators are tightly interleaved in a sequential manner, with the output of one operator being transmitted over the network for the input of the other. Consequently, operations that depend on distinct hardware resources cannot be effectively overlapped in time, leading to considerable resource underutilization. Therefore, careful orchestration of operations on various devices and efficient design of task pipelines are required to promote execution overlapping and increase resource utilization.\n\nSystem Design\n\nWe build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation and solves the related challenges. Lamina employs two kinds of acceleration devices: memory devices are used for storing KV cache and computing the attention operator, and computation devices are used for storing model parameters and computing other parts of the model. These two kinds of devices are interconnected with high-speed DCN, e.g., Infiniband or Ethernet.\n\n4.1 Fully Host-Bypassed Network Stack\n\nThe communication between GPUs across different nodes, often utilizing RDMA technologies, is a complex process that requires the coordination of multiple system agents, including the CPU, GPU, and NIC. To reduce GPU-aware networking overhead, GPUDirect RDMA (GDR) [3] is developed to allow the RDMA-capable NIC (RNIC) to directly access GPU memory. This eliminates the need for host memory as an intermediate buffer, thereby enhancing both network latency and bandwidth. However, the control path still requires CPU involvement and includes several steps, all of which lie on the critical path and contribute to network latency. Specifically, when transferring data using GPUDirect RDMA, the following steps are performed:", "original_types": ["text", "header"], "id": 911}
{"type": "section", "content": "4.2 Automated Model Converter\n\n4.2.1 Model Splitting\n\nIn the attention offloading architecture, different operators of the LLM might be executed on different hardware; hence, we need to partition the model into slices, which is achieved by cutting at the attention operators. It often involves significant modifications to the existing codebase, primarily because the desired cutting points do not align with the LLM's inherent modular structure. This misalignment complicates the partitioning process and increases the risk of errors and inconsistencies within the heterogeneous system.\n\nTo facilitate model partitioning, we develop an automated model splitter capable of transforming the LLM into individually invokable slices, illustrated in Figure 6. Given the LLM source code, the splitter uses symbolic execution to generate a weighted computation graph. The weight of each edge denotes the size of the data passed between the operators, which is derived from the model's shape specification.\n\nDue to the presence of residual connections and other intricate model constructs, directly removing the attention operator does not always result in a disconnected computation graph. Therefore, we compute the minimum weighted cut of the remaining graph, from the input to the output of the attention operator. The edges in this minimum cut, representing the context that must be saved between slice invocations, are removed from the computation graphs. This process is iteratively applied to each attention operator, ultimately yielding n + 1 model slices, where n denotes the original number of the attention operators.\n\n4.2.2 Resource Utilization Overlapping\n\nWhile the attention operators and other operators in a transformer block are executed sequentially, a closer examination of the attention computation reveals the potential for achieving partial overlapping of resource utilization. Given an attention query q and the set of token indices I, the attention computation can be carried out in a divide-and-conquer manner. Assume that I can be written as the disjoint union of two subsets I1 and I2, and let\n\nA_q(I) = ∑_{i∈I} softmax(q^Tk_i) v_i, \nS_q(I) = ∑_{i∈I} exp(q^Tk_i),\n\nwhere A_q(I) is the attention output and S_q(I) is the denominator of softmax, then A_q(I) can be easily obtained by combining the partial attention results on I1 and I2, i.e., [A_q(I1), S_q(I1)] and [A_q(I2), S_q(I2)]:\n\nA_q(I) = A_q(I1)S_q(I1) + A_q(I2)S_q(I2) \nS_q(I1) + S_q(I2).", "doc_id": "chen2024", "page": 7, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "4.2 Automated Model Converter\n\n4.2.1 Model Splitting\n\nIn the attention offloading architecture, different operators of the LLM might be executed on different hardware; hence, we need to partition the model into slices, which is achieved by cutting at the attention operators. It often involves significant modifications to the existing codebase, primarily because the desired cutting points do not align with the LLM's inherent modular structure. This misalignment complicates the partitioning process and increases the risk of errors and inconsistencies within the heterogeneous system.\n\nTo facilitate model partitioning, we develop an automated model splitter capable of transforming the LLM into individually invokable slices, illustrated in Figure 6. Given the LLM source code, the splitter uses symbolic execution to generate a weighted computation graph. The weight of each edge denotes the size of the data passed between the operators, which is derived from the model's shape specification.\n\nDue to the presence of residual connections and other intricate model constructs, directly removing the attention operator does not always result in a disconnected computation graph. Therefore, we compute the minimum weighted cut of the remaining graph, from the input to the output of the attention operator. The edges in this minimum cut, representing the context that must be saved between slice invocations, are removed from the computation graphs. This process is iteratively applied to each attention operator, ultimately yielding n + 1 model slices, where n denotes the original number of the attention operators.\n\n4.2.2 Resource Utilization Overlapping\n\nWhile the attention operators and other operators in a transformer block are executed sequentially, a closer examination of the attention computation reveals the potential for achieving partial overlapping of resource utilization. Given an attention query q and the set of token indices I, the attention computation can be carried out in a divide-and-conquer manner. Assume that I can be written as the disjoint union of two subsets I1 and I2, and let\n\nA_q(I) = ∑_{i∈I} softmax(q^Tk_i) v_i, \nS_q(I) = ∑_{i∈I} exp(q^Tk_i),\n\nwhere A_q(I) is the attention output and S_q(I) is the denominator of softmax, then A_q(I) can be easily obtained by combining the partial attention results on I1 and I2, i.e., [A_q(I1), S_q(I1)] and [A_q(I2), S_q(I2)]:\n\nA_q(I) = A_q(I1)S_q(I1) + A_q(I2)S_q(I2) \nS_q(I1) + S_q(I2).", "original_types": ["text", "header"], "id": 912}
{"type": "figure", "content": "Figure 6: The partitioned computation graph of an LLM.", "doc_id": "chen2024", "page": 7, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Figure 6: The partitioned computation graph of an LLM.", "id": 913}
{"type": "section", "content": "4.3 Execution Pipelining\n\nDue to the serial nature of transformer-based models, if there is only one batch under processing, the memory device is idle when the computation device is working, and vice versa. To address this resource underutilization problem and increase system throughput, we may run multiple batches concurrently in a pipelined fashion. With properly designed pipelining, better hardware utilization can be achieved without sacrificing latency. We propose the rotational staggered pipelining to solve this problem.", "doc_id": "chen2024", "page": 8, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "4.3 Execution Pipelining\n\nDue to the serial nature of transformer-based models, if there is only one batch under processing, the memory device is idle when the computation device is working, and vice versa. To address this resource underutilization problem and increase system throughput, we may run multiple batches concurrently in a pipelined fashion. With properly designed pipelining, better hardware utilization can be achieved without sacrificing latency. We propose the rotational staggered pipelining to solve this problem.", "original_types": ["text", "header"], "id": 914}
{"type": "section", "content": "Evaluation\n\nTestbed. We deploy Lamina on a real heterogeneous cluster with two kinds of GPU nodes. Each node consists of either eight H100 or H20 GPUs, and each GPU is paired with a dedicated ConnectX-7 NIC via PCIe switch. The GPU nodes are interconnected with 400 Gbps RoCE network. We use H100 as compute-optimized GPUs and H20 as memory-optimized GPUs for Lamina.\n\nModels. Lamina supports a wide variety of LLM architectures, including OPT [58], LLaMA [48], and LLaMA3 [9]. All these architectures have similar outlines and workload characteristics and only have minor differences irrelevant to system designs. Hence, as listed in Table 3, we choose LLaMA-33B, LLaMA-65B, and LLaMA3-70B for evaluations. All model parameters and KV caches are stored in FP16 format.\n\nWorkloads To mirror the real-world LLM use cases, we use four request traces collected from the production systems of two LLM service providers, Azure [1,40] and Kimi [41]. Due to data protection regulations, these traces only contain the sequence length of user requests but not the actual contents. Hence, we use requests of dummy tokens with the same sequence length for evaluation. The summaries of these traces, including the average prompt tokens (lp) and average generated tokens (lg), are listed in Table 4.", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Evaluation\n\nTestbed. We deploy Lamina on a real heterogeneous cluster with two kinds of GPU nodes. Each node consists of either eight H100 or H20 GPUs, and each GPU is paired with a dedicated ConnectX-7 NIC via PCIe switch. The GPU nodes are interconnected with 400 Gbps RoCE network. We use H100 as compute-optimized GPUs and H20 as memory-optimized GPUs for Lamina.\n\nModels. Lamina supports a wide variety of LLM architectures, including OPT [58], LLaMA [48], and LLaMA3 [9]. All these architectures have similar outlines and workload characteristics and only have minor differences irrelevant to system designs. Hence, as listed in Table 3, we choose LLaMA-33B, LLaMA-65B, and LLaMA3-70B for evaluations. All model parameters and KV caches are stored in FP16 format.\n\nWorkloads To mirror the real-world LLM use cases, we use four request traces collected from the production systems of two LLM service providers, Azure [1,40] and Kimi [41]. Due to data protection regulations, these traces only contain the sequence length of user requests but not the actual contents. Hence, we use requests of dummy tokens with the same sequence length for evaluation. The summaries of these traces, including the average prompt tokens (lp) and average generated tokens (lg), are listed in Table 4.", "original_types": ["text", "header"], "id": 915}
{"type": "table", "content": "Table 3: Large language models used for evaluation.\nModel Parameters L d G\nLLaMA-33B 64.7 GB 60 6656 1\nLLaMA-65B 130.1 GB 80 8192 1\nLLaMA3-70B 137.5 GB 80 8192 8", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Table 3: Large language models used for evaluation.\nModel Parameters L d G\nLLaMA-33B 64.7 GB 60 6656 1\nLLaMA-65B 130.1 GB 80 8192 1\nLLaMA3-70B 137.5 GB 80 8192 8", "id": 916}
{"type": "table", "content": "Table 4: Request traces used for evaluation.\nTrace # Requests lp lg\nAzure-Conv 19366 1154.7 211.1\nAzure-Code 8819 2047.8 27.9\nKimi-Conv 12031 12035.1 342.6\nKimi-TA 23608 8560.0 182.1", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Table 4: Request traces used for evaluation.\nTrace # Requests lp lg\nAzure-Conv 19366 1154.7 211.1\nAzure-Code 8819 2047.8 27.9\nKimi-Conv 12031 12035.1 342.6\nKimi-TA 23608 8560.0 182.1", "id": 917}
{"type": "section", "content": "Baseline system. We compare with vLLM [28], a state-of-the-art LLM serving system optimized for high throughput. vLLM also integrates optimizations from other LLM inference systems, such as continuous batching from Orca [57]. We use vLLM with homogeneous H100 GPUs and use tensor parallel for multi-GPU inference. As Lamina only focuses on the decode phase, we modify vLLM to remove the prefill phase during evaluation for a fair comparison.\n\n6.1 Serving Performance\n\nWe evaluate the serving performance of Lamina against vLLM using real-world request traces. We first use homogeneous and heterogeneous hardware settings of similar costs, listed in Table 5, for vLLM and Lamina, respectively. Compared with vLLM, Lamina replaces half of the H100 devices to H20, which is cheaper but provides more memory capacity and bandwidth. We measure the token generation throughput, time between tokens (TBT), and average batch size.", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Baseline system. We compare with vLLM [28], a state-of-the-art LLM serving system optimized for high throughput. vLLM also integrates optimizations from other LLM inference systems, such as continuous batching from Orca [57]. We use vLLM with homogeneous H100 GPUs and use tensor parallel for multi-GPU inference. As Lamina only focuses on the decode phase, we modify vLLM to remove the prefill phase during evaluation for a fair comparison.\n\n6.1 Serving Performance\n\nWe evaluate the serving performance of Lamina against vLLM using real-world request traces. We first use homogeneous and heterogeneous hardware settings of similar costs, listed in Table 5, for vLLM and Lamina, respectively. Compared with vLLM, Lamina replaces half of the H100 devices to H20, which is cheaper but provides more memory capacity and bandwidth. We measure the token generation throughput, time between tokens (TBT), and average batch size.", "original_types": ["text", "header"], "id": 918}
{"type": "table", "content": "Table 5: Equal-cost hardware configurations for evaluation.\nModel Lamina vLLM\nLLaMA-33B DOP=(1,2) 2×H100 ($20.32/hr) ($22.12/hr)\nLLaMA-65B, LLaMA3-70B DOP=(2,4) 4×H100 ($40.64/hr) ($44.24/hr)", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Table 5: Equal-cost hardware configurations for evaluation.\nModel Lamina vLLM\nLLaMA-33B DOP=(1,2) 2×H100 ($20.32/hr) ($22.12/hr)\nLLaMA-65B, LLaMA3-70B DOP=(2,4) 4×H100 ($40.64/hr) ($44.24/hr)", "id": 919}
{"type": "section", "content": "As illustrated in Figure 10, Lamina consistently achieves 16.1 ~ 90.1% higher throughput than vLLM among all models and traces, given comparable hardware costs. This enhancement is primarily attributed to the larger batch size attained by Lamina, which is 2.39× of vLLM on average. These", "doc_id": "chen2024", "page": 9, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "As illustrated in Figure 10, Lamina consistently achieves 16.1 ~ 90.1% higher throughput than vLLM among all models and traces, given comparable hardware costs. This enhancement is primarily attributed to the larger batch size attained by Lamina, which is 2.39× of vLLM on average. These", "original_types": ["text"], "id": 920}
{"type": "figure", "content": "Figure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.", "doc_id": "chen2024", "page": 10, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Figure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.", "id": 921}
{"type": "figure", "content": "Figure 11: Decoding throughput and hardware cost with various hardware configurations. The DOPs for Lamina and tensor parallelisms for vLLM are annotated in the plot. The configuration with best cost efficiency is bolded.", "doc_id": "chen2024", "page": 10, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Figure 11: Decoding throughput and hardware cost with various hardware configurations. The DOPs for Lamina and tensor parallelisms for vLLM are annotated in the plot. The configuration with best cost efficiency is bolded.", "id": 922}
{"type": "section", "content": "results demonstrate that Lamina effectively leverages the extra memory capacity provided by memory-optimized devices to boost decoding throughput. Note that the throughput and batch size of LLaMA3-70B is much larger than LLaMA-33B and LLaMA-65B; this is because LLaMA3-70B adopts GQA with a group size of 8, which results in a much smaller KV cache size per request.", "doc_id": "chen2024", "page": 10, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "results demonstrate that Lamina effectively leverages the extra memory capacity provided by memory-optimized devices to boost decoding throughput. Note that the throughput and batch size of LLaMA3-70B is much larger than LLaMA-33B and LLaMA-65B; this is because LLaMA3-70B adopts GQA with a group size of 8, which results in a much smaller KV cache size per request.", "original_types": ["text"], "id": 923}
{"type": "section", "content": "6.2 Latency Breakdown\n\nLatency is a crucial indicator of the service quality offered by LLM applications. In this subsection, we measure the time between tokens (TBT) across various system configurations, as well as the execution time for model and attention workers and the networking overhead. We use requests with fixed sequence lengths (4096 or 8192) as the workload and disable rotational pipelining to better reveal the time breakdown. As we can see from Figure 12, for smaller batch sizes, the model execution time dominates the token generation latency. The attention and networking latency rapidly increases for larger batch sizes, while the model execution time remains almost constant. This indicates that the computation resource utilization gets improved as batch size increases. Note that the observed TBT might be less than the sum of model worker time, attention worker time, and network time. This is due to the automated resource utilization overlapping optimization, which will be further profiled in subsection 6.4.", "doc_id": "chen2024", "page": 11, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "6.2 Latency Breakdown\n\nLatency is a crucial indicator of the service quality offered by LLM applications. In this subsection, we measure the time between tokens (TBT) across various system configurations, as well as the execution time for model and attention workers and the networking overhead. We use requests with fixed sequence lengths (4096 or 8192) as the workload and disable rotational pipelining to better reveal the time breakdown. As we can see from Figure 12, for smaller batch sizes, the model execution time dominates the token generation latency. The attention and networking latency rapidly increases for larger batch sizes, while the model execution time remains almost constant. This indicates that the computation resource utilization gets improved as batch size increases. Note that the observed TBT might be less than the sum of model worker time, attention worker time, and network time. This is due to the automated resource utilization overlapping optimization, which will be further profiled in subsection 6.4.", "original_types": ["text", "header"], "id": 924}
{"type": "section", "content": "6.4 Resource Utilization Overlapping\n\nTo assess the efficacy of resource utilization overlapping (subsection 4.2.2) implemented in our automated model converter, we conducted a series of experiments on the LLaMA-65B and LLaMA3-70B models, with the optimization either enabled or disabled. We use request batches of varying sizes and the context length of each request is fixed at 4096.", "doc_id": "chen2024", "page": 12, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "6.4 Resource Utilization Overlapping\n\nTo assess the efficacy of resource utilization overlapping (subsection 4.2.2) implemented in our automated model converter, we conducted a series of experiments on the LLaMA-65B and LLaMA3-70B models, with the optimization either enabled or disabled. We use request batches of varying sizes and the context length of each request is fixed at 4096.", "original_types": ["text", "header"], "id": 925}
{"type": "figure", "content": "Figure 14: Time between tokens (TBT) results with automatic resource utilization overlapping enabled and disabled.", "doc_id": "chen2024", "page": 12, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Figure 14: Time between tokens (TBT) results with automatic resource utilization overlapping enabled and disabled.", "id": 926}
{"type": "table", "content": "Table 1: Title...", "doc_id": "chen2024", "page": 12, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Table 1: Title...", "id": 927}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "chen2024", "page": 12, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Figure 1: Title...", "id": 928}
{"type": "section", "content": "8 Related Work\n\nSystem optimizations for LLM Inference. Splitwise [40] and DistServe [59] proposes prefill-decode disaggregation, which improves hardware utilization and minimizes the interference between the prefill and decode phases. Orca [57] proposes continuous batching, that batches incoming requests in iteration granularity. Compared with whole-request batching, continuous batching greatly reduces resource waste caused by early termination during the decode phase. PagedAttention [28] focuses on memory management optimizations, using fine-grained KV cache management to reduce memory waste. PagedAttention can also be used to optimize various decoding scenarios, like beam search and shared prefixes. These optimizations can all be used in our system. FlexGen [44] is a heterogeneous LLM inference system employing layer-and token-level task partitioning and scheduling. However, it does not account for the varying characteristics of different operators within a layer. LLM-tailored inference systems, like DeepSeed [11], Megatron-LM [45], and TensorRT-LLM [39], use optimizations of various aspects including kernel optimization [17, 23], advanced scheduling [8, 19, 33, 51], and efficient memory management [19].\n\n7 Discussion\n\nGenerality of our techniques. Although Lamina is built for model-attention disaggregation, relevant techniques can also be used to enable a wider range of fine-grained LLM disaggregation techniques in distributed heterogeneous environments. For example, LoRA [24] and Mixture-of-Experts (MoE) [18, 53] all add less computation-intensive operators to existing LLM architectures. Like Lamina, we may also offload the LoRA and MoE operators to less powerful but more economic remote accelerators to reduce the inference cost. Such operator-level disaggregations, unlike prefill-decode disaggregation, require frequent layer-wise communications and are considered not feasible unless an optimized networking stack like the one in Lamina is used. Alternative heterogeneous devices. In Lamina, we may use more specialized accelerating devices for optimal performance and cost. For example, we anticipate that Processing-in-Memory (PIM) devices [13, 22, 26, 29, 30, 32, 47, 54] will be a more suitable candidate for memory-optimized devices as they demonstrate even greater cost advantages alongside their larger capacity and higher bandwidth. Besides, we can also use CPU and DRAM for attention computation and KV cache storage. However, due to the relatively smaller bandwidth of host DRAM, it is preferable to also adopt sparse attention mechanisms [14, 52] to reduce the size of data read during attention computation.", "doc_id": "chen2024", "page": 12, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "8 Related Work\n\nSystem optimizations for LLM Inference. Splitwise [40] and DistServe [59] proposes prefill-decode disaggregation, which improves hardware utilization and minimizes the interference between the prefill and decode phases. Orca [57] proposes continuous batching, that batches incoming requests in iteration granularity. Compared with whole-request batching, continuous batching greatly reduces resource waste caused by early termination during the decode phase. PagedAttention [28] focuses on memory management optimizations, using fine-grained KV cache management to reduce memory waste. PagedAttention can also be used to optimize various decoding scenarios, like beam search and shared prefixes. These optimizations can all be used in our system. FlexGen [44] is a heterogeneous LLM inference system employing layer-and token-level task partitioning and scheduling. However, it does not account for the varying characteristics of different operators within a layer. LLM-tailored inference systems, like DeepSeed [11], Megatron-LM [45], and TensorRT-LLM [39], use optimizations of various aspects including kernel optimization [17, 23], advanced scheduling [8, 19, 33, 51], and efficient memory management [19].\n\n7 Discussion\n\nGenerality of our techniques. Although Lamina is built for model-attention disaggregation, relevant techniques can also be used to enable a wider range of fine-grained LLM disaggregation techniques in distributed heterogeneous environments. For example, LoRA [24] and Mixture-of-Experts (MoE) [18, 53] all add less computation-intensive operators to existing LLM architectures. Like Lamina, we may also offload the LoRA and MoE operators to less powerful but more economic remote accelerators to reduce the inference cost. Such operator-level disaggregations, unlike prefill-decode disaggregation, require frequent layer-wise communications and are considered not feasible unless an optimized networking stack like the one in Lamina is used. Alternative heterogeneous devices. In Lamina, we may use more specialized accelerating devices for optimal performance and cost. For example, we anticipate that Processing-in-Memory (PIM) devices [13, 22, 26, 29, 30, 32, 47, 54] will be a more suitable candidate for memory-optimized devices as they demonstrate even greater cost advantages alongside their larger capacity and higher bandwidth. Besides, we can also use CPU and DRAM for attention computation and KV cache storage. However, due to the relatively smaller bandwidth of host DRAM, it is preferable to also adopt sparse attention mechanisms [14, 52] to reduce the size of data read during attention computation.", "original_types": ["text", "header"], "id": 929}
{"type": "section", "content": "Conclusion\n\nIn this paper, we present model-attention disaggregation, an innovative architectural approach to improve the efficiency of LLM decoding. This approach is motivated by the observation that the LLM decoding phase can be divided into computation-intensive parts and memory-intensive parts (i.e., the attention operators). Hence, we may use computation- and memory-optimized devices for each part to improve the hardware resource utilization. Moreover, by adjusting the To realize this idea, we design a revamped latency-optimized networking stack that facilitate the frequent data transfer between remote GPUs. We also develop automated tools for transforming and optimizing existing LLMs for model-attention disaggregation. We develop and deploy Lamina on a cluster comprising heterogeneous GPUs. Evaluation on traces collected from production systems show that Lamina provides 16.1 ∼ 90.1% higher throughput than heterogeneous solutions with similar hardware costs.", "doc_id": "chen2024", "page": 13, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Conclusion\n\nIn this paper, we present model-attention disaggregation, an innovative architectural approach to improve the efficiency of LLM decoding. This approach is motivated by the observation that the LLM decoding phase can be divided into computation-intensive parts and memory-intensive parts (i.e., the attention operators). Hence, we may use computation- and memory-optimized devices for each part to improve the hardware resource utilization. Moreover, by adjusting the To realize this idea, we design a revamped latency-optimized networking stack that facilitate the frequent data transfer between remote GPUs. We also develop automated tools for transforming and optimizing existing LLMs for model-attention disaggregation. We develop and deploy Lamina on a cluster comprising heterogeneous GPUs. Evaluation on traces collected from production systems show that Lamina provides 16.1 ∼ 90.1% higher throughput than heterogeneous solutions with similar hardware costs.", "original_types": ["text", "header"], "id": 930}
{"type": "section", "content": "[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. Newton: A dram-maker’s accelerator-in-memory (aim) architecture for machine learning. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 372–385, 2020.\n\n[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus, 2023.\n\n[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\n[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and Kevin Skadron. Scaling with design constraints: Predicting the future of big chips. IEEE Micro, 31(4):16–29, 2011.\n\n[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu Kim, Yuhwan Ro, Seungwon Lee, David Wang, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim. Aquabolt-xl hbm2-pim, lpddr5-pim with in-memory processing, and axdimm with acceleration buffer. IEEE Micro, 42(3):20–30, 2022.\n\n[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\n[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagdattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, page 611–626, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jeongbin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoong Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. System architecture and software stack for gddr6-aim. In 2022 IEEE Hot Chips 34 Symposium (HCS), pages 1–25, 2022.\n\n[30] Ann Franchesca Laguna, Arman Kazemi, Michael Niemier, and X. Sharon Hu. In-memory computing based accelerator for transformer networks for long sequences. In 2021 Design, Automation and Test in Europe Conference & Exhibition (DATE), pages 1839–1844, 2021.\n\n[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2023.", "doc_id": "chen2024", "page": 14, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. Newton: A dram-maker’s accelerator-in-memory (aim) architecture for machine learning. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 372–385, 2020.\n\n[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus, 2023.\n\n[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\n[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and Kevin Skadron. Scaling with design constraints: Predicting the future of big chips. IEEE Micro, 31(4):16–29, 2011.\n\n[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu Kim, Yuhwan Ro, Seungwon Lee, David Wang, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim. Aquabolt-xl hbm2-pim, lpddr5-pim with in-memory processing, and axdimm with acceleration buffer. IEEE Micro, 42(3):20–30, 2022.\n\n[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\n[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagdattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, page 611–626, New York, NY, USA, 2023. Association for Computing Machinery.\n\n[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jeongbin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoong Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. System architecture and software stack for gddr6-aim. In 2022 IEEE Hot Chips 34 Symposium (HCS), pages 1–25, 2022.\n\n[30] Ann Franchesca Laguna, Arman Kazemi, Michael Niemier, and X. Sharon Hu. In-memory computing based accelerator for transformer networks for long sequences. In 2021 Design, Automation and Test in Europe Conference & Exhibition (DATE), pages 1839–1844, 2021.\n\n[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2023.", "original_types": ["text"], "id": 931}
{"type": "section", "content": "[32] Wantong Li, Madison Manley, James Read, Ankit Kaul, Muhammed S. Bakir, and Shimeng Yu. H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(10):1592–1602, 2023.\n\n[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. AlpaServe: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pages 663–679, 2023.\n\n[34", "doc_id": "chen2024", "page": 14, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "[32] Wantong Li, Madison Manley, James Read, Ankit Kaul, Muhammed S. Bakir, and Shimeng Yu. H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(10):1592–1602, 2023.\n\n[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. AlpaServe: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pages 663–679, 2023.\n\n[34", "original_types": ["text"], "id": 932}
{"type": "section", "content": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context, 2023.\n\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding, 2023.\n\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137–22176. PMLR, 2023.\n\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-hao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.\n\nNVIDIA. Tensorrt-llm: A tensorrt toolbox for optimized large language model inference. https://github.com/NVIDIA/TensorRT-LLM.\n\nPratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2024.\n\nRuoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: A kvcache-centric disaggregated architecture for llm serving. arXiv preprint arXiv:2407.00079, 2024.\n\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.\n\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\n\nFranyell Silfa, Jose Maria Arnau, and Antonio González. E-batch: Energy-efficient and high-throughput rnn batching. ACM Trans. Archit. Code Optim., 19(1), jan 2022.\n\nShrihari Sridharan, Jacob R. Stevens, Kaushik Roy, and Anand Raghunathan. X-former: In-memory acceleration of transformers. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(8):1223–1233, 2023.", "doc_id": "chen2024", "page": 15, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.\n\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context, 2023.\n\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding, 2023.\n\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137–22176. PMLR, 2023.\n\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xin-hao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.\n\nNVIDIA. Tensorrt-llm: A tensorrt toolbox for optimized large language model inference. https://github.com/NVIDIA/TensorRT-LLM.\n\nPratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2024.\n\nRuoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: A kvcache-centric disaggregated architecture for llm serving. arXiv preprint arXiv:2407.00079, 2024.\n\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.\n\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\n\nFranyell Silfa, Jose Maria Arnau, and Antonio González. E-batch: Energy-efficient and high-throughput rnn batching. ACM Trans. Archit. Code Optim., 19(1), jan 2022.\n\nShrihari Sridharan, Jacob R. Stevens, Kaushik Roy, and Anand Raghunathan. X-former: In-memory acceleration of transformers. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(8):1223–1233, 2023.", "original_types": ["text"], "id": 933}
{"type": "section", "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\nSamuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful visual performance model for multicore architectures. Commun. ACM, 52(4):65–76, apr 2009.\n\nBingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models, 2023.\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024.\n\nLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Offloading-efficient moe model serving, 2024.\n\nXiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen. Retransformer: Reram-based processing-in-memory architecture for transformer acceleration. In Proceedings of", "doc_id": "chen2024", "page": 15, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\nSamuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful visual performance model for multicore architectures. Commun. ACM, 52(4):65–76, apr 2009.\n\nBingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models, 2023.\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024.\n\nLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Offloading-efficient moe model serving, 2024.\n\nXiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen. Retransformer: Reram-based processing-in-memory architecture for transformer acceleration. In Proceedings of", "original_types": ["text"], "id": 934}
{"type": "section", "content": "the 39th International Conference on Computer-Aided Design, ICCAD '20, New York, NY, USA, 2020. Association for Computing Machinery.\n\n[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming Zhuang, Weifeng Zhang, Dharmesh Jani, and Peipei Zhou. Challenges and opportunities to enable large-scale computing via heterogeneous chiplets. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 765–770. IEEE, 2024.\n\n[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling long-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n\n[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-jeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n\n[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-Serve: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193–210, 2024.", "doc_id": "chen2024", "page": 16, "url": "https://arxiv.org/pdf/2405.01814", "embedded_text": "the 39th International Conference on Computer-Aided Design, ICCAD '20, New York, NY, USA, 2020. Association for Computing Machinery.\n\n[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming Zhuang, Weifeng Zhang, Dharmesh Jani, and Peipei Zhou. Challenges and opportunities to enable large-scale computing via heterogeneous chiplets. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 765–770. IEEE, 2024.\n\n[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling long-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n\n[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-jeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n\n[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-Serve: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193–210, 2024.", "original_types": ["text"], "id": 935}
{"type": "section", "content": "Abstract\n\nLarge language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored. In this paper, we show that our model, namely FLM-101B, trained with our growth strategy under a budget of $100K, reaches 80% of the baselines' performances with only 10% of their floating-point operations. We believe that further studies on progressive training will benefit the community by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is publicly available.", "doc_id": "li2025a", "page": 1, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Abstract\n\nLarge language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored. In this paper, we show that our model, namely FLM-101B, trained with our growth strategy under a budget of $100K, reaches 80% of the baselines' performances with only 10% of their floating-point operations. We believe that further studies on progressive training will benefit the community by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is publicly available.", "original_types": ["text", "header"], "id": 936}
{"type": "figure", "content": "Figure 1: An overview of different growth strategies. (a): a baseline with constant number of parameters. (b): a straightforward linear growth strategy, cost-saving being exactly 50%; (c): a superlinear strategy with > 50% cost saving; (d): sublinear strategy saving the cost by less than 50%.", "doc_id": "li2025a", "page": 1, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Figure 1: An overview of different growth strategies. (a): a baseline with constant number of parameters. (b): a straightforward linear growth strategy, cost-saving being exactly 50%; (c): a superlinear strategy with > 50% cost saving; (d): sublinear strategy saving the cost by less than 50%.", "id": 937}
{"type": "section", "content": "Introduction\n\nLarge language models (LLMs) (Radford et al. 2018; Touvron et al. 2023a; Devlin et al. 2019; Raffel et al. 2020) have consistently demonstrated their efficacy across a spectrum of applications, especially in language processing (Wang et al. 2022b,a; Fan et al. 2022; Liu et al. 2022) and multimodal tasks (Zhao et al. 2023; Meng et al. 2023). Despite variations in architectural designs, a universal challenge confronting all LLMs is the escalating cost associated with their training. Recent trends indicate a shift towards utilizing larger amounts of data (e.g., 1.4T tokens for Llama-1 (Touvron et al. 2023a), 2T tokens for Llama-2 (Touvron et al. 2023b), and 15T tokens for Llama-3 (Meta 2024)). Meanwhile, the sizes of open-sourced models continue to increase (Penedo et al. 2023; Bi et al. 2024; Mistral 2024). Consequently, a major focus within LLM research is the development of innovative methodologies that effectively mitigate training expenses, aligning with the broader objectives of Green AI (Schwartz et al. 2020).\nIn this paper, we present our exploration to train an LLM at the 100B-parameter scale using a growth strategy inspired by previous research on progressive learning (Gong et al. 2019; Gu et al. 2021; Yao et al. 2024) and neurogenesis (Eriksson et al. 1998). “Growth” means dynamic expansion of the parameter number count, from small to large, through the training progresses. Figure 1 illustrates three typical growth strategies: linear, sublinear, and superlinear. As the FLOPs of LLMs are approximately proportional to their number of parameters (Hoffmann et al. 2022), the area under the parameter curve represents the computational cost of training.\nWhile existing studies on scaling laws (Hoffmann et al. 2022) suggest that training a smaller model with more data may potentially result in higher scores on some tasks under a fixed FLOPs budget, they mainly consider the scenarios where model sizes are fixed through training. We believe that verifying the feasibility of a growth strategy (Gu et al. 2021; Shen et al. 2022; Chen et al. 2022; Yao et al. 2024) for extremely large models would be an important completion to scaling laws. To maximize computational efficiency, we strategically focus on implementing an aggressive growth strategy (Figure 1 (c)) for sanity check. We adapt the MSG (Yao et al. 2024) growth operators to train a model at 100B+ scale. We fix our budget to be $100K with 192 A800 GPUs.\nAt the 100B scale, it is impractical to conduct strict head-", "doc_id": "li2025a", "page": 1, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Introduction\n\nLarge language models (LLMs) (Radford et al. 2018; Touvron et al. 2023a; Devlin et al. 2019; Raffel et al. 2020) have consistently demonstrated their efficacy across a spectrum of applications, especially in language processing (Wang et al. 2022b,a; Fan et al. 2022; Liu et al. 2022) and multimodal tasks (Zhao et al. 2023; Meng et al. 2023). Despite variations in architectural designs, a universal challenge confronting all LLMs is the escalating cost associated with their training. Recent trends indicate a shift towards utilizing larger amounts of data (e.g., 1.4T tokens for Llama-1 (Touvron et al. 2023a), 2T tokens for Llama-2 (Touvron et al. 2023b), and 15T tokens for Llama-3 (Meta 2024)). Meanwhile, the sizes of open-sourced models continue to increase (Penedo et al. 2023; Bi et al. 2024; Mistral 2024). Consequently, a major focus within LLM research is the development of innovative methodologies that effectively mitigate training expenses, aligning with the broader objectives of Green AI (Schwartz et al. 2020).\nIn this paper, we present our exploration to train an LLM at the 100B-parameter scale using a growth strategy inspired by previous research on progressive learning (Gong et al. 2019; Gu et al. 2021; Yao et al. 2024) and neurogenesis (Eriksson et al. 1998). “Growth” means dynamic expansion of the parameter number count, from small to large, through the training progresses. Figure 1 illustrates three typical growth strategies: linear, sublinear, and superlinear. As the FLOPs of LLMs are approximately proportional to their number of parameters (Hoffmann et al. 2022), the area under the parameter curve represents the computational cost of training.\nWhile existing studies on scaling laws (Hoffmann et al. 2022) suggest that training a smaller model with more data may potentially result in higher scores on some tasks under a fixed FLOPs budget, they mainly consider the scenarios where model sizes are fixed through training. We believe that verifying the feasibility of a growth strategy (Gu et al. 2021; Shen et al. 2022; Chen et al. 2022; Yao et al. 2024) for extremely large models would be an important completion to scaling laws. To maximize computational efficiency, we strategically focus on implementing an aggressive growth strategy (Figure 1 (c)) for sanity check. We adapt the MSG (Yao et al. 2024) growth operators to train a model at 100B+ scale. We fix our budget to be $100K with 192 A800 GPUs.\nAt the 100B scale, it is impractical to conduct strict head-", "original_types": ["text", "header"], "id": 938}
{"type": "section", "content": "2. Design Overview of FLM-101B\n\nIn this section, we provide an outline of FLM-101B, detailing its architecture, pre-training methods, and configuration specifics.\n\n2.1 Architecture\n\nBackbone. Among the many existing model architectures, we adopt FreeLM (Li et al. 2023) as the backbone for our models, with modifications. FreeLM is based on GPT (Radford et al. 2019), a transformer-like architecture with a decoder-only configuration. Different from GPT, FreeLM features two pre-training objectives: the language objective and the teacher objective (Section 2.2). We preserve the GPT-3-style transformer block designs without incorporating the later modifications from Llama series. We employ the tokenizer derived from GPT-4, characterized by a vocabulary size of 100, 256.\n\n2.2 Pre-Training Setup\n\nFLM-101B. By design, FLM-101B is an English-Chinese bilingual model. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5%. Inspired by the finding that instruction data can augment LLMs' comprehension capabilities (Ouyang et al. 2022), we integrate multi-task instructionally prompted data: OIG (Open Instruction Generalist) \\({ }^{1}\\) and COIG (Chinese Open Instruction Generalist) \\({ }^{2}\\), in the pre-training stage.\n\neFLM-16B. To analyse the effect of domain-specific knowledge data (Section 4.2), we apply the FreeLM teacher signals (Li et al. 2023) to enhance factual capability. Due to computational cost, we incorporate these signals only in the smallest 16B model. This enhanced model is named eFLM-16B.\n\nSpecifically, we employ two emojis: \\(\\odot\\) (U+1F621) and \\(\\odot\\) (U+1F608) \\({ }^{3}\\), from the vocabulary to replace the original binary classification labels (Li et al. 2023). For the teacher signal, we supervise only on the emoji tokens, unifying the objective with language modeling. Moreover, we discard the original multi-task alternating approach and completely mix the samples from both sides in every batch. This strategy can enhance the consistency of data sampling distribution as well as improve training stability.", "doc_id": "li2025a", "page": 2, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "2. Design Overview of FLM-101B\n\nIn this section, we provide an outline of FLM-101B, detailing its architecture, pre-training methods, and configuration specifics.\n\n2.1 Architecture\n\nBackbone. Among the many existing model architectures, we adopt FreeLM (Li et al. 2023) as the backbone for our models, with modifications. FreeLM is based on GPT (Radford et al. 2019), a transformer-like architecture with a decoder-only configuration. Different from GPT, FreeLM features two pre-training objectives: the language objective and the teacher objective (Section 2.2). We preserve the GPT-3-style transformer block designs without incorporating the later modifications from Llama series. We employ the tokenizer derived from GPT-4, characterized by a vocabulary size of 100, 256.\n\n2.2 Pre-Training Setup\n\nFLM-101B. By design, FLM-101B is an English-Chinese bilingual model. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46.5%. Inspired by the finding that instruction data can augment LLMs' comprehension capabilities (Ouyang et al. 2022), we integrate multi-task instructionally prompted data: OIG (Open Instruction Generalist) \\({ }^{1}\\) and COIG (Chinese Open Instruction Generalist) \\({ }^{2}\\), in the pre-training stage.\n\neFLM-16B. To analyse the effect of domain-specific knowledge data (Section 4.2), we apply the FreeLM teacher signals (Li et al. 2023) to enhance factual capability. Due to computational cost, we incorporate these signals only in the smallest 16B model. This enhanced model is named eFLM-16B.\n\nSpecifically, we employ two emojis: \\(\\odot\\) (U+1F621) and \\(\\odot\\) (U+1F608) \\({ }^{3}\\), from the vocabulary to replace the original binary classification labels (Li et al. 2023). For the teacher signal, we supervise only on the emoji tokens, unifying the objective with language modeling. Moreover, we discard the original multi-task alternating approach and completely mix the samples from both sides in every batch. This strategy can enhance the consistency of data sampling distribution as well as improve training stability.", "original_types": ["text", "header"], "id": 939}
{"type": "table", "content": "Table 1: Partial configurations for different growth stages.\nMarkdown representation of the table", "doc_id": "li2025a", "page": 2, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 1: Partial configurations for different growth stages.\nMarkdown representation of the table", "id": 940}
{"type": "section", "content": "2.3 Growth Strategy\n\nThe essence of the low cost in scaling up FLM-101B is the growth strategy. Specifically, we train three models, with 16B, 51B, and 101B parameters, respectively, in a sequential manner. Each model inherits knowledge from its predecessor. This is contrary to the common practice that the models of different sizes are trained independently (Touvron et al. 2023a,b).\n\nFunction-preserving Growth. Function preservation means that before and after growth, the models yield consistent outputs given the same arbitrary inputs. This property has proven", "doc_id": "li2025a", "page": 2, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "2.3 Growth Strategy\n\nThe essence of the low cost in scaling up FLM-101B is the growth strategy. Specifically, we train three models, with 16B, 51B, and 101B parameters, respectively, in a sequential manner. Each model inherits knowledge from its predecessor. This is contrary to the common practice that the models of different sizes are trained independently (Touvron et al. 2023a,b).\n\nFunction-preserving Growth. Function preservation means that before and after growth, the models yield consistent outputs given the same arbitrary inputs. This property has proven", "original_types": ["text", "header"], "id": 941}
{"type": "table", "content": "Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.\n|\n|---|---|---|---|---|---|---|\n|Params (billion)| Tensor Parallel Size| Pipeline Parallel Size| Data Parallel Size| Number of GPUs| Batch Size| teraFLOP/s per GPU| FLOPs Utilization|\n|---|---|---|---|---|---|---|---|\n|16| 2| 1| 96| 192| 2304| 162| 51.90%|\n|51| 4| 2| 24| 192| 2304| 160| 51.30%|\n|101| 4| 4| 12| 192| 2160| 165| 52.88%|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|", "doc_id": "li2025a", "page": 3, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.\n|\n|---|---|---|---|---|---|---|\n|Params (billion)| Tensor Parallel Size| Pipeline Parallel Size| Data Parallel Size| Number of GPUs| Batch Size| teraFLOP/s per GPU| FLOPs Utilization|\n|---|---|---|---|---|---|---|---|\n|16| 2| 1| 96| 192| 2304| 162| 51.90%|\n|51| 4| 2| 24| 192| 2304| 160| 51.30%|\n|101| 4| 4| 12| 192| 2160| 165| 52.88%|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|", "id": 942}
{"type": "table", "content": "Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, net tCO2e, and their formulas are the same as (Patterson et al. 2021).\n|\n|---\n|Model| GPT-3 | Gopher | PaLM | GLM-130B | Llama-2 | FLM-101B\n\n|Params| 175B | 280B | 540B | 130B | 70B | 101B\n|GPU Hours| 3.55e6 | 3.77e6 | 8.40e6 | 1.11e6 | 1.72e6 | 1.01e5\n|Chip Power/TDP| 330 | 283 | 378.5 | 400 | 400 | 400\n|Energy (MkWh)| 1171 | 1066 | 3179 | 444 | 688 | 40\n|net tCO2e| 552 | 380 | 271 | 257 | 291 | 26\n", "doc_id": "li2025a", "page": 4, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, net tCO2e, and their formulas are the same as (Patterson et al. 2021).\n|\n|---\n|Model| GPT-3 | Gopher | PaLM | GLM-130B | Llama-2 | FLM-101B\n\n|Params| 175B | 280B | 540B | 130B | 70B | 101B\n|GPU Hours| 3.55e6 | 3.77e6 | 8.40e6 | 1.11e6 | 1.72e6 | 1.01e5\n|Chip Power/TDP| 330 | 283 | 378.5 | 400 | 400 | 400\n|Energy (MkWh)| 1171 | 1066 | 3179 | 444 | 688 | 40\n|net tCO2e| 552 | 380 | 271 | 257 | 291 | 26\n", "id": 943}
{"type": "section", "content": "Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.", "doc_id": "li2025a", "page": 5, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floating-point operations (zetta = 10^21) of the training process for reference.", "original_types": ["header"], "id": 944}
{"type": "table", "content": "Table 4\n| Model | Cost (zettaFLOPs) | Average | ARC | HellaSwag | MMLU | TruthfulQA |\n|-------|-------------------|---------|-----|----------|-------|-------------|\n| Llama-2 (13B) | 201.37 (±28.77) | 58.66 | 59.39 | 82.13 | 55.77 | 37.38 |\n| Llama-2 (7B) | 106.60 (±15.23) | 54.32 | 53.07 | 78.59 | 46.87 | 38.76 |\n| Llama (13B) | 94.81 (±13.54) | 56.08 | 56.23 | 80.93 | 47.67 | 39.48 |\n| Llama (7B) | 49.54 (±7.08) | 49.72 | 51.02 | 77.82 | 35.71 | 34.33 |\n| GLM-130B | 210.80 | 48.11 | 42.15 | 67.91 | 42.59 | 39.80 |\n\n| FLM-101B | 28.22 | 43.94 | 39.76 | 66.23 | 28.30* | 41.47 |\n\n*44.50 for a knowledge-enhanced eFLM-16B (Section 2.2, 4.2).", "doc_id": "li2025a", "page": 5, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 4\n| Model | Cost (zettaFLOPs) | Average | ARC | HellaSwag | MMLU | TruthfulQA |\n|-------|-------------------|---------|-----|----------|-------|-------------|\n| Llama-2 (13B) | 201.37 (±28.77) | 58.66 | 59.39 | 82.13 | 55.77 | 37.38 |\n| Llama-2 (7B) | 106.60 (±15.23) | 54.32 | 53.07 | 78.59 | 46.87 | 38.76 |\n| Llama (13B) | 94.81 (±13.54) | 56.08 | 56.23 | 80.93 | 47.67 | 39.48 |\n| Llama (7B) | 49.54 (±7.08) | 49.72 | 51.02 | 77.82 | 35.71 | 34.33 |\n| GLM-130B | 210.80 | 48.11 | 42.15 | 67.91 | 42.59 | 39.80 |\n\n| FLM-101B | 28.22 | 43.94 | 39.76 | 66.23 | 28.30* | 41.47 |\n\n*44.50 for a knowledge-enhanced eFLM-16B (Section 2.2, 4.2).", "id": 945}
{"type": "section", "content": "Table 5: Performance of eFLM-16B and baselines on C-eval. In this table, eFLM-16B refers to the professional-knowledge-enhanced FLM-16B. Note that C-Eval leaderboard only keeps one decimal place for the evaluation results.", "doc_id": "li2025a", "page": 5, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 5: Performance of eFLM-16B and baselines on C-eval. In this table, eFLM-16B refers to the professional-knowledge-enhanced FLM-16B. Note that C-Eval leaderboard only keeps one decimal place for the evaluation results.", "original_types": ["header"], "id": 946}
{"type": "table", "content": "Table 5\n| Model | Average | Average (Hard) | STEM | Social Science | Humanities | Others |\n|-------|---------|----------------|------|----------------|----------|----------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n| GLM-130B | 44.0 | 30.7 | 36.7 | 55.8 | 47.7 | 43.0 |\n\n| eFLM-16B | 46.1 | 28.9 | 38.3 | 53.7 | 46.8 | 52.6 |\n\n", "doc_id": "li2025a", "page": 5, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 5\n| Model | Average | Average (Hard) | STEM | Social Science | Humanities | Others |\n|-------|---------|----------------|------|----------------|----------|----------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n| GLM-130B | 44.0 | 30.7 | 36.7 | 55.8 | 47.7 | 43.0 |\n\n| eFLM-16B | 46.1 | 28.9 | 38.3 | 53.7 | 46.8 | 52.6 |\n\n", "id": 947}
{"type": "section", "content": "and Chinese is reported to be 1:1, the cost of GLM-130B for English is 210.80 zettaFLOPs, and the same for Chinese. The data ratio of FLM-101B is 53.5% : 46.5% for English and Chinese. The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).\n\nCarbon Footprint Analysis. An important measurement of a model’s environmental impact (Schwartz et al. 2020) is the carbon footprints originated from the pre-training process. We estimate carbon emission with the methods provided in (Patterson et al. 2021). We summarize the carbon footprint statistics of FLM-101B and well-known LLMs in Table 3. Our model yields only 1/10 pre-training carbon footprint of a typical LLM.", "doc_id": "li2025a", "page": 5, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "and Chinese is reported to be 1:1, the cost of GLM-130B for English is 210.80 zettaFLOPs, and the same for Chinese. The data ratio of FLM-101B is 53.5% : 46.5% for English and Chinese. The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).\n\nCarbon Footprint Analysis. An important measurement of a model’s environmental impact (Schwartz et al. 2020) is the carbon footprints originated from the pre-training process. We estimate carbon emission with the methods provided in (Patterson et al. 2021). We summarize the carbon footprint statistics of FLM-101B and well-known LLMs in Table 3. Our model yields only 1/10 pre-training carbon footprint of a typical LLM.", "original_types": ["text"], "id": 948}
{"type": "section", "content": "Table 6: Performance of the three stages of FLM on Open LLM. To reduce the computational cost during evaluation, we sample 20% and 30% items for HellaSwag and MMLU tasks, respectively.", "doc_id": "li2025a", "page": 6, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 6: Performance of the three stages of FLM on Open LLM. To reduce the computational cost during evaluation, we sample 20% and 30% items for HellaSwag and MMLU tasks, respectively.", "original_types": ["header"], "id": 949}
{"type": "table", "content": "Table 6: Performance of the three stages of FLM on Open LLM.\n| Parameters | Training Data | Average | ARC | Hellaswag | MMLU | TruthfulQA |\n|---|---|---|---|---|---|---|\n| 16B | 245.37B | 39.19 | 32.25 | 58.57 | 27.02 | 38.92 |\n| 51B | 39.64B | 41.79 | 35.32 | 64.04 | 27.66 | 40.12 |\n| 101B | 26.54B | 44.41 | 39.76 | 67.88 | 28.54 | 41.47 |\n", "doc_id": "li2025a", "page": 6, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 6: Performance of the three stages of FLM on Open LLM.\n| Parameters | Training Data | Average | ARC | Hellaswag | MMLU | TruthfulQA |\n|---|---|---|---|---|---|---|\n| 16B | 245.37B | 39.19 | 32.25 | 58.57 | 27.02 | 38.92 |\n| 51B | 39.64B | 41.79 | 35.32 | 64.04 | 27.66 | 40.12 |\n| 101B | 26.54B | 44.41 | 39.76 | 67.88 | 28.54 | 41.47 |\n", "id": 950}
{"type": "section", "content": "Rule Understanding\n\nWe consider the understanding and execution of rules being a strong indication of reasoning capability. To this end, we design rule understanding evaluation. Note that this test is different from reasoning based on the chain of thought. Detailed discussion is provided in Appendix A.2.", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Rule Understanding\n\nWe consider the understanding and execution of rules being a strong indication of reasoning capability. To this end, we design rule understanding evaluation. Note that this test is different from reasoning based on the chain of thought. Detailed discussion is provided in Appendix A.2.", "original_types": ["text", "header"], "id": 951}
{"type": "table", "content": "Table 9: Performance of FLM-101B, GPT-3, and GLM-130B on rule understanding tasks.\nMarkdown representation of the table", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 9: Performance of FLM-101B, GPT-3, and GLM-130B on rule understanding tasks.\nMarkdown representation of the table", "id": 952}
{"type": "section", "content": "Pattern Mining\n\nPattern Mining evaluation is common in IQ tests. In detail, it is the induction and deduction of the patterns emerging in a new context.", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Pattern Mining\n\nPattern Mining evaluation is common in IQ tests. In detail, it is the induction and deduction of the patterns emerging in a new context.", "original_types": ["text", "header"], "id": 953}
{"type": "table", "content": "Table 10: Performance of FLM-101B, GPT-3, and GLM-130B on pattern mining tasks.\nMarkdown representation of the table", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 10: Performance of FLM-101B, GPT-3, and GLM-130B on pattern mining tasks.\nMarkdown representation of the table", "id": 954}
{"type": "section", "content": "Anti-interference\n\nAnti-interference capability is critical for finding and utilizing information that is truly related to a specific goal, in an unseen and noisy context (Appendix Figure 3). As suggested by the cocktail party problem in speech recognition (Qian et al. 2018), we consider anti-interference ability to be important for intelligent agents. We conduct anti-interference evaluation in three task types: Multiple Key Retrieval, Single Supporting Fact Tracking, and Two Supporting Facts Tracking, as exemplified in Figure 3 in the Appendix.", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Anti-interference\n\nAnti-interference capability is critical for finding and utilizing information that is truly related to a specific goal, in an unseen and noisy context (Appendix Figure 3). As suggested by the cocktail party problem in speech recognition (Qian et al. 2018), we consider anti-interference ability to be important for intelligent agents. We conduct anti-interference evaluation in three task types: Multiple Key Retrieval, Single Supporting Fact Tracking, and Two Supporting Facts Tracking, as exemplified in Figure 3 in the Appendix.", "original_types": ["text", "header"], "id": 955}
{"type": "table", "content": "Table 11: Performance of FLM-101B, GPT-3, and GLM-130B on anti-interference evaluation.\nMarkdown representation of the table", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Table 11: Performance of FLM-101B, GPT-3, and GLM-130B on anti-interference evaluation.\nMarkdown representation of the table", "id": 956}
{"type": "section", "content": "Conclusions, Limitations, and Future Work\n\nIn this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget. The key idea of reducing the training cost of FLM-101B is to break through the fixed number of model parameters via a growth strategy. Experimental results on knowledg-oriented and IQ-related benchmarks show that FLM-101B is comparable to strong baseline models with less computational cost. Note that harmful contents may be induced from the open-sourced checkpoint, which do not represent the opinions of the authors.", "doc_id": "li2025a", "page": 7, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Conclusions, Limitations, and Future Work\n\nIn this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget. The key idea of reducing the training cost of FLM-101B is to break through the fixed number of model parameters via a growth strategy. Experimental results on knowledg-oriented and IQ-related benchmarks show that FLM-101B is comparable to strong baseline models with less computational cost. Note that harmful contents may be induced from the open-sourced checkpoint, which do not represent the opinions of the authors.", "original_types": ["text", "header"], "id": 957}
{"type": "section", "content": "as well as training stability would potentially be beneficial for future attempts of further scaling up LLMs, e.g., beyond 1T parameters.\n\nReferences\n\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu, E.; Clark, J. H.; Shafey, L. E.; Huang, Y.; Meier-Hellstern, K.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.; Ruder, S.; Tay, Y.; Xiao, K.; Xu, Y.; Zhang, Y.; Ábrego, G. H.; Ahn, J.; Austin, J.; Barham, P.; Botha, J. A.; Bradbury, J.; Brahma, S.; Brooks, K.; Catasta, M.; Cheng, Y.; Cherry, C.; Choquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave, S.; Dehghani, M.; Dev, S.; Devlin, J.; Díaz, M.; Du, N.; Dyer, E.; Feinberg, V.; Feng, F.; Fienberg, M.; Garcia, X.; Gehrmann, S.; Gonzalez, L.; and et al. 2023. PaLM 2 Technical Report. CoRR, abs/2305.10403.\n\nBi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.; Ding, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954.\n\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901.\n\nChen, C.; Yin, Y.; Shang, L.; Jiang, X.; Qin, Y.; Wang, F.; Wang, Z.; Chen, X.; Liu, Z.; and Liu, Q. 2022. bert2BERT: Towards Reusable Pretrained Language Models. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2134–2148. Association for Computational Linguistics.\n\nChen, T.; Goodfellow, I. J.; and Shlens, J. 2016. Net2Net: Accelerating Learning via Knowledge Transfer. In Bengio, Y.; and LeCun, Y., eds., 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.\n\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. CoRR, abs/1803.05457.\n\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), 4171–4186. Association for Computational Linguistics.\n\nEriksson, P. S.; Perfilieva, E.; Björk-Eriksson, T.; Alborn, A.-M.; Nordborg, C.; Peterson, D. A.; and Gage, F. H. 1998. Neurogenesis in the adult human hippocampus. Nature medicine, 4(11): 1313–1317.", "doc_id": "li2025a", "page": 8, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "as well as training stability would potentially be beneficial for future attempts of further scaling up LLMs, e.g., beyond 1T parameters.\n\nReferences\n\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu, E.; Clark, J. H.; Shafey, L. E.; Huang, Y.; Meier-Hellstern, K.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.; Ruder, S.; Tay, Y.; Xiao, K.; Xu, Y.; Zhang, Y.; Ábrego, G. H.; Ahn, J.; Austin, J.; Barham, P.; Botha, J. A.; Bradbury, J.; Brahma, S.; Brooks, K.; Catasta, M.; Cheng, Y.; Cherry, C.; Choquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave, S.; Dehghani, M.; Dev, S.; Devlin, J.; Díaz, M.; Du, N.; Dyer, E.; Feinberg, V.; Feng, F.; Fienberg, M.; Garcia, X.; Gehrmann, S.; Gonzalez, L.; and et al. 2023. PaLM 2 Technical Report. CoRR, abs/2305.10403.\n\nBi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.; Ding, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954.\n\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901.\n\nChen, C.; Yin, Y.; Shang, L.; Jiang, X.; Qin, Y.; Wang, F.; Wang, Z.; Chen, X.; Liu, Z.; and Liu, Q. 2022. bert2BERT: Towards Reusable Pretrained Language Models. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2134–2148. Association for Computational Linguistics.\n\nChen, T.; Goodfellow, I. J.; and Shlens, J. 2016. Net2Net: Accelerating Learning via Knowledge Transfer. In Bengio, Y.; and LeCun, Y., eds., 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.\n\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. CoRR, abs/1803.05457.\n\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), 4171–4186. Association for Computational Linguistics.\n\nEriksson, P. S.; Perfilieva, E.; Björk-Eriksson, T.; Alborn, A.-M.; Nordborg, C.; Peterson, D. A.; and Gage, F. H. 1998. Neurogenesis in the adult human hippocampus. Nature medicine, 4(11): 1313–1317.", "original_types": ["text", "header"], "id": 958}
{"type": "section", "content": "Fan, S.; Wang, Y.; Li, J.; Zhang, Z.; Shang, S.; and Han, P. 2022. Interactive Information Extraction by Semantic Information Graph. In Raedt, L. D., ed., Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, 4100–4106. ijcai.org.\n\nGong, L.; He, D.; Li, Z.; Qin, T.; Wang, L.; and Liu, T. 2019. Efficient training of bert by progressively stacking. In International conference on machine learning, 2337–2346. PMLR.\n\nGu, X.; Liu, L.; Yu, H.; Li, J.; Chen, C.; and Han, J. 2021. On the Transformer Growth for Progressive BERT Training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 5174–5180.\n\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multi-task Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\nHenighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.; Jackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.; Hallacy, C.; Mann, B.; Radford, A.; Ramesh, A.; Ryder, N.; Ziegler, D. M.; Schulman, J.; Amodei, D.; and McCandlish, S. 2020. Scaling Laws for Autoregressive Generative Modeling. CoRR, abs/2010.14701.\n\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.; Welbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.; van den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Simonyan, K.; Elsen, E.; Vinyals, O.; Rae, J. W.; and Sifre, L. 2022. An empirical analysis of compute-optimal large language model training. In NeurIPS.\n\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He, J. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. CoRR, abs/2305.08322.\n\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020. Scaling Laws for Neural Language Models. CoRR, abs/2001.08361.\n\nKorthikanti, V.; Casper, J.; Lym, S.; McAfee, L.; Andersch, M.; Shoeybi, M.; and Catanzaro, B. 2022. Reducing Activation Recomputation in Large Transformer Models. arXiv:2205.05198.\n\nLi, X.; Jiang, X.; Meng, X.; Sun, A.; and Wang, Y. 2023. FreeLM: Fine-Tuning-Free Language Model. CoRR, abs/2305.01616.\n\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 3214–3252. Association for Computational Linguistics.\n\nLittwin, E.; and Yang, G. 2023. Adaptive Optimization in the ∞-Width Limit. In The Eleventh International Conference", "doc_id": "li2025a", "page": 8, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Fan, S.; Wang, Y.; Li, J.; Zhang, Z.; Shang, S.; and Han, P. 2022. Interactive Information Extraction by Semantic Information Graph. In Raedt, L. D., ed., Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, 4100–4106. ijcai.org.\n\nGong, L.; He, D.; Li, Z.; Qin, T.; Wang, L.; and Liu, T. 2019. Efficient training of bert by progressively stacking. In International conference on machine learning, 2337–2346. PMLR.\n\nGu, X.; Liu, L.; Yu, H.; Li, J.; Chen, C.; and Han, J. 2021. On the Transformer Growth for Progressive BERT Training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 5174–5180.\n\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multi-task Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\nHenighan, T.; Kaplan, J.; Katz, M.; Chen, M.; Hesse, C.; Jackson, J.; Jun, H.; Brown, T. B.; Dhariwal, P.; Gray, S.; Hallacy, C.; Mann, B.; Radford, A.; Ramesh, A.; Ryder, N.; Ziegler, D. M.; Schulman, J.; Amodei, D.; and McCandlish, S. 2020. Scaling Laws for Autoregressive Generative Modeling. CoRR, abs/2010.14701.\n\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.; Cai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.; Welbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.; van den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Simonyan, K.; Elsen, E.; Vinyals, O.; Rae, J. W.; and Sifre, L. 2022. An empirical analysis of compute-optimal large language model training. In NeurIPS.\n\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He, J. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. CoRR, abs/2305.08322.\n\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and Amodei, D. 2020. Scaling Laws for Neural Language Models. CoRR, abs/2001.08361.\n\nKorthikanti, V.; Casper, J.; Lym, S.; McAfee, L.; Andersch, M.; Shoeybi, M.; and Catanzaro, B. 2022. Reducing Activation Recomputation in Large Transformer Models. arXiv:2205.05198.\n\nLi, X.; Jiang, X.; Meng, X.; Sun, A.; and Wang, Y. 2023. FreeLM: Fine-Tuning-Free Language Model. CoRR, abs/2305.01616.\n\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 3214–3252. Association for Computational Linguistics.\n\nLittwin, E.; and Yang, G. 2023. Adaptive Optimization in the ∞-Width Limit. In The Eleventh International Conference", "original_types": ["text"], "id": 959}
{"type": "section", "content": "on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n\nLiu, Y.; Wang, Y.; Sun, A.; Meng, X.; Li, J.; and Guo, J. 2022. A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. In Carpuat, M.; de Marneffe, M.; and Ruíz, I. V. M., eds., Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, 1670–1680. Association for Computational Linguistics.\n\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay Regularization in Adam. CoRR, abs/1711.05101.\n\nMeng, X.; Lin, C.; Wang, Y.; and Zhang, Y. 2023. Net-GPT: Generative Pretrained Transformer for Network Traffic. CoRR, abs/2304.09513.\n\nMeta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3/\n\nMistral. 2024. Mistral 8x22B. https://mistral.ai/news/mixtral-8x22b/\n\nNarayanan, D.; Shoeybi, M.; Casper, J.; LeGresley, P.; Patwary, M.; Korthikanti, V.; Vainbrand, D.; Kashinkunti, P.; Bernauer, J.; Catanzaro, B.; Phanishayee, A.; and Zaharia, M. 2021. Efficient Large-Scale Language Model Training on GPU Clusters. CoRR, abs/2104.04473.\n\nOpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774.\n\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions with human feedback. In NeurIPS.\n\nPatterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia, L.-M.; Rothchild, D.; So, D.; Texier, M.; and Dean, J. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350.\n\nPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.\n\nQian, Y.; Weng, C.; Chang, X.; Wang, S.; and Yu, D. 2018. Past review, current progress, and challenges ahead on the cocktail party problem. Frontiers Inf. Technol. Electron. Eng., 19(1): 40–63.\n\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. 2018. Improving language understanding by generative pre-training.\n\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language Models are Unsupervised Multitask Learners.", "doc_id": "li2025a", "page": 9, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n\nLiu, Y.; Wang, Y.; Sun, A.; Meng, X.; Li, J.; and Guo, J. 2022. A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. In Carpuat, M.; de Marneffe, M.; and Ruíz, I. V. M., eds., Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, 1670–1680. Association for Computational Linguistics.\n\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay Regularization in Adam. CoRR, abs/1711.05101.\n\nMeng, X.; Lin, C.; Wang, Y.; and Zhang, Y. 2023. Net-GPT: Generative Pretrained Transformer for Network Traffic. CoRR, abs/2304.09513.\n\nMeta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. https://ai.meta.com/blog/meta-llama-3/\n\nMistral. 2024. Mistral 8x22B. https://mistral.ai/news/mixtral-8x22b/\n\nNarayanan, D.; Shoeybi, M.; Casper, J.; LeGresley, P.; Patwary, M.; Korthikanti, V.; Vainbrand, D.; Kashinkunti, P.; Bernauer, J.; Catanzaro, B.; Phanishayee, A.; and Zaharia, M. 2021. Efficient Large-Scale Language Model Training on GPU Clusters. CoRR, abs/2104.04473.\n\nOpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774.\n\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions with human feedback. In NeurIPS.\n\nPatterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia, L.-M.; Rothchild, D.; So, D.; Texier, M.; and Dean, J. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350.\n\nPenedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.\n\nQian, Y.; Weng, C.; Chang, X.; Wang, S.; and Yu, D. 2018. Past review, current progress, and challenges ahead on the cocktail party problem. Frontiers Inf. Technol. Electron. Eng., 19(1): 40–63.\n\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.; et al. 2018. Improving language understanding by generative pre-training.\n\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language Models are Unsupervised Multitask Learners.", "original_types": ["text"], "id": 960}
{"type": "section", "content": "Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, H. F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; Rutherford, E.; Hennigan, T.; Menick, J.; Cassirer, A.; Powell, R.; van den Driessche, G.; Hendricks, L. A.; Rauh, M.; Huang, P.; Glaese, A.; Welbl, J.; Dathathri, S.; Huang, S.; Uesato, J.; Mellor, J.; Higgins, I.; Creswell, A.; McAleese, N.; Wu, A.; Elsen, E.; Jayakumar, S. M.; Buchatskaya, E.; Budden, D.; Sutherland, E.; Simonyan, K.; Paganini, M.; Sifre, L.; Martens, L.; Li, X. L.; Kuncoro, A.; Nematzadeh, A.; Gribovskaya, E.; Donato, D.; Lazaridou, A.; Mensch, A.; Lespiau, J.; Tsimpoukelli, M.; Grigorev, N.; Fritz, D.; Sottiaux, T.; Pajarskas, M.; Pohlen, T.; Gong, Z.; Toyama, D.; de Masson d’Autume, C.; Li, Y.; Terzi, T.; Mikulik, V.; Babuschkin, I.; Clark, A.; de Las Casas, D.; Guy, A.; Jones, C.; Bradbury, J.; Johnson, M. J.; Hechtman, B. A.; Weidinger, L.; Gabriel, I.; Isaac, W.; Lockhart, E.; Osindero, S.; Rimell, L.; Dyer, C.; Vinyals, O.; Ayoub, K.; Stanway, J.; Bennett, L.; Hassabis, D.; Kavukcuoglu, K.; and Irving, G. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. CoRR, abs/2112.11446.\n\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res., 21: 140:1–140:67.\n\nRajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y. 2019. ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. CoRR, abs/1910.02054.\n\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow, D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; Tow, J.; Rush, A. M.; Biderman, S.; Webson, A.; Ammanamanchi, P. S.; Wang, T.; Sagot, B.; Muennighoff, N.; del Moral, A. V.; Ruwase, O.; Bawden, R.; Bekman, S.; McMillan-Major, A.; Beltagy, I.; Nguyen, H.; Saulnier, L.; Tan, S.; Suarez, P. O.; Sanh, V.; Laurençon, H.; Jernite, Y.; Launay, J.; Mitchell, M.; Raffel, C.; Gokaslan, A.; Simhi, A.; Soroa, A.; Aji, A. F.; Alfassy, A.; Rogers, A.; Nitzav, A. K.; Xu, C.; Mou, C.; Emezue, C.; Klamm, C.; Leong, C.; van Strien, D.; Adelani, D. I.; and et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. CoRR, abs/2211.05100.\n\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2020. Green ai. Communications of the ACM, 63(12): 54–63.\n\nShen, S.; Walsh, P.; Keutzer, K.; Dodge, J.; Peters, M. E.; and Beltagy, I. 2022. Staged Training for Transformer Language Models. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvári, C.; Niu, G.; and Sabato, S., eds., International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, 19893–19908. PMLR.\n\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper, J.; and Catanzaro, B. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRR, abs/1909.08053.", "doc_id": "li2025a", "page": 9, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, H. F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; Rutherford, E.; Hennigan, T.; Menick, J.; Cassirer, A.; Powell, R.; van den Driessche, G.; Hendricks, L. A.; Rauh, M.; Huang, P.; Glaese, A.; Welbl, J.; Dathathri, S.; Huang, S.; Uesato, J.; Mellor, J.; Higgins, I.; Creswell, A.; McAleese, N.; Wu, A.; Elsen, E.; Jayakumar, S. M.; Buchatskaya, E.; Budden, D.; Sutherland, E.; Simonyan, K.; Paganini, M.; Sifre, L.; Martens, L.; Li, X. L.; Kuncoro, A.; Nematzadeh, A.; Gribovskaya, E.; Donato, D.; Lazaridou, A.; Mensch, A.; Lespiau, J.; Tsimpoukelli, M.; Grigorev, N.; Fritz, D.; Sottiaux, T.; Pajarskas, M.; Pohlen, T.; Gong, Z.; Toyama, D.; de Masson d’Autume, C.; Li, Y.; Terzi, T.; Mikulik, V.; Babuschkin, I.; Clark, A.; de Las Casas, D.; Guy, A.; Jones, C.; Bradbury, J.; Johnson, M. J.; Hechtman, B. A.; Weidinger, L.; Gabriel, I.; Isaac, W.; Lockhart, E.; Osindero, S.; Rimell, L.; Dyer, C.; Vinyals, O.; Ayoub, K.; Stanway, J.; Bennett, L.; Hassabis, D.; Kavukcuoglu, K.; and Irving, G. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. CoRR, abs/2112.11446.\n\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res., 21: 140:1–140:67.\n\nRajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y. 2019. ZeRO: Memory Optimization Towards Training A Trillion Parameter Models. CoRR, abs/1910.02054.\n\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow, D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; Tow, J.; Rush, A. M.; Biderman, S.; Webson, A.; Ammanamanchi, P. S.; Wang, T.; Sagot, B.; Muennighoff, N.; del Moral, A. V.; Ruwase, O.; Bawden, R.; Bekman, S.; McMillan-Major, A.; Beltagy, I.; Nguyen, H.; Saulnier, L.; Tan, S.; Suarez, P. O.; Sanh, V.; Laurençon, H.; Jernite, Y.; Launay, J.; Mitchell, M.; Raffel, C.; Gokaslan, A.; Simhi, A.; Soroa, A.; Aji, A. F.; Alfassy, A.; Rogers, A.; Nitzav, A. K.; Xu, C.; Mou, C.; Emezue, C.; Klamm, C.; Leong, C.; van Strien, D.; Adelani, D. I.; and et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. CoRR, abs/2211.05100.\n\nSchwartz, R.; Dodge, J.; Smith, N. A.; and Etzioni, O. 2020. Green ai. Communications of the ACM, 63(12): 54–63.\n\nShen, S.; Walsh, P.; Keutzer, K.; Dodge, J.; Peters, M. E.; and Beltagy, I. 2022. Staged Training for Transformer Language Models. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvári, C.; Niu, G.; and Sabato, S., eds., International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, 19893–19908. PMLR.\n\nShoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper, J.; and Catanzaro, B. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRR, abs/1909.08053.", "original_types": ["text"], "id": 961}
{"type": "section", "content": "Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et al. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.\n\nSu, J.; Lu, Y.; Pan, S.; Wen, B.; and Liu, Y. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. CoRR, abs/2104.09864.\n\nSun, Y.; Dong, L.; Patra, B.; Ma, S.; Huang, S.; Benhaim, A.; Chaudhary, V.; Song, X.; and Wei, F. 2023. A Length-", "doc_id": "li2025a", "page": 9, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et al. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.\n\nSu, J.; Lu, Y.; Pan, S.; Wen, B.; and Liu, Y. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. CoRR, abs/2104.09864.\n\nSun, Y.; Dong, L.; Patra, B.; Ma, S.; Huang, S.; Benhaim, A.; Chaudhary, V.; Song, X.; and Wei, F. 2023. A Length-", "original_types": ["text"], "id": 962}
{"type": "section", "content": "Extrapolatable Transformer. In Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 14590–14604. Association for Computational Linguistics.\n\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation Language Models. CoRR, abs/2302.13971.\n\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hossseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR, abs/2307.09288.\n\nValiant, L. G. 1990. A Bridging Model for Parallel Computation. Commun. ACM, 33(8): 103–111.\n\nWang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Wallach, H. M.; Larochelle, H.; Beygelzimer, A.; d’Alché-Buc, F.; Fox, E. B.; and Garnett, R., eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 3261–3275.\n\nWang, Y.; Li, X.; Sun, A.; Meng, X.; Liao, H.; and Guo, J. 2022a. CofeNet: Context and Former-Label Enhanced Net for Complicated Quotation Extraction. In Calzolari, N.; Huang, C.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.; Ryu, P.; Chen, H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue, N.; Kim, S.; Hahn, Y.; He, Z.; Lee, T. K.; Santus, E.; Bond, F.; and Na, S., eds., Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, 2438–2449. International Committee on Computational Linguistics.", "doc_id": "li2025a", "page": 10, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Extrapolatable Transformer. In Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 14590–14604. Association for Computational Linguistics.\n\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation Language Models. CoRR, abs/2302.13971.\n\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hossseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR, abs/2307.09288.\n\nValiant, L. G. 1990. A Bridging Model for Parallel Computation. Commun. ACM, 33(8): 103–111.\n\nWang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Wallach, H. M.; Larochelle, H.; Beygelzimer, A.; d’Alché-Buc, F.; Fox, E. B.; and Garnett, R., eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 3261–3275.\n\nWang, Y.; Li, X.; Sun, A.; Meng, X.; Liao, H.; and Guo, J. 2022a. CofeNet: Context and Former-Label Enhanced Net for Complicated Quotation Extraction. In Calzolari, N.; Huang, C.; Kim, H.; Pustejovsky, J.; Wanner, L.; Choi, K.; Ryu, P.; Chen, H.; Donatelli, L.; Ji, H.; Kurohashi, S.; Paggio, P.; Xue, N.; Kim, S.; Hahn, Y.; He, Z.; Lee, T. K.; Santus, E.; Bond, F.; and Na, S., eds., Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, 2438–2449. International Committee on Computational Linguistics.", "original_types": ["text"], "id": 963}
{"type": "section", "content": "Wang, Y.; Zhang, H.; Sun, A.; and Meng, X. 2022b. CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 7064–7075. Association for Computational Linguistics.\n\nWatkins, C. E.; Campbell, V. L.; Nieberding, R.; and Hallmark, R. 1995. Contemporary practice of psychological assessment by clinical psychologists. Professional psychology: Research and practice, 26(1): 54.\n\nWei, J. W.; Hou, L.; Lampinen, A. K.; Chen, X.; Huang, D.; Tay, Y.; Chen, X.; Lu, Y.; Zhou, D.; Ma, T.; and Le, Q. V. 2023. Symbol tuning improves in-context learning in language models. CoRR, abs/2305.08298.\n\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; Van Merriënboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.\n\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y.; Xu, Y.; Sun, K.; Yu, D.; Yu, C.; Tian, Y.; Dong, Q.; Liu, W.; Shi, B.; Cui, Y.; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y.; Patterson, Y.; Tian, Z.; Zhang, Y.; Zhou, H.; Liu, S.; Zhao, Z.; Zhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and Lan, Z. 2020. CLUE: A Chinese Language Understanding Evaluation Benchmark. In Scott, D.; Bel, N.; and Zong, C., eds., Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, 4762–4772. International Committee on Computational Linguistics.\n\nYang, G.; and Hu, E. J. 2021. Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, 11727–11737. PMLR.\n\nYang, G.; Hu, E. J.; Babuschkin, I.; Sidor, S.; Liu, X.; Farhi, D.; Ryder, N.; Pachocki, J.; Chen, W.; and Gao, J. 2021. Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 17084–17097.\n\nYao, Y.; and Wang, Y. 2023. Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. CoRR, abs/2304.06875.\n\nYao, Y.; Zhang, Z.; Li, J.; and Wang, Y. 2024. Masked Structural Growth for 2x Faster Language Model Pre-training. In The Twelfth International Conference on Learning Representations.", "doc_id": "li2025a", "page": 10, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Wang, Y.; Zhang, H.; Sun, A.; and Meng, X. 2022b. CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts. In Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 7064–7075. Association for Computational Linguistics.\n\nWatkins, C. E.; Campbell, V. L.; Nieberding, R.; and Hallmark, R. 1995. Contemporary practice of psychological assessment by clinical psychologists. Professional psychology: Research and practice, 26(1): 54.\n\nWei, J. W.; Hou, L.; Lampinen, A. K.; Chen, X.; Huang, D.; Tay, Y.; Chen, X.; Lu, Y.; Zhou, D.; Ma, T.; and Le, Q. V. 2023. Symbol tuning improves in-context learning in language models. CoRR, abs/2305.08298.\n\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; Van Merriënboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.\n\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y.; Xu, Y.; Sun, K.; Yu, D.; Yu, C.; Tian, Y.; Dong, Q.; Liu, W.; Shi, B.; Cui, Y.; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y.; Patterson, Y.; Tian, Z.; Zhang, Y.; Zhou, H.; Liu, S.; Zhao, Z.; Zhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and Lan, Z. 2020. CLUE: A Chinese Language Understanding Evaluation Benchmark. In Scott, D.; Bel, N.; and Zong, C., eds., Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, 4762–4772. International Committee on Computational Linguistics.\n\nYang, G.; and Hu, E. J. 2021. Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, 11727–11737. PMLR.\n\nYang, G.; Hu, E. J.; Babuschkin, I.; Sidor, S.; Liu, X.; Farhi, D.; Ryder, N.; Pachocki, J.; Chen, W.; and Gao, J. 2021. Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 17084–17097.\n\nYao, Y.; and Wang, Y. 2023. Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. CoRR, abs/2304.06875.\n\nYao, Y.; Zhang, Z.; Li, J.; and Wang, Y. 2024. Masked Structural Growth for 2x Faster Language Model Pre-training. In The Twelfth International Conference on Learning Representations.", "original_types": ["text"], "id": 964}
{"type": "section", "content": "Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. HellaSwag: Can a Machine Really Finish Your Sentence? In Korhonen, A.; Traum, D. R.; and Màrquez, L., eds., Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, 4791–4800. Association for Computational Linguistics.\n\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.; Xue, Y.; Zhai, J.; Chen, W.; Liu, Z.; Zhang, P.; Dong, Y.; and Tang, J. 2023. GLM-130B: An Open Bilingual Pre-trained Model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.", "doc_id": "li2025a", "page": 10, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. HellaSwag: Can a Machine Really Finish Your Sentence? In Korhonen, A.; Traum, D. R.; and Màrquez, L., eds., Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, 4791–4800. Association for Computational Linguistics.\n\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; Tam, W. L.; Ma, Z.; Xue, Y.; Zhai, J.; Chen, W.; Liu, Z.; Zhang, P.; Dong, Y.; and Tang, J. 2023. GLM-130B: An Open Bilingual Pre-trained Model. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.", "original_types": ["text"], "id": 965}
{"type": "section", "content": "Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.; and Wen, J. 2023. A Survey of Large Language Models. CoRR, abs/2303.18223.", "doc_id": "li2025a", "page": 11, "url": "https://arxiv.org/pdf/2309.03852", "embedded_text": "Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.; and Wen, J. 2023. A Survey of Large Language Models. CoRR, abs/2303.18223.", "original_types": ["text"], "id": 966}
{"type": "section", "content": "ABSTRACT\n\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational power and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has decreased, meaning that the users of these technologies have little to no information about their resource demands and subsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying AI’s environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized best-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping public understanding of AI’s environmental impacts, tracing their origins and their spread in both the media and scientific publications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and conclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate negative impacts in the future.", "doc_id": "luccioni2025c", "page": 1, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "ABSTRACT\n\nIn recent years, Artificial Intelligence (AI) models have grown in size and complexity, driving greater demand for computational power and natural resources. In parallel to this trend, transparency around the costs and impacts of these models has decreased, meaning that the users of these technologies have little to no information about their resource demands and subsequent impacts on the environment. Despite this dearth of adequate data, escalating demand for figures quantifying AI’s environmental impacts has led to numerous instances of misinformation evolving from inaccurate or de-contextualized best-effort estimates of greenhouse gas emissions. In this article, we explore pervasive myths and misconceptions shaping public understanding of AI’s environmental impacts, tracing their origins and their spread in both the media and scientific publications. We discuss the importance of data transparency in clarifying misconceptions and mitigating these harms, and conclude with a set of recommendations for how AI developers and policymakers can leverage this information to mitigate negative impacts in the future.", "original_types": ["text", "header"], "id": 967}
{"type": "section", "content": "Related Work", "doc_id": "luccioni2025c", "page": 2, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Related Work", "original_types": ["header"], "id": 968}
{"type": "section", "content": "Approaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as these systems have grown more impactful and widely deployed in user-facing applications. Initial studies, such as that of Strubell et al., underscored the environmental cost of training Transformer-based language models12. Research done in the following years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI models including GPT-3, T5, Meena, and Switch Transformer, providing new estimates16 and expanding the scope of analysis beyond model training to account for operational and embodied emissions17, improving methodology for software energy measurement18, and a lifecycle approach to assessing emissions from model training and deployment11. Wu et al. further advanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,19. Most recently, Luccioni, Jernite, and Strubell 20 pioneered the AI inference impact methodology, revealing generative architectures as particularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference impacts. These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22. Above and beyond energy considerations, Li et al.23 expanded the scope of AI environmental impact measurement by estimating the water footprint of GPT-3 based on publicly available information, whereas Han et al.24 assessed the public health toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. In another significant advancement, Google’s recent TPU lifecycle assessment25 offered the most comprehensive cradle-to-grave environmental analysis of AI hardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure, significantly extending existing environmental impact models. Building on many of these approaches, Morrison et al.26 performed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development, and training, enhancing the accuracy of these metrics through the use of granular underlying data. The breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating AI’s environmental impacts, and the many different perspectives that exist in this space. Whereas several standardized approaches have been proposed to measure different aspects of AI’s requirements in terms of energy and water, as well as the emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology and standards that cover all dimensions. In the next section, we examine how this translates into decreased environmental transparency in the AI industry via an empirical analysis of AI models over time.", "doc_id": "luccioni2025c", "page": 2, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Approaches to calculating the environmental impacts of AI systems have evolved significantly over the last several years, as these systems have grown more impactful and widely deployed in user-facing applications. Initial studies, such as that of Strubell et al., underscored the environmental cost of training Transformer-based language models12. Research done in the following years extended this analysis, for instance by calculating the energy use and GHG footprint for several notable AI models including GPT-3, T5, Meena, and Switch Transformer, providing new estimates16 and expanding the scope of analysis beyond model training to account for operational and embodied emissions17, improving methodology for software energy measurement18, and a lifecycle approach to assessing emissions from model training and deployment11. Wu et al. further advanced this analysis by explicitly mapping the environmental impacts across the entire AI development pipeline,19. Most recently, Luccioni, Jernite, and Strubell 20 pioneered the AI inference impact methodology, revealing generative architectures as particularly energy-intensive compared to task-specific models and underscoring the critical importance of addressing inference impacts. These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22. Above and beyond energy considerations, Li et al.23 expanded the scope of AI environmental impact measurement by estimating the water footprint of GPT-3 based on publicly available information, whereas Han et al.24 assessed the public health toll of AI training’s air pollution, finding that training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. In another significant advancement, Google’s recent TPU lifecycle assessment25 offered the most comprehensive cradle-to-grave environmental analysis of AI hardware to date, integrating embodied carbon data associated with manufacturing AI accelerators and data center infrastructure, significantly extending existing environmental impact models. Building on many of these approaches, Morrison et al.26 performed a holistic evaluation of the energy, carbon, and water impacts of AI hardware manufacturing, model development, and training, enhancing the accuracy of these metrics through the use of granular underlying data. The breadth and diversity of the analyses described in this section illustrate the multitude of factors involved in estimating AI’s environmental impacts, and the many different perspectives that exist in this space. Whereas several standardized approaches have been proposed to measure different aspects of AI’s requirements in terms of energy and water, as well as the emissions associated with model training and inference, the field is still currently lacking a comprehensive methodology and standards that cover all dimensions. In the next section, we examine how this translates into decreased environmental transparency in the AI industry via an empirical analysis of AI models over time.", "id": 969}
{"type": "section", "content": "Environmental Transparency Trends\n\nWhile there has been progress in developing more robust methodologies for measuring AI’s environmental impacts, the broader AI industry has paradoxically been trending in the opposite direction, disclosing less information over time. In order to quantify this trend, we analyze Epoch AI’s Notable AI Models dataset27, which tracks information on “models that were state of the art, highly cited, or otherwise historically notable”, with respect to transparency about the environmental impacts of those models. We examine the level of environmental impact transparency for each model based on key information from the Epoch AI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content (e.g., paper, model card, announcement). We select the time period starting in 2010 as this is the beginning of the modern “deep learning era” (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed, including all 754 models from 2010 to the first quarter of 2025. Our analysis, shown in Figure 1, reveals substantial variation in environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others provide no information at all regarding their approach. Overall, we find that models exhibit three transparency categories:", "doc_id": "luccioni2025c", "page": 2, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Environmental Transparency Trends\n\nWhile there has been progress in developing more robust methodologies for measuring AI’s environmental impacts, the broader AI industry has paradoxically been trending in the opposite direction, disclosing less information over time. In order to quantify this trend, we analyze Epoch AI’s Notable AI Models dataset27, which tracks information on “models that were state of the art, highly cited, or otherwise historically notable”, with respect to transparency about the environmental impacts of those models. We examine the level of environmental impact transparency for each model based on key information from the Epoch AI dataset (e.g., model accessibility, training compute estimation method) as well as from individual model release content (e.g., paper, model card, announcement). We select the time period starting in 2010 as this is the beginning of the modern “deep learning era” (as defined by Epoch AI), which is representative of the types of AI models currently trained and deployed, including all 754 models from 2010 to the first quarter of 2025. Our analysis, shown in Figure 1, reveals substantial variation in environmental impact transparency: some models disclose sufficient details to enable impact estimation, whereas others provide no information at all regarding their approach. Overall, we find that models exhibit three transparency categories:", "original_types": ["text", "header"], "id": 970}
{"type": "figure", "content": "Figure 1. Environmental Impact Transparency of Notable AI Models by Release Year", "doc_id": "luccioni2025c", "page": 3, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Figure 1. Environmental Impact Transparency of Notable AI Models by Release Year", "id": 971}
{"type": "section", "content": "yet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of training data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. This period includes the the work of Strubell et al.12, Luccioni11 and others. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.", "doc_id": "luccioni2025c", "page": 3, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "yet common practice, although many articles accompanying papers did provide related information about, e.g. the amount of training data used or number of epochs trained. From 2019 to 2022, transparency improved as awareness of impacts grew and open-weights model releases became more common. This period includes the the work of Strubell et al.12, Luccioni11 and others. The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.", "original_types": ["text"], "id": 972}
{"type": "section", "content": "Investigating the Urban Legends of AI's Environmental Impacts\n\nMaking sustainably-minded decisions when using AI systems requires having the necessary information about different aspects of their development and deployment. While there are empirical studies focusing on AI's environmental impacts, such as those cited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size, architecture, optimizations, hardware, location, setup, system) that they are not representative of. This fuels misinformation, undermines scientific research, and can result in decisions that are not grounded in facts²⁹. In the paragraphs below, we address some of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore their potential for spreading environmental misinformation.\n\nTraining an AI model emits as much CO₂ as five cars in their lifetimes", "doc_id": "luccioni2025c", "page": 4, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Investigating the Urban Legends of AI's Environmental Impacts\n\nMaking sustainably-minded decisions when using AI systems requires having the necessary information about different aspects of their development and deployment. While there are empirical studies focusing on AI's environmental impacts, such as those cited in previous sections, these numbers have often been taken out of context or used as proxies for conditions (e.g., model size, architecture, optimizations, hardware, location, setup, system) that they are not representative of. This fuels misinformation, undermines scientific research, and can result in decisions that are not grounded in facts²⁹. In the paragraphs below, we address some of the common estimates for the environmental impacts of AI, in an effort to contextualize their provenance and to explore their potential for spreading environmental misinformation.\n\nTraining an AI model emits as much CO₂ as five cars in their lifetimes", "original_types": ["text", "header"], "id": 973}
{"type": "section", "content": "Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,¹² which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer neural network architecture³⁰ that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.³¹ In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO₂-equivalent GHG emissions (CO₂e), or about five times the emissions of a car during its lifetime, including fuel. The research article was written for a specialized audience of AI and NLP researchers, who would have the background knowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and featuring a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked up by numerous media outlets (including MIT Technology Review³² and Forbes³³). The “five cars” number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO₂e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions. Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today’s generative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However, while the “five cars” estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI training workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the “five cars” estimate: Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO₂e,³⁴ over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO₂e³⁵ or over 40x the “five cars” estimate.", "doc_id": "luccioni2025c", "page": 4, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,¹² which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original “base” (65M) and “big” (213M parameter) variants of the Transformer neural network architecture³⁰ that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.³¹ In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO₂-equivalent GHG emissions (CO₂e), or about five times the emissions of a car during its lifetime, including fuel. The research article was written for a specialized audience of AI and NLP researchers, who would have the background knowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and featuring a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked up by numerous media outlets (including MIT Technology Review³² and Forbes³³). The “five cars” number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO₂e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions. Further, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today’s generative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However, while the “five cars” estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI training workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the “five cars” estimate: Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO₂e,³⁴ over 4x the estimate that forms the basis for the “five cars” number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO₂e³⁵ or over 40x the “five cars” estimate.", "id": 974}
{"type": "section", "content": "A request to ChatGPT consumes ten times more energy than a Google search\n\nAnother often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours (Wh) of energy, which is \"ten times more than a Google search\". This figure is often quoted in the press³⁶,³⁷ and in industry reports³⁸. Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet’s Chairman John Hennessy during a 2023 interview with Reuters, in which he said that “having an exchange with AI known as a large language model likely cost 10 times more than a standard keyword search”³⁹. This remark was used was the basis of an estimate published in October 2023 of “approximately 3 Wh per LLM interaction”⁴⁰, with the Google search number taken from a 2009 blog post from Google that stated that “Queries vary in degree of difficulty, but for the average query [...] this amounts to 0.0003 kWh of energy per search”⁴¹. This number is misleading for several reasons. First, Hennessy has no relation to", "doc_id": "luccioni2025c", "page": 4, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "A request to ChatGPT consumes ten times more energy than a Google search\n\nAnother often cited and misrepresented metric is the estimate that a single request to ChatGPT uses approximately 3 watt-hours (Wh) of energy, which is \"ten times more than a Google search\". This figure is often quoted in the press³⁶,³⁷ and in industry reports³⁸. Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet’s Chairman John Hennessy during a 2023 interview with Reuters, in which he said that “having an exchange with AI known as a large language model likely cost 10 times more than a standard keyword search”³⁹. This remark was used was the basis of an estimate published in October 2023 of “approximately 3 Wh per LLM interaction”⁴⁰, with the Google search number taken from a 2009 blog post from Google that stated that “Queries vary in degree of difficulty, but for the average query [...] this amounts to 0.0003 kWh of energy per search”⁴¹. This number is misleading for several reasons. First, Hennessy has no relation to", "original_types": ["text", "header"], "id": 975}
{"type": "section", "content": "OpenAI or Microsoft (which provides the compute for OpenAI’s services), so the comment he made was based on secondhand information. Second, even if Hennessy’s comparison were accurate, basing the search estimate on a figure that is 16 years old — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current Transformer-based models — is also bound to amplify the inaccuracy of the estimate.\n\nTo understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11, 2025, that appear when searching for “ChatGPT energy consumption” on Google News. For each article, we noted whether it mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by acknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on the energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural network architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when addressing this subject47, stating that the true figures remain unknown. It is also noteworthy that among these articles, 9% also relay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.", "doc_id": "luccioni2025c", "page": 5, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "OpenAI or Microsoft (which provides the compute for OpenAI’s services), so the comment he made was based on secondhand information. Second, even if Hennessy’s comparison were accurate, basing the search estimate on a figure that is 16 years old — at a time when Web search was done using bag-of-words or vector-based search techniques as opposed to the current Transformer-based models — is also bound to amplify the inaccuracy of the estimate.\n\nTo understand the impact of the propagation of this estimate, we analyzed 100 news articles published as of April 11, 2025, that appear when searching for “ChatGPT energy consumption” on Google News. For each article, we noted whether it mentioned the 3 Wh estimate, if it referenced others, or if it called for transparency or caution regarding figures by acknowledging uncertainty or suggesting that such statistics should be viewed critically. Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs43 (likely using the same 3 Wh figure), 11% prefer to provide global figures on the energy impact of data centers44, 8% discuss other topics, particularly DeepSeek45 and optimizations with ternary neural network architectures to improve energy efficiency46 and only 5% explicitly call for transparency or necessary caution when addressing this subject47, stating that the true figures remain unknown. It is also noteworthy that among these articles, 9% also relay the claim that training a LLM produces emissions equivalent to 5 cars in their lifetime.", "original_types": ["text"], "id": 976}
{"type": "figure", "content": "Figure 3. Analysis of media articles discussing ChatGPT energy consumption.", "doc_id": "luccioni2025c", "page": 5, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Figure 3. Analysis of media articles discussing ChatGPT energy consumption.", "id": 977}
{"type": "section", "content": "AI can reduce 10% of global emissions\nWhile the numbers around AI’s negative environmental impacts can be misinterpreted and taken out of context, so, too, can the potential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale. One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030—the equivalent of the total annual emissions of the European Union”48. The same number appears in a more recent BCG report from 2023, which was commissioned by Google and published ahead of COP2649. The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.\n\nApplying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T L.148050 or WBCSD guidance on avoided emissions51. And yet, these numbers were picked up in research52 and the media, used as evidence that the potential of AI to stop climate change is overwhelmingly positive53,54. While AI undoubtedly has potential positive applications in sectors ranging from transportation to agriculture to energy55, these global generalizations can be misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the beliefs that the benefits of AI will outweigh its costs56.", "doc_id": "luccioni2025c", "page": 5, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "AI can reduce 10% of global emissions\nWhile the numbers around AI’s negative environmental impacts can be misinterpreted and taken out of context, so, too, can the potential of AI to reduce emissions, especially by corporate actors that develop and deploy AI systems on a global scale. One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that “Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030—the equivalent of the total annual emissions of the European Union”48. The same number appears in a more recent BCG report from 2023, which was commissioned by Google and published ahead of COP2649. The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale.\n\nApplying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding—in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects—methodological requirements outlined in established recent standards like ITU-T L.148050 or WBCSD guidance on avoided emissions51. And yet, these numbers were picked up in research52 and the media, used as evidence that the potential of AI to stop climate change is overwhelmingly positive53,54. While AI undoubtedly has potential positive applications in sectors ranging from transportation to agriculture to energy55, these global generalizations can be misleading because they overlook the myriad of problems that technology alone cannot solve, while giving credibility to the beliefs that the benefits of AI will outweigh its costs56.", "original_types": ["text"], "id": 978}
{"type": "section", "content": "How to improve environmental impact disclosures in AI\n\nOpacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed procurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack the information necessary to develop evidence-based regulations. This opacity also generates cascading effects throughout value chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments. Furthermore, the absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that could drive efficiency improvements. Perhaps most critically, this lack of transparency undermines accountability mechanisms, making it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress toward sustainability goals.\n\nThis section explores how comprehensive environmental transparency can address these challenges through four interconnected pathways:\n\n1. Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and deployment;\n2. Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustainability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user enterprises;\n3. Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful comparisons; and\n4. Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.\n\nMeasurement and Disclosure", "doc_id": "luccioni2025c", "page": 6, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "How to improve environmental impact disclosures in AI\n\nOpacity in AI environmental reporting creates multiple interconnected challenges: organizations cannot make informed procurement or innovation decisions without access to reliable environmental performance data on AI, while policymakers lack the information necessary to develop evidence-based regulations. This opacity also generates cascading effects throughout value chains, as AI adoption creates unmeasured emissions that undermine corporate net zero commitments. Furthermore, the absence of standardized metrics prevents meaningful comparison between AI systems, limiting market mechanisms that could drive efficiency improvements. Perhaps most critically, this lack of transparency undermines accountability mechanisms, making it impossible to hold AI developers and deployers responsible for their environmental performance or to track progress toward sustainability goals.\n\nThis section explores how comprehensive environmental transparency can address these challenges through four interconnected pathways:\n\n1. Carrying out comprehensive measurement and disclosure by AI developers at each stage of model development and deployment;\n2. Integrating comprehensive AI environmental impacts into sustainability accounting frameworks and corporate sustainability disclosures by organizations across the entire AI value chain, from model providers and hyperscalers to end-user enterprises;\n3. Developing standardized verification and assurance frameworks to ensure data reliability and enable meaningful comparisons; and\n4. Implementing clear regulatory requirements by policymakers to ensure consistent, verifiable reporting across the industry.\n\nMeasurement and Disclosure", "original_types": ["text", "header", "list"], "id": 979}
{"type": "section", "content": "As the starting point of AI development, AI researchers and developers are able to gather empirical measurements from the systems they create at different steps of the model lifecycle. When developing models from scratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools like Code Carbon57 or no-code tools like Green Algorithms58. When using or adapting existing models, performance and efficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production. For instance, the AI Energy Score project21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets. These metrics should be reported in model cards59 and scientific publications with complete methodological transparency, including hardware specifications, geographic locations, electricity sources, measurement uncertainties, and allocation methodologies. This empirical foundation enables downstream organizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts through peer-reviewed publication of methodologies and results. AI providers across the entire value chain, including cloud infrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency with granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related environmental impacts. Government and public sector organizations should mandate transparency in all AI procurements, require open data for publicly funded research, and align AI deployments with existing net zero commitments.\n\nOrganizational Implementation and Processes", "doc_id": "luccioni2025c", "page": 6, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "As the starting point of AI development, AI researchers and developers are able to gather empirical measurements from the systems they create at different steps of the model lifecycle. When developing models from scratch, energy consumption and GHG emissions from training and inference can be estimated using programmatic tools like Code Carbon57 or no-code tools like Green Algorithms58. When using or adapting existing models, performance and efficiency testing can significantly reduce emissions by enabling the deployment of more energy-efficient models in production. For instance, the AI Energy Score project21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets. These metrics should be reported in model cards59 and scientific publications with complete methodological transparency, including hardware specifications, geographic locations, electricity sources, measurement uncertainties, and allocation methodologies. This empirical foundation enables downstream organizational GHG accounting while contributing to the broader scientific understanding of AI environmental impacts through peer-reviewed publication of methodologies and results. AI providers across the entire value chain, including cloud infrastructure providers, model hosting platforms, and API service providers, must implement comprehensive transparency with granular environmental data disclosure, enabling downstream organizations to accurately account for their AI-related environmental impacts. Government and public sector organizations should mandate transparency in all AI procurements, require open data for publicly funded research, and align AI deployments with existing net zero commitments.\n\nOrganizational Implementation and Processes", "original_types": ["text", "header"], "id": 980}
{"type": "section", "content": "As AI adoption accelerates, organizations should implement comprehensive frameworks to assess, measure, and integrate AI’s environmental impacts into existing sustainability management systems using structured approaches tailored to their specific contexts and risk profiles. The materiality assessment framework should aim to establish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct tiers of analysis intensity. Organizations developing AI systems utilizing open-source models on their infrastructure should implement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and organization-wide aggregations. Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial models or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by incorporating environmental disclosure requirements into procurement processes and contractual agreements60. Specifically, organizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services under consideration. These environmental metrics should be systematically integrated into organizations’ GHG accounting frameworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and unmodeled factors.", "doc_id": "luccioni2025c", "page": 6, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "As AI adoption accelerates, organizations should implement comprehensive frameworks to assess, measure, and integrate AI’s environmental impacts into existing sustainability management systems using structured approaches tailored to their specific contexts and risk profiles. The materiality assessment framework should aim to establish quantitative thresholds across environmental intensity and usage scale dimensions – for example creating distinct tiers of analysis intensity. Organizations developing AI systems utilizing open-source models on their infrastructure should implement comprehensive measurement protocols at multiple levels of granularity: model-specific, service or process-level, and organization-wide aggregations. Similarly, entities utilizing third-party AI services (e.g., API-based integrations of commercial models or subscription-based access for internal teams like ChatGPT, Copilot or Claude) should demand transparency by incorporating environmental disclosure requirements into procurement processes and contractual agreements60. Specifically, organizations should request access to standardized metrics (such as the AI Energy Score or an equivalent) for all AI services under consideration. These environmental metrics should be systematically integrated into organizations’ GHG accounting frameworks and non-financial performance disclosures, with explicit documentation of methodological assumptions and unmodeled factors.", "original_types": ["text"], "id": 981}
{"type": "section", "content": "Standards, Verification and Assurance\n\nEnvironmental AI disclosures require robust verification frameworks to ensure accuracy and prevent greenwashing, necessitating new assurance standards adapted to AI’s rapid evolution, distributed compute, and complex value chains. While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway across organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD (Organization for Economic Cooperation and Development). These bodies are well-positioned to develop standardized approaches for stakeholders ranging from developers to governments. Given AI’s transnational nature, coordination and harmonization of these efforts is essential. Without alignment, implementation may diverge across jurisdictions, creating further confusion in the market. However, as formal standards may take years to materialize, interim ad hoc methods (such as those outlined above) can provide valuable insights and help shape the eventual development of formal methodologies. These AI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly regarding the GHG Protocol’s treatment of electricity emissions measurement. The current allowance for market-based accounting enables companies to significantly under-report their actual AI-related emissions through renewable energy certificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility –. For AI services consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure environmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured through market mechanisms.\n\nPolicy Frameworks and Reporting", "doc_id": "luccioni2025c", "page": 7, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Standards, Verification and Assurance\n\nEnvironmental AI disclosures require robust verification frameworks to ensure accuracy and prevent greenwashing, necessitating new assurance standards adapted to AI’s rapid evolution, distributed compute, and complex value chains. While no unified standard yet exists for assessing AI sustainability, parallel efforts are underway across organizations such as the Green Software Foundation, ISO (International Organization for Standardization), and OECD (Organization for Economic Cooperation and Development). These bodies are well-positioned to develop standardized approaches for stakeholders ranging from developers to governments. Given AI’s transnational nature, coordination and harmonization of these efforts is essential. Without alignment, implementation may diverge across jurisdictions, creating further confusion in the market. However, as formal standards may take years to materialize, interim ad hoc methods (such as those outlined above) can provide valuable insights and help shape the eventual development of formal methodologies. These AI environmental disclosure frameworks must also strengthen adherence to robust GHG accounting principles, particularly regarding the GHG Protocol’s treatment of electricity emissions measurement. The current allowance for market-based accounting enables companies to significantly under-report their actual AI-related emissions through renewable energy certificates, creating the same problematic disconnect from reality that has undermined carbon offsetting credibility –. For AI services consuming substantial electricity across distributed data centers, mandatory location-based accounting would ensure environmental transparency frameworks capture the true systemic climate impacts rather than allowing them to be obscured through market mechanisms.\n\nPolicy Frameworks and Reporting", "original_types": ["text", "header"], "id": 982}
{"type": "section", "content": "Environmental transparency documentation is already commonplace for private organizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate disclosure requirements in the US, or local and state-level climate disclosure laws. However, policymakers should incorporate additional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability Reporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage, and a transition plan aligned with the Paris Agreement –, particularly as this aligns with existing provisions in the EU AI Act. Non-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria to incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices. For organizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators, semiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements. These could include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure. Enforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets and funding on compliance with disclosure standards.\n\nConclusion\n\nThe current trend toward reduced transparency around AI’s environmental impact contributes to misinformation and hinders informed decision-making across all levels, from individual researchers and developers to organizations and policymakers. This declining transparency is particularly troubling given AI’s escalating environmental impacts amid global climate concerns and looming planetary boundaries. While competition is frequently cited to justify opacity, other competitive industries, such as food (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between transparency and competition is achievable. Reversing the trend toward opacity in AI environmental reporting is essential for informed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms emerge that may alter these impacts. As members of the AI community committed to addressing the climate crisis, we aim to ensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this goal.", "doc_id": "luccioni2025c", "page": 7, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Environmental transparency documentation is already commonplace for private organizations in existing legislation such as the Corporate Sustainability Reporting Directive (CSRD) in the EU, SEC climate disclosure requirements in the US, or local and state-level climate disclosure laws. However, policymakers should incorporate additional reporting requirements specifically addressing AI system utilization under standards such as European Sustainability Reporting Standards E1 (Climate Change) which mandates the disclosure of Scope 1, 2, and 3 GHG emissions, energy usage, and a transition plan aligned with the Paris Agreement –, particularly as this aligns with existing provisions in the EU AI Act. Non-governmental sustainability rating agencies such as CDP and EcoVadis should similarly expand their assessment criteria to incorporate AI-specific environmental impact metrics, creating market incentives for improved disclosure practices. For organizations directly participating in the AI value chain (service providers, data center operators, developers, IT integrators, semiconductor and GPU manufacturers) policymakers should implement more stringent transparency requirements. These could include mandatory detailed environmental reporting disaggregated by model, usage patterns, and physical infrastructure. Enforcement mechanisms might include annual comprehensive environmental reports or conditioning access to public markets and funding on compliance with disclosure standards.\n\nConclusion\n\nThe current trend toward reduced transparency around AI’s environmental impact contributes to misinformation and hinders informed decision-making across all levels, from individual researchers and developers to organizations and policymakers. This declining transparency is particularly troubling given AI’s escalating environmental impacts amid global climate concerns and looming planetary boundaries. While competition is frequently cited to justify opacity, other competitive industries, such as food (with ingredient labeling) and healthcare (with side-effect and pricing transparency), demonstrate that a balance between transparency and competition is achievable. Reversing the trend toward opacity in AI environmental reporting is essential for informed decision-making, accountability, and sustainable technology advancement, particularly as new model paradigms emerge that may alter these impacts. As members of the AI community committed to addressing the climate crisis, we aim to ensure the sustainability of our field as it continues to expand – recognizing that increased transparency is fundamental to this goal.", "original_types": ["text", "header"], "id": 983}
{"type": "section", "content": "References\n\n1. Luers, A. et al. Will ai accelerate or delay the race to net-zero emissions? Nature 628, 718–720 (2024).\n\n2. Kshirsagar, M. et al. Becoming good at AI for good. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 664–673 (2021).\n\n3. DeepMind. Using ai to fight climate change (2009).\n\n4. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. In Ethics of data and analytics, 254–264 (Auerbach Publications, 2022).\n\n5. Buolamwini, J. & Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, 77–91 (PMLR, 2018).\n\n6. Rivera, J.-P. et al. Escalation risks from language models in military and diplomatic decision-making. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 836–898 (2024).\n\n7. Manheim, K. & Kaplan, L. Artificial intelligence: Risks to privacy and democracy. Yale JL & Tech. 21, 106 (2019).\n\n8. Summerfield, C. et al. How will advanced ai systems impact democracy? arXiv preprint arXiv:2409.06729 (2024).\n\n9. Crawford, K. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence (Yale University Press, 2021).\n\n10. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmithell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).\n\n11. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Estimating the carbon footprint of BLOOM, a 176b parameter language model. arXiv preprint arXiv:2211.02001 (2022).\n\n12. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n13. Luccioni, A. S. & Hernandez-Garcia, A. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023).\n\n14. Dodge, J. et al. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 1877–1894 (2022).\n\n15. Ligozat, A.-L., Lefèvre, J., Bugeau, A. & Combaz, J. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).\n\n16. Patterson, D. et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n17. Gupta, U. et al. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), 854–867 (IEEE, 2021).", "doc_id": "luccioni2025c", "page": 8, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "References\n\n1. Luers, A. et al. Will ai accelerate or delay the race to net-zero emissions? Nature 628, 718–720 (2024).\n\n2. Kshirsagar, M. et al. Becoming good at AI for good. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 664–673 (2021).\n\n3. DeepMind. Using ai to fight climate change (2009).\n\n4. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. In Ethics of data and analytics, 254–264 (Auerbach Publications, 2022).\n\n5. Buolamwini, J. & Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, 77–91 (PMLR, 2018).\n\n6. Rivera, J.-P. et al. Escalation risks from language models in military and diplomatic decision-making. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 836–898 (2024).\n\n7. Manheim, K. & Kaplan, L. Artificial intelligence: Risks to privacy and democracy. Yale JL & Tech. 21, 106 (2019).\n\n8. Summerfield, C. et al. How will advanced ai systems impact democracy? arXiv preprint arXiv:2409.06729 (2024).\n\n9. Crawford, K. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence (Yale University Press, 2021).\n\n10. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmithell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610–623 (2021).\n\n11. Luccioni, A. S., Viguier, S. & Ligozat, A.-L. Estimating the carbon footprint of BLOOM, a 176b parameter language model. arXiv preprint arXiv:2211.02001 (2022).\n\n12. Strubell, E., Ganesh, A. & McCallum, A. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n13. Luccioni, A. S. & Hernandez-Garcia, A. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023).\n\n14. Dodge, J. et al. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, 1877–1894 (2022).\n\n15. Ligozat, A.-L., Lefèvre, J., Bugeau, A. & Combaz, J. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).\n\n16. Patterson, D. et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n17. Gupta, U. et al. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), 854–867 (IEEE, 2021).", "original_types": ["text", "header"], "id": 984}
{"type": "section", "content": "18. Cao, Q., Lal, Y. K., Trivedi, H., Balasubramanian, A. & Balasubramanian, N. IrEne: Interpretable energy prediction for transformers. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2145–2157, DOI: 10.18653/v1/2021.acl-long.167 (Association for Computational Linguistics, Online, 2021).\n\n19. Wu, C.-J. et al. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n20. Luccioni, S., Jernite, Y. & Strubell, E. Power hungry processing: Watts driving the cost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24, 85–99, DOI: 10.1145/3630106.3658542 (ACM, 2024).\n\n21. Luccioni, S. & Gamazaychikov, B. AI Energy Score Leaderboard. https://huggingface.co/spaces/AIEnergyScore/Leaderboard (2025). Hugging Face Spaces.\n\n22. Luccioni, S. et al. Light bulbs have energy ratings—so why can’t ai chatbots? Nature 632, 736–738 (2024).\n\n23. Li\n\n24. Han, Y., Wu, Z., Li, P., Wierman, A. & Ren, S. The unpaid toll: Quantifying the public health impact of ai. arXiv preprint arXiv:2412.06288 (2024).", "doc_id": "luccioni2025c", "page": 8, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "18. Cao, Q., Lal, Y. K., Trivedi, H., Balasubramanian, A. & Balasubramanian, N. IrEne: Interpretable energy prediction for transformers. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2145–2157, DOI: 10.18653/v1/2021.acl-long.167 (Association for Computational Linguistics, Online, 2021).\n\n19. Wu, C.-J. et al. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n20. Luccioni, S., Jernite, Y. & Strubell, E. Power hungry processing: Watts driving the cost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24, 85–99, DOI: 10.1145/3630106.3658542 (ACM, 2024).\n\n21. Luccioni, S. & Gamazaychikov, B. AI Energy Score Leaderboard. https://huggingface.co/spaces/AIEnergyScore/Leaderboard (2025). Hugging Face Spaces.\n\n22. Luccioni, S. et al. Light bulbs have energy ratings—so why can’t ai chatbots? Nature 632, 736–738 (2024).\n\n23. Li\n\n24. Han, Y., Wu, Z., Li, P., Wierman, A. & Ren, S. The unpaid toll: Quantifying the public health impact of ai. arXiv preprint arXiv:2412.06288 (2024).", "original_types": ["text"], "id": 985}
{"type": "section", "content": "25. Schneider, I. et al. Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025). 2502.01671.\n\n26. Morrison, J. et al. Holistically evaluating the environmental impact of creating language models. arXiv preprint arXiv:2503.05804 (2025).\n\n27. Epoch AI. Data on notable ai models (2024). Accessed: 2025-04-06.\n\n28. OpenRouter. Openrouter leaderboard rankings. https://openrouter.ai/rankings?view=month (2025). Accessed: 2025-06-03.\n\n29. Lovins, A. B. Artificial intelligence meets natural stupidity: Managing the risks (2025).\n\n30. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n\n31. So, D., Le, Q. & Liang, C. The evolved transformer. In Chaudhuri, K. & Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning, vol. 97 of Proceedings of Machine Learning Research, 5877–5886 (PMLR, 2019).\n\n32. Hao, K. Training a single ai model can emit as much carbon as five cars in their lifetimes. MIT technology Rev. 75, 103 (2019).\n\n33. Toews, R. Deep learning’s carbon emissions problem. Forbes (2020).\n\n34. Gemma Team et al. Gemma 2: Improving open language models at a practical size (2024). 2408.00118.\n\n35. Meta. Llama 3.1 Model Card. https://huggingface.co/meta-llama/Llama-3.1-8B (2024). Hugging Face Model Card.\n\n36. Kerr, D. & NPR. AI brings soaring emissions for Google and Microsoft, a major contributor to climate change. NPR, July (2024).\n\n37. Chen, S. How much energy will ai really consume? the good, the bad and the unknown. Nature 639, 22–24 (2025).\n\n38. Aljabour, J., Wilson, T. & Patel, P. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. EPRI White Pap. no. 3002028905 (2024).\n\n39. Dastin, J. & Nellis, S. Focus: For tech giants, ai like bing and bard poses billion-dollar search problem. Reuters (2023).\n\n40. De Vries, A. The growing energy footprint of artificial intelligence. Joule 7, 2191–2194 (2023).\n\n41. Hölzle, U. Powering a google search (2009).\n\n42. Inês Trindade Pereira. ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume? Euronews (online). Accessed: 2025-06-01.\n\n43. Hamish van der Ven. AI is bad for the environment, and the problem is bigger than energy consumption. The Conversation (online). Accessed: 2025-06-01.\n\n44. Spencer Kimball. Data centers powering artificial intelligence could use more electricity than entire cities. CNBC (online). Accessed: 2025-06-01.\n\n45. James O’Donnell. DeepSeek might not be such good news for energy after all. MIT Technology Review (online). Accessed: 2025-06-01.\n\n46. Berry Zwets. Researchers claim to cut energy consumption AI 95 percent. Techzine (online). Accessed: 2025-06-01.\n\n47. Adam Clark Estes. Should you feel guilty about using AI? Vox (online). Accessed: 2025-06-01.\n\n48. Degot, C., Duranton, S., Frédeau, M. & Hutchinson, R. Reduce carbon and costs with the power of ai. Boston Consult. Group 26 (2021).", "doc_id": "luccioni2025c", "page": 9, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "25. Schneider, I. et al. Life-cycle emissions of ai hardware: A cradle-to-grave approach and generational trends (2025). 2502.01671.\n\n26. Morrison, J. et al. Holistically evaluating the environmental impact of creating language models. arXiv preprint arXiv:2503.05804 (2025).\n\n27. Epoch AI. Data on notable ai models (2024). Accessed: 2025-04-06.\n\n28. OpenRouter. Openrouter leaderboard rankings. https://openrouter.ai/rankings?view=month (2025). Accessed: 2025-06-03.\n\n29. Lovins, A. B. Artificial intelligence meets natural stupidity: Managing the risks (2025).\n\n30. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n\n31. So, D., Le, Q. & Liang, C. The evolved transformer. In Chaudhuri, K. & Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning, vol. 97 of Proceedings of Machine Learning Research, 5877–5886 (PMLR, 2019).\n\n32. Hao, K. Training a single ai model can emit as much carbon as five cars in their lifetimes. MIT technology Rev. 75, 103 (2019).\n\n33. Toews, R. Deep learning’s carbon emissions problem. Forbes (2020).\n\n34. Gemma Team et al. Gemma 2: Improving open language models at a practical size (2024). 2408.00118.\n\n35. Meta. Llama 3.1 Model Card. https://huggingface.co/meta-llama/Llama-3.1-8B (2024). Hugging Face Model Card.\n\n36. Kerr, D. & NPR. AI brings soaring emissions for Google and Microsoft, a major contributor to climate change. NPR, July (2024).\n\n37. Chen, S. How much energy will ai really consume? the good, the bad and the unknown. Nature 639, 22–24 (2025).\n\n38. Aljabour, J., Wilson, T. & Patel, P. Powering intelligence: Analyzing artificial intelligence and data center energy consumption. EPRI White Pap. no. 3002028905 (2024).\n\n39. Dastin, J. & Nellis, S. Focus: For tech giants, ai like bing and bard poses billion-dollar search problem. Reuters (2023).\n\n40. De Vries, A. The growing energy footprint of artificial intelligence. Joule 7, 2191–2194 (2023).\n\n41. Hölzle, U. Powering a google search (2009).\n\n42. Inês Trindade Pereira. ChatGPT, Deepseek & Co: How much energy do AI-powered chatbots consume? Euronews (online). Accessed: 2025-06-01.\n\n43. Hamish van der Ven. AI is bad for the environment, and the problem is bigger than energy consumption. The Conversation (online). Accessed: 2025-06-01.\n\n44. Spencer Kimball. Data centers powering artificial intelligence could use more electricity than entire cities. CNBC (online). Accessed: 2025-06-01.\n\n45. James O’Donnell. DeepSeek might not be such good news for energy after all. MIT Technology Review (online). Accessed: 2025-06-01.\n\n46. Berry Zwets. Researchers claim to cut energy consumption AI 95 percent. Techzine (online). Accessed: 2025-06-01.\n\n47. Adam Clark Estes. Should you feel guilty about using AI? Vox (online). Accessed: 2025-06-01.\n\n48. Degot, C., Duranton, S., Frédeau, M. & Hutchinson, R. Reduce carbon and costs with the power of ai. Boston Consult. Group 26 (2021).", "original_types": ["text"], "id": 986}
{"type": "section", "content": "49. Dannouni, A. et al. Accelerating climate action with ai. Boston Consult. Group Special Rep. Google (2023).\n\n50. ITU. Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions impact greenhouse gas emissions of other sectors. Tech. Rep. ITU-T L.1480, ITU (2022). Accessed: 2025-06-01.\n\n51. WBCSD. Guidance on Avoided Emissions. Tech. Rep., WBCSD (2023). Accessed: 2025-06-01.\n\n52. Das, K. P. & Chandra, J. A survey on artificial intelligence for reducing the climate footprint in healthcare. Energy Nexus 9, 100167 (2023).\n\n53. The Environment. Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).\n\n54. Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023).\n\n55. Rolnick, D. et al. Tackling climate change with machine learning. ACM Comput. Surv. (CSUR) 55, 1–96 (2022).", "doc_id": "luccioni2025c", "page": 9, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "49. Dannouni, A. et al. Accelerating climate action with ai. Boston Consult. Group Special Rep. Google (2023).\n\n50. ITU. Enabling the Net Zero transition: Assessing how the use of information and communication technology solutions impact greenhouse gas emissions of other sectors. Tech. Rep. ITU-T L.1480, ITU (2022). Accessed: 2025-06-01.\n\n51. WBCSD. Guidance on Avoided Emissions. Tech. Rep., WBCSD (2023). Accessed: 2025-06-01.\n\n52. Das, K. P. & Chandra, J. A survey on artificial intelligence for reducing the climate footprint in healthcare. Energy Nexus 9, 100167 (2023).\n\n53. The Environment. Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022).\n\n54. Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023).\n\n55. Rolnick, D. et al. Tackling climate change with machine learning. ACM Comput. Surv. (CSUR) 55, 1–96 (2022).", "original_types": ["text"], "id": 987}
{"type": "section", "content": "56. Ambrose, J. & Hern, A. Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).\n\n57. Schmidt, V. et al. Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).\n\n58. Lannelongue, L., Grealey, J. & Inouye, M. Green algorithms: Quantifying the carbon footprint of computation. Adv. Sci. 2100707 (2021).\n\n59. Mitchell, M. et al. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, 220–229 (2019).\n\n60. Luccioni, S. & Gamazaychikov, B. AI Models Hiding Their Energy Footprint? Here’s What You Can Do (2025).\n\n61. Leal Filho, W. et al. European sustainability reporting standards: An assessment of requirements and preparedness of eu companies. J. environmental management 380, 125008 (2025).\n\n62. Gamazaychikov, B. Unveiling salesforce’s blueprint for sustainable ai: Where responsibility meets innovation (2023).\n\n63. Touvron, H. et al. Llama: Open and efficient foundation language models (2023). 2302.13971.\n\n64. Mesnard, T. et al. Gemma: Open models based on gemini research and technology (2024). 2403.08295.\n\n65. Meta. Llama 4 Model Card. https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct (2025). Hugging Face Model Card.\n\n66. Meta. Llama 3 Model Card. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md (2024). GitHub Repository.\n\nAuthor contributions statement\n\nB.G. conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis. All authors wrote, edited and reviewed the manuscript.", "doc_id": "luccioni2025c", "page": 10, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "56. Ambrose, J. & Hern, A. Ai will be help rather than hindrance in hitting climate targets, bill gates says (2024).\n\n57. Schmidt, V. et al. Codecarbon: Estimate and track carbon emissions from machine learning computing (2021).\n\n58. Lannelongue, L., Grealey, J. & Inouye, M. Green algorithms: Quantifying the carbon footprint of computation. Adv. Sci. 2100707 (2021).\n\n59. Mitchell, M. et al. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, 220–229 (2019).\n\n60. Luccioni, S. & Gamazaychikov, B. AI Models Hiding Their Energy Footprint? Here’s What You Can Do (2025).\n\n61. Leal Filho, W. et al. European sustainability reporting standards: An assessment of requirements and preparedness of eu companies. J. environmental management 380, 125008 (2025).\n\n62. Gamazaychikov, B. Unveiling salesforce’s blueprint for sustainable ai: Where responsibility meets innovation (2023).\n\n63. Touvron, H. et al. Llama: Open and efficient foundation language models (2023). 2302.13971.\n\n64. Mesnard, T. et al. Gemma: Open models based on gemini research and technology (2024). 2403.08295.\n\n65. Meta. Llama 4 Model Card. https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct (2025). Hugging Face Model Card.\n\n66. Meta. Llama 3 Model Card. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md (2024). GitHub Repository.\n\nAuthor contributions statement\n\nB.G. conducted the environmental impact transparency analysis, T.A.d.C carried out the media analysis. All authors wrote, edited and reviewed the manuscript.", "original_types": ["text", "header"], "id": 988}
{"type": "section", "content": "Appendix", "doc_id": "luccioni2025c", "page": 11, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Appendix", "original_types": ["header"], "id": 989}
{"type": "table", "content": "Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel | Organization | Energy Consumption (MWh) | GHG Emissions (tCO2e)\nOLMo 20M²⁶ | Ai2 | 0.8 | 0.3\nCodeGen 350M⁶² | Salesforce | 71 | 6\nLlama 7B⁶³ | Meta | 356 | 14\nBLOOM¹¹ | Big Science | 520 | 30\nT5¹⁶ | Google | 85.7 | 47\nOLMo 2 13B²⁶ | Ai2 | 157 | 101\nGemma 2B + 9B⁶⁴ | Google | ? | 131\nGPT-3¹⁶ | OpenAI | 1,287 | 552\nLlama 4 Scout⁶⁵ | Meta | 3,500 | 1,354\nLlama 3 70B⁶⁶ | Meta | ? | 1,900\nLlama 3.1 405B³⁵ | Meta | ? | 8,930\nMax/Min Variance: | | 4,375 | 29,767", "doc_id": "luccioni2025c", "page": 11, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)\nModel | Organization | Energy Consumption (MWh) | GHG Emissions (tCO2e)\nOLMo 20M²⁶ | Ai2 | 0.8 | 0.3\nCodeGen 350M⁶² | Salesforce | 71 | 6\nLlama 7B⁶³ | Meta | 356 | 14\nBLOOM¹¹ | Big Science | 520 | 30\nT5¹⁶ | Google | 85.7 | 47\nOLMo 2 13B²⁶ | Ai2 | 157 | 101\nGemma 2B + 9B⁶⁴ | Google | ? | 131\nGPT-3¹⁶ | OpenAI | 1,287 | 552\nLlama 4 Scout⁶⁵ | Meta | 3,500 | 1,354\nLlama 3 70B⁶⁶ | Meta | ? | 1,900\nLlama 3.1 405B³⁵ | Meta | ? | 8,930\nMax/Min Variance: | | 4,375 | 29,767", "id": 990}
{"type": "table", "content": "Table 2. Range of Inference Energy Use21 (Representative Models Displayed)\nModel\tOrganization\tGPU Energy for 1k Queries (Wh)\tTask\nbert-tiny-finetuned-squadv2\tmrm8488\t0.06\tExtractive QA\nGIST-all-MiniLM-L6-v2\tavsolatorio\t0.11\tSentence Similarity\ndynamic_tinybert\tIntel\t0.21\tExtractive QA\ndistilbert-imdb\tlvwerra\t0.22\tText Classification\nquestion_answering_v2\tFalconsai\t0.23\tExtractive QA\nResnet 18\tMicrosoft\t0.30\tImage Classification\tyolos-tiny\thustvl\t1.00\tObject Detection\nVision Perceiver Conv\tGoogle\t2.64\tImage Classification\nSFR-Embedding-Mistral\tSalesforce\t5.22\tSentence Similarity\tyolos-base\thustvl\t7.98\tObject Detection\nGemma 7B\tGoogle\t18.90\tText Generation\tT5 11b\tGoogle\t27.79\tText Classification\nphi-4\tMicrosoft\t28.74\tText Generation\tT5 11b\tGoogle\t178.13\tExtractive QA\nMitsua Diffusion One\tMitsua\t186.81\tImage Generation\nMixtral 8x7B\tMistral\t615.39\tText Generation\nStable Diffusion XL Base\tStability AI\t1,639.85\tImage Generation\nLlama 3 70B\tMeta\t1,719.66\tText Generation\nQwen2.5 72B\tQwen\t1,869.55\tText Generation\nCommand-R Plus\tCohere\t3,426.38\tText Generation\nMax/Min Variance:\t57,106", "doc_id": "luccioni2025c", "page": 12, "url": "https://arxiv.org/pdf/2506.15572", "embedded_text": "Table 2. Range of Inference Energy Use21 (Representative Models Displayed)\nModel\tOrganization\tGPU Energy for 1k Queries (Wh)\tTask\nbert-tiny-finetuned-squadv2\tmrm8488\t0.06\tExtractive QA\nGIST-all-MiniLM-L6-v2\tavsolatorio\t0.11\tSentence Similarity\ndynamic_tinybert\tIntel\t0.21\tExtractive QA\ndistilbert-imdb\tlvwerra\t0.22\tText Classification\nquestion_answering_v2\tFalconsai\t0.23\tExtractive QA\nResnet 18\tMicrosoft\t0.30\tImage Classification\tyolos-tiny\thustvl\t1.00\tObject Detection\nVision Perceiver Conv\tGoogle\t2.64\tImage Classification\nSFR-Embedding-Mistral\tSalesforce\t5.22\tSentence Similarity\tyolos-base\thustvl\t7.98\tObject Detection\nGemma 7B\tGoogle\t18.90\tText Generation\tT5 11b\tGoogle\t27.79\tText Classification\nphi-4\tMicrosoft\t28.74\tText Generation\tT5 11b\tGoogle\t178.13\tExtractive QA\nMitsua Diffusion One\tMitsua\t186.81\tImage Generation\nMixtral 8x7B\tMistral\t615.39\tText Generation\nStable Diffusion XL Base\tStability AI\t1,639.85\tImage Generation\nLlama 3 70B\tMeta\t1,719.66\tText Generation\nQwen2.5 72B\tQwen\t1,869.55\tText Generation\nCommand-R Plus\tCohere\t3,426.38\tText Generation\nMax/Min Variance:\t57,106", "id": 991}
{"type": "section", "content": "Green AI\n\nRoy Schwartz* ♦ Jesse Dodge*♣ ♣ Noah A. Smith ♦ ♣ Oren Etzioni ♦\n\nAllen Institute for AI, Seattle, Washington, USA ♦ Carnegie Mellon University, Pittsburgh, Pennsylvania, USA ♣ University of Washington, Seattle, Washington, USA\n\nJuly 2019\n\nAbstract\n\nThe computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [40]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or “price tag” of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.", "doc_id": "schwartz2019", "page": 1, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Green AI\n\nRoy Schwartz* ♦ Jesse Dodge*♣ ♣ Noah A. Smith ♦ ♣ Oren Etzioni ♦\n\nAllen Institute for AI, Seattle, Washington, USA ♦ Carnegie Mellon University, Pittsburgh, Pennsylvania, USA ♣ University of Washington, Seattle, Washington, USA\n\nJuly 2019\n\nAbstract\n\nThe computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [40]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or “price tag” of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive—enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.", "original_types": ["text", "header"], "id": 992}
{"type": "figure", "content": "Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years. Figure taken from [2].", "doc_id": "schwartz2019", "page": 2, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years. Figure taken from [2].", "id": 993}
{"type": "section", "content": "Specifically, we propose making efficiency a more common evaluation criterion for AI papers alongside accuracy and related measures.\n\nAI research can be computationally expensive in a number of ways, but each provides opportunities for efficient improvements; for example, papers could be required to plot accuracy as a function of computational cost and of training set size, providing a baseline for more data-efficient research in the future. Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.\n\nOur empirical analysis in Figure 2 suggests that the AI research community has paid relatively little attention to computational efficiency. In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially, at a pace that far exceeds Moore’s Law [28]. Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This paper identifies key factors that contribute to Red AI and advocates the introduction of a simple, easy-to-compute efficiency metric that could help make some AI research greener, more inclusive, and perhaps more cognitively plausible. Green AI is part of a broader, long-standing interest in environmentally-friendly scientific research (e.g., see the journal Green Chemistry). Computer science, in particular, has a long history of investigating sustainable and energy-efficient computing (e.g., see the journal Sustainable Computing: Informatics and Systems).\n\nThe remainder of this paper is organized as follows. Section 2 analyzes practices that move deep-learning research into the realm of Red AI. Section 3 discusses our proposals for Green AI. Section 4 considers related work, and we conclude with a discussion of directions for future research.\n\n2 Red AI\n\nRed AI refers to AI research that seeks to obtain state-of-the-art results in accuracy (or related measures) through the use of massive computational power—essentially “buying” stronger results. Yet the relationship between model performance and model complexity (measured as number of parameters or inference time) has long been understood to be at best logarithmic; for a linear gain in performance, an exponentially larger model is required [18]. Similar trends exist with increasing the quantity of training data [41, 13] and the number of experiments [9]. In each of these cases, diminishing returns come at increased computational cost.\n\nThis section analyzes the factors contributing to Red AI and shows how it is resulting in diminishing returns over time (see Figure 3). We note again that Red AI work is valuable, and in fact, much of it contributes to what we know", "doc_id": "schwartz2019", "page": 2, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Specifically, we propose making efficiency a more common evaluation criterion for AI papers alongside accuracy and related measures.\n\nAI research can be computationally expensive in a number of ways, but each provides opportunities for efficient improvements; for example, papers could be required to plot accuracy as a function of computational cost and of training set size, providing a baseline for more data-efficient research in the future. Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.\n\nOur empirical analysis in Figure 2 suggests that the AI research community has paid relatively little attention to computational efficiency. In fact, as Figure 1 illustrates, the computational cost of research is increasing exponentially, at a pace that far exceeds Moore’s Law [28]. Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This paper identifies key factors that contribute to Red AI and advocates the introduction of a simple, easy-to-compute efficiency metric that could help make some AI research greener, more inclusive, and perhaps more cognitively plausible. Green AI is part of a broader, long-standing interest in environmentally-friendly scientific research (e.g., see the journal Green Chemistry). Computer science, in particular, has a long history of investigating sustainable and energy-efficient computing (e.g., see the journal Sustainable Computing: Informatics and Systems).\n\nThe remainder of this paper is organized as follows. Section 2 analyzes practices that move deep-learning research into the realm of Red AI. Section 3 discusses our proposals for Green AI. Section 4 considers related work, and we conclude with a discussion of directions for future research.\n\n2 Red AI\n\nRed AI refers to AI research that seeks to obtain state-of-the-art results in accuracy (or related measures) through the use of massive computational power—essentially “buying” stronger results. Yet the relationship between model performance and model complexity (measured as number of parameters or inference time) has long been understood to be at best logarithmic; for a linear gain in performance, an exponentially larger model is required [18]. Similar trends exist with increasing the quantity of training data [41, 13] and the number of experiments [9]. In each of these cases, diminishing returns come at increased computational cost.\n\nThis section analyzes the factors contributing to Red AI and shows how it is resulting in diminishing returns over time (see Figure 3). We note again that Red AI work is valuable, and in fact, much of it contributes to what we know", "original_types": ["text", "header"], "id": 994}
{"type": "section", "content": "AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.", "doc_id": "schwartz2019", "page": 3, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.", "original_types": ["text"], "id": 995}
{"type": "figure", "content": "Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.", "doc_id": "schwartz2019", "page": 3, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Figure 2: AI papers tend to target accuracy rather than efficiency. The figure shows the proportion of papers that target accuracy, efficiency, both or other from a sample of 60 papers from top AI conferences.", "id": 996}
{"type": "section", "content": "Expensive processing of one example\n\nOur focus is on neural models, where it is common for each training step to require inference, so we discuss training and inference cost together as “processing” an example. Some works have used increasingly expensive models which require great amounts of resources, and as a result, in these models, performing inference can require a lot of computation, and training even more so. For instance, Google’s BERT-large [8] contains roughly 350 million parameters. openAI’s openGPT2-XL model [30] contains 1.5 billion parameters. AI2, our home organization, recently released Grover [49], also containing 1.5 billion parameters. In the computer vision community, a similar trend is observed (Figure 1).\n\nSuch large models have high costs for processing each example, which leads to large training costs. BERT-large was trained on 64 TPU chips for 4 days. Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000. XLNet had a similar architecture to BERT-large, but used a more expensive objective function (in addition to an order of magnitude more data), and was trained on 512 TPU chips for 2.5 days.7 It is impossible to reproduce the best BERT-large results8 or XLNet results9 using a single GPU. Specialized models can have even more extreme costs, such as AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go [37] at a cost of over $1,000 per hour.10\n\nWhen examining variants of a single model (e.g., BERT-small and BERT-large) we see that larger models can have stronger performance, which is a valuable scientific contribution. However, this implies the financial and environmental cost of increasingly large AI models will not decrease soon, as the pace of model growth far exceeds the resulting increase in model performance [16]. As a result, more and more resources are going to be required to keep improving AI models by simply making them larger.", "doc_id": "schwartz2019", "page": 4, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Expensive processing of one example\n\nOur focus is on neural models, where it is common for each training step to require inference, so we discuss training and inference cost together as “processing” an example. Some works have used increasingly expensive models which require great amounts of resources, and as a result, in these models, performing inference can require a lot of computation, and training even more so. For instance, Google’s BERT-large [8] contains roughly 350 million parameters. openAI’s openGPT2-XL model [30] contains 1.5 billion parameters. AI2, our home organization, recently released Grover [49], also containing 1.5 billion parameters. In the computer vision community, a similar trend is observed (Figure 1).\n\nSuch large models have high costs for processing each example, which leads to large training costs. BERT-large was trained on 64 TPU chips for 4 days. Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000. XLNet had a similar architecture to BERT-large, but used a more expensive objective function (in addition to an order of magnitude more data), and was trained on 512 TPU chips for 2.5 days.7 It is impossible to reproduce the best BERT-large results8 or XLNet results9 using a single GPU. Specialized models can have even more extreme costs, such as AlphaGo, the best version of which required 1,920 CPUs and 280 GPUs to play a single game of Go [37] at a cost of over $1,000 per hour.10\n\nWhen examining variants of a single model (e.g., BERT-small and BERT-large) we see that larger models can have stronger performance, which is a valuable scientific contribution. However, this implies the financial and environmental cost of increasingly large AI models will not decrease soon, as the pace of model growth far exceeds the resulting increase in model performance [16]. As a result, more and more resources are going to be required to keep improving AI models by simply making them larger.", "original_types": ["text", "header"], "id": 997}
{"type": "figure", "content": "Figure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of training examples increases exponentially [25].", "doc_id": "schwartz2019", "page": 5, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Figure 3: Diminishing returns of training on more data: object detection accuracy increases linearly as the number of training examples increases exponentially [25].", "id": 998}
{"type": "section", "content": "The topic of massive number of experiments is not as well studied as the first two discussed above. In fact, the number of experiments performed during model construction is often underreported. Nonetheless, evidence for a logarithmic relation exists here as well, between the number of experiments and performance gains [9].\n\nDiscussion\n\nThe benefits of pouring more resources into models are certainly of interest to the AI community. Indeed, there is value in pushing the limits of model size, dataset size, and the hyperparameter search space. Currently, despite the massive amount of resources put into recent AI models, such investment still pays off in terms of downstream performance (albeit at an increasingly lower rate). Finding the point of saturation (if such exists) is an important question for the future of AI. \nOur goal in this paper is to raise awareness of the cost of Red AI, as well as encourage the AI community to recognize the value of work by researchers that take a different path, optimizing efficiency rather than accuracy. Next we turn to discuss concrete measures for making AI more green.", "doc_id": "schwartz2019", "page": 5, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "The topic of massive number of experiments is not as well studied as the first two discussed above. In fact, the number of experiments performed during model construction is often underreported. Nonetheless, evidence for a logarithmic relation exists here as well, between the number of experiments and performance gains [9].\n\nDiscussion\n\nThe benefits of pouring more resources into models are certainly of interest to the AI community. Indeed, there is value in pushing the limits of model size, dataset size, and the hyperparameter search space. Currently, despite the massive amount of resources put into recent AI models, such investment still pays off in terms of downstream performance (albeit at an increasingly lower rate). Finding the point of saturation (if such exists) is an important question for the future of AI. \nOur goal in this paper is to raise awareness of the cost of Red AI, as well as encourage the AI community to recognize the value of work by researchers that take a different path, optimizing efficiency rather than accuracy. Next we turn to discuss concrete measures for making AI more green.", "original_types": ["text", "header"], "id": 999}
{"type": "section", "content": "the cost of an experiment decomposes into the cost of a processing a single example, the size of the dataset, and the number of experiments (Equation 1), reducing the amount of work in each of these steps will result in AI that is more green.\n\nWhen reporting the amount of work done by a model, we want to measure a quantity that allows for a fair comparison between different models. As a result, this measure should ideally be stable across different labs, at different times, and using different hardware.\n\nCarbon emission\n\nCarbon emission is appealing as it is a quantity we want to directly minimize. Nonetheless it is impractical to measure the exact amount of carbon released by training or executing a model, and accordingly—generating an AI result, as this amount depends highly on the local electricity infrastructure. As a result, it is not comparable between researchers in different locations or even the same location at different times.\n\nElectricity usage\n\nElectricity usage is correlated with carbon emission while being time- and location-agnostic. Moreover, GPUs often report the amount of electricity each of their cores consume at each time point, which facilitates the estimation of the total amount of electricity consumed by generating an AI result. Nonetheless, this measure is hardware dependent, and as a result does not allow for a fair comparison between different models.\n\nElapsed real time\n\nThe total running time for generating an AI result is a natural measure for efficiency, as all other things being equal, a faster model is doing less computational work. Nonetheless, this measure is highly influenced by factors such as the underlying hardware, other jobs running on the same machine, and the number of cores used. These factors hinder the comparison between different models, as well as the decoupling of modeling contributions from hardware improvements.\n\nNumber of parameters\n\nAnother common measure of efficiency is the number of parameters (learnable or total) used by the model. As with run time, this measure is correlated with the amount of work. Unlike the other measures described above, it does not depend on the underlying hardware. Moreover, this measure also highly correlates with the amount of memory consumed by the model. Nonetheless, different algorithms make different use of their parameters, for instance by making the model deeper vs. wider. As a result, different models with a similar number of parameters often perform different amounts of work.\n\nFPO", "doc_id": "schwartz2019", "page": 6, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "the cost of an experiment decomposes into the cost of a processing a single example, the size of the dataset, and the number of experiments (Equation 1), reducing the amount of work in each of these steps will result in AI that is more green.\n\nWhen reporting the amount of work done by a model, we want to measure a quantity that allows for a fair comparison between different models. As a result, this measure should ideally be stable across different labs, at different times, and using different hardware.\n\nCarbon emission\n\nCarbon emission is appealing as it is a quantity we want to directly minimize. Nonetheless it is impractical to measure the exact amount of carbon released by training or executing a model, and accordingly—generating an AI result, as this amount depends highly on the local electricity infrastructure. As a result, it is not comparable between researchers in different locations or even the same location at different times.\n\nElectricity usage\n\nElectricity usage is correlated with carbon emission while being time- and location-agnostic. Moreover, GPUs often report the amount of electricity each of their cores consume at each time point, which facilitates the estimation of the total amount of electricity consumed by generating an AI result. Nonetheless, this measure is hardware dependent, and as a result does not allow for a fair comparison between different models.\n\nElapsed real time\n\nThe total running time for generating an AI result is a natural measure for efficiency, as all other things being equal, a faster model is doing less computational work. Nonetheless, this measure is highly influenced by factors such as the underlying hardware, other jobs running on the same machine, and the number of cores used. These factors hinder the comparison between different models, as well as the decoupling of modeling contributions from hardware improvements.\n\nNumber of parameters\n\nAnother common measure of efficiency is the number of parameters (learnable or total) used by the model. As with run time, this measure is correlated with the amount of work. Unlike the other measures described above, it does not depend on the underlying hardware. Moreover, this measure also highly correlates with the amount of memory consumed by the model. Nonetheless, different algorithms make different use of their parameters, for instance by making the model deeper vs. wider. As a result, different models with a similar number of parameters often perform different amounts of work.\n\nFPO", "original_types": ["text", "header"], "id": 1000}
{"type": "section", "content": "As a concrete measure, we suggest reporting the total number of floating point operations (FPO) required to generate a result.13 FPO provides an estimate to the amount of work performed by a computational process. It is computed analytically by defining a cost to two base operations, ADD and MUL. Based on these operations, the FPO cost of any machine learning abstract operation (e.g., a tanh operation, a matrix multiplication, a convolution operation, or the BERT model) can be computed as a recursive function of these two operations. FPO has been used in the past to quantify the energy footprint of a model [27, 43, 12, 42], but is not widely adopted in AI. FPO has several appealing properties. First, it directly computes the amount of work done by the running machine when executing a specific instance of a model, and is thus tied to the amount of energy consumed. Second, FPO is agnostic to the hardware on which the model is run. This facilitates fair comparisons between different approaches, unlike the measures described above. Third, FPO is strongly correlated with the running time of the model [4]. Unlike asymptotic runtime, FPO also considers the amount of work done at each time step. Several packages exist for computing FPO in various neural network libraries,14 though none of them contains all the building blocks required to construct all modern AI models. We encourage the builders of neural network libraries to implement such functionality directly.", "doc_id": "schwartz2019", "page": 6, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "As a concrete measure, we suggest reporting the total number of floating point operations (FPO) required to generate a result.13 FPO provides an estimate to the amount of work performed by a computational process. It is computed analytically by defining a cost to two base operations, ADD and MUL. Based on these operations, the FPO cost of any machine learning abstract operation (e.g., a tanh operation, a matrix multiplication, a convolution operation, or the BERT model) can be computed as a recursive function of these two operations. FPO has been used in the past to quantify the energy footprint of a model [27, 43, 12, 42], but is not widely adopted in AI. FPO has several appealing properties. First, it directly computes the amount of work done by the running machine when executing a specific instance of a model, and is thus tied to the amount of energy consumed. Second, FPO is agnostic to the hardware on which the model is run. This facilitates fair comparisons between different approaches, unlike the measures described above. Third, FPO is strongly correlated with the running time of the model [4]. Unlike asymptotic runtime, FPO also considers the amount of work done at each time step. Several packages exist for computing FPO in various neural network libraries,14 though none of them contains all the building blocks required to construct all modern AI models. We encourage the builders of neural network libraries to implement such functionality directly.", "original_types": ["text"], "id": 1001}
{"type": "figure", "content": "Figure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top): model parameters (in million), FPO (in billions), top-1 accuracy on ImageNet. (4a): Different models: AlexNet [20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17]. (4b): Comparison of different sizes (measured by the number of layers) of the ResNet model [14].", "doc_id": "schwartz2019", "page": 7, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Figure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top): model parameters (in million), FPO (in billions), top-1 accuracy on ImageNet. (4a): Different models: AlexNet [20], ResNet [14], ResNext [47], DPN107 [5], SENet154 [17]. (4b): Comparison of different sizes (measured by the number of layers) of the ResNet model [14].", "id": 1002}
{"type": "section", "content": "Discussion Efficient machine learning approaches have received attention in the research community, but are generally not motivated by being green. For example, a significant amount of work in the computer vision community has addressed efficient inference, which is necessary for real-time processing of images for applications like self-driving cars [24, 31, 22], or for placing models on devices such as mobile phones [16, 34]. Most of these approaches target efficient model inference [32, 50, 12],15 and thus only minimize the cost of processing a single example, while ignoring the other two red practices discussed in Section 2.16\n\nThe above examples indicate that the path to making AI green depends on how it is used. When developing a new model, much of the research process involves training many model variants on a training set and performing inference on a small development set. In such a setting, more efficient training procedures can lead to greater savings, while in a production setting more efficient inference can be more important. We advocate for a holistic view of computational savings which doesn’t sacrifice in some areas to make advances in others.", "doc_id": "schwartz2019", "page": 7, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "Discussion Efficient machine learning approaches have received attention in the research community, but are generally not motivated by being green. For example, a significant amount of work in the computer vision community has addressed efficient inference, which is necessary for real-time processing of images for applications like self-driving cars [24, 31, 22], or for placing models on devices such as mobile phones [16, 34]. Most of these approaches target efficient model inference [32, 50, 12],15 and thus only minimize the cost of processing a single example, while ignoring the other two red practices discussed in Section 2.16\n\nThe above examples indicate that the path to making AI green depends on how it is used. When developing a new model, much of the research process involves training many model variants on a training set and performing inference on a small development set. In such a setting, more efficient training procedures can lead to greater savings, while in a production setting more efficient inference can be more important. We advocate for a holistic view of computational savings which doesn’t sacrifice in some areas to make advances in others.", "original_types": ["text"], "id": 1003}
{"type": "section", "content": "3.3 Additional Ways to Promote Green AI\n\nIn addition to reporting the FPO cost of the final reported number, we encourage researchers to report the budget/accuracy curve observed during training. In a recent paper [9], we observed that selecting the better performing model on a given task depends highly on the amount of compute available during model development. We introduced a method for computing the expected best validation performance of a model as a function of the given budget. We argue that reporting this curve will allow users to make wiser decisions about their selection of models and highlight the stability of different approaches. We further advocate for making efficiency an official contribution in major AI conferences, by advising reviewers to recognize and value contributions that do not strictly improve state of the art, but have other benefits such as efficiency. Finally, we note that the trend of releasing pretrained models publicly is a green success, and we would like to encourage organizations to continue to release their models in order to save others the costs of retraining them.", "doc_id": "schwartz2019", "page": 8, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "3.3 Additional Ways to Promote Green AI\n\nIn addition to reporting the FPO cost of the final reported number, we encourage researchers to report the budget/accuracy curve observed during training. In a recent paper [9], we observed that selecting the better performing model on a given task depends highly on the amount of compute available during model development. We introduced a method for computing the expected best validation performance of a model as a function of the given budget. We argue that reporting this curve will allow users to make wiser decisions about their selection of models and highlight the stability of different approaches. We further advocate for making efficiency an official contribution in major AI conferences, by advising reviewers to recognize and value contributions that do not strictly improve state of the art, but have other benefits such as efficiency. Finally, we note that the trend of releasing pretrained models publicly is a green success, and we would like to encourage organizations to continue to release their models in order to save others the costs of retraining them.", "original_types": ["text", "header"], "id": 1004}
{"type": "section", "content": "5 Conclusion\n\nThe vision of Green AI raises many exciting research directions that help to overcome the inclusiveness challenges of Red AI. Progress will reduce the computational expense with a minimal reduction in performance, or even improve performance as more efficient methods are discovered. Also, it would seem that Green AI could be moving us in a more cognitively plausible direction as the brain is highly efficient.\n\nIt’s important to reiterate that we see Green AI as a valuable option not an exclusive mandate—of course, both Green AI and Red AI have contributions to make. We want to increase the prevalence of Green AI by highlighting its benefits, advocating a standard measure of efficiency. Below, we point to a few important green research directions, and highlight a few open questions.\n\nResearch on building space or time efficient models is often motivated by fitting a model on a small device (such as a phone) or fast enough to process examples in real time, such as image captioning for the blind (see Section 3.1). Some modern models don’t even fit on a single GPU (see Section 2). Here we argue for a far broader approach.\n\nData efficiency has received significant attention over the years [35, 19]. Modern research in vision and NLP often involves first pretraining a model on large “raw” (unannotated) data then fine-tuning it to a task of interest through supervised learning. A strong result in this area often involves achieving similar performance to a baseline with fewer training examples or fewer gradient steps. Most recent work has addressed fine-tuning data [29], but pretraining efficiency is also important. In either case, one simple technique to improve in this area is to simply report performance with different amounts of training data. For example, reporting performance of contextual embedding models trained on 10 million, 100 million, 1 billion, and 10 billion tokens would facilitate faster development of new models, as they can first be compared at the smallest data sizes. Research here is of value not just to make training less expensive, but because in areas such as low resource languages or historical domains it is extremely hard to generate more data, so to progress we must make more efficient use of what is available.\n\nFinally, the total number of experiments run to get a final result is often underreported and underdiscussed [9]. The few instances researchers have of full reporting of the hyperparameter search, architecture evaluations, and ablations that went into a reported experimental result have surprised the community [40]. While many hyperparameter optimization algorithms exist which can reduce the computational expense required to reach a given level of performance [3, 10], simple improvements here can have a large impact. For example, stopping training early for models which are clearly underperforming can lead to great savings [21].", "doc_id": "schwartz2019", "page": 9, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "5 Conclusion\n\nThe vision of Green AI raises many exciting research directions that help to overcome the inclusiveness challenges of Red AI. Progress will reduce the computational expense with a minimal reduction in performance, or even improve performance as more efficient methods are discovered. Also, it would seem that Green AI could be moving us in a more cognitively plausible direction as the brain is highly efficient.\n\nIt’s important to reiterate that we see Green AI as a valuable option not an exclusive mandate—of course, both Green AI and Red AI have contributions to make. We want to increase the prevalence of Green AI by highlighting its benefits, advocating a standard measure of efficiency. Below, we point to a few important green research directions, and highlight a few open questions.\n\nResearch on building space or time efficient models is often motivated by fitting a model on a small device (such as a phone) or fast enough to process examples in real time, such as image captioning for the blind (see Section 3.1). Some modern models don’t even fit on a single GPU (see Section 2). Here we argue for a far broader approach.\n\nData efficiency has received significant attention over the years [35, 19]. Modern research in vision and NLP often involves first pretraining a model on large “raw” (unannotated) data then fine-tuning it to a task of interest through supervised learning. A strong result in this area often involves achieving similar performance to a baseline with fewer training examples or fewer gradient steps. Most recent work has addressed fine-tuning data [29], but pretraining efficiency is also important. In either case, one simple technique to improve in this area is to simply report performance with different amounts of training data. For example, reporting performance of contextual embedding models trained on 10 million, 100 million, 1 billion, and 10 billion tokens would facilitate faster development of new models, as they can first be compared at the smallest data sizes. Research here is of value not just to make training less expensive, but because in areas such as low resource languages or historical domains it is extremely hard to generate more data, so to progress we must make more efficient use of what is available.\n\nFinally, the total number of experiments run to get a final result is often underreported and underdiscussed [9]. The few instances researchers have of full reporting of the hyperparameter search, architecture evaluations, and ablations that went into a reported experimental result have surprised the community [40]. While many hyperparameter optimization algorithms exist which can reduce the computational expense required to reach a given level of performance [3, 10], simple improvements here can have a large impact. For example, stopping training early for models which are clearly underperforming can lead to great savings [21].", "original_types": ["text", "header"], "id": 1005}
{"type": "section", "content": "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019.\n\n[9] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your work: Improved reporting of experimental results. In Proc. of EMNLP, 2019.\n\n[10] Jesse Dodge, Kevin Jamieson, and Noah A. Smith. Open loop hyperparameter optimization and determinantal point processes. In Proc. of AutoML, 2017.\n\n[11] Clement Duhart, Gershon Dublon, Brian Mayton, Glorianna Davenport, and Joseph A. Paradiso. Deep learning for wildlife conservation and restoration efforts, 2019. ICML Workshop on Climate Change.\n\n[12] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. In Proc. of CVPR, 2018.\n\n[13] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24:8–12, 2009.\n\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of CVPR, 2016.\n\n[15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n\n[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications, 2017. arXiv:1704.04861.\n\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proc. of CVPR, 2018.\n\n[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, and Kevin Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. In Proc. of CVPR, 2017.\n\n[19] Sanket Kamthe and Marc Peter Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. In Proc. of AISTATS, 2018.\n\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of NeurIPS, 2012.\n\n[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. In Proc. of ICLR, 2017.\n\n[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. Ssd: Single shot multibox detector. In Proc. of ECCV, 2016.\n\n[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach, 2019. arXiv:1907.11692.\n\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShuffleNet V2: Practical guidelines for efficient cnn architecture design. In Proc. of ECCV, 2018.", "doc_id": "schwartz2019", "page": 10, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019.\n\n[9] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your work: Improved reporting of experimental results. In Proc. of EMNLP, 2019.\n\n[10] Jesse Dodge, Kevin Jamieson, and Noah A. Smith. Open loop hyperparameter optimization and determinantal point processes. In Proc. of AutoML, 2017.\n\n[11] Clement Duhart, Gershon Dublon, Brian Mayton, Glorianna Davenport, and Joseph A. Paradiso. Deep learning for wildlife conservation and restoration efforts, 2019. ICML Workshop on Climate Change.\n\n[12] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. MorphNet: Fast & simple resource-constrained structure learning of deep networks. In Proc. of CVPR, 2018.\n\n[13] Alon Halevy, Peter Norvig, and Fernando Pereira. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24:8–12, 2009.\n\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of CVPR, 2016.\n\n[15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n\n[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications, 2017. arXiv:1704.04861.\n\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proc. of CVPR, 2018.\n\n[18] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, and Kevin Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. In Proc. of CVPR, 2017.\n\n[19] Sanket Kamthe and Marc Peter Deisenroth. Data-efficient reinforcement learning with probabilistic model predictive control. In Proc. of AISTATS, 2018.\n\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of NeurIPS, 2012.\n\n[21] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. In Proc. of ICLR, 2017.\n\n[22] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. Ssd: Single shot multibox detector. In Proc. of ECCV, 2016.\n\n[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach, 2019. arXiv:1907.11692.\n\n[24] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShuffleNet V2: Practical guidelines for efficient cnn architecture design. In Proc. of ECCV, 2018.", "original_types": ["text"], "id": 1006}
{"type": "section", "content": "[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proc. ECCV, 2018.", "doc_id": "schwartz2019", "page": 10, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "[25] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proc. ECCV, 2018.", "original_types": ["text"], "id": 1007}
{"type": "section", "content": "[26] Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In Proc. of EMNLP, 2018.\n\n[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In Proc. of ICLR, 2017.\n\n[28] Gordon E. Moore. Cramming more components onto integrated circuits, 1965.\n\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.\n\n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. OpenAI Blog.\n\n[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In Proc. of ECCV, 2016.\n\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proc. of CVPR, 2016.\n\n[33] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. Körding, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and Joshua Bengio. Tackling climate change with machine learning, 2019. arXiv:1905.12616.\n\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In Proc. of CVPR, 2018.\n\n[35] Roy Schwartz, Sam Thomson, and Noah A. Smith. SoPa: Bridging CNNs, RNNs, and weighted finite-state machines. In Proc. of ACL, 2018.\n\n[36] Yoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, James Manyika, Juan Carlos Niebles, Terah Lyons, John Etchemendy, and Z Bauer. The AI index 2018 annual report. AI Index Steering Committee, Human-Centered AI Initiative, Stanford University. Available at http://cdn.aiindex.org/2018/AI%20Index%202018%20Annual%20Report.pdf, 202018, 2018.\n\n[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.\n\n[38] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017. arXiv:1712.01815.", "doc_id": "schwartz2019", "page": 11, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "[26] Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In Proc. of EMNLP, 2018.\n\n[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In Proc. of ICLR, 2017.\n\n[28] Gordon E. Moore. Cramming more components onto integrated circuits, 1965.\n\n[29] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.\n\n[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. OpenAI Blog.\n\n[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In Proc. of ECCV, 2016.\n\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proc. of CVPR, 2016.\n\n[33] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. Körding, Carla Gomes, Andrew Y. Ng, Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and Joshua Bengio. Tackling climate change with machine learning, 2019. arXiv:1905.12616.\n\n[34] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In Proc. of CVPR, 2018.\n\n[35] Roy Schwartz, Sam Thomson, and Noah A. Smith. SoPa: Bridging CNNs, RNNs, and weighted finite-state machines. In Proc. of ACL, 2018.\n\n[36] Yoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, James Manyika, Juan Carlos Niebles, Terah Lyons, John Etchemendy, and Z Bauer. The AI index 2018 annual report. AI Index Steering Committee, Human-Centered AI Initiative, Stanford University. Available at http://cdn.aiindex.org/2018/AI%20Index%202018%20Annual%20Report.pdf, 202018, 2018.\n\n[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.\n\n[38] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017. arXiv:1712.01815.", "original_types": ["text"], "id": 1008}
{"type": "section", "content": "[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017.\n\n[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proc. of ACL, 2019.", "doc_id": "schwartz2019", "page": 11, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354, 2017.\n\n[40] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP. In Proc. of ACL, 2019.", "original_types": ["text"], "id": 1009}
{"type": "section", "content": "[41] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proc. of ICCV, 2017.\n\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n\n[43] Tom Veniat and Ludovic Denoyer. Learning time/memory-efficient deep architectures with budgeted super networks. In Proc. of CVPR, 2018.\n\n[44] Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, and Dieter Fox. Early fusion for goal directed robotic vision. In Proc. of IROS, 2019.\n\n[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019. arXiv:1905.00537.\n\n[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR, 2019.\n\n[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. of CVPR, 2017.\n\n[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding, 2019. arXiv:1906.08237.\n\n[49] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news, 2019. arXiv:1905.12616.\n\n[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In Proc. of CVPR, 2018.\n\n[51] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proc. of ICLR, 2017.", "doc_id": "schwartz2019", "page": 12, "url": "https://arxiv.org/pdf/1907.10597", "embedded_text": "[41] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proc. of ICCV, 2017.\n\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n\n[43] Tom Veniat and Ludovic Denoyer. Learning time/memory-efficient deep architectures with budgeted super networks. In Proc. of CVPR, 2018.\n\n[44] Aaron Walsman, Yonatan Bisk, Saadia Gabriel, Dipendra Misra, Yoav Artzi, Yejin Choi, and Dieter Fox. Early fusion for goal directed robotic vision. In Proc. of IROS, 2019.\n\n[45] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019. arXiv:1905.00537.\n\n[46] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR, 2019.\n\n[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. of CVPR, 2017.\n\n[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding, 2019. arXiv:1906.08237.\n\n[49] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news, 2019. arXiv:1905.12616.\n\n[50] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In Proc. of CVPR, 2018.\n\n[51] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proc. of ICLR, 2017.", "original_types": ["text"], "id": 1010}
{"type": "section", "content": "Abstract\n\nThis paper introduces an infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models in commercial datacenters. The framework combines public API performance data with company-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes model-level energy, water, and carbon metrics. Results show the most energy-intensive models exceed 29 Wh per long prompt, over 65× the most efficient systems. Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset. These findings highlight a growing paradox: as AI becomes cheaper and faster, global adoption drives disproportionate resource consumption. Our methodology offers a standardized, empirically grounded basis for sustainability benchmarking and accountability in AI deployment.", "doc_id": "jegham2025", "page": 1, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Abstract\n\nThis paper introduces an infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models in commercial datacenters. The framework combines public API performance data with company-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost and provide a dynamically updated dashboard that visualizes model-level energy, water, and carbon metrics. Results show the most energy-intensive models exceed 29 Wh per long prompt, over 65× the most efficient systems. Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset. These findings highlight a growing paradox: as AI becomes cheaper and faster, global adoption drives disproportionate resource consumption. Our methodology offers a standardized, empirically grounded basis for sustainability benchmarking and accountability in AI deployment.", "original_types": ["text", "header"], "id": 1011}
{"type": "section", "content": "2. Related Work\n\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a growing body of work attempting to quantify the energy, carbon, and water costs associated with training and deploying LLMs.", "doc_id": "jegham2025", "page": 2, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "2. Related Work\n\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a growing body of work attempting to quantify the energy, carbon, and water costs associated with training and deploying LLMs.", "original_types": ["text", "header"], "id": 1012}
{"type": "section", "content": "3 Preliminaries\n\nTo capture infrastructure-level overhead in data center operations, we apply three standard environmental multipliers: Power Usage Effectiveness (PUE) [19, 20], Water Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factor (CIF) [21, 22].\n\nPUE accounts for non-computational energy overheads such as cooling, lighting, and power distribution. Defined as the ratio of total data center energy consumption to IT-specific energy use.\n\nWUE captures the water used per kilowatt-hour of IT energy, encompassing on-site cooling (Scope 1), off-site electricity generation (Scope 2), and embodied water from hardware manufacturing and transport (Scope 3). WUE can be computed based on either water withdrawal (the total volume drawn from natural or municipal sources) or water consumption (the portion of withdrawn water permanently lost, primarily through evaporation).\n\nCIF measures carbon emissions per kilowatt-hour of energy consumed, largely driven by the regional electricity mix. Emissions are categorized as direct on-site combustion (Scope 1), off-site electricity generation (Scope 2), and embodied emissions from manufacturing and transport (Scope 3).", "doc_id": "jegham2025", "page": 3, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "3 Preliminaries\n\nTo capture infrastructure-level overhead in data center operations, we apply three standard environmental multipliers: Power Usage Effectiveness (PUE) [19, 20], Water Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factor (CIF) [21, 22].\n\nPUE accounts for non-computational energy overheads such as cooling, lighting, and power distribution. Defined as the ratio of total data center energy consumption to IT-specific energy use.\n\nWUE captures the water used per kilowatt-hour of IT energy, encompassing on-site cooling (Scope 1), off-site electricity generation (Scope 2), and embodied water from hardware manufacturing and transport (Scope 3). WUE can be computed based on either water withdrawal (the total volume drawn from natural or municipal sources) or water consumption (the portion of withdrawn water permanently lost, primarily through evaporation).\n\nCIF measures carbon emissions per kilowatt-hour of energy consumed, largely driven by the regional electricity mix. Emissions are categorized as direct on-site combustion (Scope 1), off-site electricity generation (Scope 2), and embodied emissions from manufacturing and transport (Scope 3).", "original_types": ["text", "header"], "id": 1013}
{"type": "table", "content": "Table 1: Deployment and infrastructure specifications of models.\n```markdown\n| Model | Launch Date | Company | Host | Hardware | Critical Power (kW) | PUE | WUE (on-site, L/kWh) | WUE (off-site, L/kWh) | CIF (kgCO₂e/kWh) |\n|-------|-------------|---------|------|----------|-------------------|------|-------------------|-------------------|------------------|\n| GPT-4.1 | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4.1 mini | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4.1 nano | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o4-mini (high) | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3 | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3-mini (high) | Jan, 2025 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3-mini | Jan, 2025 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o1 | Dec, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o1-mini | Sep, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4o (Mar '25) | May, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4o mini | July, 2024 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| GPT-4 Turbo | Nov, 2023 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| GPT-4 | Mar, 2023 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| DeepSeek-R1 | Jan, 2025 | Deepseek | Deepseek | DGX H800 [8] | 10.20 [38] | 1.27 [39] | 1.20 [39] | 6.016 [35] | 0.6 [40] \n| DeepSeek-V3 | Dec, 2024 | Deepseek | Deepseek | DGX H800 [8] | 10.20 [38] | 1.27 [39] | 1.20 [39] | 6.016 [35] | 0.6 [40] \n| DeepSeek-R1 | Jan, 2025 | Deepseek | Deepseek | DGX H200/H100 | 10.20 | 1.12 | 0.30 | 4.35 | 0.35 \n| DeepSeek-V3 | Dec, 2024 | Deepseek | Deepseek | DGX H200/H100 | 10.20 | 1.12 | 0.30 | 4.35 | 0.35 \n| Claude-3.7 Sonnet | Feb, 2025 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| Claude-3.5 Sonnet | Jun, 2024 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| Claude-3.5 Haiku | Nov, 2024 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| LLaMA-3.3 70B | Dec, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2-vision 90B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2-vision 11B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2 3B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2 1B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-405B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-70B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-8B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3-70B | Apr, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3-8B | Apr, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n\n*DGX A100 was estimated for GPT-4o mini, GPT-4 Turbo, and GPT-4. Justification and estimation details are provided in Section 4.3.1.", "doc_id": "jegham2025", "page": 4, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 1: Deployment and infrastructure specifications of models.\n```markdown\n| Model | Launch Date | Company | Host | Hardware | Critical Power (kW) | PUE | WUE (on-site, L/kWh) | WUE (off-site, L/kWh) | CIF (kgCO₂e/kWh) |\n|-------|-------------|---------|------|----------|-------------------|------|-------------------|-------------------|------------------|\n| GPT-4.1 | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4.1 mini | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4.1 nano | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o4-mini (high) | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3 | Apr, 2025 | OpenAI | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3-mini (high) | Jan, 2025 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o3-mini | Jan, 2025 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o1 | Dec, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| o1-mini | Sep, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4o (Mar '25) | May, 2024 | Microsoft Azure | DGX H200/H100 [30, 31] | 10.20 [32] | 1.12 [33] | 0.30 [34] | 4.35 [35] | 0.35 [36] \n| GPT-4o mini | July, 2024 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| GPT-4 Turbo | Nov, 2023 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| GPT-4 | Mar, 2023 | OpenAI | Microsoft Azure | DGX A100* | 6.50[37] | 1.12 | 0.30 | 4.35 | 0.35 \n| DeepSeek-R1 | Jan, 2025 | Deepseek | Deepseek | DGX H800 [8] | 10.20 [38] | 1.27 [39] | 1.20 [39] | 6.016 [35] | 0.6 [40] \n| DeepSeek-V3 | Dec, 2024 | Deepseek | Deepseek | DGX H800 [8] | 10.20 [38] | 1.27 [39] | 1.20 [39] | 6.016 [35] | 0.6 [40] \n| DeepSeek-R1 | Jan, 2025 | Deepseek | Deepseek | DGX H200/H100 | 10.20 | 1.12 | 0.30 | 4.35 | 0.35 \n| DeepSeek-V3 | Dec, 2024 | Deepseek | Deepseek | DGX H200/H100 | 10.20 | 1.12 | 0.30 | 4.35 | 0.35 \n| Claude-3.7 Sonnet | Feb, 2025 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| Claude-3.5 Sonnet | Jun, 2024 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| Claude-3.5 Haiku | Nov, 2024 | Anthropic | AWS | DGX H200/H100 [41, 42] | 10.20 | 1.14 [43] | 0.18 [43] | 5.11 [35] | 0.287 [44] \n| LLaMA-3.3 70B | Dec, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2-vision 90B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2-vision 11B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2 3B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.2 1B | Sep, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-405B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-70B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3.1-8B | Jul, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3-70B | Apr, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n| LLaMA-3-8B | Apr, 2024 | Meta | AWS | DGX H200/H100 | 10.20 | 1.14 | 0.18 | 5.11 | 0.287 \n\n*DGX A100 was estimated for GPT-4o mini, GPT-4 Turbo, and GPT-4. Justification and estimation details are provided in Section 4.3.1.", "id": 1014}
{"type": "section", "content": "power draw as the H100, with system-level energy characteristics that are nearly identical [50]. While the H200 achieves greater energy efficiency due to faster memory and higher bandwidth, and the H800 may exhibit reduced performance due to export-related firmware limitations, both maintain the same peak power draw, thermal design profile, and system-level utilization characteristics as the H100 [38, 50]. These architectural differences affect throughput and latency, resulting in higher or lower energy consumed per token, but do not impact total system power demand under load. We therefore treat H100, H200, and H800 as equivalent in our power modeling, since our estimates are based on power draw and utilization rather than task-level performance.\n\nEnvironmental multipliers such as PUE, WUE, and CIF are assigned according to each cloud provider’s data center locations and corresponding regional grid characteristics. For OpenAI and DeepSeek models hosted on Microsoft Azure, we use Azure-reported PUE and site-level WUE values, while CIF and source-level WUE are derived from the specific geographic locations of Microsoft data centers around the world. For AWS-hosted models, including those from Anthropic and Meta, we apply AWS-reported PUE and site-level WUE, and compute CIF and source-level WUE based on the regional distribution of AWS data centers used for inference. For DeepSeek models that are deployed in Chinese datacenters, we adopt the average PUE and site-level WUE of the thirty most efficient data centers in China, while CIF and source-level WUE are determined using the regional locations of its known or reported data center deployments.\n\n4.2 Per-Query Energy Consumption Estimation\n\nTo quantify the energy required for a single inference, we introduce a probabilistic framework that captures the stochastic nature of LLM workloads. The model integrates standardized performance data [51], which report latency to first-token generation (L) and tokens-per-second (TPS, denoted R) across empirical quantiles (5th, 25th, 50th, 75th, and 95th percentiles) and three representative prompt configurations: short-form (100 input, 300 output tokens), medium (1,000 input, 1,000 output), and long-form (10,000 input, 1,500 output), reflecting variability across multiple test runs for each model and prompt configuration.\n\nTo model realistic runtime behavior, we construct a joint distribution of L and R using a Gaussian copula with correlation coefficient ρ = −0.3, capturing the negative dependence typically observed between latency and TPS. From this distribution, we draw 10,000 correlated samples (Li, Ri), each representing one plausible inference scenario. The culmination of this infrastructure-aware framework is the introduction of our novel formula to precisely estimate the per-query energy consumption:", "doc_id": "jegham2025", "page": 4, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "power draw as the H100, with system-level energy characteristics that are nearly identical [50]. While the H200 achieves greater energy efficiency due to faster memory and higher bandwidth, and the H800 may exhibit reduced performance due to export-related firmware limitations, both maintain the same peak power draw, thermal design profile, and system-level utilization characteristics as the H100 [38, 50]. These architectural differences affect throughput and latency, resulting in higher or lower energy consumed per token, but do not impact total system power demand under load. We therefore treat H100, H200, and H800 as equivalent in our power modeling, since our estimates are based on power draw and utilization rather than task-level performance.\n\nEnvironmental multipliers such as PUE, WUE, and CIF are assigned according to each cloud provider’s data center locations and corresponding regional grid characteristics. For OpenAI and DeepSeek models hosted on Microsoft Azure, we use Azure-reported PUE and site-level WUE values, while CIF and source-level WUE are derived from the specific geographic locations of Microsoft data centers around the world. For AWS-hosted models, including those from Anthropic and Meta, we apply AWS-reported PUE and site-level WUE, and compute CIF and source-level WUE based on the regional distribution of AWS data centers used for inference. For DeepSeek models that are deployed in Chinese datacenters, we adopt the average PUE and site-level WUE of the thirty most efficient data centers in China, while CIF and source-level WUE are determined using the regional locations of its known or reported data center deployments.\n\n4.2 Per-Query Energy Consumption Estimation\n\nTo quantify the energy required for a single inference, we introduce a probabilistic framework that captures the stochastic nature of LLM workloads. The model integrates standardized performance data [51], which report latency to first-token generation (L) and tokens-per-second (TPS, denoted R) across empirical quantiles (5th, 25th, 50th, 75th, and 95th percentiles) and three representative prompt configurations: short-form (100 input, 300 output tokens), medium (1,000 input, 1,000 output), and long-form (10,000 input, 1,500 output), reflecting variability across multiple test runs for each model and prompt configuration.\n\nTo model realistic runtime behavior, we construct a joint distribution of L and R using a Gaussian copula with correlation coefficient ρ = −0.3, capturing the negative dependence typically observed between latency and TPS. From this distribution, we draw 10,000 correlated samples (Li, Ri), each representing one plausible inference scenario. The culmination of this infrastructure-aware framework is the introduction of our novel formula to precisely estimate the per-query energy consumption:", "original_types": ["text", "header"], "id": 1015}
{"type": "section", "content": "Let \\( L_i \\) captures the initialization latency and \\( \\frac{Output Length}{R_i} \\) represents the time it takes to generate the response. Also, let \\( P_{GPU} \\) and \\( P_{non-GPU} \\) denote the rated power draw (in kW) of the GPU subsystem and the non-GPU subsystem (e.g., CPUs, SSDs, network, and cooling control electronics), respectively. The parameters \\( U_{GPU,min} \\) and \\( U_{GPU,max} \\) represent the minimum and maximum GPU utilization fractions observed during inference, while \\( U_{non-GPU} \\) represents the average utilization fraction for non-GPU components. PUE factor is also incorporated to account for datacenter-level overheads.\n\nWe compute energy consumption at the lower and upper utilization bounds as:\n\nWe also define an expected per-query energy as a weighted combination of both scenarios (w_max = 0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query energy outcomes. The final metrics are reported as the sample mean and standard deviation:\n\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center efficiency, enabling robust and reproducible estimation of per-query energy consumption across diverse inference conditions.\n\n4.3 Hardware-Class Attribution\n\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7–20B), Small (20–40B), Medium (40–70B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly. Models that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g., GPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini) as Medium, and models labeled “Nano” such as GPT-4.1 nano as Small based on reported model performance (e.g., TPS, latency, and reasoning capabilities) [51].", "doc_id": "jegham2025", "page": 5, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Let \\( L_i \\) captures the initialization latency and \\( \\frac{Output Length}{R_i} \\) represents the time it takes to generate the response. Also, let \\( P_{GPU} \\) and \\( P_{non-GPU} \\) denote the rated power draw (in kW) of the GPU subsystem and the non-GPU subsystem (e.g., CPUs, SSDs, network, and cooling control electronics), respectively. The parameters \\( U_{GPU,min} \\) and \\( U_{GPU,max} \\) represent the minimum and maximum GPU utilization fractions observed during inference, while \\( U_{non-GPU} \\) represents the average utilization fraction for non-GPU components. PUE factor is also incorporated to account for datacenter-level overheads.\n\nWe compute energy consumption at the lower and upper utilization bounds as:\n\nWe also define an expected per-query energy as a weighted combination of both scenarios (w_max = 0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query energy outcomes. The final metrics are reported as the sample mean and standard deviation:\n\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center efficiency, enabling robust and reproducible estimation of per-query energy consumption across diverse inference conditions.\n\n4.3 Hardware-Class Attribution\n\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7–20B), Small (20–40B), Medium (40–70B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly. Models that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g., GPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini) as Medium, and models labeled “Nano” such as GPT-4.1 nano as Small based on reported model performance (e.g., TPS, latency, and reasoning capabilities) [51].", "original_types": ["text", "header"], "id": 1016}
{"type": "section", "content": "AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming demand, they are generally constrained to a narrow range below 16 to preserve responsiveness. Benchmarks [51] show that even for large prompts, most models maintain a first-token latency below one second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch sizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-sensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary calculations, as it represents a practical midpoint between common deployment scenarios. A detailed sensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The number of GPUs and their allocated power draw utilization rates for H100 systems are estimated from Splitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100 systems, we adopt measurements from Patel et al. and Kakolyris et al.’s work [56, 57]. Per-request GPU and non-GPU utilization rates are calculated as:\n\nwhere \\( G \\) is the number of GPUs assigned per model, \\( N = 8 \\) is the number of GPUs per node, and \\( B = 8 \\) is the batch size. \\( D_{GPU} \\) denotes the assigned GPUs’ power draw, expressed as a fraction of their maximum power draw, while \\( D_{non-GPU} = 0.5 \\) represents the conservatively assigned fixed utilization fraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power draw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes, as deployment-specific telemetry is unavailable to determine whether such capacity is reassigned, load-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across model classes. Values are rounded to typical intervals observed during inference, accounting for input processing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.", "doc_id": "jegham2025", "page": 5, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization while maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming demand, they are generally constrained to a narrow range below 16 to preserve responsiveness. Benchmarks [51] show that even for large prompts, most models maintain a first-token latency below one second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch sizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-sensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary calculations, as it represents a practical midpoint between common deployment scenarios. A detailed sensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The number of GPUs and their allocated power draw utilization rates for H100 systems are estimated from Splitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100 systems, we adopt measurements from Patel et al. and Kakolyris et al.’s work [56, 57]. Per-request GPU and non-GPU utilization rates are calculated as:\n\nwhere \\( G \\) is the number of GPUs assigned per model, \\( N = 8 \\) is the number of GPUs per node, and \\( B = 8 \\) is the batch size. \\( D_{GPU} \\) denotes the assigned GPUs’ power draw, expressed as a fraction of their maximum power draw, while \\( D_{non-GPU} = 0.5 \\) represents the conservatively assigned fixed utilization fraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power draw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes, as deployment-specific telemetry is unavailable to determine whether such capacity is reassigned, load-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across model classes. Values are rounded to typical intervals observed during inference, accounting for input processing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.", "original_types": ["text"], "id": 1017}
{"type": "table", "content": "Table 2: Estimated node-level GPU and non-GPU utilization by model class for H100 and A100.\n```markdown\n| Class | GPU Count | D_GPU (H100) | D_GPU (A100) | U_GPU total (H100) | U_GPU total (A100) | U_non-GPU total |\n|-------|-----------|-------------|--------------|---------------------|---------------------|-----------------\n| Nano  | 1         | 35–65%      | 80–90%       | 0.55–1.00%          | 1.25–1.5%           | 0.87%           \n| Micro | 1         | 50–80%      | 90–100%      | 0.75–1.25%          | 1.5–1.6%            | 0.87%           \n| Small | 2         | 55–80%      | N/A          | 1.70–2.50%          | N/A                 | 1.6%            \n| Medium| 4         | 50–70%      | 100–110%     | 3.00–4.50%          | 6.25–7%             | 3.125%          \n| Large | 8         | 45–60%      | 100–120%     | 5.50–7.50%          | 12.5–15.0%          | 6.25%           \n|-------|-----------|-------------|--------------|---------------------|---------------------|-----------------\n```", "doc_id": "jegham2025", "page": 6, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 2: Estimated node-level GPU and non-GPU utilization by model class for H100 and A100.\n```markdown\n| Class | GPU Count | D_GPU (H100) | D_GPU (A100) | U_GPU total (H100) | U_GPU total (A100) | U_non-GPU total |\n|-------|-----------|-------------|--------------|---------------------|---------------------|-----------------\n| Nano  | 1         | 35–65%      | 80–90%       | 0.55–1.00%          | 1.25–1.5%           | 0.87%           \n| Micro | 1         | 50–80%      | 90–100%      | 0.75–1.25%          | 1.5–1.6%            | 0.87%           \n| Small | 2         | 55–80%      | N/A          | 1.70–2.50%          | N/A                 | 1.6%            \n| Medium| 4         | 50–70%      | 100–110%     | 3.00–4.50%          | 6.25–7%             | 3.125%          \n| Large | 8         | 45–60%      | 100–120%     | 5.50–7.50%          | 12.5–15.0%          | 6.25%           \n|-------|-----------|-------------|--------------|---------------------|---------------------|-----------------\n```", "id": 1018}
{"type": "figure", "content": "Figure 1: (Left) Mean energy consumption of GPT-4o and GPT-4o mini across providers and GPU types, measured by output size. (Right) Distribution of TPS (averaged across output sizes)", "doc_id": "jegham2025", "page": 6, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 1: (Left) Mean energy consumption of GPT-4o and GPT-4o mini across providers and GPU types, measured by output size. (Right) Distribution of TPS (averaged across output sizes)", "id": 1019}
{"type": "section", "content": "4.3.1 GPT-4, GPT-4 Turbo, and GPT-4o mini Hardware Estimation\n\nIn our experiment, we observed a performance discrepancy: GPT-4o mini showed significantly lower throughput and higher latency on OpenAI’s API compared to Microsoft Azure under identical prompt settings, as shown in Figure 1. Both variants also underperformed relative to OpenAI’s GPT-4o, with 60% and 27% lower TPS, respectively. Given GPT-4o mini’s smaller size and H200’s architectural advantages, its performance would be expected to match or exceed GPT-4o if served on H200 infrastructure. The observed gap is inconsistent with H200 deployment and suggests that GPT-4o mini is running on A100 or H100 systems. Notably, Azure’s version outperforms OpenAI’s by 47% on average, further supporting the likelihood that Azure uses H100 and OpenAI retains A100. Therefore, to validate our hardware estimations, we tested this hypothesis using two-way ANOVA and Tukey HSD (Table 3). At 300-token prompts, energy consumption was statistically similar across platforms, as expected given the small computational load. However, at larger output sizes, significant differences emerged: OpenAI’s presumed A100 deployment differed from Azure’s H100 deployment with \\(p < 0.05\\), and Azure’s H100 also outperformed OpenAI’s assumed H100 with \\(p < 0.05\\), reinforcing the likelihood that OpenAI’s GPT-4o mini is not served on H100. We therefore consider GPT-4o mini to be running on A100. Additionally, with reports that GPT-4 was trained and deployed on A100 systems [58], and given the architectural continuity between GPT-4 and GPT-4 Turbo and their low throughput, high latency, and impending deprecation [59], we also consider they are running on A100 architecture since it is unlikely that they have migrated to newer hardware.", "doc_id": "jegham2025", "page": 6, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "4.3.1 GPT-4, GPT-4 Turbo, and GPT-4o mini Hardware Estimation\n\nIn our experiment, we observed a performance discrepancy: GPT-4o mini showed significantly lower throughput and higher latency on OpenAI’s API compared to Microsoft Azure under identical prompt settings, as shown in Figure 1. Both variants also underperformed relative to OpenAI’s GPT-4o, with 60% and 27% lower TPS, respectively. Given GPT-4o mini’s smaller size and H200’s architectural advantages, its performance would be expected to match or exceed GPT-4o if served on H200 infrastructure. The observed gap is inconsistent with H200 deployment and suggests that GPT-4o mini is running on A100 or H100 systems. Notably, Azure’s version outperforms OpenAI’s by 47% on average, further supporting the likelihood that Azure uses H100 and OpenAI retains A100. Therefore, to validate our hardware estimations, we tested this hypothesis using two-way ANOVA and Tukey HSD (Table 3). At 300-token prompts, energy consumption was statistically similar across platforms, as expected given the small computational load. However, at larger output sizes, significant differences emerged: OpenAI’s presumed A100 deployment differed from Azure’s H100 deployment with \\(p < 0.05\\), and Azure’s H100 also outperformed OpenAI’s assumed H100 with \\(p < 0.05\\), reinforcing the likelihood that OpenAI’s GPT-4o mini is not served on H100. We therefore consider GPT-4o mini to be running on A100. Additionally, with reports that GPT-4 was trained and deployed on A100 systems [58], and given the architectural continuity between GPT-4 and GPT-4 Turbo and their low throughput, high latency, and impending deprecation [59], we also consider they are running on A100 architecture since it is unlikely that they have migrated to newer hardware.", "original_types": ["text"], "id": 1020}
{"type": "table", "content": "Table 3: Tukey HSD Adjusted \\(p\\)-values for energy consumption differences by provider, GPU system, and prompt size\n```markdown\n| Group 1 | Group 2 | 300 tokens | 1000 tokens | 1500 tokens |\n|----------|----------|------------|-------------|-------------|\n| Azure (H100) | OpenAI (A100) | 0.979 | 0.0009 | <0.0001 |\n| Azure (H100) | OpenAI (H100) | 0.951 | 0.0001 | <0.0001 |\n|----------|----------|------------|-------------|-------------|\n```", "doc_id": "jegham2025", "page": 6, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 3: Tukey HSD Adjusted \\(p\\)-values for energy consumption differences by provider, GPU system, and prompt size\n```markdown\n| Group 1 | Group 2 | 300 tokens | 1000 tokens | 1500 tokens |\n|----------|----------|------------|-------------|-------------|\n| Azure (H100) | OpenAI (A100) | 0.979 | 0.0009 | <0.0001 |\n| Azure (H100) | OpenAI (H100) | 0.951 | 0.0001 | <0.0001 |\n|----------|----------|------------|-------------|-------------|\n```", "id": 1021}
{"type": "section", "content": "4.4 Per-Query Water Consumption and Carbon Emissions Estimation\n\nThis study focuses exclusively on operational emissions and resource consumption during the inference phase of the model. Accordingly, embodied emissions and water use from hardware manufacturing and supply chains (Scope 3) are excluded due to their limited relevance to real-time deployment and the risk of inflating per-query estimates when applied without deployment-specific attribution or when model lifecycles remain ongoing. For water usage, we focus solely on water consumption (water permanently removed from the source). For carbon emissions, we exclude Scope 1 emissions as they are generally negligible compared to Scope 2 emissions due to the infrequent use of on-site fuel combustion for backup generators and facility heating in data centers [60]. For example, Scope 1 emissions accounted for only 1.6% of Microsoft’s Scope 2 emissions in 2023 [36], a figure that includes executive air travel, ground transportation, refrigerant leakage, and on-site fuel use, further diminishing the share attributable to data center operations. Accordingly, our analysis focuses exclusively on Scope 2 emissions, which capture the carbon intensity of electricity consumed during inference. A more detailed discussion of these considerations is provided in Appendix B.\n\nWater consumption and carbon emissions per query are calculated as:\n\nWater (L) = \\(\\frac{E_{query}}{PUE}\\) \\cdot WUE_{site} + \\(\\frac{E_{query} \\cdot WUE_{source}}{PUE}\\)\n\nCarbon (kgCO₂e) = \\(E_{query} \\cdot CIF\\)\n\n4.5 Eco-Efficiency via Data Envelopment Analysis (DEA)\n\nWe apply cross-efficiency DEA to evaluate the effectiveness of each model in converting environmental resources into functional intelligence. Inputs include per-query energy consumption, PUE, WUEsource, WUESite, and CIF. The output is the Artificial Intelligence Index, a composite score weighted across multiple benchmark domains [51]. Specifically, reasoning and knowledge tasks (MMLU-Pro [61], HLE [62], GPQA [63]) collectively contribute 50% of the index (1/6 each); mathematical proficiency (MATH-500 [64], AIME [65]) contributes 25% (1/8 each); and coding ability (SciCode [66], LiveCodeBench [67]) accounts for the remaining 25% (1/8 each).\n\nIn contrast to standard Charnes-Cooper-Rhodes (CCR) or Banker-Charnes-Cooper (BCC) models, which enable each model to choose its optimal weightings, sometimes inflating performance, cross-efficiency assesses each model based on its own and all peer weightings. This approach reduces self-evaluation bias and recognizes models that maintain strong performance from various efficiency viewpoints. The resulting scores offer a more robust and comparative measure of eco-efficiency. Full results and additional discussion are provided in Appendix C.", "doc_id": "jegham2025", "page": 7, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "4.4 Per-Query Water Consumption and Carbon Emissions Estimation\n\nThis study focuses exclusively on operational emissions and resource consumption during the inference phase of the model. Accordingly, embodied emissions and water use from hardware manufacturing and supply chains (Scope 3) are excluded due to their limited relevance to real-time deployment and the risk of inflating per-query estimates when applied without deployment-specific attribution or when model lifecycles remain ongoing. For water usage, we focus solely on water consumption (water permanently removed from the source). For carbon emissions, we exclude Scope 1 emissions as they are generally negligible compared to Scope 2 emissions due to the infrequent use of on-site fuel combustion for backup generators and facility heating in data centers [60]. For example, Scope 1 emissions accounted for only 1.6% of Microsoft’s Scope 2 emissions in 2023 [36], a figure that includes executive air travel, ground transportation, refrigerant leakage, and on-site fuel use, further diminishing the share attributable to data center operations. Accordingly, our analysis focuses exclusively on Scope 2 emissions, which capture the carbon intensity of electricity consumed during inference. A more detailed discussion of these considerations is provided in Appendix B.\n\nWater consumption and carbon emissions per query are calculated as:\n\nWater (L) = \\(\\frac{E_{query}}{PUE}\\) \\cdot WUE_{site} + \\(\\frac{E_{query} \\cdot WUE_{source}}{PUE}\\)\n\nCarbon (kgCO₂e) = \\(E_{query} \\cdot CIF\\)\n\n4.5 Eco-Efficiency via Data Envelopment Analysis (DEA)\n\nWe apply cross-efficiency DEA to evaluate the effectiveness of each model in converting environmental resources into functional intelligence. Inputs include per-query energy consumption, PUE, WUEsource, WUESite, and CIF. The output is the Artificial Intelligence Index, a composite score weighted across multiple benchmark domains [51]. Specifically, reasoning and knowledge tasks (MMLU-Pro [61], HLE [62], GPQA [63]) collectively contribute 50% of the index (1/6 each); mathematical proficiency (MATH-500 [64], AIME [65]) contributes 25% (1/8 each); and coding ability (SciCode [66], LiveCodeBench [67]) accounts for the remaining 25% (1/8 each).\n\nIn contrast to standard Charnes-Cooper-Rhodes (CCR) or Banker-Charnes-Cooper (BCC) models, which enable each model to choose its optimal weightings, sometimes inflating performance, cross-efficiency assesses each model based on its own and all peer weightings. This approach reduces self-evaluation bias and recognizes models that maintain strong performance from various efficiency viewpoints. The resulting scores offer a more robust and comparative measure of eco-efficiency. Full results and additional discussion are provided in Appendix C.", "original_types": ["text", "header", "equation"], "id": 1022}
{"type": "section", "content": "Experimental Evaluation\n\nWe benchmark the environmental footprint of 30 LLMs across three modalities: Energy consumption, water usage, and carbon emissions, based on equations 2, 4, and 5, respectively. For the long-form query evaluation, GPT-4 and LLaMA-3 (8B and 70B) are excluded due to context window limitations.", "doc_id": "jegham2025", "page": 8, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Experimental Evaluation\n\nWe benchmark the environmental footprint of 30 LLMs across three modalities: Energy consumption, water usage, and carbon emissions, based on equations 2, 4, and 5, respectively. For the long-form query evaluation, GPT-4 and LLaMA-3 (8B and 70B) are excluded due to context window limitations.", "original_types": ["text", "header"], "id": 1023}
{"type": "table", "content": "Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).\nMarkdown representation of the table", "doc_id": "jegham2025", "page": 8, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).\nMarkdown representation of the table", "id": 1024}
{"type": "figure", "content": "Figure 3: Energy consumption per model across three prompt sizes (Wh, log-scale).", "doc_id": "jegham2025", "page": 8, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 3: Energy consumption per model across three prompt sizes (Wh, log-scale).", "id": 1025}
{"type": "section", "content": "5.2 Water and Carbon Emissions", "doc_id": "jegham2025", "page": 9, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "5.2 Water and Carbon Emissions", "original_types": ["header"], "id": 1026}
{"type": "figure", "content": "Figure 4: Water consumption and carbon emissions per model.", "doc_id": "jegham2025", "page": 9, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 4: Water consumption and carbon emissions per model.", "id": 1027}
{"type": "figure", "content": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total annual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption of GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.", "doc_id": "jegham2025", "page": 10, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total annual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption of GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.", "id": 1028}
{"type": "section", "content": "Validation Against Public Disclosures\n\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent statements provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman reported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing that GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level inference. Our framework estimates 0.42 Wh (±0.13 Wh) for a short GPT-4o prompt (0.37 Wh without datacenter overhead), within 19% of Altman’s figure. Similarly, the results for Mistral Large 2 align closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approximately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e, ±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment within one standard deviation. Together, these alignments between independent disclosures and our modeled results suggest that the framework reproduces realistic operational conditions for modern LLM inference.", "doc_id": "jegham2025", "page": 10, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Validation Against Public Disclosures\n\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent statements provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman reported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing that GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level inference. Our framework estimates 0.42 Wh (±0.13 Wh) for a short GPT-4o prompt (0.37 Wh without datacenter overhead), within 19% of Altman’s figure. Similarly, the results for Mistral Large 2 align closely with Mistral’s published life-cycle assessment (LCA) report [69], which cites approximately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e, ±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment within one standard deviation. Together, these alignments between independent disclosures and our modeled results suggest that the framework reproduces realistic operational conditions for modern LLM inference.", "original_types": ["text"], "id": 1029}
{"type": "section", "content": "6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\n\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into real-world benchmarks. Based on scaled inference volumes, GPT-4o’s annual water consumption is projected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly equivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2 million people. Importantly, this consumption refers to evaporated freshwater permanently removed from local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount of freshwater equivalent to the annual drinking needs of almost 1.2 million people.", "doc_id": "jegham2025", "page": 11, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\n\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into real-world benchmarks. Based on scaled inference volumes, GPT-4o’s annual water consumption is projected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly equivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2 million people. Importantly, this consumption refers to evaporated freshwater permanently removed from local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount of freshwater equivalent to the annual drinking needs of almost 1.2 million people.", "original_types": ["text", "header"], "id": 1030}
{"type": "figure", "content": "Figure 6: Energy consumption of GPT-5 across query lengths and reasoning modes", "doc_id": "jegham2025", "page": 12, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 6: Energy consumption of GPT-5 across query lengths and reasoning modes", "id": 1031}
{"type": "section", "content": "8 Discussion and Policy Implications\n\n8.1 The Critical Role of Infrastructure in AI Sustainability\n\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While model design enhances theoretical efficiency, real-world outcomes can substantially diverge based on deployment conditions and factors such as renewable energy usage and hardware efficiency. For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek models highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on DeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than their Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient cooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability gains can stem as much from datacenter design as from model optimization. These observations underscore that true AI sustainability will hinge on coordinated progress in hardware efficiency, renewable energy sources, and infrastructure-aware deployment strategies.\n\n8.2 Rebound Effects and the Jevons Paradox\n\nAlthough large language models consume significantly less energy, water, and carbon per task than human labor [75], these efficiency gains do not inherently reduce overall environmental impact. As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand. The acceleration and affordability of AI remove traditional human and resource constraints, enabling unprecedented levels of usage. Consequently, the cumulative environmental burden threatens to overwhelm the sustainability baselines that AI efficiency improvements initially sought to mitigate. As such, sustainable AI deployment must focus on systemic frameworks that assess how well models balance capability with environmental cost. In response, we propose DEA as a principled method for benchmarking model-level eco-efficiency.\n\n8.3 Policy Implications", "doc_id": "jegham2025", "page": 12, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "8 Discussion and Policy Implications\n\n8.1 The Critical Role of Infrastructure in AI Sustainability\n\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While model design enhances theoretical efficiency, real-world outcomes can substantially diverge based on deployment conditions and factors such as renewable energy usage and hardware efficiency. For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek models highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on DeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than their Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient cooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability gains can stem as much from datacenter design as from model optimization. These observations underscore that true AI sustainability will hinge on coordinated progress in hardware efficiency, renewable energy sources, and infrastructure-aware deployment strategies.\n\n8.2 Rebound Effects and the Jevons Paradox\n\nAlthough large language models consume significantly less energy, water, and carbon per task than human labor [75], these efficiency gains do not inherently reduce overall environmental impact. As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand. The acceleration and affordability of AI remove traditional human and resource constraints, enabling unprecedented levels of usage. Consequently, the cumulative environmental burden threatens to overwhelm the sustainability baselines that AI efficiency improvements initially sought to mitigate. As such, sustainable AI deployment must focus on systemic frameworks that assess how well models balance capability with environmental cost. In response, we propose DEA as a principled method for benchmarking model-level eco-efficiency.\n\n8.3 Policy Implications", "original_types": ["text"], "id": 1032}
{"type": "section", "content": "As AI systems scale globally, ensuring environmental sustainability requires both model-level optimizations and systemic regulation of infrastructure. Government agencies should encourage thresholds on the permissible environmental footprint per inference regarding energy, water, and carbon emissions that AI models must not exceed. These thresholds can be met through architectural innovations, such as sparsity and quantization, or through infrastructure-level optimizations like more efficient hardware, cleaner energy sourcing, and improved cooling systems. Our methodology offers a standardized, scalable framework to quantify these efforts. Incorporating technologies like dielectric liquid cooling offers a promising path to reduce or eliminate water use in data centers drastically [77]. Transparency must also be elevated through system-level reporting of per-inference energy, water, and carbon metrics. Additionally, deployment strategies, such as batching, should be integrated into sustainability planning, as larger batch sizes can reduce per-query energy use by improving hardware utilization with only minimal impact on latency.", "doc_id": "jegham2025", "page": 12, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "As AI systems scale globally, ensuring environmental sustainability requires both model-level optimizations and systemic regulation of infrastructure. Government agencies should encourage thresholds on the permissible environmental footprint per inference regarding energy, water, and carbon emissions that AI models must not exceed. These thresholds can be met through architectural innovations, such as sparsity and quantization, or through infrastructure-level optimizations like more efficient hardware, cleaner energy sourcing, and improved cooling systems. Our methodology offers a standardized, scalable framework to quantify these efforts. Incorporating technologies like dielectric liquid cooling offers a promising path to reduce or eliminate water use in data centers drastically [77]. Transparency must also be elevated through system-level reporting of per-inference energy, water, and carbon metrics. Additionally, deployment strategies, such as batching, should be integrated into sustainability planning, as larger batch sizes can reduce per-query energy use by improving hardware utilization with only minimal impact on latency.", "original_types": ["text"], "id": 1033}
{"type": "section", "content": "Conclusion, Limitations, and Future Work\n\nThis paper introduces the first large-scale, infrastructure-aware framework for benchmarking the environmental footprint of LLM inference, integrating API performance, environmental multipliers, and statistical inference to assess energy, water, and carbon costs under real-world conditions. By applying cross-efficiency DEA, we contextualize environmental impact in terms of functional performance, revealing that eco-efficiency hinges not only on model design but also on infrastructure. Our GPT-4o case study emphasizes the Jevons Paradox: As AI becomes cheaper and faster, total usage expands, intensifying environmental strain despite gains in per-query efficiency. Additionally, our GPT-5 case study sheds lights on the importance of prompt-level efficiency and adaptive routing. Without structural shifts in how LLMs are designed, deployed, and used, these invisible costs will continue to rise, threatening to offset the societal benefits that made these systems valuable in the first place. This work establishes a standardized, scalable framework for benchmarking the environmental footprint of LLM inference in real-world data center deployments, providing a basis for transparent, infrastructure-aware sustainability assessment and future regulation.\n\nOur work inherits certain limitations that we acknowledge: we avoid overstating model-specific footprints by conservatively including only the energy drawn by actively assigned GPUs. This is due to the lack of means to determine whether unused GPUs’ capacity is reassigned, load-balanced, or left inactive. Isolating non-GPU power consumption was also difficult. We applied a fixed utilization estimate from prior studies, acknowledging that their variation across inference workloads is typically significantly lower than that of GPUs. Moreover, for proprietary models without disclosed size, we classified their scale based on observed API performance. Future work should address these limitations as more detailed telemetry and facility-level reporting become available. Additionally, future studies should also extend beyond text generation to evaluate image, video, and audio generation, which are likely to impose greater environmental costs due to higher computational intensity.", "doc_id": "jegham2025", "page": 13, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Conclusion, Limitations, and Future Work\n\nThis paper introduces the first large-scale, infrastructure-aware framework for benchmarking the environmental footprint of LLM inference, integrating API performance, environmental multipliers, and statistical inference to assess energy, water, and carbon costs under real-world conditions. By applying cross-efficiency DEA, we contextualize environmental impact in terms of functional performance, revealing that eco-efficiency hinges not only on model design but also on infrastructure. Our GPT-4o case study emphasizes the Jevons Paradox: As AI becomes cheaper and faster, total usage expands, intensifying environmental strain despite gains in per-query efficiency. Additionally, our GPT-5 case study sheds lights on the importance of prompt-level efficiency and adaptive routing. Without structural shifts in how LLMs are designed, deployed, and used, these invisible costs will continue to rise, threatening to offset the societal benefits that made these systems valuable in the first place. This work establishes a standardized, scalable framework for benchmarking the environmental footprint of LLM inference in real-world data center deployments, providing a basis for transparent, infrastructure-aware sustainability assessment and future regulation.\n\nOur work inherits certain limitations that we acknowledge: we avoid overstating model-specific footprints by conservatively including only the energy drawn by actively assigned GPUs. This is due to the lack of means to determine whether unused GPUs’ capacity is reassigned, load-balanced, or left inactive. Isolating non-GPU power consumption was also difficult. We applied a fixed utilization estimate from prior studies, acknowledging that their variation across inference workloads is typically significantly lower than that of GPUs. Moreover, for proprietary models without disclosed size, we classified their scale based on observed API performance. Future work should address these limitations as more detailed telemetry and facility-level reporting become available. Additionally, future studies should also extend beyond text generation to evaluate image, video, and audio generation, which are likely to impose greater environmental costs due to higher computational intensity.", "original_types": ["text", "header"], "id": 1034}
{"type": "section", "content": "[12] David Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Xinlei Chen, and Andrew Ng. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\n\n[13] Shaolei Li. Making ai less “thirsty”: Uncovering and addressing the secret water footprint of ai models. arXiv preprint arXiv:2304.03271, 2023.\n\n[14] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. Trends in ai inference energy consumption: Beyond the performance-vs-parameter laws of deep learning. Sustainable Computing: Informatics and Systems, 38:100857, 2023.\n\n[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Codecarbon: Estimate and track carbon emissions from machine learning training. https://github.com/mlco2/codecarbon, 2022.\n\n[16] Microsoft Corporation. 2024 environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, May 2024.\n\n[17] Google. 2024 environmental report. https://sustainability.google/reports/google-2024-environmental-report/, July 2024.\n\n[18] Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, and Sagar Sen. The price of prompting: Profiling energy use in large language models inference. arXiv preprint arXiv:2407.16893, 2024.\n\n[19] The Green Grid. PUE™: A Comprehensive Examination of the Metric. February 2012. White Paper 49.\n\n[20] International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC). Information technology – Data centres – Key performance indicators – Part 2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/63211.html.\n\n[21] U.S. Environmental Protection Agency (EPA). Emissions & Generation Resource Integrated Database (eGRID). https://www.epa.gov/egrid, 2025.\n\n[22] International Energy Agency (IEA). Emissions Factors. 2025.\n\n[23] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693–13696, 2020.\n\n[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[26] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–9. IEEE, 2023.", "doc_id": "jegham2025", "page": 14, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[12] David Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Xinlei Chen, and Andrew Ng. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\n\n[13] Shaolei Li. Making ai less “thirsty”: Uncovering and addressing the secret water footprint of ai models. arXiv preprint arXiv:2304.03271, 2023.\n\n[14] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo. Trends in ai inference energy consumption: Beyond the performance-vs-parameter laws of deep learning. Sustainable Computing: Informatics and Systems, 38:100857, 2023.\n\n[15] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Codecarbon: Estimate and track carbon emissions from machine learning training. https://github.com/mlco2/codecarbon, 2022.\n\n[16] Microsoft Corporation. 2024 environmental sustainability report. https://www.microsoft.com/en-us/corporate-responsibility/sustainability/report, May 2024.\n\n[17] Google. 2024 environmental report. https://sustainability.google/reports/google-2024-environmental-report/, July 2024.\n\n[18] Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, and Sagar Sen. The price of prompting: Profiling energy use in large language models inference. arXiv preprint arXiv:2407.16893, 2024.\n\n[19] The Green Grid. PUE™: A Comprehensive Examination of the Metric. February 2012. White Paper 49.\n\n[20] International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC). Information technology – Data centres – Key performance indicators – Part 2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/63211.html.\n\n[21] U.S. Environmental Protection Agency (EPA). Emissions & Generation Resource Integrated Database (eGRID). https://www.epa.gov/egrid, 2025.\n\n[22] International Energy Agency (IEA). Emissions Factors. 2025.\n\n[23] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693–13696, 2020.\n\n[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[26] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–9. IEEE, 2023.", "original_types": ["text"], "id": 1035}
{"type": "section", "content": "[27] Zeyu Yang, Karel Adamek, and Wesley Armour. Double-exponential increases in inference energy: The cost of the race for accuracy. arXiv preprint arXiv:2412.09731, 2024.\n\n[28] Sasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the cost of ai deployment? In Proceedings of the 2024 ACM conference on fairness, accountability, and transparency, pages 85–99, 2024.\n\n[29] Anthony Harding and Juan Moreno-Cruz. Watts and bots: The energy implications of ai adoption. arXiv preprint arXiv:2409.06626, 2024.", "doc_id": "jegham2025", "page": 14, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[27] Zeyu Yang, Karel Adamek, and Wesley Armour. Double-exponential increases in inference energy: The cost of the race for accuracy. arXiv preprint arXiv:2412.09731, 2024.\n\n[28] Sasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the cost of ai deployment? In Proceedings of the 2024 ACM conference on fairness, accountability, and transparency, pages 85–99, 2024.\n\n[29] Anthony Harding and Juan Moreno-Cruz. Watts and bots: The energy implications of ai adoption. arXiv preprint arXiv:2409.06626, 2024.", "original_types": ["text"], "id": 1036}
{"type": "section", "content": "[30] Dallin Grimm. Nvidia ceo hand-delivers world’s fastest ai system to openai. https://www.tomshardware.com/tech-industry/artificial-intelligence/, April 2024.\n\n[31] NVIDIA. NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows. https://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-as-demand-for-ai-grows, March 2023.\n\n[32] Imran Latif, Alex C. Newkirk, Matthew R. Carbone, Arslan Munir, Yuewei Lin, Jonathan Koomey, Xi Yu, and Zhihua Dong. Single-node power demand during ai training: Measurements on an 8-gpu nvidia h100 system. IEEE Access, 13:61740–61747, 2025. doi: 10.1109/ACCESS.2025.3554728.\n\n[33] Noelle Walsh. How microsoft measures datacenter water and energy use to improve azure cloud sustainability. https://azure.microsoft.com/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/, April 2022. Microsoft Azure Blog.\n\n[34] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for cooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/, December 2024. Microsoft Cloud Blog.\n\n[35] World Resources Institute. Guidance for calculating water use embedded in purchased electricity. Technical report, World Resources Institute, 2024.\n\n[36] Microsoft Corporation. 2024 environmental sustainability report data fact sheet. https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/2024-Environmental-Sustainability-Report-Data-Fact.pdf, May 2024. Comprehensive environmental metrics including greenhouse gas emissions, energy consumption, water usage, waste management, and land protection for fiscal year 2023.\n\n[37] NVIDIA Corporation. Nvidia dgx a100: The universal system for ai infrastructure. https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020. Datasheet detailing specifications and features of the NVIDIA DGX A100 system.\n\n[38] NVIDIA Corporation. Nvidia dgx h800 system. https://viperatech.com/shop/nvidia-dgx-h800-systems/, 2024. High-performance AI system featuring 8x NVIDIA H800 GPUs, 640 GB HBM3 memory, and up to 32 petaFLOPS FP8 performance.\n\n[39] Hequan Wu. Academician hequan wu: Green and low-carbon development of data centers requires multi-dimensional coordination of “source, grid, load, and storage”. https://www.cace.org.cn/News/NContent?key=04e714e4e006d433617f5d7148df2eb0, April 2024. China Communications Enterprise Association News.\n\n[40] Wenli Ni, Xiurong Hu, Hongyang Du, Yulin Kang, Yi Ju, and Qunwei Wang. Co2 emission-mitigation pathways for china’s data centers. Resources, Conservation and Recycling, 202: 107383, 2024.", "doc_id": "jegham2025", "page": 15, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[30] Dallin Grimm. Nvidia ceo hand-delivers world’s fastest ai system to openai. https://www.tomshardware.com/tech-industry/artificial-intelligence/, April 2024.\n\n[31] NVIDIA. NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows. https://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-as-demand-for-ai-grows, March 2023.\n\n[32] Imran Latif, Alex C. Newkirk, Matthew R. Carbone, Arslan Munir, Yuewei Lin, Jonathan Koomey, Xi Yu, and Zhihua Dong. Single-node power demand during ai training: Measurements on an 8-gpu nvidia h100 system. IEEE Access, 13:61740–61747, 2025. doi: 10.1109/ACCESS.2025.3554728.\n\n[33] Noelle Walsh. How microsoft measures datacenter water and energy use to improve azure cloud sustainability. https://azure.microsoft.com/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/, April 2022. Microsoft Azure Blog.\n\n[34] Steve Solomon. Sustainable by design: Next-generation datacenters consume zero water for cooling. https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/12/09/sustainable-by-design-next-generation-datacenters-consume-zero-water-for-cooling/, December 2024. Microsoft Cloud Blog.\n\n[35] World Resources Institute. Guidance for calculating water use embedded in purchased electricity. Technical report, World Resources Institute, 2024.\n\n[36] Microsoft Corporation. 2024 environmental sustainability report data fact sheet. https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/2024-Environmental-Sustainability-Report-Data-Fact.pdf, May 2024. Comprehensive environmental metrics including greenhouse gas emissions, energy consumption, water usage, waste management, and land protection for fiscal year 2023.\n\n[37] NVIDIA Corporation. Nvidia dgx a100: The universal system for ai infrastructure. https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf, 2020. Datasheet detailing specifications and features of the NVIDIA DGX A100 system.\n\n[38] NVIDIA Corporation. Nvidia dgx h800 system. https://viperatech.com/shop/nvidia-dgx-h800-systems/, 2024. High-performance AI system featuring 8x NVIDIA H800 GPUs, 640 GB HBM3 memory, and up to 32 petaFLOPS FP8 performance.\n\n[39] Hequan Wu. Academician hequan wu: Green and low-carbon development of data centers requires multi-dimensional coordination of “source, grid, load, and storage”. https://www.cace.org.cn/News/NContent?key=04e714e4e006d433617f5d7148df2eb0, April 2024. China Communications Enterprise Association News.\n\n[40] Wenli Ni, Xiurong Hu, Hongyang Du, Yulin Kang, Yi Ju, and Qunwei Wang. Co2 emission-mitigation pathways for china’s data centers. Resources, Conservation and Recycling, 202: 107383, 2024.", "original_types": ["text"], "id": 1037}
{"type": "section", "content": "[41] AWS News Blog. New amazon ec2 p5 instances powered by nvidia h100 tensor core gpus for accelerating generative ai and hpc applications. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/.\n\n[42] AWS News Blog. New amazon ec2 p5e instances with nvidia h200 tensor core gpus and efav3 networking. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-instances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking, 2024.\n\n[43] Amazon.com, Inc. 2023 amazon sustainability report. Technical report, Amazon.com, Inc., 2024.\n\n[44] Electricity Maps. Electricity maps — live carbon intensity map. https://app.electricitymaps.com/map/, 2025.", "doc_id": "jegham2025", "page": 15, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[41] AWS News Blog. New amazon ec2 p5 instances powered by nvidia h100 tensor core gpus for accelerating generative ai and hpc applications. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/.\n\n[42] AWS News Blog. New amazon ec2 p5e instances with nvidia h200 tensor core gpus and efav3 networking. https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-instances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking, 2024.\n\n[43] Amazon.com, Inc. 2023 amazon sustainability report. Technical report, Amazon.com, Inc., 2024.\n\n[44] Electricity Maps. Electricity maps — live carbon intensity map. https://app.electricitymaps.com/map/, 2025.", "original_types": ["text"], "id": 1038}
{"type": "section", "content": "[45] NVIDIA Corporation. NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX H100 Systems – Electrical Specifications, October 2024.\n\n[46] Arman Shehabi, Sarah J. Smith, Nathaniel Horner, Inês Azevedo, Richard Brown, Jonathan Koomey, Eric Masanet, Dale Sartor, Magnus Herrlin, and William Lintner. 2024 united states data center energy usage report. Technical report, Lawrence Berkeley National Laboratory, December 2024.\n\n[47] Rani Borkar. Microsoft and nvidia partnership continues to deliver on the promise of ai. https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-partnership-continues-to-deliver-on-the-promise-of-ai/, March 2024. Microsoft Azure Blog.\n\n[48] NVIDIA. Project ceiba. https://resources.nvidia.com/en-us-dgx-cloud/project-ceiba-video?ncid=so-twit-266831&ncid=no-ncid, 2023.\n\n[49] The New York Times. Nvidia’s h20 chip faces new u.s. export restrictions to china. https://www.nytimes.com/2025/04/15/technology/nvidia-h20-chip-china-restrictions.html, April 2025.\n\n[50] NVIDIA Corporation. NVIDIA DGX H100/H200 System User Guide, 2025.\n\n[51] Artificial Analysis. Artificial analysis: Ai model & api providers analysis. https://artificialanalysis.ai, 2025.\n\n[52] NVIDIA. Triton inference server user guide: Dynamic batching. https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html, 2024.\n\n[53] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: Inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1362–1379. IEEE Computer Society, 2024.\n\n[54] Ankit Vora, Avik Chaudhuri, Deepak Narayanan, and Matei Zaharia. Splitwise: Efficient generative llm inference using phase-splitting. In Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024.\n\n[55] Xing Chen, Daniel Lo, Sitao Xiang, Daniel Kang, and Kunle Olukotun. A latency processing unit: A latency-optimized and highly scalable processor for large language model inference. In Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024.\n\n[56] P. Patel et al. Characterizing power management opportunities for llms in the cloud. In Proceedings of the 29th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.\n\n[57] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Sotirios Xydis, and Dimitrios Soudris. Slo-aware gpu dvfs for energy-efficient llm inference serving. IEEE Computer Architecture Letters, 2024.\n\n[58] Dylan Patel and Gerald Wong. Gpt-4 architecture, infrastructure, training dataset, costs, vision, moe. https://semianalysis.com/2023/07/10/gpt-4-architecture-infrastructure/, July 2023.", "doc_id": "jegham2025", "page": 16, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[45] NVIDIA Corporation. NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX H100 Systems – Electrical Specifications, October 2024.\n\n[46] Arman Shehabi, Sarah J. Smith, Nathaniel Horner, Inês Azevedo, Richard Brown, Jonathan Koomey, Eric Masanet, Dale Sartor, Magnus Herrlin, and William Lintner. 2024 united states data center energy usage report. Technical report, Lawrence Berkeley National Laboratory, December 2024.\n\n[47] Rani Borkar. Microsoft and nvidia partnership continues to deliver on the promise of ai. https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-partnership-continues-to-deliver-on-the-promise-of-ai/, March 2024. Microsoft Azure Blog.\n\n[48] NVIDIA. Project ceiba. https://resources.nvidia.com/en-us-dgx-cloud/project-ceiba-video?ncid=so-twit-266831&ncid=no-ncid, 2023.\n\n[49] The New York Times. Nvidia’s h20 chip faces new u.s. export restrictions to china. https://www.nytimes.com/2025/04/15/technology/nvidia-h20-chip-china-restrictions.html, April 2025.\n\n[50] NVIDIA Corporation. NVIDIA DGX H100/H200 System User Guide, 2025.\n\n[51] Artificial Analysis. Artificial analysis: Ai model & api providers analysis. https://artificialanalysis.ai, 2025.\n\n[52] NVIDIA. Triton inference server user guide: Dynamic batching. https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html, 2024.\n\n[53] Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: Inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1362–1379. IEEE Computer Society, 2024.\n\n[54] Ankit Vora, Avik Chaudhuri, Deepak Narayanan, and Matei Zaharia. Splitwise: Efficient generative llm inference using phase-splitting. In Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024.\n\n[55] Xing Chen, Daniel Lo, Sitao Xiang, Daniel Kang, and Kunle Olukotun. A latency processing unit: A latency-optimized and highly scalable processor for large language model inference. In Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024.\n\n[56] P. Patel et al. Characterizing power management opportunities for llms in the cloud. In Proceedings of the 29th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024.\n\n[57] Andreas Kosmas Kakolyris, Dimosthenis Masouros, Sotirios Xydis, and Dimitrios Soudris. Slo-aware gpu dvfs for energy-efficient llm inference serving. IEEE Computer Architecture Letters, 2024.\n\n[58] Dylan Patel and Gerald Wong. Gpt-4 architecture, infrastructure, training dataset, costs, vision, moe. https://semianalysis.com/2023/07/10/gpt-4-architecture-infrastructure/, July 2023.", "original_types": ["text"], "id": 1039}
{"type": "section", "content": "[59] OpenAI. Deprecations - openai api. https://platform.openai.com/docs/deprecations, 2025.\n\n[60] Tuğana Aslan, Peter Holzapfel, Lutz Stobbe, Andreas Grimm, Nils F Nissen, and Matthias Finkbeiner. Toward climate neutral data centers: Greenhouse gas inventory, scenarios, and strategies. iScience, 28(1), 2025.", "doc_id": "jegham2025", "page": 16, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[59] OpenAI. Deprecations - openai api. https://platform.openai.com/docs/deprecations, 2025.\n\n[60] Tuğana Aslan, Peter Holzapfel, Lutz Stobbe, Andreas Grimm, Nils F Nissen, and Matthias Finkbeiner. Toward climate neutral data centers: Greenhouse gas inventory, scenarios, and strategies. iScience, 28(1), 2025.", "original_types": ["text"], "id": 1040}
{"type": "section", "content": "[61] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:95266–95290, 2025.\n\n[62] Dan Hendrycks et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. URL https://arxiv.org/abs/2501.14249.\n\n[63] David Rein et al. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n\n[64] HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/HuggingFaceH4/MATH-500, 2024.\n\n[65] Maxwell-Jia. Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/AIME_2024, 2024.\n\n[66] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: A research coding benchmark curated by scientists. Advances in Neural Information Processing Systems, 37:30624–30650, 2024.\n\n[67] Fanjia Yan et al. Livecodebench: Holistic and contamination free evaluation of llms for code. arXiv preprint arXiv:2403.07974, 2024.\n\n[68] Sam Altman. The gentle singularity. https://blog.samaltman.com/the-gentle-singularity, 2025.\n\n[69] Mistral AI. Our contribution to a global environmental standard for AI, Jul 2025. URL https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai.\n\n[70] Reuters. Openai’s weekly active users surpass 400 million. https://www.reuters.com/technology/artificial-intelligence/openais-weekly-active-users-surpass-400-million-2025-02-20/, February 2025.\n\n[71] Emma Roth. Chatgpt now has over 300 million weekly users. https://www.theverge.com/2024/12/4/24313097/chatgpt-300-million-weekly-users, December 2024.\n\n[72] Shubham Singh. Chatgpt statistics (2025): Dau & mau data worldwide. https://www.demandsage.com/chatgpt-statistics/, April 2025.\n\n[73] Anthony Cardillo. How many google searches are there per day? (march 2025). https://explodingtopics.com/blog/google-searches-per-day, April 2025.\n\n[74] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025.\n\n[75] Shaolei Ren, Bill Tomlinson, Rebecca W Black, and Andrew W Torrance. Reconciling the contrasting narratives on the environmental impact of large language models. Scientific Reports, 14(1):26310, 2024.\n\n[76] John M Polimeni and Raluca Iorgulescu Polimeni. Jevons’ paradox and the myth of technological liberation. Ecological Complexity, 3(4):344–353, 2006.\n\n[77] Aleksandar Ristic-Smith and Daniel J. Rogers. Compact two-phase immersion cooling with dielectric fluid for pcb-based power electronics. IEEE Open Journal of Power Electronics, 5:1107–1118, 2024. doi: 10.1109/OJPEL.2024.3432989.", "doc_id": "jegham2025", "page": 17, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "[61] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:95266–95290, 2025.\n\n[62] Dan Hendrycks et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. URL https://arxiv.org/abs/2501.14249.\n\n[63] David Rein et al. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\n\n[64] HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/HuggingFaceH4/MATH-500, 2024.\n\n[65] Maxwell-Jia. Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/AIME_2024, 2024.\n\n[66] Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, et al. Scicode: A research coding benchmark curated by scientists. Advances in Neural Information Processing Systems, 37:30624–30650, 2024.\n\n[67] Fanjia Yan et al. Livecodebench: Holistic and contamination free evaluation of llms for code. arXiv preprint arXiv:2403.07974, 2024.\n\n[68] Sam Altman. The gentle singularity. https://blog.samaltman.com/the-gentle-singularity, 2025.\n\n[69] Mistral AI. Our contribution to a global environmental standard for AI, Jul 2025. URL https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai.\n\n[70] Reuters. Openai’s weekly active users surpass 400 million. https://www.reuters.com/technology/artificial-intelligence/openais-weekly-active-users-surpass-400-million-2025-02-20/, February 2025.\n\n[71] Emma Roth. Chatgpt now has over 300 million weekly users. https://www.theverge.com/2024/12/4/24313097/chatgpt-300-million-weekly-users, December 2024.\n\n[72] Shubham Singh. Chatgpt statistics (2025): Dau & mau data worldwide. https://www.demandsage.com/chatgpt-statistics/, April 2025.\n\n[73] Anthony Cardillo. How many google searches are there per day? (march 2025). https://explodingtopics.com/blog/google-searches-per-day, April 2025.\n\n[74] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025.\n\n[75] Shaolei Ren, Bill Tomlinson, Rebecca W Black, and Andrew W Torrance. Reconciling the contrasting narratives on the environmental impact of large language models. Scientific Reports, 14(1):26310, 2024.\n\n[76] John M Polimeni and Raluca Iorgulescu Polimeni. Jevons’ paradox and the myth of technological liberation. Ecological Complexity, 3(4):344–353, 2006.\n\n[77] Aleksandar Ristic-Smith and Daniel J. Rogers. Compact two-phase immersion cooling with dielectric fluid for pcb-based power electronics. IEEE Open Journal of Power Electronics, 5:1107–1118, 2024. doi: 10.1109/OJPEL.2024.3432989.", "original_types": ["text"], "id": 1041}
{"type": "section", "content": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.", "doc_id": "jegham2025", "page": 18, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.", "original_types": ["header"], "id": 1042}
{"type": "table", "content": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.\n| Batch Size | D_GPU | U_GPU total | U_non-GPU total |\n|---|---|---|---|\n| 4 | 40-55% | 10-13.5% | 12.5% |\n| 8 | 45-60% | 5.5-7.5% | 6.25% |\n| 16 | 55-70% | 3.5-4.5% | 3.125% |\n", "doc_id": "jegham2025", "page": 18, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Table 5: Estimated node-level GPU and non-GPU utilization by batch size for GPT-4o.\n| Batch Size | D_GPU | U_GPU total | U_non-GPU total |\n|---|---|---|---|\n| 4 | 40-55% | 10-13.5% | 12.5% |\n| 8 | 45-60% | 5.5-7.5% | 6.25% |\n| 16 | 55-70% | 3.5-4.5% | 3.125% |\n", "id": 1043}
{"type": "figure", "content": "Figure 8: Cross efficiency DEA scores. Bar labels show the AI Index (top) and cross-efficiency score (bottom).", "doc_id": "jegham2025", "page": 19, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "Figure 8: Cross efficiency DEA scores. Bar labels show the AI Index (top) and cross-efficiency score (bottom).", "id": 1044}
{"type": "section", "content": "manufacturing, emissions from global logistics, and hardware retirement. For instance, Microsoft’s Scope 3 CO2e emissions in 2023 accounted for 66% of the total emissions [16]. Yet, these values are highly variable across vendors, manufacturing locations, and fabrication nodes, and they lack deployment-specific attribution when applied to real-time inference tasks.\n\nMoreover, given that many large-scale models are continually updated and deployed across evolving infrastructures, ascribing a fixed fraction of embodied emissions or water per query is both methodologically fragile and likely to result in overestimation. Applying complete hardware manufacturing footprints to ongoing inference, without amortizing them over the expected hardware lifespan or query volume, risks artificially inflating per-query environmental costs.\n\nIn light of this, we excluded Scope 3 from our prompt-level framework, as its inclusion would introduce non-trivial uncertainty and potentially distort comparative eco-efficiency across models. Nevertheless, the long-term sustainability of AI infrastructure will depend on extending lifecycle accountability beyond the inference phase; future work is encouraged to adopt comprehensive lifecycle analyses (LCA) that integrate Scope 3 considerations once transparent and standardized data become available.\n\nCross-efffficiency DEA Results\n\nBefore presenting the eco-efficiency results, it is worth noting that Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4, and GPT-4 Turbo were excluded due to the lack of benchmark results on certain tests. Since cross-efficiency requires complete inputs and outputs, these models could not be fairly evaluated.\n\nAs shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These results suggest that downsizing reasoning models can yield meaningful sustainability gains without compromising performance.\n\nAt the opposite end, DeepSeek-R1 (0.067) and DeepSeek-V3 (0.059) recorded the lowest efficiency scores. Despite their advanced reasoning capabilities, their high energy, water, and carbon costs indicate significant infrastructural inefficiencies. Their Azure-hosted variants performed better, DeepSeek-R1 (0.539) and DeepSeek-V3 (0.523), yet remained below most OpenAI and Anthropic systems. Among OpenAI models, GPT-4.1 mini (0.580) and GPT-4.1 nano (0.508) balanced output quality and sustainability particularly well. LLaMA models clustered between 0.4 and 0.6, reflecting efficient power use but limited reasoning performance.", "doc_id": "jegham2025", "page": 19, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "manufacturing, emissions from global logistics, and hardware retirement. For instance, Microsoft’s Scope 3 CO2e emissions in 2023 accounted for 66% of the total emissions [16]. Yet, these values are highly variable across vendors, manufacturing locations, and fabrication nodes, and they lack deployment-specific attribution when applied to real-time inference tasks.\n\nMoreover, given that many large-scale models are continually updated and deployed across evolving infrastructures, ascribing a fixed fraction of embodied emissions or water per query is both methodologically fragile and likely to result in overestimation. Applying complete hardware manufacturing footprints to ongoing inference, without amortizing them over the expected hardware lifespan or query volume, risks artificially inflating per-query environmental costs.\n\nIn light of this, we excluded Scope 3 from our prompt-level framework, as its inclusion would introduce non-trivial uncertainty and potentially distort comparative eco-efficiency across models. Nevertheless, the long-term sustainability of AI infrastructure will depend on extending lifecycle accountability beyond the inference phase; future work is encouraged to adopt comprehensive lifecycle analyses (LCA) that integrate Scope 3 considerations once transparent and standardized data become available.\n\nCross-efffficiency DEA Results\n\nBefore presenting the eco-efficiency results, it is worth noting that Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4, and GPT-4 Turbo were excluded due to the lack of benchmark results on certain tests. Since cross-efficiency requires complete inputs and outputs, these models could not be fairly evaluated.\n\nAs shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint. GPT-4o (Mar) (0.789) and o3 (0.758) also performed well. These results suggest that downsizing reasoning models can yield meaningful sustainability gains without compromising performance.\n\nAt the opposite end, DeepSeek-R1 (0.067) and DeepSeek-V3 (0.059) recorded the lowest efficiency scores. Despite their advanced reasoning capabilities, their high energy, water, and carbon costs indicate significant infrastructural inefficiencies. Their Azure-hosted variants performed better, DeepSeek-R1 (0.539) and DeepSeek-V3 (0.523), yet remained below most OpenAI and Anthropic systems. Among OpenAI models, GPT-4.1 mini (0.580) and GPT-4.1 nano (0.508) balanced output quality and sustainability particularly well. LLaMA models clustered between 0.4 and 0.6, reflecting efficient power use but limited reasoning performance.", "original_types": ["text", "header"], "id": 1045}
{"type": "section", "content": "In summary, eco-efficiency relies on both output quality and environmental cost. OpenAI’s smaller reasoning models and Claude 3.7 Sonnet strike that balance most effectively, while DeepSeek and LLaMA demonstrate the limitations of concentrating on capability or sustainability alone.", "doc_id": "jegham2025", "page": 20, "url": "https://arxiv.org/pdf/2505.09598", "embedded_text": "In summary, eco-efficiency relies on both output quality and environmental cost. OpenAI’s smaller reasoning models and Claude 3.7 Sonnet strike that balance most effectively, while DeepSeek and LLaMA demonstrate the limitations of concentrating on capability or sustainability alone.", "original_types": ["text"], "id": 1046}
{"type": "section", "content": "Abstract\n\nThe rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.", "doc_id": "khan2025", "page": 1, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Abstract\n\nThe rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.", "original_types": ["text", "header"], "id": 1047}
{"type": "section", "content": "II. RELATED WORK\n\nIn recent years, the convergence of environmental sustainability and AI has led to the emergence of “in Green AI”, focusing on reducing the carbon footprint of large-scale models through optimization techniques. This section reviews key studies that have contributed to this field, highlighting their contributions to sustainable AI practices.", "doc_id": "khan2025", "page": 2, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "II. RELATED WORK\n\nIn recent years, the convergence of environmental sustainability and AI has led to the emergence of “in Green AI”, focusing on reducing the carbon footprint of large-scale models through optimization techniques. This section reviews key studies that have contributed to this field, highlighting their contributions to sustainable AI practices.", "original_types": ["text", "header"], "id": 1048}
{"type": "table", "content": "Common Carbon Emission Metrics in Green AI\nMetric\tUnit\tDefinition\tReference\nCarbon Dioxide Equivalent (CO2e)\tMetric tons (tCO2e)\tA measure of greenhouse gases expressed as CO2 equivalent\tIPCC, GHG Protocol\nCarbon Intensity\tgCO2/kWh\tCO2 emissions per unit of electricity consumed\tInternational Energy Agency\nScope 1 Emissions\ttCO2e\tDirect emissions from controlled sources\tGHG Protocol\nScope 2 Emissions\ttCO2e\tIndirect emissions from purchased electricity\tGHG Protocol\nScope 3 Emissions\ttCO2e\tIndirect emissions across value chains\tGHG Protocol\nNet Emissions\tZero\ttCO2e\tBalance when emissions equal removals\tUNFCCC\nEnergy Consumption\tMWh\tTotal energy consumed\tIEA, EIA\nGlobal Warming Potential (GWP)\tRatio\tHeat trapped by a gas compared to CO2\tIPCC\nCarbon Offsets\ttCO2e\tCredits for emissions reduction or removal\tVCS, Gold Standard\nCarbon Capture and Storage (CCS)\ttCO2 captured\tCO2 removed and stored to prevent release\tIEA, IPCC", "doc_id": "khan2025", "page": 2, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Common Carbon Emission Metrics in Green AI\nMetric\tUnit\tDefinition\tReference\nCarbon Dioxide Equivalent (CO2e)\tMetric tons (tCO2e)\tA measure of greenhouse gases expressed as CO2 equivalent\tIPCC, GHG Protocol\nCarbon Intensity\tgCO2/kWh\tCO2 emissions per unit of electricity consumed\tInternational Energy Agency\nScope 1 Emissions\ttCO2e\tDirect emissions from controlled sources\tGHG Protocol\nScope 2 Emissions\ttCO2e\tIndirect emissions from purchased electricity\tGHG Protocol\nScope 3 Emissions\ttCO2e\tIndirect emissions across value chains\tGHG Protocol\nNet Emissions\tZero\ttCO2e\tBalance when emissions equal removals\tUNFCCC\nEnergy Consumption\tMWh\tTotal energy consumed\tIEA, EIA\nGlobal Warming Potential (GWP)\tRatio\tHeat trapped by a gas compared to CO2\tIPCC\nCarbon Offsets\ttCO2e\tCredits for emissions reduction or removal\tVCS, Gold Standard\nCarbon Capture and Storage (CCS)\ttCO2 captured\tCO2 removed and stored to prevent release\tIEA, IPCC", "id": 1049}
{"type": "section", "content": "B. Quantization Techniques in LLMs\n\nQuantization [11] has emerged as a transformative approach in optimizing LLMs, addressing the dual challenges of computational efficiency and environmental sustainability. It works by converting model parameters from high-precision formats (e.g., 32-bit floating-point) to lower-precision formats (e.g., 8-bit or even 4-bit), thereby reducing memory requirements and accelerating computation. This technique aligns closely with the goals of Green AI, as it minimizes resource usage while maintaining acceptable accuracy.\nResearch efforts have showcased the potential of quantization as a key technique for enhancing energy efficiency in AI systems. For instance, GPTQ (Accurate Post-Training Quantization for Generative Pre-trained Transformers) [9] introduces a method for post-training quantization that retains high model performance despite significant reductions in parameter precision. This method enables efficient deployment of LLMs on edge devices and other constrained environments, directly addressing the environmental concerns highlighted in studies like GreenTrainer [12]. Another notable approach is", "doc_id": "khan2025", "page": 2, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "B. Quantization Techniques in LLMs\n\nQuantization [11] has emerged as a transformative approach in optimizing LLMs, addressing the dual challenges of computational efficiency and environmental sustainability. It works by converting model parameters from high-precision formats (e.g., 32-bit floating-point) to lower-precision formats (e.g., 8-bit or even 4-bit), thereby reducing memory requirements and accelerating computation. This technique aligns closely with the goals of Green AI, as it minimizes resource usage while maintaining acceptable accuracy.\nResearch efforts have showcased the potential of quantization as a key technique for enhancing energy efficiency in AI systems. For instance, GPTQ (Accurate Post-Training Quantization for Generative Pre-trained Transformers) [9] introduces a method for post-training quantization that retains high model performance despite significant reductions in parameter precision. This method enables efficient deployment of LLMs on edge devices and other constrained environments, directly addressing the environmental concerns highlighted in studies like GreenTrainer [12]. Another notable approach is", "original_types": ["text", "header"], "id": 1050}
{"type": "section", "content": "C. Trade-Offs Between Accuracy and Optimization\n\nThe interplay between accuracy and optimization in machine learning models, particularly LLMs, underscores the challenges in balancing performance with resource efficiency. Techniques like FrugalGPT, described by Chen et al. (2023) [5], illustrate how cascading models and leveraging prompt adaptation can reduce costs by up to 98% without compromising accuracy. Similarly, FrugalML shows how selectively routing queries to different APIs can maintain performance while cutting costs by up to 90% [5], [6]. However, the effectiveness of these strategies varies across tasks. For example, reducing token lengths or approximating model outputs might preserve general quality but risks performance drops in nuanced applications like sentiment analysis or summarization [22]. These findings reveal that while cost and carbon footprint reductions are achievable, ensuring minimal trade-offs in precision, recall, or F1 score remains a complex optimization problem, often requiring task-specific calibrations [5], [22].\n\nIII. CASE STUDY: SUSTAINABLE DEPLOYMENT OF LARGE LANGUAGE MODELS\n\nA. Problem Definition\n\nLLMs have become integral to various natural language processing applications, yet their soaring computational demands pose significant sustainability challenges. These include high energy consumption, carbon emissions, and escalating operational costs, particularly when using cloud-based infrastructures [4]. To address these concerns, this study proposes a framework for LLM deployment that emphasizes local inference, aiming to mitigate environmental impact while preserving model performance and user experience.\n\nFormally, consider a classification problem with input data\n\nX = {x1, x2, ..., xN},\n\nand corresponding ground-truth labels\n\nY = {y1, y2, ..., yN}, y_i ∈ {1, 2, ..., K},\n\nAn LLM-based classifier f_θ(·) predicts ŷ_i = f_θ(x_i). We seek to minimize energy consumption and carbon emissions while maintaining high predictive accuracy, where accuracy can be quantified using standard metrics such as precision, recall, and F1-score.\n\nB. Framework Overview\n\nThe proposed framework (Figure. 1) tackles energy efficiency in LLM deployment through three interconnected components: local inference optimization, the selection of energy-efficient LLMs, and a comprehensive evaluation methodology. These components function synergistically to reduce energy consumption without sacrificing predictive accuracy or responsiveness.", "doc_id": "khan2025", "page": 3, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "C. Trade-Offs Between Accuracy and Optimization\n\nThe interplay between accuracy and optimization in machine learning models, particularly LLMs, underscores the challenges in balancing performance with resource efficiency. Techniques like FrugalGPT, described by Chen et al. (2023) [5], illustrate how cascading models and leveraging prompt adaptation can reduce costs by up to 98% without compromising accuracy. Similarly, FrugalML shows how selectively routing queries to different APIs can maintain performance while cutting costs by up to 90% [5], [6]. However, the effectiveness of these strategies varies across tasks. For example, reducing token lengths or approximating model outputs might preserve general quality but risks performance drops in nuanced applications like sentiment analysis or summarization [22]. These findings reveal that while cost and carbon footprint reductions are achievable, ensuring minimal trade-offs in precision, recall, or F1 score remains a complex optimization problem, often requiring task-specific calibrations [5], [22].\n\nIII. CASE STUDY: SUSTAINABLE DEPLOYMENT OF LARGE LANGUAGE MODELS\n\nA. Problem Definition\n\nLLMs have become integral to various natural language processing applications, yet their soaring computational demands pose significant sustainability challenges. These include high energy consumption, carbon emissions, and escalating operational costs, particularly when using cloud-based infrastructures [4]. To address these concerns, this study proposes a framework for LLM deployment that emphasizes local inference, aiming to mitigate environmental impact while preserving model performance and user experience.\n\nFormally, consider a classification problem with input data\n\nX = {x1, x2, ..., xN},\n\nand corresponding ground-truth labels\n\nY = {y1, y2, ..., yN}, y_i ∈ {1, 2, ..., K},\n\nAn LLM-based classifier f_θ(·) predicts ŷ_i = f_θ(x_i). We seek to minimize energy consumption and carbon emissions while maintaining high predictive accuracy, where accuracy can be quantified using standard metrics such as precision, recall, and F1-score.\n\nB. Framework Overview\n\nThe proposed framework (Figure. 1) tackles energy efficiency in LLM deployment through three interconnected components: local inference optimization, the selection of energy-efficient LLMs, and a comprehensive evaluation methodology. These components function synergistically to reduce energy consumption without sacrificing predictive accuracy or responsiveness.", "original_types": ["text", "header"], "id": 1051}
{"type": "figure", "content": "Figure 1. Detailed Overview of the Proposed Optimization Framework", "doc_id": "khan2025", "page": 3, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Figure 1. Detailed Overview of the Proposed Optimization Framework", "id": 1052}
{"type": "section", "content": "1) Local Inference Optimization: Unlike traditional cloud-based methods that rely on centralized data centers, local inference allows models to run directly on user devices while maintaining data privacy. By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10]. To achieve efficient local inference, the framework employs a quantization process [9], which lowers the numerical precision of model parameters. Specifically, we define a uniform quantization function Q_b(·) that maps 32-bit weight tensors to a b-bit representation:\n\nQ_b(w) = round(w - min(w) / Δ),\n\nwhere Δ is a scaling factor determined by the range (max(w) - min(w)) of the weights. In this work, we use a 4-bit quantization strategy (b = 4), which substantially reduces computational and memory requirements without significantly compromising model performance. We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.\n\n2) Selection of Energy-Efficient Pre-trained LLMs: In addition to local inference optimization, the framework includes a careful selection of pre-trained LLMs that are specifically designed for low computational overhead. These models, including Llama3.2 [1], Phi3.2 [21], Mistral [13], Qwen [8], and Llava [14], stand out for their smaller parameter counts, streamlined architectures, and selective attention mechanisms. Such features align well with edge-oriented design principles,", "doc_id": "khan2025", "page": 3, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "1) Local Inference Optimization: Unlike traditional cloud-based methods that rely on centralized data centers, local inference allows models to run directly on user devices while maintaining data privacy. By minimizing data transmission between clients and remote servers, this method significantly reduces both network overhead and carbon footprint [10]. To achieve efficient local inference, the framework employs a quantization process [9], which lowers the numerical precision of model parameters. Specifically, we define a uniform quantization function Q_b(·) that maps 32-bit weight tensors to a b-bit representation:\n\nQ_b(w) = round(w - min(w) / Δ),\n\nwhere Δ is a scaling factor determined by the range (max(w) - min(w)) of the weights. In this work, we use a 4-bit quantization strategy (b = 4), which substantially reduces computational and memory requirements without significantly compromising model performance. We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.\n\n2) Selection of Energy-Efficient Pre-trained LLMs: In addition to local inference optimization, the framework includes a careful selection of pre-trained LLMs that are specifically designed for low computational overhead. These models, including Llama3.2 [1], Phi3.2 [21], Mistral [13], Qwen [8], and Llava [14], stand out for their smaller parameter counts, streamlined architectures, and selective attention mechanisms. Such features align well with edge-oriented design principles,", "original_types": ["text"], "id": 1053}
{"type": "section", "content": "3) Evaluation Methodology\n\nThe central problem tackled here is a classification task for which we use standard evaluation metrics, including precision, recall, and F1-score. We measure these metrics both before and after applying our quantization approach to understand any performance trade-offs. Furthermore, we track energy usage and estimate carbon footprints by monitoring power consumption and utilizing emission factor data. Let E denote the total energy consumed (in kWh), and let α be the emission factor (kg CO2 per kWh). We define the carbon footprint CF as:\n\nCF = E × α.\n\nC. Expected Outcomes\n\nThe proposed framework is expected to significantly reduce energy consumption and carbon emissions during LLM inference, while maintaining accuracy and responsiveness comparable to standard cloud-based methods. These findings support the goals of Green AI, showing that sustainable solutions can deliver high performance without burdening users or compromising model quality.\n\nIV. Experimental Setup\n\nA. Hardware and Software Setting\n\nThe hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable). The system type is a 64-bit operating system with an x64-based processor, running on Windows 11 Pro.\n\nWe use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions.\n\nBaselines\n\nWe used the following instruction-tuned models in Table. II for inference, each configured with specific hyperparameters tailored to their architecture and target tasks.", "doc_id": "khan2025", "page": 4, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "3) Evaluation Methodology\n\nThe central problem tackled here is a classification task for which we use standard evaluation metrics, including precision, recall, and F1-score. We measure these metrics both before and after applying our quantization approach to understand any performance trade-offs. Furthermore, we track energy usage and estimate carbon footprints by monitoring power consumption and utilizing emission factor data. Let E denote the total energy consumed (in kWh), and let α be the emission factor (kg CO2 per kWh). We define the carbon footprint CF as:\n\nCF = E × α.\n\nC. Expected Outcomes\n\nThe proposed framework is expected to significantly reduce energy consumption and carbon emissions during LLM inference, while maintaining accuracy and responsiveness comparable to standard cloud-based methods. These findings support the goals of Green AI, showing that sustainable solutions can deliver high performance without burdening users or compromising model quality.\n\nIV. Experimental Setup\n\nA. Hardware and Software Setting\n\nThe hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable). The system type is a 64-bit operating system with an x64-based processor, running on Windows 11 Pro.\n\nWe use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions.\n\nBaselines\n\nWe used the following instruction-tuned models in Table. II for inference, each configured with specific hyperparameters tailored to their architecture and target tasks.", "original_types": ["text", "header", "equation"], "id": 1054}
{"type": "table", "content": "Table II: Baseline Models and Inference Hyperparameters\nMarkdown representation of the table", "doc_id": "khan2025", "page": 4, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Table II: Baseline Models and Inference Hyperparameters\nMarkdown representation of the table", "id": 1055}
{"type": "figure", "content": "Figure 2: Sentiment Assessment Instructions and Indicators Checklist", "doc_id": "khan2025", "page": 4, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Figure 2: Sentiment Assessment Instructions and Indicators Checklist", "id": 1056}
{"type": "section", "content": "V. Results\n\nA. Accuracy vs Memory Usage\n\nTable III shows significant reductions in carbon emissions across all models, with some achieving up to 45% after optimization. These results demonstrate the effectiveness of quantization and local inference in lowering energy use and computational overhead, while maintaining model performance. Such improvements make these methods well-suited for deployment on edge devices and in resource-constrained environments.\n\nHowever, the impact on performance metrics such as accuracy, F1 score, recall, and precision varies. While the reduction in carbon footprint is consistent, performance trade-offs are evident, with some metrics experiencing marginal improvements", "doc_id": "khan2025", "page": 4, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "V. Results\n\nA. Accuracy vs Memory Usage\n\nTable III shows significant reductions in carbon emissions across all models, with some achieving up to 45% after optimization. These results demonstrate the effectiveness of quantization and local inference in lowering energy use and computational overhead, while maintaining model performance. Such improvements make these methods well-suited for deployment on edge devices and in resource-constrained environments.\n\nHowever, the impact on performance metrics such as accuracy, F1 score, recall, and precision varies. While the reduction in carbon footprint is consistent, performance trade-offs are evident, with some metrics experiencing marginal improvements", "original_types": ["text", "header"], "id": 1057}
{"type": "section", "content": "TABLE III\n\nCOMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLMs BEFORE AND AFTER OPTIMIZATION. CARBON EMISSIONS ARE CALCULATED PER INFERENCE TASK.", "doc_id": "khan2025", "page": 5, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "TABLE III\n\nCOMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLMs BEFORE AND AFTER OPTIMIZATION. CARBON EMISSIONS ARE CALCULATED PER INFERENCE TASK.", "original_types": ["text", "header"], "id": 1058}
{"type": "table", "content": "TABLE III\nComparison of performance metrics and carbon emissions for five LLMs before and after optimization. Carbon emissions are calculated per inference task.", "doc_id": "khan2025", "page": 5, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "TABLE III\nComparison of performance metrics and carbon emissions for five LLMs before and after optimization. Carbon emissions are calculated per inference task.", "id": 1059}
{"type": "figure", "content": "Fig. 3. Key Examples of Sentiment Analysis Experiments", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Fig. 3. Key Examples of Sentiment Analysis Experiments", "id": 1060}
{"type": "table", "content": "Sentiment Evaluation", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Sentiment Evaluation", "id": 1061}
{"type": "section", "content": "Text: $AAPL afternoon selloff as usual will be brutal. Get ready to lose a ton of money.\n\nLabel: Negative\n\nPredicted: Negative\n\nReasoning: “afternoon selloff as usual will be brutal”, “get ready to lose a ton of money” strongly negative sentiment.\n\nEvaluation: Consistent and correct; matches Negative sentiment with reasoning (e.g., “brutal” and financial loss).", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Text: $AAPL afternoon selloff as usual will be brutal. Get ready to lose a ton of money.\n\nLabel: Negative\n\nPredicted: Negative\n\nReasoning: “afternoon selloff as usual will be brutal”, “get ready to lose a ton of money” strongly negative sentiment.\n\nEvaluation: Consistent and correct; matches Negative sentiment with reasoning (e.g., “brutal” and financial loss).", "original_types": ["text"], "id": 1062}
{"type": "table", "content": "Sentiment Evaluation", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Sentiment Evaluation", "id": 1063}
{"type": "section", "content": "Text: Dealers said the share was hit by investor disappointment over a refining margin of 9.48 USD per barrel and shipping performance decline.\n\nLabel: Negative\n\nPredicted: Neutral\n\nReasoning: Negative: “disappointment” and declining EBIT suggest negativity; Neutral: data tone is factual.\n\nEvaluation: Inconsistent prediction; factual tone doesn’t negate “disappointment” and negative financial performance.", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Text: Dealers said the share was hit by investor disappointment over a refining margin of 9.48 USD per barrel and shipping performance decline.\n\nLabel: Negative\n\nPredicted: Neutral\n\nReasoning: Negative: “disappointment” and declining EBIT suggest negativity; Neutral: data tone is factual.\n\nEvaluation: Inconsistent prediction; factual tone doesn’t negate “disappointment” and negative financial performance.", "original_types": ["text"], "id": 1064}
{"type": "table", "content": "Sentiment Evaluation", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Sentiment Evaluation", "id": 1065}
{"type": "section", "content": "Text: Recent deliveries include refinery technology to Anrak Aluminium and sintering plants to Bhushan Steel.\n\nLabel: Neutral\n\nPredicted: Neutral\n\nReasoning: Purely factual information, no emotive or evaluative language.\n\nEvaluation: Consistent and correct; neutral tone matches factual details.", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Text: Recent deliveries include refinery technology to Anrak Aluminium and sintering plants to Bhushan Steel.\n\nLabel: Neutral\n\nPredicted: Neutral\n\nReasoning: Purely factual information, no emotive or evaluative language.\n\nEvaluation: Consistent and correct; neutral tone matches factual details.", "original_types": ["text"], "id": 1066}
{"type": "table", "content": "Sentiment Evaluation", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Sentiment Evaluation", "id": 1067}
{"type": "section", "content": "Text: Cinema Series concludes with a profile of Finnish inventor Olavi Linden, whose work has led to dozens of design awards.\n\nLabel: Neutral\n\nPredicted: Positive\n\nReasoning: Positive: “dozens of design awards” implies achievement; Neutral: factual and descriptive tone; “concludes” is neutral.\n\nEvaluation: Inconsistent; factual tone suggests Neutral, despite positive implications of “awards.”", "doc_id": "khan2025", "page": 6, "url": "https://arxiv.org/pdf/2504.06307", "embedded_text": "Text: Cinema Series concludes with a profile of Finnish inventor Olavi Linden, whose work has led to dozens of design awards.\n\nLabel: Neutral\n\nPredicted: Positive\n\nReasoning: Positive: “dozens of design awards” implies achievement; Neutral: factual and descriptive tone; “concludes” is neutral.\n\nEvaluation: Inconsistent; factual tone suggests Neutral, despite positive implications of “awards.”", "original_types": ["text"], "id": 1068}
{"type": "section", "content": "Abstract\n\nWe live in a world that is experiencing an unprecedented boom of AI applications that increasingly penetrate all sectors of private and public life, from education, media, medicine, and mobility to the industrial and professional workspace. As this world is simultaneously grappling with climate change, the climate effects and environmental implications of the development and use of AI have become an important subject of public and academic debate. In this paper, we aim to provide guidance on the climate-related regulation for data centers and AI specifically, and discuss how to operationalize these requirements. We also highlight challenges and room for improvement, and make a number of policy proposals to this end. In particular, we propose a specific interpretation of the AI Act to bring reporting on the previously unaddressed energy consumption from AI inferences back into the scope. We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications. Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level. We also argue for an interpretation of the AI Act that includes environmental concerns in the mandatory risk assessments (sustainability risk assessment, SIA), and provide guidance on its operationalization. The EU data center regulation proves to be a good first step but requires further development by including binding renewable energy and efficiency targets for data centers. Overall, we make twelve concrete policy proposals, in four main areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.", "doc_id": "ebert2024", "page": 1, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Abstract\n\nWe live in a world that is experiencing an unprecedented boom of AI applications that increasingly penetrate all sectors of private and public life, from education, media, medicine, and mobility to the industrial and professional workspace. As this world is simultaneously grappling with climate change, the climate effects and environmental implications of the development and use of AI have become an important subject of public and academic debate. In this paper, we aim to provide guidance on the climate-related regulation for data centers and AI specifically, and discuss how to operationalize these requirements. We also highlight challenges and room for improvement, and make a number of policy proposals to this end. In particular, we propose a specific interpretation of the AI Act to bring reporting on the previously unaddressed energy consumption from AI inferences back into the scope. We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications. Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level. We also argue for an interpretation of the AI Act that includes environmental concerns in the mandatory risk assessments (sustainability risk assessment, SIA), and provide guidance on its operationalization. The EU data center regulation proves to be a good first step but requires further development by including binding renewable energy and efficiency targets for data centers. Overall, we make twelve concrete policy proposals, in four main areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.", "original_types": ["text", "header"], "id": 1069}
{"type": "section", "content": "1. Introduction\n\nthe EU’s data center regulation remains incomplete, lacking binding efficiency and renewable energy targets. These shortcomings threaten to undermine the most promising legislative avenues for tackling AI’s climate effects. The EU’s recent push for massive investments in computing infrastructure under the AI Continent Action Plan, including the establishment of AI Gigafactories [25], is a late but necessary impetus for technological and strategic autonomy but must not endanger the Union’s climate goals under the Green New Deal.\n\nAgainst this background, this paper makes three key contributions. First, we offer the first thorough analysis of the AI Act’s final version from an environmental perspective. Unlike prior scholarship, which has examined these areas separately, we also scrutinize the interplay between data center regulation and direct AI regulation. AI sustainability is not governed by a single legal instrument but rather emerges from the interaction of multiple frameworks and actors. This fragmented approach creates legal uncertainty and regulatory loopholes — notably the omission of AI inference from reporting obligations and the failure to mandate Sustainability Impact Assessments (SIA). We then propose a novel interpretation of the AI Act to include AI inferences in the reporting obligations of general-purpose AI providers, and to incorporate sustainability considerations into the mandatory risk assessment framework to effectuate the goal of environmental protection enshrined in Art. 1 and Recitals 1, 2 and 8.\n\nSecond, we examine the practical challenges of implementing AI-related environmental obligations and provide technical recommendations. The crux here is to operationalize existing rules and make climate reporting work by developing metrics that are both meaningful and workable in practice.\n\nThird, we identify areas where the AI Act and related regulatory frameworks require future amendments. In light of upcoming evaluation rounds, we advocate for closing loopholes and reinstating environmental obligations that were weakened during the legislative process. While the current political momentum seems to favor deregulation, the climate emergency persists. Therefore, it is crucial to improve the best legal tools still available, and to advocate for a nuanced legal framework to bring the twin transitions of AI and climate together. Our twelve policy proposals address four key areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.\n\n2. Related Work", "doc_id": "ebert2024", "page": 2, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "1. Introduction\n\nthe EU’s data center regulation remains incomplete, lacking binding efficiency and renewable energy targets. These shortcomings threaten to undermine the most promising legislative avenues for tackling AI’s climate effects. The EU’s recent push for massive investments in computing infrastructure under the AI Continent Action Plan, including the establishment of AI Gigafactories [25], is a late but necessary impetus for technological and strategic autonomy but must not endanger the Union’s climate goals under the Green New Deal.\n\nAgainst this background, this paper makes three key contributions. First, we offer the first thorough analysis of the AI Act’s final version from an environmental perspective. Unlike prior scholarship, which has examined these areas separately, we also scrutinize the interplay between data center regulation and direct AI regulation. AI sustainability is not governed by a single legal instrument but rather emerges from the interaction of multiple frameworks and actors. This fragmented approach creates legal uncertainty and regulatory loopholes — notably the omission of AI inference from reporting obligations and the failure to mandate Sustainability Impact Assessments (SIA). We then propose a novel interpretation of the AI Act to include AI inferences in the reporting obligations of general-purpose AI providers, and to incorporate sustainability considerations into the mandatory risk assessment framework to effectuate the goal of environmental protection enshrined in Art. 1 and Recitals 1, 2 and 8.\n\nSecond, we examine the practical challenges of implementing AI-related environmental obligations and provide technical recommendations. The crux here is to operationalize existing rules and make climate reporting work by developing metrics that are both meaningful and workable in practice.\n\nThird, we identify areas where the AI Act and related regulatory frameworks require future amendments. In light of upcoming evaluation rounds, we advocate for closing loopholes and reinstating environmental obligations that were weakened during the legislative process. While the current political momentum seems to favor deregulation, the climate emergency persists. Therefore, it is crucial to improve the best legal tools still available, and to advocate for a nuanced legal framework to bring the twin transitions of AI and climate together. Our twelve policy proposals address four key areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.\n\n2. Related Work", "original_types": ["text", "header"], "id": 1070}
{"type": "section", "content": "Due to the interdisciplinary nature of sustainable AI, related works originate from three main fields—computer science; humanities and policy; and legal research. In recent years, the computer science and AI community has become increasingly aware of the environmental effects of AI, which led to both policy-oriented [8, 14, 29, 37, 44, 45, 52, 54, 56, 70, 76] and technical contributions, particularly on the environmental effects of data centers operations and AI-related computing [2, 30, 62], studies for tracking [1, 19, 23, 51, 53, 55, 64, 79, 82] and techniques for reducing the emissions of data centers and AI models [7, 31, 43, 46, 57]. The water consumption of AI training has also been a concern [49, 83]. Broader scholarship and policy work includes ethical [15, 27], informational [20, 54, 63, 75], and social perspectives [6]. Numerous contributions also analyze and evaluate the various means in which AI may be used to mitigate, and adapt to, climate change [13, 17, 34, 41, 47, 68, 69, 75], as also acknowledged in the AI Act (Recitals 4 and 142). However, from a legal perspective, the problem of sustainable AI remains underexplored. Existing contributions date from before the AI Act’s final version [63], which differs significantly from previous proposals (see below, 5.2.) or do not engage with its provisions in detail [33]. Other, complimentary work focuses uniquely on the DSA [11] or data center regulation [16].\n\n3. Technical Background\n\nFrom a technical perspective, it is important to distinguish between (pre-)training, fine-tuning, and inference. Training refers to the process of initially adjusting a model’s parameters or weights to fit the data. This process is highly compute-intensive and typically requires a significant amount of energy [37, 53, 55, 79]. LLM performance strongly depends on the model scale (number of parameters), which in turn requires more training data [38, 66], and hence more resources. Such scaling can even be expected with algorithmically advanced models, such as DeepSeek R1 [3].", "doc_id": "ebert2024", "page": 2, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Due to the interdisciplinary nature of sustainable AI, related works originate from three main fields—computer science; humanities and policy; and legal research. In recent years, the computer science and AI community has become increasingly aware of the environmental effects of AI, which led to both policy-oriented [8, 14, 29, 37, 44, 45, 52, 54, 56, 70, 76] and technical contributions, particularly on the environmental effects of data centers operations and AI-related computing [2, 30, 62], studies for tracking [1, 19, 23, 51, 53, 55, 64, 79, 82] and techniques for reducing the emissions of data centers and AI models [7, 31, 43, 46, 57]. The water consumption of AI training has also been a concern [49, 83]. Broader scholarship and policy work includes ethical [15, 27], informational [20, 54, 63, 75], and social perspectives [6]. Numerous contributions also analyze and evaluate the various means in which AI may be used to mitigate, and adapt to, climate change [13, 17, 34, 41, 47, 68, 69, 75], as also acknowledged in the AI Act (Recitals 4 and 142). However, from a legal perspective, the problem of sustainable AI remains underexplored. Existing contributions date from before the AI Act’s final version [63], which differs significantly from previous proposals (see below, 5.2.) or do not engage with its provisions in detail [33]. Other, complimentary work focuses uniquely on the DSA [11] or data center regulation [16].\n\n3. Technical Background\n\nFrom a technical perspective, it is important to distinguish between (pre-)training, fine-tuning, and inference. Training refers to the process of initially adjusting a model’s parameters or weights to fit the data. This process is highly compute-intensive and typically requires a significant amount of energy [37, 53, 55, 79]. LLM performance strongly depends on the model scale (number of parameters), which in turn requires more training data [38, 66], and hence more resources. Such scaling can even be expected with algorithmically advanced models, such as DeepSeek R1 [3].", "original_types": ["text", "header"], "id": 1071}
{"type": "section", "content": "4.1 EU Data Collection and Reporting Obligations for Data Centers\n\nIn the EU, data collection and reporting obligations for data centers were established by two recent legal acts, Art. 12 of the recast Energy Efficiency Directive EU/2023/1791 of September 13, 2023 (\"EED\"), and the Commission Delegated Regulation EU/2024/1364 of March 14, 2024 (\"Delegated Regulation\"). The new rules apply to all data centers in the EU with a power demand of the installed information technology (IT) of at least 500kW, which includes small-sized data centers. Data center operators are required to collect, make publicly available and report to a EU database information that is deemed relevant for the sustainability assessment of the data centers and the industry as a whole. The reporting is mandated on an annual basis.\n\nThe required data includes energy consumption, power utilization, temperature set points, waste heat utilisation, water usage and use of renewable energy (EED, Annex VII(c)). Notably, while the EED focuses on energy and power, the reporting of water usage is a significant step forward as both energy and water consumption have raised concerns in AI settings [49, 55]. In addition, the Water Framework Directive can be harnessed to limit the overall amount water a data center may consume, and also control for any potential loss of water quality [33].\n\nThe Delegated Regulation provides specific key performance indicators and methodology. Most notable is the requirement to measure and report the energy consumption of the installed information technology. Following the standard-methodology for the calculation of PUE,1 the energy consumption must be measured at the uninterruptible power system (UPS) or, if not existent, at the power distribution unit (PDU) or at another point specified by the data center (see Delegated Regulation, Annex II(1)(e); see also the appendix, Figure 1 as Categories 1-3.\n\nThe data centers must report to the EU database directly or via a national reporting scheme, if such a scheme is implemented by the Member State. From the reported data the Commission calculates the data center sustainability indicators which are made publicly available on an aggregate level. They include the power usage effectiveness (PUE), the water usage effectiveness (WUE)2, the energy reuse factor (ERF)3, and the renewable energy factor (REF)4.\n\n4.2 Energy Management Systems and Energy Audits", "doc_id": "ebert2024", "page": 3, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "4.1 EU Data Collection and Reporting Obligations for Data Centers\n\nIn the EU, data collection and reporting obligations for data centers were established by two recent legal acts, Art. 12 of the recast Energy Efficiency Directive EU/2023/1791 of September 13, 2023 (\"EED\"), and the Commission Delegated Regulation EU/2024/1364 of March 14, 2024 (\"Delegated Regulation\"). The new rules apply to all data centers in the EU with a power demand of the installed information technology (IT) of at least 500kW, which includes small-sized data centers. Data center operators are required to collect, make publicly available and report to a EU database information that is deemed relevant for the sustainability assessment of the data centers and the industry as a whole. The reporting is mandated on an annual basis.\n\nThe required data includes energy consumption, power utilization, temperature set points, waste heat utilisation, water usage and use of renewable energy (EED, Annex VII(c)). Notably, while the EED focuses on energy and power, the reporting of water usage is a significant step forward as both energy and water consumption have raised concerns in AI settings [49, 55]. In addition, the Water Framework Directive can be harnessed to limit the overall amount water a data center may consume, and also control for any potential loss of water quality [33].\n\nThe Delegated Regulation provides specific key performance indicators and methodology. Most notable is the requirement to measure and report the energy consumption of the installed information technology. Following the standard-methodology for the calculation of PUE,1 the energy consumption must be measured at the uninterruptible power system (UPS) or, if not existent, at the power distribution unit (PDU) or at another point specified by the data center (see Delegated Regulation, Annex II(1)(e); see also the appendix, Figure 1 as Categories 1-3.\n\nThe data centers must report to the EU database directly or via a national reporting scheme, if such a scheme is implemented by the Member State. From the reported data the Commission calculates the data center sustainability indicators which are made publicly available on an aggregate level. They include the power usage effectiveness (PUE), the water usage effectiveness (WUE)2, the energy reuse factor (ERF)3, and the renewable energy factor (REF)4.\n\n4.2 Energy Management Systems and Energy Audits", "original_types": ["text", "header"], "id": 1072}
{"type": "section", "content": "The EED also requires that Member States mandate companies with an average annual energy consumption of more than 10 TJ to conduct an energy audit at least every four years and those with a consumption of more than 85 TJ to implement an energy management system including regular energy audits (Art. 11 EED). This would also apply to operators of data centers. The Directive sets up certain minimum criteria for energy audits (Annex VI EED) and refers to the relevant international or European standards (Recital 80 EED). The legal minimum criteria, however, do not dictate how energy consumption should be measured.", "doc_id": "ebert2024", "page": 3, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "The EED also requires that Member States mandate companies with an average annual energy consumption of more than 10 TJ to conduct an energy audit at least every four years and those with a consumption of more than 85 TJ to implement an energy management system including regular energy audits (Art. 11 EED). This would also apply to operators of data centers. The Directive sets up certain minimum criteria for energy audits (Annex VI EED) and refers to the relevant international or European standards (Recital 80 EED). The legal minimum criteria, however, do not dictate how energy consumption should be measured.", "original_types": ["text"], "id": 1073}
{"type": "section", "content": "4.5 Discussion and Interim Conclusion on Data Center Regulation\n\nThe regulation of data centers in the context of AI’s environmental impact, particularly regarding energy and water consumption, presents both advantages and shortcomings. The increasing growth of AI-related activities, such as training and inference, places significant pressure on the environmental footprint of data centers. While the EU has implemented generic data center regulations, such as those outlined in the Energy Efficiency Directive and the Delegated Regulation, these rules also indirectly govern the environmental impact of AI by imposing reporting and data collection requirements. Notably, these regulations require the reporting of both energy and water consumption, a critical aspect given the rising concerns over resource use in AI applications.\n\nOne of the key strengths of the EU’s approach is the establishment of specific reporting obligations. Data center operators must collect and publicly report energy consumption, power utilization, water usage, waste heat utilization, and the use of renewable energy. These measures help create transparency and provide a foundation for future efficiency improvements. Additionally, the German implementation of the EED goes beyond mere reporting by setting specific targets for energy efficiency and renewable energy use in data centers, as well as requiring smaller data centers to comply. Germany’s approach might serve as a potential blueprint for broader EU regulation, particularly mandating data center operators to inform customers about their direct energy consumption, an essential factor for optimizing AI-related energy use.\n\nHowever, the regulations also present several shortcomings. While data collection and reporting obligations are useful, the absence of binding efficiency and renewable energy targets at the EU level is a major limitation. Although the Commission is expected to propose further legislative measures by 2025, the current lack of enforceable standards means that data centers could continue to consume vast amounts of energy and water without significant reductions in their environmental impact. Moreover, while Germany has introduced stricter targets, these do not extend to all Member States, potentially leading to a fragmented regulatory environment across the EU.", "doc_id": "ebert2024", "page": 4, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "4.5 Discussion and Interim Conclusion on Data Center Regulation\n\nThe regulation of data centers in the context of AI’s environmental impact, particularly regarding energy and water consumption, presents both advantages and shortcomings. The increasing growth of AI-related activities, such as training and inference, places significant pressure on the environmental footprint of data centers. While the EU has implemented generic data center regulations, such as those outlined in the Energy Efficiency Directive and the Delegated Regulation, these rules also indirectly govern the environmental impact of AI by imposing reporting and data collection requirements. Notably, these regulations require the reporting of both energy and water consumption, a critical aspect given the rising concerns over resource use in AI applications.\n\nOne of the key strengths of the EU’s approach is the establishment of specific reporting obligations. Data center operators must collect and publicly report energy consumption, power utilization, water usage, waste heat utilization, and the use of renewable energy. These measures help create transparency and provide a foundation for future efficiency improvements. Additionally, the German implementation of the EED goes beyond mere reporting by setting specific targets for energy efficiency and renewable energy use in data centers, as well as requiring smaller data centers to comply. Germany’s approach might serve as a potential blueprint for broader EU regulation, particularly mandating data center operators to inform customers about their direct energy consumption, an essential factor for optimizing AI-related energy use.\n\nHowever, the regulations also present several shortcomings. While data collection and reporting obligations are useful, the absence of binding efficiency and renewable energy targets at the EU level is a major limitation. Although the Commission is expected to propose further legislative measures by 2025, the current lack of enforceable standards means that data centers could continue to consume vast amounts of energy and water without significant reductions in their environmental impact. Moreover, while Germany has introduced stricter targets, these do not extend to all Member States, potentially leading to a fragmented regulatory environment across the EU.", "original_types": ["text", "header"], "id": 1074}
{"type": "section", "content": "5.3 Transparency\n\nThe first climate-related obligation in the AI Act concerns transparency and reporting, however not with respect to data centers, but concerning providers of high-risk and GPAI models [4]. Yet, the rules come with significant ambiguities and loopholes. In the following, we detail the six most important ones.", "doc_id": "ebert2024", "page": 5, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "5.3 Transparency\n\nThe first climate-related obligation in the AI Act concerns transparency and reporting, however not with respect to data centers, but concerning providers of high-risk and GPAI models [4]. Yet, the rules come with significant ambiguities and loopholes. In the following, we detail the six most important ones.", "original_types": ["text", "header"], "id": 1075}
{"type": "section", "content": "5.4 Environmental Risk Assessment and Mitigation\n\nIn tackling the climate effects of AI and ICT more generally, it is arguably crucial to move beyond mere transparency provisions towards more substantive goals and obligations. Indeed, the AI Act does contain some language to this effect. For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.\n\nCrucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5]. In Art. 1(1) and Recital 1, the purpose of the AI Act is defined as protecting health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection. However, in the doctrine of the Charter of Fundamental Rights of the European Union (the Charter), environmental protection (Art. 37 of the Charta) is merely an objective rule, not a fundamental right [60]. Democracy and the rule of law are not enshrined in a particular Article of the Charter but serve as guiding principles that permeate the Charter and all of the fundamental rights. Accordingly, they (only) find a mention in the preamble of the Charter.\n\nIn our view, the European legislator did not err on the doctrinal classification of democracy, the rule of law and environmental protection in the Charter by including these principles. While it is conceivable that a legislative error may have occurred in miscategorising the objective rule in Art. 37 of the Charta, that seems very unlikely for the basic principles merely expressed in the preamble.", "doc_id": "ebert2024", "page": 6, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "5.4 Environmental Risk Assessment and Mitigation\n\nIn tackling the climate effects of AI and ICT more generally, it is arguably crucial to move beyond mere transparency provisions towards more substantive goals and obligations. Indeed, the AI Act does contain some language to this effect. For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.\n\nCrucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5]. In Art. 1(1) and Recital 1, the purpose of the AI Act is defined as protecting health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection. However, in the doctrine of the Charter of Fundamental Rights of the European Union (the Charter), environmental protection (Art. 37 of the Charta) is merely an objective rule, not a fundamental right [60]. Democracy and the rule of law are not enshrined in a particular Article of the Charter but serve as guiding principles that permeate the Charter and all of the fundamental rights. Accordingly, they (only) find a mention in the preamble of the Charter.\n\nIn our view, the European legislator did not err on the doctrinal classification of democracy, the rule of law and environmental protection in the Charter by including these principles. While it is conceivable that a legislative error may have occurred in miscategorising the objective rule in Art. 37 of the Charta, that seems very unlikely for the basic principles merely expressed in the preamble.", "original_types": ["text", "header"], "id": 1076}
{"type": "section", "content": "Levels for Measuring and Estimating Energy Consumption\n\nThere are several levels within a data center based on which energy consumption may be measured or estimated [4]. These include (1) the data center level, (2) the cumulative server level, (3) the GPU-Level and other hardware within a server and (4) various other levels. In this section, we outline each level along with their benefits, drawbacks, and estimation methods for when measurement is unavailable.\n\nData Center Level\n\nOn the data center level, the power required to operate the entire data center is measured, including both the direct power consumption of computing equipment and the additional overhead for cooling and maintaining the data center. This approach provides the most extensive and complete figures since it represents the actual energy usage, but also assumes that a data center is exclusively utilized for the pre-training by the model provider. It encourages the selection of an efficient data center. Additionally, data centers have average PUE values of 1.58, so this overhead makes up a significant portion of the energy consumption. On the other hand, the power usage resulting solely from the model’s architecture, the quantity of training data, the efficiency of the implementation, and experimental settings is very important but is somewhat skewed by the efficiency of the data center.\n\nIf data-center level power consumption measurement is not available, using the PUE factor for estimation is deemed appropriate. To calculate total energy usage, the PUE factor is multiplied by the raw computational power consumption measured or estimated at the cumulative server or rack level (see below). This might be reasonable, if only parts of a data center are utilized and only measurements closer to the ICT equipment are available.\n\nCumulative Server Level\n\nA large-scale model is trained across many servers in a distributed manner. Each server includes GPUs responsible for the primary computation. To accurately monitor the power consumption over time, a local power distribution unit (PDU), capable of measuring the provided power, is attached to each server. Aggregating these measurements yields a highly precise figure of the total energy consumption attributable to the model’s computations. Instead of aggregating local PDUs, the usage of primary PDUs or uninterruptible power supply (UPS) systems already measuring at the rack level or even many racks is also suitable (See also Appendix, Figure 1), as long as the measurements precisely match fully the utilized hardware resources by the model\n\nGPU-Level and other hardware within a server", "doc_id": "ebert2024", "page": 7, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Levels for Measuring and Estimating Energy Consumption\n\nThere are several levels within a data center based on which energy consumption may be measured or estimated [4]. These include (1) the data center level, (2) the cumulative server level, (3) the GPU-Level and other hardware within a server and (4) various other levels. In this section, we outline each level along with their benefits, drawbacks, and estimation methods for when measurement is unavailable.\n\nData Center Level\n\nOn the data center level, the power required to operate the entire data center is measured, including both the direct power consumption of computing equipment and the additional overhead for cooling and maintaining the data center. This approach provides the most extensive and complete figures since it represents the actual energy usage, but also assumes that a data center is exclusively utilized for the pre-training by the model provider. It encourages the selection of an efficient data center. Additionally, data centers have average PUE values of 1.58, so this overhead makes up a significant portion of the energy consumption. On the other hand, the power usage resulting solely from the model’s architecture, the quantity of training data, the efficiency of the implementation, and experimental settings is very important but is somewhat skewed by the efficiency of the data center.\n\nIf data-center level power consumption measurement is not available, using the PUE factor for estimation is deemed appropriate. To calculate total energy usage, the PUE factor is multiplied by the raw computational power consumption measured or estimated at the cumulative server or rack level (see below). This might be reasonable, if only parts of a data center are utilized and only measurements closer to the ICT equipment are available.\n\nCumulative Server Level\n\nA large-scale model is trained across many servers in a distributed manner. Each server includes GPUs responsible for the primary computation. To accurately monitor the power consumption over time, a local power distribution unit (PDU), capable of measuring the provided power, is attached to each server. Aggregating these measurements yields a highly precise figure of the total energy consumption attributable to the model’s computations. Instead of aggregating local PDUs, the usage of primary PDUs or uninterruptible power supply (UPS) systems already measuring at the rack level or even many racks is also suitable (See also Appendix, Figure 1), as long as the measurements precisely match fully the utilized hardware resources by the model\n\nGPU-Level and other hardware within a server", "original_types": ["text", "header", "subheader"], "id": 1077}
{"type": "section", "content": "The measurement may be based on the energy usage of particular components as determined by on-chip sensors. Nvidia GPUs and certain CPUs already provide straightforward power consumption monitoring, therefore estimations are usually not necessary. However, despite GPU power consumption being a significant factor and its usage correlating with the total power usage, it substantially under-represents the actual energy consumption since it measures just a single component. CPU power usage is a relatively minor factor in consumption. Most other server components cannot be measured. We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.\n\nOther levels\n\nOther measurement levels, such as Workload, Cloud Instance, or Virtual Machine, involve high complexity and numerous assumptions, resulting in a lack of standardized measurement or estimation methods with considerable uncertainty. We advise against using these levels for power consumption tracking.\n\nInterim Conclusion on the Level of Measurement\n\nIn our analysis, we argue that energy consumption should be measured and", "doc_id": "ebert2024", "page": 7, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "The measurement may be based on the energy usage of particular components as determined by on-chip sensors. Nvidia GPUs and certain CPUs already provide straightforward power consumption monitoring, therefore estimations are usually not necessary. However, despite GPU power consumption being a significant factor and its usage correlating with the total power usage, it substantially under-represents the actual energy consumption since it measures just a single component. CPU power usage is a relatively minor factor in consumption. Most other server components cannot be measured. We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.\n\nOther levels\n\nOther measurement levels, such as Workload, Cloud Instance, or Virtual Machine, involve high complexity and numerous assumptions, resulting in a lack of standardized measurement or estimation methods with considerable uncertainty. We advise against using these levels for power consumption tracking.\n\nInterim Conclusion on the Level of Measurement\n\nIn our analysis, we argue that energy consumption should be measured and", "original_types": ["text", "subheader"], "id": 1078}
{"type": "section", "content": "Measurement or Estimation\n\nAlthough actual measurement is more onerous, it also yields more precise results for energy consumption reporting. Major model providers are likely to already measure power consumption as it is a primary cost factor and highly linked to computational power. Despite the availability of power consumption data, companies may be tempted to use estimated values to protect sensitive information. This practice should be restricted by legally prioritizing measurements over estimations in Annex XI Section 1(2)(e).\n\nThe same reasoning applies to smaller entities relying on cloud computing services for fine-tuning, for example such offered by Amazon Web Services or Microsoft Azure. Fine-tuning is crucial for employing foundational models in task-specific applications and tailoring them to specific datasets. The higher the expense of initial model trainings, the stronger the incentive to perform fine-tuning rather than retraining the model. Therefore, this is the type of adaptation that most businesses will focus on to effectively integrate large AI models into practical products.", "doc_id": "ebert2024", "page": 8, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Measurement or Estimation\n\nAlthough actual measurement is more onerous, it also yields more precise results for energy consumption reporting. Major model providers are likely to already measure power consumption as it is a primary cost factor and highly linked to computational power. Despite the availability of power consumption data, companies may be tempted to use estimated values to protect sensitive information. This practice should be restricted by legally prioritizing measurements over estimations in Annex XI Section 1(2)(e).\n\nThe same reasoning applies to smaller entities relying on cloud computing services for fine-tuning, for example such offered by Amazon Web Services or Microsoft Azure. Fine-tuning is crucial for employing foundational models in task-specific applications and tailoring them to specific datasets. The higher the expense of initial model trainings, the stronger the incentive to perform fine-tuning rather than retraining the model. Therefore, this is the type of adaptation that most businesses will focus on to effectively integrate large AI models into practical products.", "original_types": ["text", "header"], "id": 1079}
{"type": "section", "content": "Policy Proposals\n\nAlthough the AI Act attempts to address climate concerns through various reporting obligations, these measures largely lack consistency and clarity. We identify twelve policy recommendations that should be integrated into the evaluation report due in August 2028 (Article 111(6)), as well as any interpretive guidelines from the AI Office and other agencies, and in reviews and potential textual revisions prior to that date (see also [4]). These measures can be grouped in four categories: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.\n\nEnergy and Environmental Reporting Obligations\n\nThe current AI Act overlooks key environmental factors related to AI systems [56]. The following proposals aim to ensure comprehensive energy and environmental accountability for AI systems, both for development and inference phases.\n\n• {'type': 'text', 'content': 'Inclusion of Energy Consumption From Inferences: We propose an interpretation that would at least bring energy consumption for single inferences back into the scope (see also [4]). Adoption of this interpretation by courts, authorities and industry, however, is uncertain. Therefore, both single and overall inferences should be included as a reporting category in Annex XI and Annex XII (vis-à-vis authorities and downstream providers) through delegated acts from the Commission (Articles 53(5) and (6)) and future recommendations from the AI Office.'}\n• {'type': 'text', 'content': 'Indirect Emissions and Water Consumption Reporting: The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration [37]) and water consumption [49]. Reporting should include: Providers reporting water usage, and deployers reporting application-related emissions, allowing for estimates where precise measurements are impossible, particularly when dealing with future scenarios.'}\n• {'type': 'text', 'content': 'Energy Reporting at the Cumulative Server Level: Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable. These principles should guide the development of technical standards under Article 40 of the AI Act, as well as the potential implementation of the Global Digital Compact on a global scale.'}\n\nLegal and Regulatory Clarifications on AI Models and Providers\n\nAmbiguities in the AI Act regarding provider responsibilities for AI models need to be clarified to ensure that entities know when and what to comply with. The following proposals seek ensure that companies understand their obligations, that open-source models are on par with closed models, and that the regulatory framework sets effective incentives at the source (data centers) to mitigate the environmental impact of AI model development and deployment.", "doc_id": "ebert2024", "page": 9, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Policy Proposals\n\nAlthough the AI Act attempts to address climate concerns through various reporting obligations, these measures largely lack consistency and clarity. We identify twelve policy recommendations that should be integrated into the evaluation report due in August 2028 (Article 111(6)), as well as any interpretive guidelines from the AI Office and other agencies, and in reviews and potential textual revisions prior to that date (see also [4]). These measures can be grouped in four categories: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.\n\nEnergy and Environmental Reporting Obligations\n\nThe current AI Act overlooks key environmental factors related to AI systems [56]. The following proposals aim to ensure comprehensive energy and environmental accountability for AI systems, both for development and inference phases.\n\n• {'type': 'text', 'content': 'Inclusion of Energy Consumption From Inferences: We propose an interpretation that would at least bring energy consumption for single inferences back into the scope (see also [4]). Adoption of this interpretation by courts, authorities and industry, however, is uncertain. Therefore, both single and overall inferences should be included as a reporting category in Annex XI and Annex XII (vis-à-vis authorities and downstream providers) through delegated acts from the Commission (Articles 53(5) and (6)) and future recommendations from the AI Office.'}\n• {'type': 'text', 'content': 'Indirect Emissions and Water Consumption Reporting: The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration [37]) and water consumption [49]. Reporting should include: Providers reporting water usage, and deployers reporting application-related emissions, allowing for estimates where precise measurements are impossible, particularly when dealing with future scenarios.'}\n• {'type': 'text', 'content': 'Energy Reporting at the Cumulative Server Level: Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable. These principles should guide the development of technical standards under Article 40 of the AI Act, as well as the potential implementation of the Global Digital Compact on a global scale.'}\n\nLegal and Regulatory Clarifications on AI Models and Providers\n\nAmbiguities in the AI Act regarding provider responsibilities for AI models need to be clarified to ensure that entities know when and what to comply with. The following proposals seek ensure that companies understand their obligations, that open-source models are on par with closed models, and that the regulatory framework sets effective incentives at the source (data centers) to mitigate the environmental impact of AI model development and deployment.", "original_types": ["text", "header", "list"], "id": 1080}
{"type": "section", "content": "Clarifying Provider Status for Model Modifications\n\nThe current definition of provider status needs to focus on substantial modifications to AI models, specifically those that involve changing the model’s weights. Reporting obligations and the change-of-provider threshold should be tied to computational costs incurred during significant modifications, with a minimum computational cost threshold ensuring that only energy-intensive changes trigger additional reporting [4].\n\nElimination of the Open-Source Exemption\n\nThe open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.\n\nEnergy Efficiency and Renewable Energy Targets for Data Centers\n\nThe EED and Delegated Regulation should be amended in three regards, following the German Energy Efficiency Act: they should set binding efficiency targets (PUE) and renewable energy targets for data centers; and include an information obligation for data center operators and cloud service providers vis-à-vis their customers regarding the attributable energy consumption. These measures should be included in the 2025 evaluation report. This would both increase transparency and facilitate more sustainable decisions by market participants.\n\nTransparency and Accountability Mechanisms\n\nTo promote public trust and accountability in AI’s environmental impact, the following measures are recommended:\n\n• {'type': 'text', 'content': 'Public Access to Climate-Related Disclosures: All climate-related disclosures, including energy consumption, should be made accessible to the general public [4]. If only aggregate data are shared, trade secrets can be protected while allowing for public scrutiny by analysts, academics, and NGOs. Public transparency would drive market pressure, reputational incentives, and public accountability; it would, arguably, encourage companies to reduce their environmental impact.'}\n• {'type': 'text', 'content': 'Energy Reporting for High-Risk AI (HRAI): For High-Risk AI (HRAI) systems, the Act should require the disclosure of energy consumption rather than computational resources to eliminate inaccuracies and enhance comparability.'}\n• {'type': 'text', 'content': 'Environmental Risk Assessments: Providers should be required to include environmental risks in their risk assessments. The language in Art. 1(1) and Recital 1, as well as Art. 9 and 55 AI Act should be clarified to reflect this. This will ensure that environmental impacts are systematically evaluated alongside other risks during AI system development and deployment.'}\n\nBeyond Transparency\n\nIn addition to the existing proposals, more far-reaching measures that go beyond reporting and transparency may need to be considered in the future, particularly as AI’s energy demands grow.", "doc_id": "ebert2024", "page": 9, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Clarifying Provider Status for Model Modifications\n\nThe current definition of provider status needs to focus on substantial modifications to AI models, specifically those that involve changing the model’s weights. Reporting obligations and the change-of-provider threshold should be tied to computational costs incurred during significant modifications, with a minimum computational cost threshold ensuring that only energy-intensive changes trigger additional reporting [4].\n\nElimination of the Open-Source Exemption\n\nThe open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.\n\nEnergy Efficiency and Renewable Energy Targets for Data Centers\n\nThe EED and Delegated Regulation should be amended in three regards, following the German Energy Efficiency Act: they should set binding efficiency targets (PUE) and renewable energy targets for data centers; and include an information obligation for data center operators and cloud service providers vis-à-vis their customers regarding the attributable energy consumption. These measures should be included in the 2025 evaluation report. This would both increase transparency and facilitate more sustainable decisions by market participants.\n\nTransparency and Accountability Mechanisms\n\nTo promote public trust and accountability in AI’s environmental impact, the following measures are recommended:\n\n• {'type': 'text', 'content': 'Public Access to Climate-Related Disclosures: All climate-related disclosures, including energy consumption, should be made accessible to the general public [4]. If only aggregate data are shared, trade secrets can be protected while allowing for public scrutiny by analysts, academics, and NGOs. Public transparency would drive market pressure, reputational incentives, and public accountability; it would, arguably, encourage companies to reduce their environmental impact.'}\n• {'type': 'text', 'content': 'Energy Reporting for High-Risk AI (HRAI): For High-Risk AI (HRAI) systems, the Act should require the disclosure of energy consumption rather than computational resources to eliminate inaccuracies and enhance comparability.'}\n• {'type': 'text', 'content': 'Environmental Risk Assessments: Providers should be required to include environmental risks in their risk assessments. The language in Art. 1(1) and Recital 1, as well as Art. 9 and 55 AI Act should be clarified to reflect this. This will ensure that environmental impacts are systematically evaluated alongside other risks during AI system development and deployment.'}\n\nBeyond Transparency\n\nIn addition to the existing proposals, more far-reaching measures that go beyond reporting and transparency may need to be considered in the future, particularly as AI’s energy demands grow.", "original_types": ["text", "header", "list"], "id": 1081}
{"type": "section", "content": "8 Objections and Solutions\n\nImplementing the proposed policy recommendations presents both practical and political challenges, particularly given the EU’s broader regulatory landscape and the evolving international context. This section outlines four key objections to our proposals and sketches solutions. First, a central priority of the new European Commission is the simplification of the digital acquis (‘fitness check’) [59]. There is concern that adding further reporting requirements and regulatory obligations could create administrative burdens that counteract this goal. However, several of our proposals, particularly standardized energy reporting metrics and clearer environmental risk assessments, align with the EU’s push for regulatory clarity and streamlining. Rather than adding complexity, these measures would, arguably, make compliance more predictable and feasible.\n\n9 Conclusion\n\nAI systems, many of them containing energy-intensive generative AI components, are increasingly integrated into economic and societal processes. Importantly, as AI intersects with robotics, writing code and steering devices, it is poised to merge decisively into industrial processes, proliferating its deployment, but also its climate effects. Currently, this intersection between the AI and the green transition sits at a regulatory blind spot, inadequately addressed in current regulation. In this paper, we take a closer look at sustainability-related data center regulation and the sustainability provisions in the AI Act. While they present a good first step, they too often lack ambition, clarity and consistency, or present significant challenges in implementation. To counter these shortcomings, we provide interpretations of the AI Act in line with its purpose of environmental protection, provide guidance on operationalizing the requirements, and make twelve concrete proposals, grouped into four areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability; and Future Far-Reaching Measures beyond Transparency.", "doc_id": "ebert2024", "page": 10, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "8 Objections and Solutions\n\nImplementing the proposed policy recommendations presents both practical and political challenges, particularly given the EU’s broader regulatory landscape and the evolving international context. This section outlines four key objections to our proposals and sketches solutions. First, a central priority of the new European Commission is the simplification of the digital acquis (‘fitness check’) [59]. There is concern that adding further reporting requirements and regulatory obligations could create administrative burdens that counteract this goal. However, several of our proposals, particularly standardized energy reporting metrics and clearer environmental risk assessments, align with the EU’s push for regulatory clarity and streamlining. Rather than adding complexity, these measures would, arguably, make compliance more predictable and feasible.\n\n9 Conclusion\n\nAI systems, many of them containing energy-intensive generative AI components, are increasingly integrated into economic and societal processes. Importantly, as AI intersects with robotics, writing code and steering devices, it is poised to merge decisively into industrial processes, proliferating its deployment, but also its climate effects. Currently, this intersection between the AI and the green transition sits at a regulatory blind spot, inadequately addressed in current regulation. In this paper, we take a closer look at sustainability-related data center regulation and the sustainability provisions in the AI Act. While they present a good first step, they too often lack ambition, clarity and consistency, or present significant challenges in implementation. To counter these shortcomings, we provide interpretations of the AI Act in line with its purpose of environmental protection, provide guidance on operationalizing the requirements, and make twelve concrete proposals, grouped into four areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability; and Future Far-Reaching Measures beyond Transparency.", "original_types": ["text", "header"], "id": 1082}
{"type": "section", "content": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference'17, July 2017, Washington, DC, USA\n\nLearning.\n\nJordan Aljbour, Tom Wilson, and Poorvi Patel. 2024. Powering Intelligence: Analyzing Artificial Intelligence and Data Center Energy Consumption. EPRI White Paper no. 3002028905 (2024).\n\nDario Amodei. 2025. On DeepSeek and Export Controls. https://darioamodei.com/on-deepseek-and-export-controls\n\nAnonymous. 2024. Anonymized Reference. (2024). This reference has been anonymized for blind review.\n\nAnonymous. 2024. Anonymized Reference. This reference has been anonymized for blind review.\n\nPedram Bakhtiarifard, Pınar Tözün, Christian Igel, and Raghavendra Selvan. 2025. Climate And Resource Awareness is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race). arXiv preprint arXiv:2502.20016 (2025).\n\nAnton Beloglazov, Rajkumar Buyya, Young Choon Lee, et al. 2011. A Taxonomy and Survey of Energy-Efficient Data Centers and Cloud Computing Systems. Advances in Computers 82 (2011), 47–111.\n\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610–623.\n\nBenedetta Brevini. 2021. Is AI Good for the Planet? Polity Press, Cambridge, UK.\n\nBenedetta Brevini. 2024. An eco-political economy of AI to understand the complexities of its environmental costs. CEPR VoxEU Column (June 2024). https://cepr.org/voxeu/columns/eco-political-economy-ai-understand-complexities-its-environmental-costs\n\nIlaria Buri and Rachel Griffin. 2024. Platform Regulation in Times of Environmental Collapse: The Digital Services Act and the Climate Emergency. Available at SSRN 4867333 (2024).\n\nBusiness Network Editors. 2024. Efforts Toward Decarbonizing Data Centers Accelerate. https://businessnetwork.jp/article/17962/. https://businessnetwork.jp/article/17962/ Accessed April 22, 2025.\n\nLin Chen, Zhonghao Chen, Yubing Zhang, Yunfei Liu, Ahmed I Osman, Mohamed Farghali, Jianmin Hua, Ahmed Al-Fatesh, Ikko Ihara, David W Rooney, et al. 2023. Artificial intelligence-based solutions for climate change: a review. Environmental Chemistry Letters 21, 5 (2023), 2525–2557.\n\nAndrew A Chien. 2021. Good, better, best: How sustainable should computing be? Commun. ACM 64, 12 (2021), 6–7.\n\nMark Coeckelbergh. 2021. AI for climate: freedom, justice, and other ethical and political challenges. AI and Ethics 1, 1 (2021), 67–72.\n\nJessica Commins and Kristina Irion. 2025. Towards Planet Proof Computing: Law and Policy of Data Centre Sustainability in the European Union. Technology and Regulation 2025 (2025). https://techreg.org/article/view/2025-1-Commins-Irion\n\nJosh Cowls, Andreas Tsamados, Mariarosaria Taddeo, and Luciano Floridi. 2023. The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.", "doc_id": "ebert2024", "page": 11, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference'17, July 2017, Washington, DC, USA\n\nLearning.\n\nJordan Aljbour, Tom Wilson, and Poorvi Patel. 2024. Powering Intelligence: Analyzing Artificial Intelligence and Data Center Energy Consumption. EPRI White Paper no. 3002028905 (2024).\n\nDario Amodei. 2025. On DeepSeek and Export Controls. https://darioamodei.com/on-deepseek-and-export-controls\n\nAnonymous. 2024. Anonymized Reference. (2024). This reference has been anonymized for blind review.\n\nAnonymous. 2024. Anonymized Reference. This reference has been anonymized for blind review.\n\nPedram Bakhtiarifard, Pınar Tözün, Christian Igel, and Raghavendra Selvan. 2025. Climate And Resource Awareness is Imperative to Achieving Sustainable AI (and Preventing a Global AI Arms Race). arXiv preprint arXiv:2502.20016 (2025).\n\nAnton Beloglazov, Rajkumar Buyya, Young Choon Lee, et al. 2011. A Taxonomy and Survey of Energy-Efficient Data Centers and Cloud Computing Systems. Advances in Computers 82 (2011), 47–111.\n\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610–623.\n\nBenedetta Brevini. 2021. Is AI Good for the Planet? Polity Press, Cambridge, UK.\n\nBenedetta Brevini. 2024. An eco-political economy of AI to understand the complexities of its environmental costs. CEPR VoxEU Column (June 2024). https://cepr.org/voxeu/columns/eco-political-economy-ai-understand-complexities-its-environmental-costs\n\nIlaria Buri and Rachel Griffin. 2024. Platform Regulation in Times of Environmental Collapse: The Digital Services Act and the Climate Emergency. Available at SSRN 4867333 (2024).\n\nBusiness Network Editors. 2024. Efforts Toward Decarbonizing Data Centers Accelerate. https://businessnetwork.jp/article/17962/. https://businessnetwork.jp/article/17962/ Accessed April 22, 2025.\n\nLin Chen, Zhonghao Chen, Yubing Zhang, Yunfei Liu, Ahmed I Osman, Mohamed Farghali, Jianmin Hua, Ahmed Al-Fatesh, Ikko Ihara, David W Rooney, et al. 2023. Artificial intelligence-based solutions for climate change: a review. Environmental Chemistry Letters 21, 5 (2023), 2525–2557.\n\nAndrew A Chien. 2021. Good, better, best: How sustainable should computing be? Commun. ACM 64, 12 (2021), 6–7.\n\nMark Coeckelbergh. 2021. AI for climate: freedom, justice, and other ethical and political challenges. AI and Ethics 1, 1 (2021), 67–72.\n\nJessica Commins and Kristina Irion. 2025. Towards Planet Proof Computing: Law and Policy of Data Centre Sustainability in the European Union. Technology and Regulation 2025 (2025). https://techreg.org/article/view/2025-1-Commins-Irion\n\nJosh Cowls, Andreas Tsamados, Mariarosaria Taddeo, and Luciano Floridi. 2023. The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.", "original_types": ["text"], "id": 1083}
{"type": "section", "content": "Kate Crawford. 2021. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press, New Haven, CT. https://yalebooks.yale.edu/book/9780300209570/atlas-of-ai\n\nMiyuru Dayarathna, Yonggang Wen, and Rui Fan. 2015. Data center energy consumption modeling: A survey. IEEE Communications surveys & tutorials 18, 1 (2015), 732–794.\n\nAlex De Vries. 2023. The growing energy footprint of artificial intelligence. Joule 7, 10 (2023), 2191–2194.\n\nDepartment of Finance, Australian Government. 2023. Net Zero in Government Operations Strategy. Technical Report. Department of Finance. https://www.finance.gov.au/sites/default/files/2023-11/Net_Zero_Government_Operations_Strategy.pdf\n\nDepartment of General Services. 2014. Management Memo MM 14-09: Energy Efficiency in Data Centers and Server Rooms. https://www.dgs.ca.gov/Resources/ManagementMemos. Accessed April 22, 2025.\n\nJesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency. 1877–1894.\n\nHamid R. Ekbia and Bonnie A. Nardi. 2017. Heteromation, and Other Stories of Computing and Capitalism. MIT Press, Cambridge, MA. https://mitpress.mit.edu/books/heteromation-and-other-stories-computing-and-capitalism\n\nEuropean Commission. 2025. AI Continent Action Plan. https://digital-strategy.ec.europa.eu/en/library/ai-continent-action-plan. COM(2025) 165 final.\n\nEuropean Commission Joint Research Centre. 2023. EU Code of Conduct for Data Centres: Towards More Innovative, Sustainable, and Secure Data Centre Facilities. https://joint-research-centre.ec.europa.eu/jrc-news-and-updates/eu-code-conduct-data-centres-towards-more-innovative-sustainable-and-secure-data-centre-facilities-2023-09-05_en Accessed: 2025-03-18.\n\nLuciano Floridi. 2021. The European legislation on AI: a brief analysis of its philosophical approach. Philosophy & Technology 34, 2 (2021), 215–222.\n\nMozilla Foundation. 2024. Open-Source AI for Environmental Justice. https://foundation.mozilla.org/en/blog/open-source-AI-for-environmental-justice/\n\nCharlotte Freitag, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S Blair, and Adrian Friday. 2021. The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations. Patterns 2, 9 (2021).\n\nGianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell, Scott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of United States Data Centers in the Artificial Intelligence Era. arXiv preprint arXiv:2411.09786 (2024).\n\nBaşak Güler and Aylin Yener. 2021. A framework for sustainable federated learning. In 2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt). IEEE, 1–8.", "doc_id": "ebert2024", "page": 11, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Kate Crawford. 2021. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press, New Haven, CT. https://yalebooks.yale.edu/book/9780300209570/atlas-of-ai\n\nMiyuru Dayarathna, Yonggang Wen, and Rui Fan. 2015. Data center energy consumption modeling: A survey. IEEE Communications surveys & tutorials 18, 1 (2015), 732–794.\n\nAlex De Vries. 2023. The growing energy footprint of artificial intelligence. Joule 7, 10 (2023), 2191–2194.\n\nDepartment of Finance, Australian Government. 2023. Net Zero in Government Operations Strategy. Technical Report. Department of Finance. https://www.finance.gov.au/sites/default/files/2023-11/Net_Zero_Government_Operations_Strategy.pdf\n\nDepartment of General Services. 2014. Management Memo MM 14-09: Energy Efficiency in Data Centers and Server Rooms. https://www.dgs.ca.gov/Resources/ManagementMemos. Accessed April 22, 2025.\n\nJesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency. 1877–1894.\n\nHamid R. Ekbia and Bonnie A. Nardi. 2017. Heteromation, and Other Stories of Computing and Capitalism. MIT Press, Cambridge, MA. https://mitpress.mit.edu/books/heteromation-and-other-stories-computing-and-capitalism\n\nEuropean Commission. 2025. AI Continent Action Plan. https://digital-strategy.ec.europa.eu/en/library/ai-continent-action-plan. COM(2025) 165 final.\n\nEuropean Commission Joint Research Centre. 2023. EU Code of Conduct for Data Centres: Towards More Innovative, Sustainable, and Secure Data Centre Facilities. https://joint-research-centre.ec.europa.eu/jrc-news-and-updates/eu-code-conduct-data-centres-towards-more-innovative-sustainable-and-secure-data-centre-facilities-2023-09-05_en Accessed: 2025-03-18.\n\nLuciano Floridi. 2021. The European legislation on AI: a brief analysis of its philosophical approach. Philosophy & Technology 34, 2 (2021), 215–222.\n\nMozilla Foundation. 2024. Open-Source AI for Environmental Justice. https://foundation.mozilla.org/en/blog/open-source-AI-for-environmental-justice/\n\nCharlotte Freitag, Mike Berners-Lee, Kelly Widdicks, Bran Knowles, Gordon S Blair, and Adrian Friday. 2021. The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations. Patterns 2, 9 (2021).\n\nGianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell, Scott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of United States Data Centers in the Artificial Intelligence Era. arXiv preprint arXiv:2411.09786 (2024).\n\nBaşak Güler and Aylin Yener. 2021. A framework for sustainable federated learning. In 2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt). IEEE, 1–8.", "original_types": ["text"], "id": 1084}
{"type": "section", "content": "Philipp Hacker. 2024. The AI Act between Digital and Sectoral Regulations. Bertelsmann Stiftung, Gütersloh, Germany. doi:10.11586/2024188\n\nPhilipp Hacker. 2024. Sustainable AI regulation. Common Market Law Review 61 (2024), 345–386.\n\nAbdullah Hamdan, Kenechi I Ibekwe, Victor I Ilojiana, Samuel Sonko, Emmanuel A Etukudoh, et al. 2024. AI in renewable energy: A review of predictive maintenance and energy optimization. International Journal of Science and Research Archive 11, 1 (2024), 718–729.\n\nKaren Hao. 2024. Microsoft’s Hypocrisy on AI. https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/ Accessed: 2025-03-18.\n\nNatali Helberger and Nicholas Diakopoulos. 2023. ChatGPT and the AI Act. Internet Policy Review 12, 1 (2023). doi:10.14763/2023.1.1682\n\nLynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n\nMaximilian Kasy. 2024. The Political Economy of AI: Towards Democratic Control of the Means of Prediction. IZA Discussion Paper No. 16948. Institute of Labor Economics (IZA). https://www.econstor.eu/bitstream/10419/295971/1/dp16948.pdf\n\nBecky Kazansky, Malavika Karak, Thiago Perosa, Quartz Tsui, Solana Baker, and The Engine Room. 2022. At the Confluence of Digital Rights and Climate & Environmental Justice: A Landscape Review. Technical Report. The Engine Room. https://engn.it/climatejusticedigitalrights\n\nBenjamin Kelly. 2022. Ethical AI and the Environment. The iJournal: Student Journal of the Faculty of Information 7, 2 (2022), 5–11.\n\nTugrul Keskin and Ryan David Kiggins (Eds.). 2021. Towards an International Political Economy of Artificial Intelligence. Palgrave Macmillan.\n\nTamara Kneese and Meg Young. 2024. Carbon emissions in the tailpipe of Generative AI. Harvard Data Science Review 5 (2024).\n\nBran Knowles. 2021. ACM TechBrief: Computing and climate change.\n\nBran Knowles, Kelly Widdicks, Gordon Blair, Mike Berners-Lee, and Adrian Friday. 2022. Our house is on fire: The climate emergency and computing’s responsibility. Commun. ACM 65, 6 (2022), 38–40.\n\nJonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls assessing the Internet’s energy and carbon impacts. Joule 5, 7 (2021), 1625–1628.\n\nNeeta Kumari and Soumya Pandey. 2023. Application of artificial intelligence in environmental sustainability and climate change. In Visualization techniques for climate change with machine learning and artificial intelligence. Elsevier, 293–316.", "doc_id": "ebert2024", "page": 11, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Philipp Hacker. 2024. The AI Act between Digital and Sectoral Regulations. Bertelsmann Stiftung, Gütersloh, Germany. doi:10.11586/2024188\n\nPhilipp Hacker. 2024. Sustainable AI regulation. Common Market Law Review 61 (2024), 345–386.\n\nAbdullah Hamdan, Kenechi I Ibekwe, Victor I Ilojiana, Samuel Sonko, Emmanuel A Etukudoh, et al. 2024. AI in renewable energy: A review of predictive maintenance and energy optimization. International Journal of Science and Research Archive 11, 1 (2024), 718–729.\n\nKaren Hao. 2024. Microsoft’s Hypocrisy on AI. https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/ Accessed: 2025-03-18.\n\nNatali Helberger and Nicholas Diakopoulos. 2023. ChatGPT and the AI Act. Internet Policy Review 12, 1 (2023). doi:10.14763/2023.1.1682\n\nLynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n\nMaximilian Kasy. 2024. The Political Economy of AI: Towards Democratic Control of the Means of Prediction. IZA Discussion Paper No. 16948. Institute of Labor Economics (IZA). https://www.econstor.eu/bitstream/10419/295971/1/dp16948.pdf\n\nBecky Kazansky, Malavika Karak, Thiago Perosa, Quartz Tsui, Solana Baker, and The Engine Room. 2022. At the Confluence of Digital Rights and Climate & Environmental Justice: A Landscape Review. Technical Report. The Engine Room. https://engn.it/climatejusticedigitalrights\n\nBenjamin Kelly. 2022. Ethical AI and the Environment. The iJournal: Student Journal of the Faculty of Information 7, 2 (2022), 5–11.\n\nTugrul Keskin and Ryan David Kiggins (Eds.). 2021. Towards an International Political Economy of Artificial Intelligence. Palgrave Macmillan.\n\nTamara Kneese and Meg Young. 2024. Carbon emissions in the tailpipe of Generative AI. Harvard Data Science Review 5 (2024).\n\nBran Knowles. 2021. ACM TechBrief: Computing and climate change.\n\nBran Knowles, Kelly Widdicks, Gordon Blair, Mike Berners-Lee, and Adrian Friday. 2022. Our house is on fire: The climate emergency and computing’s responsibility. Commun. ACM 65, 6 (2022), 38–40.\n\nJonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls assessing the Internet’s energy and carbon impacts. Joule 5, 7 (2021), 1625–1628.\n\nNeeta Kumari and Soumya Pandey. 2023. Application of artificial intelligence in environmental sustainability and climate change. In Visualization techniques for climate change with machine learning and artificial intelligence. Elsevier, 293–316.", "original_types": ["text"], "id": 1085}
{"type": "section", "content": "Guozhu Li, Zixuan Sun, Qingqin Wang, Shuai Wang, Kailiang Huang, Naini Zhao, Yanqiang Di, Xudong Zhao, and Zishang Zhu. 2023. China’s green data center development:Policies and carbon reduction technology path. Environmental Research 231 (2023), 116248. doi:10.1016/j.envres.2023.116248\n\nPengfei Li\n\nPengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. 2023. Towards Environmentally Equitable AI via Geographical Load Balancing. arXiv preprint arXiv:2307.05494 (2023). https://arxiv.org/abs/2307.05494\n\nAlexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023).\n\nAlexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025).\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research 24, 253 (2023), 1–15.\n\nSasha Luccioni, Boris Gamazaychikov, Sara Hooker, Régis Pierrard, Emma Strubell, Yacine Jernite, and Carole-Jean Wu. 2024. Light bulbs have energy ratings—so why can’t AI chatbots? Nature 632, 8026 (2024), 736–738.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. 2024. Power hungry processing: Watts driving the cost of AI deployment?. In The 2024 ACM Conference on", "doc_id": "ebert2024", "page": 11, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Guozhu Li, Zixuan Sun, Qingqin Wang, Shuai Wang, Kailiang Huang, Naini Zhao, Yanqiang Di, Xudong Zhao, and Zishang Zhu. 2023. China’s green data center development:Policies and carbon reduction technology path. Environmental Research 231 (2023), 116248. doi:10.1016/j.envres.2023.116248\n\nPengfei Li\n\nPengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. 2023. Towards Environmentally Equitable AI via Geographical Load Balancing. arXiv preprint arXiv:2307.05494 (2023). https://arxiv.org/abs/2307.05494\n\nAlexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023).\n\nAlexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025).\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research 24, 253 (2023), 1–15.\n\nSasha Luccioni, Boris Gamazaychikov, Sara Hooker, Régis Pierrard, Emma Strubell, Yacine Jernite, and Carole-Jean Wu. 2024. Light bulbs have energy ratings—so why can’t AI chatbots? Nature 632, 8026 (2024), 736–738.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. 2024. Power hungry processing: Watts driving the cost of AI deployment?. In The 2024 ACM Conference on", "original_types": ["text"], "id": 1086}
{"type": "section", "content": "Conference'17, July 2017, Washington, DC, USA\n\nKai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker\n\nFairness, Accountability, and Transparency. 85–99.\n\n[56] Sasha Luccioni, Bruna Trevelin, and Margaret Mitchell. 2024. The Environmental Impacts of AI – Primer. Hugging Face Blog (2024). https://huggingface.co/blog/sasha/ai-environment-primer Accessed: 2024-10-08.\n\n[57] Kasper Groes Albin Ludvigsen. 2022. How to Estimate and Reduce the Carbon Footprint of Machine Learning Models. https://towardsdatascience.com/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880/ Accessed: 2025-03-18.\n\n[58] Stuart Mills. 2024. Algorithms, Bytes, and Chips: The Emerging Political Economy of Foundation Models. SSRN Electronic Journal (May 2024). doi:10.2139/ssrn.4834417\n\n[59] MLex. 2025. EU AI Act to be reviewed but no major changes expected, AI Office boss says. https://www.mlex.com/mlex/articles/2300091/eu-ai-act-to-be-reviewed-but-no-major-changes-expected-ai-office-boss-says Accessed: 2025-03-17.\n\n[60] Elisa Morgera and Gracia Marín Durán. 2022. Article 37–Environmental Protection. In The EU Charter of Fundamental Rights (2 ed.), Tamara Hervey et al. (Eds.). Hart/Nomos, Baden-Baden.\n\n[61] Safiya Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, New York, NY. https://nyupress.org/9781479837243/algorithms-of-oppression/\n\n[62] OECD. 2022. Measuring the Environmental Impacts of Artificial Intelligence Compute and Applications. https://www.oecd.org/content/dam/oecd/en/publications/reports/2022/11/measuring-the-environmental-impacts-of-artificial-intelligence-compute-and-applications_3ddded5/7babf571-en.pdf Accessed: 2025-03-18.\n\n[63] Ugo Pagallo, Jacopo Ciani Sciolla, and Massimo Durante. 2022. The environmental challenges of AI in EU law: lessons learned from the Artificial Intelligence Act (AIA) with its drawbacks. Transforming Government: People, Process and Policy 16, 3 (2022), 359–376.\n\n[64] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[65] Billy Perrigo. 2023. Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. Time (2023). https://time.com/6247678/openai-chatgpt-kenya-workers/\n\n[66] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021).\n\n[67] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits. arXiv preprint arXiv:2305.05733 (2023). https://arxiv.org/abs/2305.05733", "doc_id": "ebert2024", "page": 12, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Conference'17, July 2017, Washington, DC, USA\n\nKai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker\n\nFairness, Accountability, and Transparency. 85–99.\n\n[56] Sasha Luccioni, Bruna Trevelin, and Margaret Mitchell. 2024. The Environmental Impacts of AI – Primer. Hugging Face Blog (2024). https://huggingface.co/blog/sasha/ai-environment-primer Accessed: 2024-10-08.\n\n[57] Kasper Groes Albin Ludvigsen. 2022. How to Estimate and Reduce the Carbon Footprint of Machine Learning Models. https://towardsdatascience.com/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880/ Accessed: 2025-03-18.\n\n[58] Stuart Mills. 2024. Algorithms, Bytes, and Chips: The Emerging Political Economy of Foundation Models. SSRN Electronic Journal (May 2024). doi:10.2139/ssrn.4834417\n\n[59] MLex. 2025. EU AI Act to be reviewed but no major changes expected, AI Office boss says. https://www.mlex.com/mlex/articles/2300091/eu-ai-act-to-be-reviewed-but-no-major-changes-expected-ai-office-boss-says Accessed: 2025-03-17.\n\n[60] Elisa Morgera and Gracia Marín Durán. 2022. Article 37–Environmental Protection. In The EU Charter of Fundamental Rights (2 ed.), Tamara Hervey et al. (Eds.). Hart/Nomos, Baden-Baden.\n\n[61] Safiya Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, New York, NY. https://nyupress.org/9781479837243/algorithms-of-oppression/\n\n[62] OECD. 2022. Measuring the Environmental Impacts of Artificial Intelligence Compute and Applications. https://www.oecd.org/content/dam/oecd/en/publications/reports/2022/11/measuring-the-environmental-impacts-of-artificial-intelligence-compute-and-applications_3ddded5/7babf571-en.pdf Accessed: 2025-03-18.\n\n[63] Ugo Pagallo, Jacopo Ciani Sciolla, and Massimo Durante. 2022. The environmental challenges of AI in EU law: lessons learned from the Artificial Intelligence Act (AIA) with its drawbacks. Transforming Government: People, Process and Policy 16, 3 (2022), 359–376.\n\n[64] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[65] Billy Perrigo. 2023. Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. Time (2023). https://time.com/6247678/openai-chatgpt-kenya-workers/\n\n[66] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021).\n\n[67] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits. arXiv preprint arXiv:2305.05733 (2023). https://arxiv.org/abs/2305.05733", "original_types": ["text"], "id": 1087}
{"type": "section", "content": "[68] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\n[69] Tahereh Saheb, Mohamad Dehghani, and Tayebeh Saheb. 2022. Artificial intelligence for sustainable energy: A contextual topic modeling and content analysis. Sustainable Computing: Informatics and Systems 35 (2022), 100699.\n\n[70] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. Commun. ACM 63, 12 (2020), 54–63.\n\n[71] Singapore Infocomm Media Development Authority. 2024. Green Data Centre Roadmap. https://www.imda.gov.sg/-/media/imda/files/how-we-can-help/green-dc-roadmap/green-dc-roadmap.pdf. https://www.imda.gov.sg/-/media/imda/files/how-we-can-help/green-dc-roadmap/green-dc-roadmap.pdf Accessed April 22, 2025.\n\n[72] Marisa Sotolongo. 2023. Defining environmental justice communities: Evaluating digital infrastructure in Southeastern states for Justice40 benefits allocation. Applied Geography 158 (2023), 103057.\n\n[73] State of California. 2012. Green Building Action Plan – For Implementation of Executive Order B-18-12. https://archive.gov.ca.gov/archive/gov39/wp-content/uploads/2017/09/Green_Building_Action_Plan_B.18.12.pdf Accessed: 2025-04-22.\n\n[74] Statista. 2025. Average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide. https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.\n\n[75] Amy L Stein. 2020. Artificial intelligence and climate change. Yale J. on Reg. 37 (2020), 890.\n\n[76] Mariarosaria Taddeo, Andreas Tsamados, Josh Cowls, and Luciano Floridi. 2021. Artificial intelligence and the climate emergency: opportunities, challenges, and recommendations. One Earth 4, 6 (2021), 776–779.\n\n[77] The Green Grid. 2023. IT & Power Efficiency Survey Results (for EUC). https://www.thegreengrid.org/en/resources/library-and-tools/572-IT-%26-Power-Efficiency-Survey-Results-%28for-EUC%29 Accessed: 2025-03-18.\n\n[78] U.S. Congress. 2024. S.3732 - Artificial Intelligence Environmental Impacts Act of 2024. https://www.congress.gov/bill/118th-congress/senate-bill/3732/ Accessed: 2025-03-18.", "doc_id": "ebert2024", "page": 12, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "[68] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\n[69] Tahereh Saheb, Mohamad Dehghani, and Tayebeh Saheb. 2022. Artificial intelligence for sustainable energy: A contextual topic modeling and content analysis. Sustainable Computing: Informatics and Systems 35 (2022), 100699.\n\n[70] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. Commun. ACM 63, 12 (2020), 54–63.\n\n[71] Singapore Infocomm Media Development Authority. 2024. Green Data Centre Roadmap. https://www.imda.gov.sg/-/media/imda/files/how-we-can-help/green-dc-roadmap/green-dc-roadmap.pdf. https://www.imda.gov.sg/-/media/imda/files/how-we-can-help/green-dc-roadmap/green-dc-roadmap.pdf Accessed April 22, 2025.\n\n[72] Marisa Sotolongo. 2023. Defining environmental justice communities: Evaluating digital infrastructure in Southeastern states for Justice40 benefits allocation. Applied Geography 158 (2023), 103057.\n\n[73] State of California. 2012. Green Building Action Plan – For Implementation of Executive Order B-18-12. https://archive.gov.ca.gov/archive/gov39/wp-content/uploads/2017/09/Green_Building_Action_Plan_B.18.12.pdf Accessed: 2025-04-22.\n\n[74] Statista. 2025. Average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide. https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/ Accessed: 2025-03-18.\n\n[75] Amy L Stein. 2020. Artificial intelligence and climate change. Yale J. on Reg. 37 (2020), 890.\n\n[76] Mariarosaria Taddeo, Andreas Tsamados, Josh Cowls, and Luciano Floridi. 2021. Artificial intelligence and the climate emergency: opportunities, challenges, and recommendations. One Earth 4, 6 (2021), 776–779.\n\n[77] The Green Grid. 2023. IT & Power Efficiency Survey Results (for EUC). https://www.thegreengrid.org/en/resources/library-and-tools/572-IT-%26-Power-Efficiency-Survey-Results-%28for-EUC%29 Accessed: 2025-03-18.\n\n[78] U.S. Congress. 2024. S.3732 - Artificial Intelligence Environmental Impacts Act of 2024. https://www.congress.gov/bill/118th-congress/senate-bill/3732/ Accessed: 2025-03-18.", "original_types": ["text"], "id": 1088}
{"type": "section", "content": "Measuring Points for IT Equipment in Data Centers", "doc_id": "ebert2024", "page": 13, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Measuring Points for IT Equipment in Data Centers", "original_types": ["header"], "id": 1089}
{"type": "figure", "content": "Figure 1: Measuring points for IT equipment energy consumption, from Annex II of the Delegated Regulation EU/2024/1364 of March 14, 2024", "doc_id": "ebert2024", "page": 13, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Figure 1: Measuring points for IT equipment energy consumption, from Annex II of the Delegated Regulation EU/2024/1364 of March 14, 2024", "id": 1090}
{"type": "section", "content": "Policy Proposals Overview", "doc_id": "ebert2024", "page": 14, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Policy Proposals Overview", "original_types": ["header"], "id": 1091}
{"type": "table", "content": "Table 1: Summary of Policy Proposals for AI and Environmental Impact\nEnergy and Environmental Reporting\n\nEnergy consumption from inferences: Include energy consumption from both single and cumulative inferences in reporting.\n\nIndirect emissions and water consumption: Mandate reporting of indirect emissions and water use in AI applications.\n\nCumulative server energy reporting: Require energy consumption to be measured and reported at the cumulative server level.\n\nLegal and Regulatory Clarifications\n\nClarifying provider status for AI modifications: Limit provider status to substantial AI model modifications.\n\nElimination of open-source exemption: Remove the exemption that allows open-source models to bypass reporting obligations.\n\nRenewable energy and efficiency targets for data centers: Introduce binding energy efficiency and renewable energy targets for data centers.\n\nTransparency and Accountability\n\nPublic access to climate-related disclosures: Ensure that climate-related disclosures are accessible to the public, not just authorities.\n\nEnergy reporting for High-Risk AI: Replace computational resource reporting with direct energy consumption disclosures for High-Risk AI systems.\n\nInclusion of environmental risks in assessments: Clarify that environmental risks must be part of risk assessments for AI providers.\n\nFuture Far-Reaching Measures\n\nRestrictions during peak energy hours: Limit AI training and non-essential inference tasks to non-peak energy demand periods.\n\nObligation to create renewable energy sources: Require AI companies and data centers to develop renewable energy sources to offset excess energy use.\n\n Tradable energy budgets for AI: Introduce a market-based system with tradable energy budgets for AI operations, similar to the EU ETS.", "doc_id": "ebert2024", "page": 14, "url": "https://arxiv.org/pdf/2410.06681", "embedded_text": "Table 1: Summary of Policy Proposals for AI and Environmental Impact\nEnergy and Environmental Reporting\n\nEnergy consumption from inferences: Include energy consumption from both single and cumulative inferences in reporting.\n\nIndirect emissions and water consumption: Mandate reporting of indirect emissions and water use in AI applications.\n\nCumulative server energy reporting: Require energy consumption to be measured and reported at the cumulative server level.\n\nLegal and Regulatory Clarifications\n\nClarifying provider status for AI modifications: Limit provider status to substantial AI model modifications.\n\nElimination of open-source exemption: Remove the exemption that allows open-source models to bypass reporting obligations.\n\nRenewable energy and efficiency targets for data centers: Introduce binding energy efficiency and renewable energy targets for data centers.\n\nTransparency and Accountability\n\nPublic access to climate-related disclosures: Ensure that climate-related disclosures are accessible to the public, not just authorities.\n\nEnergy reporting for High-Risk AI: Replace computational resource reporting with direct energy consumption disclosures for High-Risk AI systems.\n\nInclusion of environmental risks in assessments: Clarify that environmental risks must be part of risk assessments for AI providers.\n\nFuture Far-Reaching Measures\n\nRestrictions during peak energy hours: Limit AI training and non-essential inference tasks to non-peak energy demand periods.\n\nObligation to create renewable energy sources: Require AI companies and data centers to develop renewable energy sources to offset excess energy use.\n\n Tradable energy budgets for AI: Introduce a market-based system with tradable energy budgets for AI operations, similar to the EU ETS.", "id": 1092}
{"type": "section", "content": "Abstract\n\nThis paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.\n\nI. Introduction", "doc_id": "wu2021a", "page": 1, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Abstract\n\nThis paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.\n\nI. Introduction", "original_types": ["text", "header"], "id": 1093}
{"type": "section", "content": "Artificial Intelligence (AI) is one of the fastest growing domains spanning research and product development and significant investment in AI is taking place across nearly every industry, policy, and academic research. This investment in AI has also stimulated novel applications in domains such as science, medicine, finance, and education. Figure 1 analyzes the number of papers published within the scientific disciplines, illustrating the growth trend in recent years1. \nAI plays an instrumental role to push the boundaries of knowledge and sparks novel, more efficient approaches to conventional tasks. AI is applied to predict protein structures radically better than previous methods. It has the potential to revolutionize biological sciences by providing in-silico methods for tasks only possible in a physical laboratory setting [1]. AI is demonstrated to achieve human-level conversation tasks, such as the Blender Bot [2], and play games at superhuman levels, such as AlphaZero [3]. AI is used to discover new electrocatalysts for efficient and scalable ways to store and utilize renewable energy [4], predicting renewable energy availability in advance to improve energy utilization [5], operating hyperscale data centers efficiently [6], growing plants using less natural resources [7], and, at the same time, being used to tackle climate changes [8], [9]. It is projected that, in the next five years, the market for AI will increase by 10× into hundreds of billions of dollars [10]. All of these investments in research, development, and deployment have led to a super-linear growth in AI data, models, and infrastructure capacity. With the dramatic growth of AI, it is imperative to understand the environmental implications, challenges, and opportunities of this nascent technology. This is because technologies tend to create a self-accelerating growth cycle, putting new demands on the environment. \nThis work explores the environmental impact of AI from a holistic perspective. More specifically, we present the challenges and opportunities to designing sustainable AI computing across the key phases of the machine learning (ML) development process — Data, Experimentation, Training, and Inference — for a variety of AI use cases at Facebook, such as vision, language, speech, recommendation and ranking. The solution space spans across our fleet of datacenters and on-device computing. Given particular use cases, we consider the impact of AI data, algorithms, and system hardware. Finally, we consider emissions across the life cycle of hardware systems, from manufacturing to operational use.", "doc_id": "wu2021a", "page": 1, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Artificial Intelligence (AI) is one of the fastest growing domains spanning research and product development and significant investment in AI is taking place across nearly every industry, policy, and academic research. This investment in AI has also stimulated novel applications in domains such as science, medicine, finance, and education. Figure 1 analyzes the number of papers published within the scientific disciplines, illustrating the growth trend in recent years1. \nAI plays an instrumental role to push the boundaries of knowledge and sparks novel, more efficient approaches to conventional tasks. AI is applied to predict protein structures radically better than previous methods. It has the potential to revolutionize biological sciences by providing in-silico methods for tasks only possible in a physical laboratory setting [1]. AI is demonstrated to achieve human-level conversation tasks, such as the Blender Bot [2], and play games at superhuman levels, such as AlphaZero [3]. AI is used to discover new electrocatalysts for efficient and scalable ways to store and utilize renewable energy [4], predicting renewable energy availability in advance to improve energy utilization [5], operating hyperscale data centers efficiently [6], growing plants using less natural resources [7], and, at the same time, being used to tackle climate changes [8], [9]. It is projected that, in the next five years, the market for AI will increase by 10× into hundreds of billions of dollars [10]. All of these investments in research, development, and deployment have led to a super-linear growth in AI data, models, and infrastructure capacity. With the dramatic growth of AI, it is imperative to understand the environmental implications, challenges, and opportunities of this nascent technology. This is because technologies tend to create a self-accelerating growth cycle, putting new demands on the environment. \nThis work explores the environmental impact of AI from a holistic perspective. More specifically, we present the challenges and opportunities to designing sustainable AI computing across the key phases of the machine learning (ML) development process — Data, Experimentation, Training, and Inference — for a variety of AI use cases at Facebook, such as vision, language, speech, recommendation and ranking. The solution space spans across our fleet of datacenters and on-device computing. Given particular use cases, we consider the impact of AI data, algorithms, and system hardware. Finally, we consider emissions across the life cycle of hardware systems, from manufacturing to operational use.", "original_types": ["text"], "id": 1094}
{"type": "figure", "content": "Figure 1: The growth of ML is exceeding that of many other scientific disciplines. Significant research growth in machine learning is observed in recent years as illustrated by the increasing cumulative number of papers published in machine learning with respect to other scientific disciplines based on the monthly count (y-axis measures the cumulative number of articles on arXiv).", "doc_id": "wu2021a", "page": 1, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 1: The growth of ML is exceeding that of many other scientific disciplines. Significant research growth in machine learning is observed in recent years as illustrated by the increasing cumulative number of papers published in machine learning with respect to other scientific disciplines based on the monthly count (y-axis measures the cumulative number of articles on arXiv).", "id": 1095}
{"type": "section", "content": "AI Model Growth\n\nThe ever-increasing data volume has also driven a super-linear trend in model size growth. Figure 2(a) depicts the 1000× model size increase for GPT3-based language translation tasks [12], [13], whereas for Baidu’s search engine, the model of 1000× larger in size improves accuracy in AUC by 0.030. Despite small, the accuracy improvement can lead to significantly higher-quality search outcomes [14]. Similarly, Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11]. Despite the large increase in model sizes, the memory capacity of GPU-based AI accelerators, e.g. 32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100, 2021), has increased by < 2× every 2 years. The resource requirements for strong AI scaling clearly outpaces that of system hardware.", "doc_id": "wu2021a", "page": 2, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "AI Model Growth\n\nThe ever-increasing data volume has also driven a super-linear trend in model size growth. Figure 2(a) depicts the 1000× model size increase for GPT3-based language translation tasks [12], [13], whereas for Baidu’s search engine, the model of 1000× larger in size improves accuracy in AUC by 0.030. Despite small, the accuracy improvement can lead to significantly higher-quality search outcomes [14]. Similarly, Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11]. Despite the large increase in model sizes, the memory capacity of GPU-based AI accelerators, e.g. 32GB (NVIDIA V100, 2018) to 80GB (NVIDIA A100, 2021), has increased by < 2× every 2 years. The resource requirements for strong AI scaling clearly outpaces that of system hardware.", "original_types": ["text", "header"], "id": 1096}
{"type": "figure", "content": "Figure 2: Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years. (a) The 1000× model size growth has led to higher model accuracy for various ML tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size. (b) At Facebook, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2 times increase in the data ingestion bandwidth demand. (c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11]. (d) The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.", "doc_id": "wu2021a", "page": 2, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 2: Deep learning has witnessed an exponential growth in data, model parameters, and system resources over the recent years. (a) The 1000× model size growth has led to higher model accuracy for various ML tasks. For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size. (b) At Facebook, the amount of data for recommendation use cases has roughly doubled between 2019 and 2021, leading to 3.2 times increase in the data ingestion bandwidth demand. (c) Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11]. (d) The explosive growth in AI has driven 2.9× and 2.5× capacity increases for AI training and inference, respectively.", "id": 1097}
{"type": "section", "content": "AI Infrastructure Growth\n\nThe strong performance scaling demand for ML motivates a variety of scale-out solutions [11], [18] by leveraging parallelism at scale with a massive collection of training accelerators. Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years. In addition, we observe trillions of inference per day across Facebook’s data centers—more than doubling in the past 3 years. The increase in inference demands has also led to an 2.5× increase in AI inference infrastructure capacity. Last but not least, the carbon footprint of AI goes beyond its operational energy consumption. The embodied carbon footprint of systems is becoming a dominating factor for AI’s overall environmental impact (Section III) [19].\n\nThe Elephant in the Room\n\nDespite the positive societal benefits [20], the endless pursuit of achieving higher model quality has led to the exponential scaling of AI with significant energy and environmental footprint implications. Although recent work shows the carbon footprint of training one large ML model, such as Meena [21], is equivalent to 242,231 miles driven by an average passenger vehicle [22], this is only one aspect; to fully understand the real environmental impact we must consider the AI ecosystem holistically going forward — beyond looking at model training alone and by accounting for both operational and embodied carbon footprint of AI. We must look at the ML pipeline end-to-end: data collection, model exploration and experimentation, model training, model optimization and run-time inference. The frequency of training and scale of each stage of the ML development cycle matter. From the systems perspective, the life cycle of ML software and system hardware, including manufacturing and operational use, must also be considered.", "doc_id": "wu2021a", "page": 2, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "AI Infrastructure Growth\n\nThe strong performance scaling demand for ML motivates a variety of scale-out solutions [11], [18] by leveraging parallelism at scale with a massive collection of training accelerators. Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years. In addition, we observe trillions of inference per day across Facebook’s data centers—more than doubling in the past 3 years. The increase in inference demands has also led to an 2.5× increase in AI inference infrastructure capacity. Last but not least, the carbon footprint of AI goes beyond its operational energy consumption. The embodied carbon footprint of systems is becoming a dominating factor for AI’s overall environmental impact (Section III) [19].\n\nThe Elephant in the Room\n\nDespite the positive societal benefits [20], the endless pursuit of achieving higher model quality has led to the exponential scaling of AI with significant energy and environmental footprint implications. Although recent work shows the carbon footprint of training one large ML model, such as Meena [21], is equivalent to 242,231 miles driven by an average passenger vehicle [22], this is only one aspect; to fully understand the real environmental impact we must consider the AI ecosystem holistically going forward — beyond looking at model training alone and by accounting for both operational and embodied carbon footprint of AI. We must look at the ML pipeline end-to-end: data collection, model exploration and experimentation, model training, model optimization and run-time inference. The frequency of training and scale of each stage of the ML development cycle matter. From the systems perspective, the life cycle of ML software and system hardware, including manufacturing and operational use, must also be considered.", "original_types": ["text", "header"], "id": 1098}
{"type": "section", "content": "Model Development Phases and AI System Hardware Life Cycle\n\nFigure 3 depicts the major development phases for ML — Data Processing, Experimentation, Training, and Inference (Section II-A) — over the life cycle of AI system hardware (Section II-B). Driven by distinct objectives of AI research and advanced product development, infrastructure is designed and built specifically to maximize data storage and ingestion efficiency for the phase of Data Processing, developer efficiency for the phase of Experimentation, training throughput efficiency for the phase of Training, and tail-latency bounded throughput efficiency for Inference.\n\nMachine Learning Model Development Cycle\n\nML researchers extract features from data during the Data Processing phase and apply weights to individual features based on feature importance to the model optimization objective. During Experimentation, the researchers design, implement and evaluate the quality of proposed algorithms, model architectures, modeling techniques, and/or training methods for determining model parameters. This model exploration process is computationally-intensive. A large collection of diverse ML ideas are explored simultaneously at-scale. Thus, during this phase, we observe unique system resource requirements from the large pool of training experiments. Within Facebook’s ML research cluster, 50% (p50) of ML training experiments take up to 1.5 GPU days while 99% (p99) of the experiments complete within 24 GPU days. There are a number of large-scale, trillion parameter models which require over 500 GPUs days.\n\nOnce a ML solution is determined as promising, it moves into Training where the ML solution is evaluated using extensive production data — data that is more recent, is larger in quantity, and contains richer features. The process often requires additional hyper-parameter tuning. Depending on the ML task requirement, the models can be trained/re-trained at different frequencies. For example, models supporting Facebook’s Search service were trained at an hourly cadence whereas the Language Translation models were trained weekly [24]. A p50 production model training workflow takes 2.96 GPU days while a training workflow at p99 can take up to 125 GPU days.\n\nFinally, for Inference, the best-performing model is deployed, producing trillions of daily predictions to serve billions of users worldwide. The total compute cycles for inference predictions are expected to exceed the corresponding training cycles for the deployed model.\n\nMachine Learning System Life Cycle", "doc_id": "wu2021a", "page": 3, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Model Development Phases and AI System Hardware Life Cycle\n\nFigure 3 depicts the major development phases for ML — Data Processing, Experimentation, Training, and Inference (Section II-A) — over the life cycle of AI system hardware (Section II-B). Driven by distinct objectives of AI research and advanced product development, infrastructure is designed and built specifically to maximize data storage and ingestion efficiency for the phase of Data Processing, developer efficiency for the phase of Experimentation, training throughput efficiency for the phase of Training, and tail-latency bounded throughput efficiency for Inference.\n\nMachine Learning Model Development Cycle\n\nML researchers extract features from data during the Data Processing phase and apply weights to individual features based on feature importance to the model optimization objective. During Experimentation, the researchers design, implement and evaluate the quality of proposed algorithms, model architectures, modeling techniques, and/or training methods for determining model parameters. This model exploration process is computationally-intensive. A large collection of diverse ML ideas are explored simultaneously at-scale. Thus, during this phase, we observe unique system resource requirements from the large pool of training experiments. Within Facebook’s ML research cluster, 50% (p50) of ML training experiments take up to 1.5 GPU days while 99% (p99) of the experiments complete within 24 GPU days. There are a number of large-scale, trillion parameter models which require over 500 GPUs days.\n\nOnce a ML solution is determined as promising, it moves into Training where the ML solution is evaluated using extensive production data — data that is more recent, is larger in quantity, and contains richer features. The process often requires additional hyper-parameter tuning. Depending on the ML task requirement, the models can be trained/re-trained at different frequencies. For example, models supporting Facebook’s Search service were trained at an hourly cadence whereas the Language Translation models were trained weekly [24]. A p50 production model training workflow takes 2.96 GPU days while a training workflow at p99 can take up to 125 GPU days.\n\nFinally, for Inference, the best-performing model is deployed, producing trillions of daily predictions to serve billions of users worldwide. The total compute cycles for inference predictions are expected to exceed the corresponding training cycles for the deployed model.\n\nMachine Learning System Life Cycle", "original_types": ["text", "header"], "id": 1099}
{"type": "section", "content": "Life Cycle Analysis (LCA) is a common methodology to assess the carbon emissions over the product life cycle. There are four major phases: manufacturing, transport, product use, and recycling². From the perspective of AI’s carbon footprint analysis, manufacturing and product use are the focus. Thus, in this work, we consider the overall carbon footprint of AI by including manufacturing — carbon emissions from building infrastructures specifically for AI (i.e., embodied carbon footprint) and product use — carbon emissions from the use of AI (i.e., operational carbon footprint).\n\nWhile quantifying the exact breakdown between operational and embodied carbon footprint is a complex process, we estimate the significance of embodied carbon emissions using Facebook’s Greenhouse Gas (GHG) emission statistics³. In this case, more than 50% of Facebook’s emissions owe to its value chain — Scope 3 of Facebook’s GHG emission. As a result, a significant embodied carbon cost is paid upfront for every system component brought into Facebook’s fleet of datacenters, where AI is the biggest growth driver.", "doc_id": "wu2021a", "page": 3, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Life Cycle Analysis (LCA) is a common methodology to assess the carbon emissions over the product life cycle. There are four major phases: manufacturing, transport, product use, and recycling². From the perspective of AI’s carbon footprint analysis, manufacturing and product use are the focus. Thus, in this work, we consider the overall carbon footprint of AI by including manufacturing — carbon emissions from building infrastructures specifically for AI (i.e., embodied carbon footprint) and product use — carbon emissions from the use of AI (i.e., operational carbon footprint).\n\nWhile quantifying the exact breakdown between operational and embodied carbon footprint is a complex process, we estimate the significance of embodied carbon emissions using Facebook’s Greenhouse Gas (GHG) emission statistics³. In this case, more than 50% of Facebook’s emissions owe to its value chain — Scope 3 of Facebook’s GHG emission. As a result, a significant embodied carbon cost is paid upfront for every system component brought into Facebook’s fleet of datacenters, where AI is the biggest growth driver.", "original_types": ["text"], "id": 1100}
{"type": "section", "content": "III. AI Computing’s Carbon Footprint\n\nA. Carbon Footprint Analysis for Industry-Scale ML Training and Deployment\n\nFigure 4 illustrates the operational carbon emissions for model training and inference across the ML tasks. We analyze six representative machine learning models in production at Facebook4. LM refers to Facebook’s Transformer-based Universal Language Model for text translation [25]. RM1 – RM5 represent five unique deep learning recommendation and ranking models for various Facebook products [26], [27].\n\nWe compare the carbon footprint of Facebook’s production ML models with seven large-scale, open-source (OSS) models: BERT-NAS, T5, Meena, GShard-600B, Switch Transformer, and GPT-3. Note, we present the operational carbon footprint of the OSS model training from [28], [21]. The operational carbon footprint results can vary based on the exact AI systems used and the carbon intensity of the energy mixture. Models with more parameters do not necessarily result in longer training time nor higher carbon emissions. Training the Switch Transformer model equipped with 1.5 trillion parameters [29] produces significantly less carbon emission than that of GPT-3 (750 billion parameters) [13]. This illustrates the carbon footprint advantage of operationally-efficient model architectures.", "doc_id": "wu2021a", "page": 4, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "III. AI Computing’s Carbon Footprint\n\nA. Carbon Footprint Analysis for Industry-Scale ML Training and Deployment\n\nFigure 4 illustrates the operational carbon emissions for model training and inference across the ML tasks. We analyze six representative machine learning models in production at Facebook4. LM refers to Facebook’s Transformer-based Universal Language Model for text translation [25]. RM1 – RM5 represent five unique deep learning recommendation and ranking models for various Facebook products [26], [27].\n\nWe compare the carbon footprint of Facebook’s production ML models with seven large-scale, open-source (OSS) models: BERT-NAS, T5, Meena, GShard-600B, Switch Transformer, and GPT-3. Note, we present the operational carbon footprint of the OSS model training from [28], [21]. The operational carbon footprint results can vary based on the exact AI systems used and the carbon intensity of the energy mixture. Models with more parameters do not necessarily result in longer training time nor higher carbon emissions. Training the Switch Transformer model equipped with 1.5 trillion parameters [29] produces significantly less carbon emission than that of GPT-3 (750 billion parameters) [13]. This illustrates the carbon footprint advantage of operationally-efficient model architectures.", "original_types": ["text", "header"], "id": 1101}
{"type": "figure", "content": "Fig. 4. The carbon footprint of the LM model is dominated by Inference whereas, for RM1 – RM5, the carbon footprint of Training versus Inference is roughly equal. The average carbon footprint for ML training tasks at Facebook is 1.8 times larger than that of Meena used in modern conversational agents and 0.3 times of GPT-3’s carbon footprint. Carbon footprint for inference tasks is included for models that are used in production. Note: the operational carbon footprint of AI does not correlate with the number of model parameters. The OSS large-scale ML tasks are based on the vanilla model architectures from [21] and may not be reflective of production use cases.", "doc_id": "wu2021a", "page": 4, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Fig. 4. The carbon footprint of the LM model is dominated by Inference whereas, for RM1 – RM5, the carbon footprint of Training versus Inference is roughly equal. The average carbon footprint for ML training tasks at Facebook is 1.8 times larger than that of Meena used in modern conversational agents and 0.3 times of GPT-3’s carbon footprint. Carbon footprint for inference tasks is included for models that are used in production. Note: the operational carbon footprint of AI does not correlate with the number of model parameters. The OSS large-scale ML tasks are based on the vanilla model architectures from [21] and may not be reflective of production use cases.", "id": 1102}
{"type": "figure", "content": "Fig. 6. Optimization is an iterative process — we have achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack.", "doc_id": "wu2021a", "page": 5, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Fig. 6. Optimization is an iterative process — we have achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack.", "id": 1103}
{"type": "section", "content": "The embodied footprint as the production footprint of Apple’s 28-core CPU with dual AMD Radeon GPUs (2000kg CO2) [31]. For CPU-only systems, we assume half the embodied emissions. Based on the characterization of model training and inference at Facebook, we assume an average utilization of 30-60% over the 3- to 5-year lifetime for servers. Figure 5 presents the overall carbon footprint for the large scale ML tasks at Facebook, spanning both operational and embodied carbon footprint. Based on the assumptions of location-based renewable energy availability, the split between the embodied and (location-based) operational carbon footprint is roughly 30% / 70% for the large scale ML tasks. Taking into account carbon-free energy, such as solar, the operational carbon footprint can be significantly reduced, leaving the manufacturing carbon cost as the dominating source of AI’s carbon footprint.", "doc_id": "wu2021a", "page": 5, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "The embodied footprint as the production footprint of Apple’s 28-core CPU with dual AMD Radeon GPUs (2000kg CO2) [31]. For CPU-only systems, we assume half the embodied emissions. Based on the characterization of model training and inference at Facebook, we assume an average utilization of 30-60% over the 3- to 5-year lifetime for servers. Figure 5 presents the overall carbon footprint for the large scale ML tasks at Facebook, spanning both operational and embodied carbon footprint. Based on the assumptions of location-based renewable energy availability, the split between the embodied and (location-based) operational carbon footprint is roughly 30% / 70% for the large scale ML tasks. Taking into account carbon-free energy, such as solar, the operational carbon footprint can be significantly reduced, leaving the manufacturing carbon cost as the dominating source of AI’s carbon footprint.", "original_types": ["text"], "id": 1104}
{"type": "figure", "content": "Fig. 7. For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.", "doc_id": "wu2021a", "page": 5, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Fig. 7. For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.", "id": 1105}
{"type": "section", "content": "The compounded benefits highlight the need for cross-stack optimizations. \n\nOptimizing the Carbon Footprint of LMs: We dive into a specific machine learning task at Facebook: language translation using a Transformer-based architecture (LM). LM is designed based on the state-of-the-art cross-lingual understanding through self-supervision. Figure 7 analyzes the power footprint improvements over a collection of optimization steps for LM: platform-level caching, GPU acceleration, low precision format on accelerator, and model optimization. In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800×. We outline the optimization benefits from each area below.\n\n- Platform-Level Caching. Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7×. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks. Using DRAM and Flash storage devices as caches, these pre-computed embeddings can be shared across applications and use cases.\n- GPU acceleration. In addition to caching, deploying LM across GPU-based specialized AI hardware unlocks an additional 10.1× energy efficiency improvement.\n- Algorithmic optimization. Finally, algorithmic optimizations provide an additional 12× energy efficiency reduction. Halving precision (e.g., going from 32-bit to 16-bit operations) provides a 2.4× energy efficiency improvement on GPUs. Another 5× energy efficiency gain can be achieved by using custom operators to schedule encoding steps within a single kernel of the Transformer module, such as [32].\n\nOptimizing the Carbon Footprint of RMs: The LM analysis is used as an example to highlight the optimization opportunities available with judicious cross-stack, hardware/software optimization. In addition to optimizing the carbon footprint for the language translation task, we describe additional optimization techniques tailored for ranking and", "doc_id": "wu2021a", "page": 5, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "The compounded benefits highlight the need for cross-stack optimizations. \n\nOptimizing the Carbon Footprint of LMs: We dive into a specific machine learning task at Facebook: language translation using a Transformer-based architecture (LM). LM is designed based on the state-of-the-art cross-lingual understanding through self-supervision. Figure 7 analyzes the power footprint improvements over a collection of optimization steps for LM: platform-level caching, GPU acceleration, low precision format on accelerator, and model optimization. In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800×. We outline the optimization benefits from each area below.\n\n- Platform-Level Caching. Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7×. These improvements are a result of pre-computing and caching frequently accessed embeddings for language translation tasks. Using DRAM and Flash storage devices as caches, these pre-computed embeddings can be shared across applications and use cases.\n- GPU acceleration. In addition to caching, deploying LM across GPU-based specialized AI hardware unlocks an additional 10.1× energy efficiency improvement.\n- Algorithmic optimization. Finally, algorithmic optimizations provide an additional 12× energy efficiency reduction. Halving precision (e.g., going from 32-bit to 16-bit operations) provides a 2.4× energy efficiency improvement on GPUs. Another 5× energy efficiency gain can be achieved by using custom operators to schedule encoding steps within a single kernel of the Transformer module, such as [32].\n\nOptimizing the Carbon Footprint of RMs: The LM analysis is used as an example to highlight the optimization opportunities available with judicious cross-stack, hardware/software optimization. In addition to optimizing the carbon footprint for the language translation task, we describe additional optimization techniques tailored for ranking and", "original_types": ["text"], "id": 1106}
{"type": "figure", "content": "Figure 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B). Despite the significant operational power footprint reduction, we continue to see the overall electricity demand for AI to increase over time — an example of Jevon’s Paradox, where efficiency improvement stimulates additional novel AI use cases.", "doc_id": "wu2021a", "page": 6, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B). Despite the significant operational power footprint reduction, we continue to see the overall electricity demand for AI to increase over time — an example of Jevon’s Paradox, where efficiency improvement stimulates additional novel AI use cases.", "id": 1107}
{"type": "section", "content": "A major infrastructure challenge faced by deep learning RM training and deployment (RM1 – RM5) is the fast-rising memory capacity and bandwidth demands (Figure 2). There are two primary sub-nets in a RM: the dense fully-connected (FC) network and the sparse embedding-based network. The FC network is constructed with multi-layer perceptions (MLPs), thus computationally-intensive. The embedding network is used to project hundreds of sparse, high-dimensional features to low-dimension vectors. It can easily contribute to over 95% of the total model size. For a number of important recommendation and ranking use cases, the embedding operation dominates the inference execution time [27], [33].\n\nTo tackle the significant memory capacity and bandwidth requirement, we deploy model quantization for RMs [34]. Quantization offers two primary efficiency benefits: the low-precision data representation reduces the amount of computation requirement and, at the same time, lowers the overall memory capacity need. By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%. This has led to 20.7% reduction in memory bandwidth consumption. Furthermore, the memory capacity reduction enabled by quantization unblocks novel systems with lower on-chip memory. For example, for RM1, quantization has enabled RM deployment on highly power-efficient systems with smaller on-chip memory, leading to an end-to-end inference latency improvement of 2.5 times.", "doc_id": "wu2021a", "page": 6, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "A major infrastructure challenge faced by deep learning RM training and deployment (RM1 – RM5) is the fast-rising memory capacity and bandwidth demands (Figure 2). There are two primary sub-nets in a RM: the dense fully-connected (FC) network and the sparse embedding-based network. The FC network is constructed with multi-layer perceptions (MLPs), thus computationally-intensive. The embedding network is used to project hundreds of sparse, high-dimensional features to low-dimension vectors. It can easily contribute to over 95% of the total model size. For a number of important recommendation and ranking use cases, the embedding operation dominates the inference execution time [27], [33].\n\nTo tackle the significant memory capacity and bandwidth requirement, we deploy model quantization for RMs [34]. Quantization offers two primary efficiency benefits: the low-precision data representation reduces the amount of computation requirement and, at the same time, lowers the overall memory capacity need. By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%. This has led to 20.7% reduction in memory bandwidth consumption. Furthermore, the memory capacity reduction enabled by quantization unblocks novel systems with lower on-chip memory. For example, for RM1, quantization has enabled RM deployment on highly power-efficient systems with smaller on-chip memory, leading to an end-to-end inference latency improvement of 2.5 times.", "original_types": ["text"], "id": 1108}
{"type": "figure", "content": "Figure 9. As accelerator utilization improves over time, both operational and embodied carbon footprints of AI improve. Carbon-free energy helps reduce the operational carbon footprint, making embodied carbon cost the dominating factor. To reduce the rising carbon footprint of AI computing at-scale, we must complement efficiency and utilization optimization with novel approaches to reduce the remaining embodied carbon footprint of AI systems.", "doc_id": "wu2021a", "page": 7, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 9. As accelerator utilization improves over time, both operational and embodied carbon footprints of AI improve. Carbon-free energy helps reduce the operational carbon footprint, making embodied carbon cost the dominating factor. To reduce the rising carbon footprint of AI computing at-scale, we must complement efficiency and utilization optimization with novel approaches to reduce the remaining embodied carbon footprint of AI systems.", "id": 1109}
{"type": "section", "content": "data privacy, we expect to see more computation being shifted away from data centers to the edge, where access to renewable energy may be limited. The edge-cloud space for AI poses interesting design opportunities (Section IV-C).", "doc_id": "wu2021a", "page": 7, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "data privacy, we expect to see more computation being shifted away from data centers to the edge, where access to renewable energy may be limited. The edge-cloud space for AI poses interesting design opportunities (Section IV-C).", "original_types": ["text"], "id": 1110}
{"type": "section", "content": "resource-intensive, involving training many models, especially when using simple approaches. Strubell et al. show that grid-search NAS can incur over 3000× environmental footprint overhead [28]. Utilizing much more sample-efficient NAS and HPO methods [45], [46] can translate directly into carbon footprint improvement. In addition to reducing the number of training experiments, one can also reduce the training time of each experiment. By detecting and stopping under-performing training workflows early, unnecessary training cycles can be eliminated.\n\nMulti-objective optimization explores the Pareto frontier of efficient model quality and system resource trade-offs. If used early in the model exploration process, it enables more informed decisions about which model to train fully and deploy given certain infrastructure capacity. Beyond model accuracy and timing performance [47], [48], [49], [50], energy and carbon footprint can be directly incorporated into the cost function as optimization objectives to enable discovery of environmentally-friendly models. Furthermore, when training is decoupled from NAS, sub-networks tailoring to specialized system hardware can be selected without additional training [51], [52], [53], [54]. Such approaches can significantly reduce the overall training time, however, at the expense of increased embodied carbon footprint.\n\nDeveloping resource-efficient model architectures fundamentally reduce the overall system capacity need of ML tasks. From the systems perspective, accelerator memory is scarce. However, DNNs, such as neural recommendation models, require significantly higher memory capacity and bandwidth [55], [33]. This motivates researchers to develop memory-efficient model architectures. For example, the Tensor-Train compression technique (TT-Rec) achieves more than 100× memory capacity reduction with negligible training time and accuracy trade-off [56]. Similarly, the design space trade-off between memory capacity requirement, training time, and model accuracy is also explored in Deep Hash Embedding (DHE) [57]. While training time increases lead to higher operational carbon footprint, in the case of TT-Rec and DHE, the memory-efficient model architectures require significantly lower memory capacity while better utilizing the computational capability of training accelerators, resulting in lower embodied carbon footprint.", "doc_id": "wu2021a", "page": 8, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "resource-intensive, involving training many models, especially when using simple approaches. Strubell et al. show that grid-search NAS can incur over 3000× environmental footprint overhead [28]. Utilizing much more sample-efficient NAS and HPO methods [45], [46] can translate directly into carbon footprint improvement. In addition to reducing the number of training experiments, one can also reduce the training time of each experiment. By detecting and stopping under-performing training workflows early, unnecessary training cycles can be eliminated.\n\nMulti-objective optimization explores the Pareto frontier of efficient model quality and system resource trade-offs. If used early in the model exploration process, it enables more informed decisions about which model to train fully and deploy given certain infrastructure capacity. Beyond model accuracy and timing performance [47], [48], [49], [50], energy and carbon footprint can be directly incorporated into the cost function as optimization objectives to enable discovery of environmentally-friendly models. Furthermore, when training is decoupled from NAS, sub-networks tailoring to specialized system hardware can be selected without additional training [51], [52], [53], [54]. Such approaches can significantly reduce the overall training time, however, at the expense of increased embodied carbon footprint.\n\nDeveloping resource-efficient model architectures fundamentally reduce the overall system capacity need of ML tasks. From the systems perspective, accelerator memory is scarce. However, DNNs, such as neural recommendation models, require significantly higher memory capacity and bandwidth [55], [33]. This motivates researchers to develop memory-efficient model architectures. For example, the Tensor-Train compression technique (TT-Rec) achieves more than 100× memory capacity reduction with negligible training time and accuracy trade-off [56]. Similarly, the design space trade-off between memory capacity requirement, training time, and model accuracy is also explored in Deep Hash Embedding (DHE) [57]. While training time increases lead to higher operational carbon footprint, in the case of TT-Rec and DHE, the memory-efficient model architectures require significantly lower memory capacity while better utilizing the computational capability of training accelerators, resulting in lower embodied carbon footprint.", "original_types": ["text"], "id": 1111}
{"type": "section", "content": "Developing efficient training algorithms is a long-time objective of research in optimization and numerical methods [58]. Evaluations of optimization methods should account for all experimentation efforts required to tune optimizer hyperparameters, not just the method performance after tuning [59], [60]. In addition, significant research has gone into algorithmic approaches to efficiently scale training [61], [62] by reducing communication cost via compression [63], [64], pipelining [65], and sharding [66], [67]. The advances have enabled efficient scaling to larger models and larger datasets. We expect efficient training methods to continue as an important domain. While this paper has focused on supervised learning relying labeled data, algorithmic efficiency extends to other learning paradigms including self-supervised and semi-supervised learning (Appendix C).", "doc_id": "wu2021a", "page": 8, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Developing efficient training algorithms is a long-time objective of research in optimization and numerical methods [58]. Evaluations of optimization methods should account for all experimentation efforts required to tune optimizer hyperparameters, not just the method performance after tuning [59], [60]. In addition, significant research has gone into algorithmic approaches to efficiently scale training [61], [62] by reducing communication cost via compression [63], [64], pipelining [65], and sharding [66], [67]. The advances have enabled efficient scaling to larger models and larger datasets. We expect efficient training methods to continue as an important domain. While this paper has focused on supervised learning relying labeled data, algorithmic efficiency extends to other learning paradigms including self-supervised and semi-supervised learning (Appendix C).", "original_types": ["text"], "id": 1112}
{"type": "figure", "content": "Figure 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.", "doc_id": "wu2021a", "page": 8, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.", "id": 1113}
{"type": "section", "content": "The Implications of General-Purpose Processors, General-Purpose Accelerators, Reconfigurable Systems, and ASICs for AI\n\nThere is a wide variety of system hardware choices for AI from general-purpose processors (CPUs), general-purpose accelerators (GPUs or TPUs), field-programmable gate arrays (FPGAs) [81], to application-specific integrated circuit (ASIC), such as Eyeriss [82]. The exact system deployment choice can be multifaceted — the cadence of ML algorithm and model architecture evolution, the diversity of ML use cases and the respective system resource requirements, and the maturity of the software stack. While ML accelerator deployment brings a step-function improvement in operational energy efficiency, it may not necessarily reduce the carbon footprint of AI computing overall. This is because of the upfront embodied carbon footprint associated with the different system hardware choices. From the environmental sustainability perspective, the optimal point depends on the compounding factor of operational efficiency improvement over generations of ML algorithms/models, deployment lifetime and embodied carbon footprint of the system hardware. Thus, to design for environmental sustainability, one must strike a careful balance between efficiency and flexibility and, at the same time, consider environmental impact as a key design dimension for next-generation AI systems.\n\nCarbon-Efficient Scheduling for AI Computing At-Scale\n\nAs the electricity consumption of hyperscale data centers continues to rise, data center operators have devoted significant investment to neutralize operational carbon footprint. By operating large-scale computing infrastructures with carbon free energy, technology companies are taking an important step to address the environmental implications of computing. More can be done however.\n\nAs the renewable energy proportion in the electricity grid increases, fluctuations in energy generation will increase due to the intermittent nature of renewable energy sources (i.e. wind, solar). Elastic carbon-aware workload scheduling techniques can be used in and across datacenters to predict and exploit the intermittent energy generation patterns [83]. However such scheduling algorithms might require server over-provisioning to allow for flexibility of shifting workloads to times when carbon-free energy is available. Furthermore, any additional server capacity comes with manufacturing carbon cost which needs to be incorporated into the design space. Alternatively, energy storage (e.g. batteries, pumped hydro, flywheels, molten salt) can be used to store renewable energy during peak generation times for use during low generation times. There is an interesting design space to achieve 24/7 carbon-free AI computing.\n\nOn-Device Learning", "doc_id": "wu2021a", "page": 9, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "The Implications of General-Purpose Processors, General-Purpose Accelerators, Reconfigurable Systems, and ASICs for AI\n\nThere is a wide variety of system hardware choices for AI from general-purpose processors (CPUs), general-purpose accelerators (GPUs or TPUs), field-programmable gate arrays (FPGAs) [81], to application-specific integrated circuit (ASIC), such as Eyeriss [82]. The exact system deployment choice can be multifaceted — the cadence of ML algorithm and model architecture evolution, the diversity of ML use cases and the respective system resource requirements, and the maturity of the software stack. While ML accelerator deployment brings a step-function improvement in operational energy efficiency, it may not necessarily reduce the carbon footprint of AI computing overall. This is because of the upfront embodied carbon footprint associated with the different system hardware choices. From the environmental sustainability perspective, the optimal point depends on the compounding factor of operational efficiency improvement over generations of ML algorithms/models, deployment lifetime and embodied carbon footprint of the system hardware. Thus, to design for environmental sustainability, one must strike a careful balance between efficiency and flexibility and, at the same time, consider environmental impact as a key design dimension for next-generation AI systems.\n\nCarbon-Efficient Scheduling for AI Computing At-Scale\n\nAs the electricity consumption of hyperscale data centers continues to rise, data center operators have devoted significant investment to neutralize operational carbon footprint. By operating large-scale computing infrastructures with carbon free energy, technology companies are taking an important step to address the environmental implications of computing. More can be done however.\n\nAs the renewable energy proportion in the electricity grid increases, fluctuations in energy generation will increase due to the intermittent nature of renewable energy sources (i.e. wind, solar). Elastic carbon-aware workload scheduling techniques can be used in and across datacenters to predict and exploit the intermittent energy generation patterns [83]. However such scheduling algorithms might require server over-provisioning to allow for flexibility of shifting workloads to times when carbon-free energy is available. Furthermore, any additional server capacity comes with manufacturing carbon cost which needs to be incorporated into the design space. Alternatively, energy storage (e.g. batteries, pumped hydro, flywheels, molten salt) can be used to store renewable energy during peak generation times for use during low generation times. There is an interesting design space to achieve 24/7 carbon-free AI computing.\n\nOn-Device Learning", "original_types": ["text", "header"], "id": 1114}
{"type": "section", "content": "On-device AI is becoming more ubiquitously adopted to enable model personalization [84], [85], [86] while improving data privacy [87], [88], [89], [90], yet its impact in terms of carbon emission is often overlooked. On-device learning emits non-negligible carbon. Figure 11 illustrates that the operational carbon footprint for training a small ML task using federated learning (FL) is comparable to that of training an orders-of-magnitude larger Transformer-based model in a centralized setting. As FL trains local models on client devices and periodically aggregates the model parameters for a global model, without collecting raw user data [87], the FL process can emit non-negligible carbon at the edge due to both computation and wireless communication. It is important to reduce AI’s environmental footprint at the edge. With the ever-increasing demand for on-device use cases over billions of client devices, such as teaching AI to understand the physical environment from the first-person perception [91] or personalizing AI tasks, the carbon footprint for on-device AI can add up to a dire amount quickly. Also, renewable energy is far more limited for client devices compared to datacenters. Optimizing the overall energy efficiency of FL and on-device AI is an important first step [92], [93], [94], [95], [96]. Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices. It is particularly challenging to amortize the embodied carbon footprint because client devices are often under-utilized [97].\n\nV. Call-to-Action\n\nA. Development of Easy-to-Adopt Telemetry for Assessing AI’s Environmental Footprint\n\nWhile the open source community has started building tools to enable automatic measurement of AI training’s environmental footprint [39], [40], [98], [99] and the ML research community requiring a broader impact statement for the submitted research manuscript, more can be done in order to incorporate efficiency and sustainability into the design process. Enabling carbon accounting methodologies and telemetry that is easy to adopt is an important step to quantify the significance of our progress in developing AI technologies in an environmentally-responsible manner. While assessing the novelty and quality of ML solutions, it is crucial to consider sustainability metrics including energy consumption and carbon footprint along with measures of model quality and system performance.", "doc_id": "wu2021a", "page": 9, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "On-device AI is becoming more ubiquitously adopted to enable model personalization [84], [85], [86] while improving data privacy [87], [88], [89], [90], yet its impact in terms of carbon emission is often overlooked. On-device learning emits non-negligible carbon. Figure 11 illustrates that the operational carbon footprint for training a small ML task using federated learning (FL) is comparable to that of training an orders-of-magnitude larger Transformer-based model in a centralized setting. As FL trains local models on client devices and periodically aggregates the model parameters for a global model, without collecting raw user data [87], the FL process can emit non-negligible carbon at the edge due to both computation and wireless communication. It is important to reduce AI’s environmental footprint at the edge. With the ever-increasing demand for on-device use cases over billions of client devices, such as teaching AI to understand the physical environment from the first-person perception [91] or personalizing AI tasks, the carbon footprint for on-device AI can add up to a dire amount quickly. Also, renewable energy is far more limited for client devices compared to datacenters. Optimizing the overall energy efficiency of FL and on-device AI is an important first step [92], [93], [94], [95], [96]. Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices. It is particularly challenging to amortize the embodied carbon footprint because client devices are often under-utilized [97].\n\nV. Call-to-Action\n\nA. Development of Easy-to-Adopt Telemetry for Assessing AI’s Environmental Footprint\n\nWhile the open source community has started building tools to enable automatic measurement of AI training’s environmental footprint [39], [40], [98], [99] and the ML research community requiring a broader impact statement for the submitted research manuscript, more can be done in order to incorporate efficiency and sustainability into the design process. Enabling carbon accounting methodologies and telemetry that is easy to adopt is an important step to quantify the significance of our progress in developing AI technologies in an environmentally-responsible manner. While assessing the novelty and quality of ML solutions, it is crucial to consider sustainability metrics including energy consumption and carbon footprint along with measures of model quality and system performance.", "original_types": ["text", "header"], "id": 1115}
{"type": "section", "content": "Metrics for AI Model and System Life Cycles\n\nStandard carbon footprint accounting methods for AI’s overall carbon footprint are at a nascent stage. We need simple, easy-to-adopt metrics to make fair and useful comparisons between AI innovations. Many different aspects must be accounted for, including the life cycles of both AI models (Data, Experimentation, Training, Deployment) and system hardware (Manufacturing and Use) (Section II).\n\nIn addition to incorporating an efficiency measure as part of leader boards for various ML tasks, data [100], models7, training algorithms [101], environmental impact must also be considered and adopted by AI system hardware developers. For example, MLPerf [102], [103], [104] is the industry standard for ML system performance comparison. The industry has witnessed significantly higher system performance speedup, outstripping what is enabled by Moore’s Law [105], [106]. Moreover, an algorithm efficiency benchmark is under development8. The MLPerf benchmark standards can advance the field of AI in an environmentally-competitive manner by enabling the measurement of energy and/or carbon footprint.\n\nCarbon Impact Statements and Model Cards\n\nWe believe it is important for all published research papers to disclose the operational and embodied carbon footprint of proposed design; we are only at the beginning of this journey9. Note, while embodied carbon footprints for AI hardware may not be readily available, describing hardware platforms, the number of machines, total runtime used to produce results presented in a research manuscript is an important first step. In addition, new models must be associated with a model card that, among other aspects of data sets and models [107], describes the model’s overall carbon footprint to train and conduct inference.\n\nVI. Key Takeaways\n\nThe Growth of AI: Deep learning has witnessed an exponential growth in training data, model parameters, and system resources over the recent years (Figure 2). The amount of data for AI has grown by 2.4×, leading to 3.2× increase in the data ingestion bandwidth demand at Facebook. Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021. The explosive growth in AI use cases has driven 2.9× and 2.5× capacity increases for AI training and inference at Facebook over the recent 18 months, respectively. The environmental footprint of AI is staggering (Figure 4, Figure 5).", "doc_id": "wu2021a", "page": 10, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Metrics for AI Model and System Life Cycles\n\nStandard carbon footprint accounting methods for AI’s overall carbon footprint are at a nascent stage. We need simple, easy-to-adopt metrics to make fair and useful comparisons between AI innovations. Many different aspects must be accounted for, including the life cycles of both AI models (Data, Experimentation, Training, Deployment) and system hardware (Manufacturing and Use) (Section II).\n\nIn addition to incorporating an efficiency measure as part of leader boards for various ML tasks, data [100], models7, training algorithms [101], environmental impact must also be considered and adopted by AI system hardware developers. For example, MLPerf [102], [103], [104] is the industry standard for ML system performance comparison. The industry has witnessed significantly higher system performance speedup, outstripping what is enabled by Moore’s Law [105], [106]. Moreover, an algorithm efficiency benchmark is under development8. The MLPerf benchmark standards can advance the field of AI in an environmentally-competitive manner by enabling the measurement of energy and/or carbon footprint.\n\nCarbon Impact Statements and Model Cards\n\nWe believe it is important for all published research papers to disclose the operational and embodied carbon footprint of proposed design; we are only at the beginning of this journey9. Note, while embodied carbon footprints for AI hardware may not be readily available, describing hardware platforms, the number of machines, total runtime used to produce results presented in a research manuscript is an important first step. In addition, new models must be associated with a model card that, among other aspects of data sets and models [107], describes the model’s overall carbon footprint to train and conduct inference.\n\nVI. Key Takeaways\n\nThe Growth of AI: Deep learning has witnessed an exponential growth in training data, model parameters, and system resources over the recent years (Figure 2). The amount of data for AI has grown by 2.4×, leading to 3.2× increase in the data ingestion bandwidth demand at Facebook. Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021. The explosive growth in AI use cases has driven 2.9× and 2.5× capacity increases for AI training and inference at Facebook over the recent 18 months, respectively. The environmental footprint of AI is staggering (Figure 4, Figure 5).", "original_types": ["text", "header"], "id": 1116}
{"type": "section", "content": "A Holistic Approach: To ensure an environmentally-sustainable growth of AI, we must consider the AI ecosystem holistically going forward. We must look at the machine learning pipelines end-to-end — data collection, model exploration and experimentation, model training, optimization and runtime inference (Section II). The frequency of training and scale of each stage of the ML pipeline must be considered to understand salient bottlenecks to sustainable AI. From the system’s perspective, the life cycle of model development and system hardware, including manufacturing and operational use, must also be accounted for.\n\nEfficiency Optimization\n\nOptimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. Along with other efficiency optimization at-scale, this has translated into 25.8% operational energy footprint reduction over the two-year period. More must be done to bend the environmental impact from the exponential growth of AI (Figure 8 and Figure 9).\n\nAn Sustainability Mindset for AI: Optimization beyond efficiency across the software and hardware stack at scale is crucial to enabling future sustainable AI systems. To develop AI technologies responsibly, we must achieve competitive model accuracy at a fixed or even reduced computational and environmental cost. We chart out potentially high-impact research and development directions across the data, algorithms and model, experimentation and system hardware, and telemetry dimensions for AI at datacenters and at the edge (Section IV).\n\nWe must take a deliberate approach when developing AI research and technologies, considering the environmental impact of innovations and taking a responsible approach to technology development [108]. That is, we need AI to be green and environmentally-sustainable.", "doc_id": "wu2021a", "page": 10, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "A Holistic Approach: To ensure an environmentally-sustainable growth of AI, we must consider the AI ecosystem holistically going forward. We must look at the machine learning pipelines end-to-end — data collection, model exploration and experimentation, model training, optimization and runtime inference (Section II). The frequency of training and scale of each stage of the ML pipeline must be considered to understand salient bottlenecks to sustainable AI. From the system’s perspective, the life cycle of model development and system hardware, including manufacturing and operational use, must also be accounted for.\n\nEfficiency Optimization\n\nOptimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. Along with other efficiency optimization at-scale, this has translated into 25.8% operational energy footprint reduction over the two-year period. More must be done to bend the environmental impact from the exponential growth of AI (Figure 8 and Figure 9).\n\nAn Sustainability Mindset for AI: Optimization beyond efficiency across the software and hardware stack at scale is crucial to enabling future sustainable AI systems. To develop AI technologies responsibly, we must achieve competitive model accuracy at a fixed or even reduced computational and environmental cost. We chart out potentially high-impact research and development directions across the data, algorithms and model, experimentation and system hardware, and telemetry dimensions for AI at datacenters and at the edge (Section IV).\n\nWe must take a deliberate approach when developing AI research and technologies, considering the environmental impact of innovations and taking a responsible approach to technology development [108]. That is, we need AI to be green and environmentally-sustainable.", "original_types": ["text", "header"], "id": 1117}
{"type": "section", "content": "REFERENCES\n\n[1] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis, “Highly accurate protein structure prediction with alphafold,” Nature, 2021.\n\n[2] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue generation,” arXiv:2107.07566, 2021.\n\n[3] D. Silver, T. Hubert, J. Schrittwieser, and D. Hassabis, “AlphaZero: Shedding new light on chess, shogi, and Go,” 2018.\n\n[4] C. L. Zitnick, L. Chanussot, A. Das, S. Goyal, J. Heras-Domingo, C. Ho, W. Hu, T. Lavril, A. Palizhati, M. Riviere, M. Shuaibi, A. Sriram, K. Tran, B. Wood, J. Yoon, D. Parikh, and Z. Ulissi, “An introduction to electrocatalyst design using machine learning for renewable energy storage,” arXiv preprint arXiv:2010.09435, 2020.\n\n[5] C. Elkin and S. Witherspoon, “Machine learning can boost the value of wind energy,” 2019.\n\n[6] R. Evans and J. Gao, “DeepMind AI Reduces Google Data Centre Cooling Bill by 40%,” 2016.\n\n[7] K. Sheikh, “A Growing Presence on the Farm: Robots,” February 2020.\n\n[8] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. Lacoste, K. Sankaran, A. S. Ross, N. Milojevic-Dupont, N. Jaques, A. Waldman-Brown, A. Luccioni, T. Maharaj, E. D. Sherwin, S. K. Mukkavilli, K. P. Kording, C. Gomes, A. Y. Ng, D. Hassabis, J. C. Platt, F. Creutzig, J. Chayes, and Y. Bengio, “Tackling climate change with machine learning,” arXiv:1906.05433, 2019.\n\n[9] R. Nishant, M. Kennedy, and J. Corbett, “Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda,” International Journal of Information Management, vol. 53, 2020.\n\n[10] Facts and Factors, “Global artificial intelligence market,” 2021.\n\n[11] D. Mudigere, Y. Hao, J. Huang, A. Tulloch, S. Sridharan, X. Liu, M. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko, A. Basant, Y. Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli, C. Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y. Ma, J. Yang, E. Wen, H. Li, L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. R. Kishore, T. Graf, A. Eisenman, K. K. Matam, A. Gangidi, G. J. Chen, M. Krishnan, A. Nayak, K. Nair, B. Muthiah, M. khorashadi, P. Bhattacharya, P. Lapukhov, M. Naumov, L. Qiao, M. Smelyanskiy, B. Jia, and V. Rao, “Software-hardware co-design for fast and scalable training of deep learning recommendation models,” arXiv preprint arXiv:2104.05158, 2021.\n\n[12] D. Hernandez and T. B. Brown, “Measuring the algorithmic efficiency of neural networks,” arXiv preprint arXiv:2005.04305, 2020.", "doc_id": "wu2021a", "page": 11, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "REFERENCES\n\n[1] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis, “Highly accurate protein structure prediction with alphafold,” Nature, 2021.\n\n[2] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue generation,” arXiv:2107.07566, 2021.\n\n[3] D. Silver, T. Hubert, J. Schrittwieser, and D. Hassabis, “AlphaZero: Shedding new light on chess, shogi, and Go,” 2018.\n\n[4] C. L. Zitnick, L. Chanussot, A. Das, S. Goyal, J. Heras-Domingo, C. Ho, W. Hu, T. Lavril, A. Palizhati, M. Riviere, M. Shuaibi, A. Sriram, K. Tran, B. Wood, J. Yoon, D. Parikh, and Z. Ulissi, “An introduction to electrocatalyst design using machine learning for renewable energy storage,” arXiv preprint arXiv:2010.09435, 2020.\n\n[5] C. Elkin and S. Witherspoon, “Machine learning can boost the value of wind energy,” 2019.\n\n[6] R. Evans and J. Gao, “DeepMind AI Reduces Google Data Centre Cooling Bill by 40%,” 2016.\n\n[7] K. Sheikh, “A Growing Presence on the Farm: Robots,” February 2020.\n\n[8] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. Lacoste, K. Sankaran, A. S. Ross, N. Milojevic-Dupont, N. Jaques, A. Waldman-Brown, A. Luccioni, T. Maharaj, E. D. Sherwin, S. K. Mukkavilli, K. P. Kording, C. Gomes, A. Y. Ng, D. Hassabis, J. C. Platt, F. Creutzig, J. Chayes, and Y. Bengio, “Tackling climate change with machine learning,” arXiv:1906.05433, 2019.\n\n[9] R. Nishant, M. Kennedy, and J. Corbett, “Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda,” International Journal of Information Management, vol. 53, 2020.\n\n[10] Facts and Factors, “Global artificial intelligence market,” 2021.\n\n[11] D. Mudigere, Y. Hao, J. Huang, A. Tulloch, S. Sridharan, X. Liu, M. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko, A. Basant, Y. Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli, C. Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y. Ma, J. Yang, E. Wen, H. Li, L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. R. Kishore, T. Graf, A. Eisenman, K. K. Matam, A. Gangidi, G. J. Chen, M. Krishnan, A. Nayak, K. Nair, B. Muthiah, M. khorashadi, P. Bhattacharya, P. Lapukhov, M. Naumov, L. Qiao, M. Smelyanskiy, B. Jia, and V. Rao, “Software-hardware co-design for fast and scalable training of deep learning recommendation models,” arXiv preprint arXiv:2104.05158, 2021.\n\n[12] D. Hernandez and T. B. Brown, “Measuring the algorithmic efficiency of neural networks,” arXiv preprint arXiv:2005.04305, 2020.", "original_types": ["text", "header"], "id": 1118}
{"type": "section", "content": "[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n\n[14] P. Nayak, “Understanding searches better than ever before,” 2019.\n\n[15] X. Yi, Y.-F. Chen, S. Ramesh, V. Rajashekhhar, L. Hong, N. Fiedel, N. Seshadri, L. Heldt, X. Wu, and E. H. Chi, “Factorized deep retrieval and distributed tensorflow serving,” in Proceedings of Machine Learning and Systems, 2018.\n\n[16] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, “Distributed hierarchical gpu parameter server for massive scale deep learning ads systems,” arXiv preprint arXiv:2003.05622, 2020.\n\n[17] M. Lui, Y. Yetim, O. Ozkan, Z. Zhao, S.-Y. Tsai, C.-J. Wu, and M. Hempstead, “Understanding capacity-driven scale-out neural recommendation inference,” in Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software, 2021.\n\n[18] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He, “Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning,” arXiv preprint arXiv:2104.07857, 2021.\n\n[19] U. Gupta, Y. Kim, S. Lee, J. Tse, H. S. Lee, G. Wei, D. Brooks, and C. Wu, “Chasing carbon: The elusive environmental footprint of computing,” in Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, 2021.\n\n[20] N. Tomasev, J. Cornebise, F. Hutter, S. Mohamed, A. Picciariello, B. Connelly, D. Belgrave, D. Ezer, F. C. van der Haert, F. Mugisha, G. Abila, H. Arai, H. Almiraat, J. Proskurnia, K. Snyder, M. Otake-Matsuura, M. Othman, T. Glasmachers, W. D. Wever, Y. Teh, M. E. Khan, R. D. Winne, T. Schaul, and C. Clopath, “Ai for social good: unlocking the opportunity for positive impact,” Nature Communications, vol. 11, 2020.\n\n[21] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,” arXiv preprint arXiv:2104.10350, 2021.\n\n[22] EPA, “United states environmental protection agency greenhouse gas equivalencies calculator,” 2021.\n\n[23] Facebook, “2020 sustainability report,” 2021.\n\n[24] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang, “Applied machine learning at facebook: A datacenter infrastructure perspective,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2018.", "doc_id": "wu2021a", "page": 11, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n\n[14] P. Nayak, “Understanding searches better than ever before,” 2019.\n\n[15] X. Yi, Y.-F. Chen, S. Ramesh, V. Rajashekhhar, L. Hong, N. Fiedel, N. Seshadri, L. Heldt, X. Wu, and E. H. Chi, “Factorized deep retrieval and distributed tensorflow serving,” in Proceedings of Machine Learning and Systems, 2018.\n\n[16] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, “Distributed hierarchical gpu parameter server for massive scale deep learning ads systems,” arXiv preprint arXiv:2003.05622, 2020.\n\n[17] M. Lui, Y. Yetim, O. Ozkan, Z. Zhao, S.-Y. Tsai, C.-J. Wu, and M. Hempstead, “Understanding capacity-driven scale-out neural recommendation inference,” in Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software, 2021.\n\n[18] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He, “Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning,” arXiv preprint arXiv:2104.07857, 2021.\n\n[19] U. Gupta, Y. Kim, S. Lee, J. Tse, H. S. Lee, G. Wei, D. Brooks, and C. Wu, “Chasing carbon: The elusive environmental footprint of computing,” in Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, 2021.\n\n[20] N. Tomasev, J. Cornebise, F. Hutter, S. Mohamed, A. Picciariello, B. Connelly, D. Belgrave, D. Ezer, F. C. van der Haert, F. Mugisha, G. Abila, H. Arai, H. Almiraat, J. Proskurnia, K. Snyder, M. Otake-Matsuura, M. Othman, T. Glasmachers, W. D. Wever, Y. Teh, M. E. Khan, R. D. Winne, T. Schaul, and C. Clopath, “Ai for social good: unlocking the opportunity for positive impact,” Nature Communications, vol. 11, 2020.\n\n[21] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,” arXiv preprint arXiv:2104.10350, 2021.\n\n[22] EPA, “United states environmental protection agency greenhouse gas equivalencies calculator,” 2021.\n\n[23] Facebook, “2020 sustainability report,” 2021.\n\n[24] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang, “Applied machine learning at facebook: A datacenter infrastructure perspective,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2018.", "original_types": ["text"], "id": 1119}
{"type": "section", "content": "[25] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, “Unsupervised cross-lingual representation learning at scale,” arXiv preprint arXiv:1911.02116, 2020.\n\n[26] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich, I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira, X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy, “Deep learning recommendation model for personalization and recommendation systems,” arXiv preprint arXiv:1906.00091, 2019.\n\n[27] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee, A. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang, “The architectural implications of facebook’s dnn-based personalized recommendation,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2020.\n\n[28] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in nlp,” arXiv preprint arXiv:1906.02243, 2019.\n\n[29] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” CoRR, vol. abs/2101.03961, 2021.\n\n[30] D. Adiwardana and T. Luong, “Towards a conversational agent that can chat about... anything,” 2020.\n\n[31] Apple, “Product environmental report Mac Pro,” 2019.\n\n[32] NVIDIA, “Faster Transformer,” 2021.\n\n[33] L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril, A. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher, D. Mudigere, M. Naumov, M. Schatz, M. Smelyanskiy, X. Wang, B. Reagen, C.-J. Wu, M. Hempstead, and X. Zhang, “Recnmp: Accelerating personalized recommendation with near-memory processing,” in Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\n[34] Z. Deng, J. Park, P. T. P. Tang, H. Liu, J. Yang, H. Yuen, J. Huang, D. Khudia, X. Wei, E. Wen, D. Choudhary, R. Krishnamoorthi, C.-J. Wu, S. Nadathur, C. Kim, M. Naumov, S. Naghshineh, and M. Smelyanskiy, “Low-precision hardware architectures meet recommendation model inference at scale,” IEEE Micro, vol. 41, no. 5, pp. 93–100, 2021.\n\n[35] L. Wesolowski, B. Acun, V. Andrei, A. Aziz, G. Dankel, C. Gregg, X. Meng, C. Meurillon, D. Sheahan, L. Tian, J. Yang, P. Yu, and K. Hazelwood, “Datacenter-scale analysis and optimization of gpu machine learning workloads,” IEEE Micro, vol. 41, no. 5, 2021.\n\n[36] A. Sriraman, A. Dhanotia, and T. F. Wenisch, “Softsku: Optimizing server architectures for microservice diversity @scale,” in Proceedings of the 46th International Symposium on Computer Architecture, Association for Computing Machinery, 2019.", "doc_id": "wu2021a", "page": 11, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "[25] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, “Unsupervised cross-lingual representation learning at scale,” arXiv preprint arXiv:1911.02116, 2020.\n\n[26] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich, I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira, X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy, “Deep learning recommendation model for personalization and recommendation systems,” arXiv preprint arXiv:1906.00091, 2019.\n\n[27] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee, A. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang, “The architectural implications of facebook’s dnn-based personalized recommendation,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2020.\n\n[28] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in nlp,” arXiv preprint arXiv:1906.02243, 2019.\n\n[29] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” CoRR, vol. abs/2101.03961, 2021.\n\n[30] D. Adiwardana and T. Luong, “Towards a conversational agent that can chat about... anything,” 2020.\n\n[31] Apple, “Product environmental report Mac Pro,” 2019.\n\n[32] NVIDIA, “Faster Transformer,” 2021.\n\n[33] L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril, A. Firoozshahian, K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher, D. Mudigere, M. Naumov, M. Schatz, M. Smelyanskiy, X. Wang, B. Reagen, C.-J. Wu, M. Hempstead, and X. Zhang, “Recnmp: Accelerating personalized recommendation with near-memory processing,” in Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\n[34] Z. Deng, J. Park, P. T. P. Tang, H. Liu, J. Yang, H. Yuen, J. Huang, D. Khudia, X. Wei, E. Wen, D. Choudhary, R. Krishnamoorthi, C.-J. Wu, S. Nadathur, C. Kim, M. Naumov, S. Naghshineh, and M. Smelyanskiy, “Low-precision hardware architectures meet recommendation model inference at scale,” IEEE Micro, vol. 41, no. 5, pp. 93–100, 2021.\n\n[35] L. Wesolowski, B. Acun, V. Andrei, A. Aziz, G. Dankel, C. Gregg, X. Meng, C. Meurillon, D. Sheahan, L. Tian, J. Yang, P. Yu, and K. Hazelwood, “Datacenter-scale analysis and optimization of gpu machine learning workloads,” IEEE Micro, vol. 41, no. 5, 2021.\n\n[36] A. Sriraman, A. Dhanotia, and T. F. Wenisch, “Softsku: Optimizing server architectures for microservice diversity @scale,” in Proceedings of the 46th International Symposium on Computer Architecture, Association for Computing Machinery, 2019.", "original_types": ["text"], "id": 1120}
{"type": "section", "content": "[37] A. Sriraman and A. Dhanotia, “Accelerometer: Understanding acceleration opportunities for data center overheads at hyperscale,” in Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, 2020.\n\n[38] C. Tang, K. Yu, K. Veeraraghavan, J. Kaldor, S. Michelson, T. Kooburat, A. Anbudurai, M. Clark, K. Gogia, L. Cheng, B. Christensen, A. Gartrell,", "doc_id": "wu2021a", "page": 11, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "[37] A. Sriraman and A. Dhanotia, “Accelerometer: Understanding acceleration opportunities for data center overheads at hyperscale,” in Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, 2020.\n\n[38] C. Tang, K. Yu, K. Veeraraghavan, J. Kaldor, S. Michelson, T. Kooburat, A. Anbudurai, M. Clark, K. Gogia, L. Cheng, B. Christensen, A. Gartrell,", "original_types": ["text"], "id": 1121}
{"type": "section", "content": "M. Khutornenko, S. Kulkarni, M. Pawlowski, T. Pelkonen, A. Rodrigues, R. Tibrewal, V. Venkatesan, and P. Zhang, “Twine: A unified cluster management system for shared infrastructure,” in Proceedings of the USENIX Symposium on Operating Systems Design and Implementation, 2020.\n\nA. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quantifying the carbon emissions of machine learning,” Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019, 2019.\n\nP. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, “Towards the systematic reporting of the energy and carbon footprints of machine learning,” CoRR, vol. abs/2002.05651, 2020.\n\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmithell, “On the dangers of stochastic parrots: Can language models be too big?,” in Proceedings of the ACM Conference on Fairness, Accountability, and Transparency, 2021.\n\nN. Sachdeva, C.-J. Wu, and J. McAuley, “Svp-cf: Selection via proxy for collaborative filtering data,” arXiv preprint arXiv:2107.04984, 2021.\n\nE. Valavi, J. Hestness, N. Ardalani, and M. Iansiti, Time and the Value of Data. Working papers, Harvard Business School, 2020.\n\nM. Zhao, N. Agarwal, A. Basant, B. Gedik, S. Pan, M. Ozdal, R. Komuravelli, J. Pan, T. Bao, H. Lu, S. Narayanan, J. Langman, K. Wilfong, H. Rastogi, C. Wu, C. Kozyrakis, and P. Pol, “Understanding and co-designing the data ingestion pipeline for industry-scale recsys training,” CoRR, vol. abs/2108.09373, 2021.\n\nR. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu, and I. Guyon, “Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020,” CoRR, vol. abs/2104.10201, 2021.\n\nP. Ren, Y. Xiao, X. Chang, P.-y. Huang, Z. Li, X. Chen, and X. Wang, “A comprehensive survey of neural architecture search: Challenges and solutions,” ACM Comput. Surv., vol. 54, no. 4, 2021.\n\nQ. Song, D. Cheng, H. Zhou, J. Yang, Y. Tian, and X. Hu, “Towards automated neural interaction discovery for click-through rate prediction,” Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nM. R. Joglekar, C. Li, M. Chen, T. Xu, X. Wang, J. K. Adams, P. Khaitan, J. Liu, and Q. V. Le, “Neural input search for large scale recommendation models,” in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nM. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,” arXiv preprint arXiv:1905.11946, 2020.\n\nD. Eriksson, P. I. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu, S. Zhao, A. Aly, G. Venkatesh, and M. Balandat, “Latency-aware neural architecture search with multi-objective bayesian optimization,” CoRR, vol. abs/2106.11890, 2021.\n\nH. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one network and specialize it for efficient deployment,” arXiv preprint arXiv:1908.09791, 2020.", "doc_id": "wu2021a", "page": 12, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "M. Khutornenko, S. Kulkarni, M. Pawlowski, T. Pelkonen, A. Rodrigues, R. Tibrewal, V. Venkatesan, and P. Zhang, “Twine: A unified cluster management system for shared infrastructure,” in Proceedings of the USENIX Symposium on Operating Systems Design and Implementation, 2020.\n\nA. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quantifying the carbon emissions of machine learning,” Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019, 2019.\n\nP. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, “Towards the systematic reporting of the energy and carbon footprints of machine learning,” CoRR, vol. abs/2002.05651, 2020.\n\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmithell, “On the dangers of stochastic parrots: Can language models be too big?,” in Proceedings of the ACM Conference on Fairness, Accountability, and Transparency, 2021.\n\nN. Sachdeva, C.-J. Wu, and J. McAuley, “Svp-cf: Selection via proxy for collaborative filtering data,” arXiv preprint arXiv:2107.04984, 2021.\n\nE. Valavi, J. Hestness, N. Ardalani, and M. Iansiti, Time and the Value of Data. Working papers, Harvard Business School, 2020.\n\nM. Zhao, N. Agarwal, A. Basant, B. Gedik, S. Pan, M. Ozdal, R. Komuravelli, J. Pan, T. Bao, H. Lu, S. Narayanan, J. Langman, K. Wilfong, H. Rastogi, C. Wu, C. Kozyrakis, and P. Pol, “Understanding and co-designing the data ingestion pipeline for industry-scale recsys training,” CoRR, vol. abs/2108.09373, 2021.\n\nR. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu, and I. Guyon, “Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020,” CoRR, vol. abs/2104.10201, 2021.\n\nP. Ren, Y. Xiao, X. Chang, P.-y. Huang, Z. Li, X. Chen, and X. Wang, “A comprehensive survey of neural architecture search: Challenges and solutions,” ACM Comput. Surv., vol. 54, no. 4, 2021.\n\nQ. Song, D. Cheng, H. Zhou, J. Yang, Y. Tian, and X. Hu, “Towards automated neural interaction discovery for click-through rate prediction,” Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nM. R. Joglekar, C. Li, M. Chen, T. Xu, X. Wang, J. K. Adams, P. Khaitan, J. Liu, and Q. V. Le, “Neural input search for large scale recommendation models,” in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nM. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,” arXiv preprint arXiv:1905.11946, 2020.\n\nD. Eriksson, P. I. Chuang, S. Daulton, P. Xia, A. Shrivastava, A. Babu, S. Zhao, A. Aly, G. Venkatesh, and M. Balandat, “Latency-aware neural architecture search with multi-objective bayesian optimization,” CoRR, vol. abs/2106.11890, 2021.\n\nH. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one network and specialize it for efficient deployment,” arXiv preprint arXiv:1908.09791, 2020.", "original_types": ["text"], "id": 1122}
{"type": "section", "content": "D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu, and D. Marculescu, “Single-path nas: Designing hardware-efficient convnets in less than 4 hours,” arXiv preprint arXiv:1904.02877, 2019.\n\nW. Chen, X. Gong, and Z. Wang, “Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective,” arXiv preprint arXiv:2102.11535, 2021.\n\nJ. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture search without training,” arXiv preprint arXiv:2006.04647, 2021.\n\nB. Acun, M. Murphy, X. Wang, J. Nie, C. Wu, and K. Hazelwood, “Understanding training efficiency of deep learning recommendation models at scale,” in Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, 2021.\n\nC. Yin, B. Acun, X. Liu, and C.-J. Wu, “TT-Rec: Tensor train compression for deep learning recommendation models,” in Proceedings of the Conference on Machine Learning and Systems, 2021.\n\nW.-C. Kang, D. Z. Cheng, T. Yao, X. Yi, T. Chen, L. Hong, and E. H. Chi, “Learning to embed categorical features without embedding tables for recommendation,” arXiv preprint arXiv:2010.10784, 2021.\n\nA. S. Nemirovskij and D. B. Yudin, Problem complexity and method efficiency in optimization. Wiley-Interscience, 1983.\n\nD. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl, “On empirical comparisons of optimizers for deep learning,” arXiv preprint arXiv:1910.05446, 2019.\n\nP. T. Sivaprasad, F. Mai, T. Vogels, M. Jaggi, and F. Fleuret, “Optimizer benchmarking needs to account for hyperparameter tuning,” in Proceedings of the International Conference on Machine Learning, 2020.\n\nP. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training imagenet in 1 hour,” arXiv preprint arXiv:1706.02677, 2017.\n\nM. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine translation,” arXiv preprint arXiv:1806.00187, 2018.\n\nD. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “Qsgd: Communication-efficient sgd via gradient quantization and encoding,” in Proceedings of the Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nT. Vogels, S. P. Karinireddy, and M. Jaggi, “Powersgd: Practical low-rank gradient compression for distributed optimization,” in Proceedings of the Advances In Neural Information Processing Systems, vol. 32, 2019.\n\nY. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al., “Gpipe: Efficient training of giant neural networks using pipeline parallelism,” in Proceedings of the Advances in neural information processing systems, vol. 32, 2019.\n\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory optimizations toward training trillion parameter models,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2020.", "doc_id": "wu2021a", "page": 12, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "D. Stamoulis, R. Ding, D. Wang, D. Lymberopoulos, B. Priyantha, J. Liu, and D. Marculescu, “Single-path nas: Designing hardware-efficient convnets in less than 4 hours,” arXiv preprint arXiv:1904.02877, 2019.\n\nW. Chen, X. Gong, and Z. Wang, “Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective,” arXiv preprint arXiv:2102.11535, 2021.\n\nJ. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture search without training,” arXiv preprint arXiv:2006.04647, 2021.\n\nB. Acun, M. Murphy, X. Wang, J. Nie, C. Wu, and K. Hazelwood, “Understanding training efficiency of deep learning recommendation models at scale,” in Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, 2021.\n\nC. Yin, B. Acun, X. Liu, and C.-J. Wu, “TT-Rec: Tensor train compression for deep learning recommendation models,” in Proceedings of the Conference on Machine Learning and Systems, 2021.\n\nW.-C. Kang, D. Z. Cheng, T. Yao, X. Yi, T. Chen, L. Hong, and E. H. Chi, “Learning to embed categorical features without embedding tables for recommendation,” arXiv preprint arXiv:2010.10784, 2021.\n\nA. S. Nemirovskij and D. B. Yudin, Problem complexity and method efficiency in optimization. Wiley-Interscience, 1983.\n\nD. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl, “On empirical comparisons of optimizers for deep learning,” arXiv preprint arXiv:1910.05446, 2019.\n\nP. T. Sivaprasad, F. Mai, T. Vogels, M. Jaggi, and F. Fleuret, “Optimizer benchmarking needs to account for hyperparameter tuning,” in Proceedings of the International Conference on Machine Learning, 2020.\n\nP. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training imagenet in 1 hour,” arXiv preprint arXiv:1706.02677, 2017.\n\nM. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine translation,” arXiv preprint arXiv:1806.00187, 2018.\n\nD. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “Qsgd: Communication-efficient sgd via gradient quantization and encoding,” in Proceedings of the Advances in Neural Information Processing Systems, vol. 30, 2017.\n\nT. Vogels, S. P. Karinireddy, and M. Jaggi, “Powersgd: Practical low-rank gradient compression for distributed optimization,” in Proceedings of the Advances In Neural Information Processing Systems, vol. 32, 2019.\n\nY. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al., “Gpipe: Efficient training of giant neural networks using pipeline parallelism,” in Proceedings of the Advances in neural information processing systems, vol. 32, 2019.\n\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory optimizations toward training trillion parameter models,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2020.", "original_types": ["text"], "id": 1123}
{"type": "section", "content": "J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters,” in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nN. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon, “In-datacenter performance analysis of a tensor processing unit,” in Proceedings of the ACM/IEEE International Symposium on Computer Architecture, 2017.\n\nJ. Hamilton, “AWS Inferentia Machine Learning Processor,” 2018.\n\nAzure, “New Azure HPC and partner offerings at Supercomputing 19,” 2019.\n\nNVIDIA, “GPUs for Virtualization,” 2021.\n\nA. Spiridonov, “New Cloud TPU VMs make training your ML models on TPUs easier than ever,” 2021.\n\nM. Gschwind, T. Kaldewey, and D. Tam, “Optimizing the efficiency of deep learning through accelerator virtualization,” IBM Journal of Research and Development, vol. 61, no. 4-5, 2017.\n\nS. Ghodrati, B. H. Ahn, J. Kyung Kim, S. Kinzer, B. R. Yatham, N. Alla, H. Sharma, M. Alian, E. Ebrahimi, N. S. Kim, C. Young, and H. Esmaeilzadeh, “Planaria: Dynamic architecture fission for spatial multi-tenant acceleration of deep neural networks,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2020.\n\nS.-C. Kao and T. Krishna, “Domain-specific genetic algorithm for multi-tenant dnnaccelerator scheduling,” arXiv preprint arXiv:2104.13997, 2021.\n\nM. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang, “Analysis of large-scale multi-tenant gpu clusters for dnn training workloads,” in Proceedings of the USENIX Annual Technical Conference, 2019.\n\nP. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives for deep learning applications,” arXiv preprint arXiv:1902.04610, 2019.\n\nR. Jain and J. Wullert, “Challenges: Environmental design for pervasive computing systems,” in Proceedings of the International Conference on Mobile Computing and Networking, 2002.\n\nJ. Chang, J. Meza, P. Ranganathan, C. Bash, and A. Shah, “Green server design: Beyond operational energy to sustainability,” in Proceedings of", "doc_id": "wu2021a", "page": 12, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters,” in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n\nN. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers, R. Boyle, P.-l. Cantin, C. Chao, C. Clark, J. Coriell, M. Daley, M. Dau, J. Dean, B. Gelb, T. V. Ghaemmaghami, R. Gottipati, W. Gulland, R. Hagmann, C. R. Ho, D. Hogberg, J. Hu, R. Hundt, D. Hurt, J. Ibarz, A. Jaffey, A. Jaworski, A. Kaplan, H. Khaitan, D. Killebrew, A. Koch, N. Kumar, S. Lacy, J. Laudon, J. Law, D. Le, C. Leary, Z. Liu, K. Lucke, A. Lundin, G. MacKean, A. Maggiore, M. Mahony, K. Miller, R. Nagarajan, R. Narayanaswami, R. Ni, K. Nix, T. Norrie, M. Omernick, N. Penukonda, A. Phelps, J. Ross, M. Ross, A. Salek, E. Samadiani, C. Severn, G. Sizikov, M. Snelham, J. Souter, D. Steinberg, A. Swing, M. Tan, G. Thorson, B. Tian, H. Toma, E. Tuttle, V. Vasudevan, R. Walter, W. Wang, E. Wilcox, and D. H. Yoon, “In-datacenter performance analysis of a tensor processing unit,” in Proceedings of the ACM/IEEE International Symposium on Computer Architecture, 2017.\n\nJ. Hamilton, “AWS Inferentia Machine Learning Processor,” 2018.\n\nAzure, “New Azure HPC and partner offerings at Supercomputing 19,” 2019.\n\nNVIDIA, “GPUs for Virtualization,” 2021.\n\nA. Spiridonov, “New Cloud TPU VMs make training your ML models on TPUs easier than ever,” 2021.\n\nM. Gschwind, T. Kaldewey, and D. Tam, “Optimizing the efficiency of deep learning through accelerator virtualization,” IBM Journal of Research and Development, vol. 61, no. 4-5, 2017.\n\nS. Ghodrati, B. H. Ahn, J. Kyung Kim, S. Kinzer, B. R. Yatham, N. Alla, H. Sharma, M. Alian, E. Ebrahimi, N. S. Kim, C. Young, and H. Esmaeilzadeh, “Planaria: Dynamic architecture fission for spatial multi-tenant acceleration of deep neural networks,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2020.\n\nS.-C. Kao and T. Krishna, “Domain-specific genetic algorithm for multi-tenant dnnaccelerator scheduling,” arXiv preprint arXiv:2104.13997, 2021.\n\nM. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang, “Analysis of large-scale multi-tenant gpu clusters for dnn training workloads,” in Proceedings of the USENIX Annual Technical Conference, 2019.\n\nP. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives for deep learning applications,” arXiv preprint arXiv:1902.04610, 2019.\n\nR. Jain and J. Wullert, “Challenges: Environmental design for pervasive computing systems,” in Proceedings of the International Conference on Mobile Computing and Networking, 2002.\n\nJ. Chang, J. Meza, P. Ranganathan, C. Bash, and A. Shah, “Green server design: Beyond operational energy to sustainability,” in Proceedings of", "original_types": ["text"], "id": 1124}
{"type": "section", "content": "the International Conference on Power Aware Computing and Systems, 2010.\n\nM. Garcia Bardon, P. Wuytens, L.-A. Ragnarsson, G. Mirabelli, D. Jang, G. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais, “DTCO including sustainability: Power-performance-area-cost-environmental score (PPACE) analysis for logic technologies,” in Proceedings of the IEEE International Electron Devices Meeting, 2020.\n\nA. Putnam, A. M. Caulfield, E. S. Chung, D. Chiou, K. Constantinides, J. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray, M. Haselman, S. Hauck, S. Heil, A. Hormati, J.-Y. Kim, S. Lanka, J. Larus, E. Peterson, S. Pope, A. Smith, J. Thong, P. Y. Xiao, and D. Burger, “A reconfigurable fabric for accelerating large-scale datacenter services,” IEEE Micro, 2015.\n\nY.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks,” in Proceedings of the ACM/IEEE International Symposium on Computer Architecture, 2016.\n\nA. Radovanovic, R. Koningstein, I. Schneider, B. Chen, A. Duarte, B. Roy, D. Xiao, M. Haridasan, P. Hung, N. Care, et al., “Carbon-aware computing for datacenters,” arXiv preprint arXiv:2106.11750, 2021.\n\nH. Cai, C. Gan, L. Zhu, and S. Han, “Tinytl: Reduce memory, not parameters for efficient on-device learning,” arXiv preprint arXiv:2007.11622, 2020.\n\nK. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and D. Ramage, “Federated evaluation of on-device personalization,” arXiv preprint arXiv:1910.10252, 2019.\n\nK. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konečnỳ, S. Mazzocchi, H. B. McMahan, et al., “Towards federated learning at scale: System design,” arXiv preprint arXiv:1902.01046, 2019.\n\nA. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile keyboard prediction,” arXiv preprint arXiv:1811.03604, 2018.\n\nT. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and F. Beaufays, “Applied federated learning: Improving google keyboard query suggestions,” arXiv preprint arXiv:1812.02903, 2018.\n\nS. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated learning for emoji prediction in a mobile keyboard,” arXiv preprint arXiv:1906.04329, 2019.\n\nD. Huba, J. Nguyen, K. Malik, R. Zhu, M. Rabbat, A. Yousefpour, C.-J. Wu, H. Zhan, P. Ustinov, H. Srinivas, K. Wang, A. Shoumikhin, J. Min, and M. Malek, “Papaya: Practical, private, and scalable federated learning,” arXiv:2111.04877, 2021.", "doc_id": "wu2021a", "page": 13, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "the International Conference on Power Aware Computing and Systems, 2010.\n\nM. Garcia Bardon, P. Wuytens, L.-A. Ragnarsson, G. Mirabelli, D. Jang, G. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais, “DTCO including sustainability: Power-performance-area-cost-environmental score (PPACE) analysis for logic technologies,” in Proceedings of the IEEE International Electron Devices Meeting, 2020.\n\nA. Putnam, A. M. Caulfield, E. S. Chung, D. Chiou, K. Constantinides, J. Demme, H. Esmaeilzadeh, J. Fowers, G. P. Gopal, J. Gray, M. Haselman, S. Hauck, S. Heil, A. Hormati, J.-Y. Kim, S. Lanka, J. Larus, E. Peterson, S. Pope, A. Smith, J. Thong, P. Y. Xiao, and D. Burger, “A reconfigurable fabric for accelerating large-scale datacenter services,” IEEE Micro, 2015.\n\nY.-H. Chen, J. Emer, and V. Sze, “Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks,” in Proceedings of the ACM/IEEE International Symposium on Computer Architecture, 2016.\n\nA. Radovanovic, R. Koningstein, I. Schneider, B. Chen, A. Duarte, B. Roy, D. Xiao, M. Haridasan, P. Hung, N. Care, et al., “Carbon-aware computing for datacenters,” arXiv preprint arXiv:2106.11750, 2021.\n\nH. Cai, C. Gan, L. Zhu, and S. Han, “Tinytl: Reduce memory, not parameters for efficient on-device learning,” arXiv preprint arXiv:2007.11622, 2020.\n\nK. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and D. Ramage, “Federated evaluation of on-device personalization,” arXiv preprint arXiv:1910.10252, 2019.\n\nK. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konečnỳ, S. Mazzocchi, H. B. McMahan, et al., “Towards federated learning at scale: System design,” arXiv preprint arXiv:1902.01046, 2019.\n\nA. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile keyboard prediction,” arXiv preprint arXiv:1811.03604, 2018.\n\nT. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and F. Beaufays, “Applied federated learning: Improving google keyboard query suggestions,” arXiv preprint arXiv:1812.02903, 2018.\n\nS. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated learning for emoji prediction in a mobile keyboard,” arXiv preprint arXiv:1906.04329, 2019.\n\nD. Huba, J. Nguyen, K. Malik, R. Zhu, M. Rabbat, A. Yousefpour, C.-J. Wu, H. Zhan, P. Ustinov, H. Srinivas, K. Wang, A. Shoumikhin, J. Min, and M. Malek, “Papaya: Practical, private, and scalable federated learning,” arXiv:2111.04877, 2021.", "original_types": ["text"], "id": 1125}
{"type": "section", "content": "K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, C. Fuegen, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik, “Ego4d: Around the world in 3,000 hours of egocentric video,” arXiv:2110.07058, 2021.\n\nY. G. Kim and C.-J. Wu, “Autofl: Enabling heterogeneity-aware energy efficient federated learning,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2021.\n\nY. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang, “Neurosurgeon: Collaborative intelligence between the cloud and mobile edge,” in Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, 2017.\n\nY. G. Kim and C.-J. Wu, “Autoscale: Energy efficiency optimization for stochastic edge inference using reinforcement learning,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2020.\n\nT.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efficient convolutional neural networks using energy-aware pruning,” arXiv:1611.05128, 2017.\n\nD. Stamoulis, T.-W. R. Chin, A. K. Prakash, H. Fang, S. Sajja, M. Bognar, and D. Marculescu, “Designing adaptive neural networks for energy-constrained image classification,” in Proceedings of the International Conference on Computer-Aided Design, 2018.\n\nC. Gao, A. Gutierrez, M. Rajan, R. G. Dreslinski, T. Mudge, and C.-J. Wu, “A study of mobile device utilization,” in Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software, 2015.\n\nV. Schmidt, K. Goyal, A. Joshi, B. Feld, L. Conell, N. Laskaris, D. Blank, J. Wilson, S. Friedler, and S. Luccioni, “CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing,” 2021.\n\nK. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, “Energy usage reports: Environmental awareness as part of algorithmic accountability,” Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019, 2019.", "doc_id": "wu2021a", "page": 13, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, C. Fuegen, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik, “Ego4d: Around the world in 3,000 hours of egocentric video,” arXiv:2110.07058, 2021.\n\nY. G. Kim and C.-J. Wu, “Autofl: Enabling heterogeneity-aware energy efficient federated learning,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2021.\n\nY. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang, “Neurosurgeon: Collaborative intelligence between the cloud and mobile edge,” in Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, 2017.\n\nY. G. Kim and C.-J. Wu, “Autoscale: Energy efficiency optimization for stochastic edge inference using reinforcement learning,” in Proceedings of the IEEE/ACM International Symposium on Microarchitecture, 2020.\n\nT.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efficient convolutional neural networks using energy-aware pruning,” arXiv:1611.05128, 2017.\n\nD. Stamoulis, T.-W. R. Chin, A. K. Prakash, H. Fang, S. Sajja, M. Bognar, and D. Marculescu, “Designing adaptive neural networks for energy-constrained image classification,” in Proceedings of the International Conference on Computer-Aided Design, 2018.\n\nC. Gao, A. Gutierrez, M. Rajan, R. G. Dreslinski, T. Mudge, and C.-J. Wu, “A study of mobile device utilization,” in Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software, 2015.\n\nV. Schmidt, K. Goyal, A. Joshi, B. Feld, L. Conell, N. Laskaris, D. Blank, J. Wilson, S. Friedler, and S. Luccioni, “CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing,” 2021.\n\nK. Lottick, S. Susai, S. A. Friedler, and J. P. Wilson, “Energy usage reports: Environmental awareness as part of algorithmic accountability,” Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019, 2019.", "original_types": ["text"], "id": 1126}
{"type": "section", "content": "D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams, “Dynabench: Rethinking benchmarking in NLP,” arXiv preprint arXiv:2104.14337, 2021.\n\nD. Hernandez and T. B. Brown, “Measuring the algorithmic efficiency of neural networks,” arXiv preprint arXiv:2005.04305, 2020.\n\nP. Mattson, V. J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter, P. Micikevicius, D. Patterson, G. Schmuelling, H. Tang, G.-Y. Wei, and C.-J. Wu, “Mlperf: An industry standard benchmark suite for machine learning performance,” IEEE Micro, vol. 40, no. 2, pp. 8–16, 2020.\n\nV. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, and C.-J. Wu, “The vision behind mlperf: Understanding ai inference performance,” IEEE Micro, vol. 41, no. 3, pp. 10–18, 2021.\n\nV. J. Reddi, D. Kanter, P. Mattson, J. Duke, T. Nguyen, R. Chukka, K. Shiring, K.-S. Tan, M. Charlebois, W. Chou, M. El-Khamy, J. Hong, M. Buch, C. Trinh, T. Atta-fosu, F. Cakir, M. Charkhabi, X. Chen, J. Chiang, D. Dexter, W. Heo, G. Schmuelling, M. Shabani, and D. Zika, “Mlperf mobile inference benchmark,” arXiv:2012.02328, 2021.\n\nP. Mattson, C. Cheng, G. Diamos, C. Coleman, P. Micikevicius, D. Patterson, H. Tang, G.-Y. Wei, P. Bailis, V. Bittorf, D. Brooks, D. Chen, D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang, D. Kang, D. Kanter, N. Kumar, J. Liao, D. Narayanan, T. Oguntebi, G. Pekhimenko, L. Pentecost, V. Janapa Reddi, T. Robie, T. St John, C.-J. Wu, L. Xu, C. Young, and M. Zaharia, “Mlperf training benchmark,” in Proceedings of Machine Learning and Systems, vol. 2, 2020.\n\nV. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou, “Mlperf inference benchmark,” in Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\nM. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model reporting,” Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nC.-J. Wu, S. Manne, P. Ranganathan, S. Bird, and S. Greenstein, “Socio-technological challenges and opportunities: Paths forward,” arXiv preprint arXiv:2108.06738, 2021.\n\nR. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” arXiv preprint arXiv:1907.10597, 2019.", "doc_id": "wu2021a", "page": 13, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, and A. Williams, “Dynabench: Rethinking benchmarking in NLP,” arXiv preprint arXiv:2104.14337, 2021.\n\nD. Hernandez and T. B. Brown, “Measuring the algorithmic efficiency of neural networks,” arXiv preprint arXiv:2005.04305, 2020.\n\nP. Mattson, V. J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter, P. Micikevicius, D. Patterson, G. Schmuelling, H. Tang, G.-Y. Wei, and C.-J. Wu, “Mlperf: An industry standard benchmark suite for machine learning performance,” IEEE Micro, vol. 40, no. 2, pp. 8–16, 2020.\n\nV. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, and C.-J. Wu, “The vision behind mlperf: Understanding ai inference performance,” IEEE Micro, vol. 41, no. 3, pp. 10–18, 2021.\n\nV. J. Reddi, D. Kanter, P. Mattson, J. Duke, T. Nguyen, R. Chukka, K. Shiring, K.-S. Tan, M. Charlebois, W. Chou, M. El-Khamy, J. Hong, M. Buch, C. Trinh, T. Atta-fosu, F. Cakir, M. Charkhabi, X. Chen, J. Chiang, D. Dexter, W. Heo, G. Schmuelling, M. Shabani, and D. Zika, “Mlperf mobile inference benchmark,” arXiv:2012.02328, 2021.\n\nP. Mattson, C. Cheng, G. Diamos, C. Coleman, P. Micikevicius, D. Patterson, H. Tang, G.-Y. Wei, P. Bailis, V. Bittorf, D. Brooks, D. Chen, D. Dutta, U. Gupta, K. Hazelwood, A. Hock, X. Huang, D. Kang, D. Kanter, N. Kumar, J. Liao, D. Narayanan, T. Oguntebi, G. Pekhimenko, L. Pentecost, V. Janapa Reddi, T. Robie, T. St John, C.-J. Wu, L. Xu, C. Young, and M. Zaharia, “Mlperf training benchmark,” in Proceedings of Machine Learning and Systems, vol. 2, 2020.\n\nV. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou, “Mlperf inference benchmark,” in Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\nM. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru, “Model cards for model reporting,” Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nC.-J. Wu, S. Manne, P. Ranganathan, S. Bird, and S. Greenstein, “Socio-technological challenges and opportunities: Paths forward,” arXiv preprint arXiv:2108.06738, 2021.\n\nR. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” arXiv preprint arXiv:1907.10597, 2019.", "original_types": ["text"], "id": 1127}
{"type": "section", "content": "K. Maeng, S. Bharuka, I. Gao, M. C. Jeffrey, V. Saraph, B.-Y. Su, C. Trippel, J. Yang, M. Rabbat, B. Lucia, and C.-J. Wu, “Cpr: Understanding and improving failure tolerant training for deep learning recommendation with partial recovery,” in Proceedings of the Conference on Machine Learning and Systems, 2021.\n\nA. Eisenman, K. K. Matam, S. Ingram, D. Mudigere, R. Krishnamoorthi, K. Nair, M. Smelyanskiy, and M. Annavaram, “Check-n-run: A checkpointing system for training deep learning recommendation models,” arXiv preprint arXiv:2010.08679, 2021.\n\nH. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy, B. Muthiah, and S. Sankar, “Silent data corruptions at scale,” arXiv preprint arXiv:2102.11245, 2021.", "doc_id": "wu2021a", "page": 13, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "K. Maeng, S. Bharuka, I. Gao, M. C. Jeffrey, V. Saraph, B.-Y. Su, C. Trippel, J. Yang, M. Rabbat, B. Lucia, and C.-J. Wu, “Cpr: Understanding and improving failure tolerant training for deep learning recommendation with partial recovery,” in Proceedings of the Conference on Machine Learning and Systems, 2021.\n\nA. Eisenman, K. K. Matam, S. Ingram, D. Mudigere, R. Krishnamoorthi, K. Nair, M. Smelyanskiy, and M. Annavaram, “Check-n-run: A checkpointing system for training deep learning recommendation models,” arXiv preprint arXiv:2010.08679, 2021.\n\nH. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy, B. Muthiah, and S. Sankar, “Silent data corruptions at scale,” arXiv preprint arXiv:2102.11245, 2021.", "original_types": ["text"], "id": 1128}
{"type": "section", "content": "[113] P. H. Hochschild, P. Turner, J. C. Mogul, R. Govindaraju, P. Ranganathan, D. E. Culler, and A. Vahdat, “Cores that don’t count,” in Proceedings of the Workshop on Hot Topics in Operating Systems, 2021.\n\n[114] X. Qiu, T. Parcollet, J. Fernandez-Marques, P. P. B. de Gusmao, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, “A first look into the carbon footprint of federated learning,” arXiv preprint arXiv:2102.07627, 2021.\n\n[115] H. Wang, B. Kim, J. Xie, and Z. Han, “How is energy consumed in smartphone deep learning apps? executing locally vs. remotely,” in Proceedings of the IEEE Global Communications Conference, 2019.\n\n[116] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan, K. Hazelwood, E. Isaac, Y. Jia, B. Jia, T. Leyvand, H. Lu, Y. Lu, L. Qiao, B. Reagen, J. Spisak, F. Sun, A. Tulloch, P. Vajda, X. Wang, Y. Wang, B. Wasti, Y. Wu, R. Xian, S. Yoo, and P. Zhang, “Machine learning at facebook: Understanding inference at the edge,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2019.\n\n[117] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.\n\n[118] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” in Proceedings of the International conference on machine learning, pp. 1597–1607, 2020.\n\n[119] M. Assran, M. Caron, I. Misra, P. Bojanowski, A. Joulin, N. Ballas, and M. Rabbat, “Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples,” arXiv preprint arXiv:2104.13963, 2021.\n\n[120] L. M. Dery, P. Michel, A. Talwalkar, and G. Neubig, “Should we be pre-training? an argument for end-task aware training as an alternative,” arXiv preprint arXiv:2109.07437, 2021.", "doc_id": "wu2021a", "page": 14, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "[113] P. H. Hochschild, P. Turner, J. C. Mogul, R. Govindaraju, P. Ranganathan, D. E. Culler, and A. Vahdat, “Cores that don’t count,” in Proceedings of the Workshop on Hot Topics in Operating Systems, 2021.\n\n[114] X. Qiu, T. Parcollet, J. Fernandez-Marques, P. P. B. de Gusmao, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, “A first look into the carbon footprint of federated learning,” arXiv preprint arXiv:2102.07627, 2021.\n\n[115] H. Wang, B. Kim, J. Xie, and Z. Han, “How is energy consumed in smartphone deep learning apps? executing locally vs. remotely,” in Proceedings of the IEEE Global Communications Conference, 2019.\n\n[116] C.-J. Wu, D. Brooks, K. Chen, D. Chen, S. Choudhury, M. Dukhan, K. Hazelwood, E. Isaac, Y. Jia, B. Jia, T. Leyvand, H. Lu, Y. Lu, L. Qiao, B. Reagen, J. Spisak, F. Sun, A. Tulloch, P. Vajda, X. Wang, Y. Wang, B. Wasti, Y. Wu, R. Xian, S. Yoo, and P. Zhang, “Machine learning at facebook: Understanding inference at the edge,” in Proceedings of the IEEE International Symposium on High Performance Computer Architecture, 2019.\n\n[117] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.\n\n[118] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” in Proceedings of the International conference on machine learning, pp. 1597–1607, 2020.\n\n[119] M. Assran, M. Caron, I. Misra, P. Bojanowski, A. Joulin, N. Ballas, and M. Rabbat, “Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples,” arXiv preprint arXiv:2104.13963, 2021.\n\n[120] L. M. Dery, P. Michel, A. Talwalkar, and G. Neubig, “Should we be pre-training? an argument for end-task aware training as an alternative,” arXiv preprint arXiv:2109.07437, 2021.", "original_types": ["text"], "id": 1129}
{"type": "figure", "content": "Figure 12: Model quality of recommendation use cases improves as we scale up the amount of data and/or the number of model parameters (e.g., embedding cardinality or dimension), leading to higher energy and carbon footprint. Maximizing model accuracy for the specific recommendation use case comes with significant energy cost — Roughly 4× energy saving can be achieved with only 0.004 model quality degradation (green vs. yellow stars).", "doc_id": "wu2021a", "page": 14, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Figure 12: Model quality of recommendation use cases improves as we scale up the amount of data and/or the number of model parameters (e.g., embedding cardinality or dimension), leading to higher energy and carbon footprint. Maximizing model accuracy for the specific recommendation use case comes with significant energy cost — Roughly 4× energy saving can be achieved with only 0.004 model quality degradation (green vs. yellow stars).", "id": 1130}
{"type": "section", "content": "APPENDIX\n\nDespite the recent calls-to-action [28], [39], [40], [41], the overall community remains under-invested in research that aims at deeply understanding and minimizing the cost of AI. There are several factors that may have contributed to the current state of AI:", "doc_id": "wu2021a", "page": 14, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "APPENDIX\n\nDespite the recent calls-to-action [28], [39], [40], [41], the overall community remains under-invested in research that aims at deeply understanding and minimizing the cost of AI. There are several factors that may have contributed to the current state of AI:", "original_types": ["text"], "id": 1131}
{"type": "section", "content": "Efficient, Environmentally-Sustainable AI Systems\n\nDisaggregating Machine Learning Pipeline Stages: As depicted in Figure 3, the overall training throughput efficiency for large-scale ML models depends on the throughput performance of both data ingestion and pre-processing and model training. Disaggregating the data ingestion and pre-processing stage of the machine learning pipeline from model training is the de-facto approach for industry-scale machine learning model training. This allows training accelerator, network and storage I/O bandwidth utilization to scale independently, thereby increasing the overall model training throughput by 56% [44]. Disaggregation with well-designed check-pointing support [110], [111] improves training fault tolerance as well. By doing so, failure on nodes that are responsible for data ingestion and pre-processing can be recovered efficiently without requiring re-runs of the entire training experiment. From a sustainability perspective, disaggregating the data storage and ingestion stage from model training maximizes infrastructure efficiency by using less system resources to achieve higher training throughput, resulting in lower embodied carbon footprint. By increasing fault tolerance, the operational carbon footprint is reduced at the same time.\n\nFault-Tolerant AI Systems and Hardware: One way to amortize the rising embodied carbon cost of AI infrastructures is to extend hardware lifetime. However, hardware ages — depending on the wear-out characteristics, increasingly more errors can surface over time and result in silent data corruption, leading to erroneous computation, model accuracy degradation, non-deterministic ML execution, or fatal system failure. In a large fleet of processors, silent data corruption can occur frequently enough to have disruptive impact on service productivity [112], [113]. Decommissioning an AI system entirely because of hardware faults is expensive from the perspective of resource and environmental footprints. System architects can design differential reliability levels for micro architectural components on an AI system depending on the ML model execution characteristics. Alternatively, algorithmic fault tolerance can be built into deep learning programming frameworks to provide a code execution path that is cognizant of hardware wear-out characteristics.", "doc_id": "wu2021a", "page": 15, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "Efficient, Environmentally-Sustainable AI Systems\n\nDisaggregating Machine Learning Pipeline Stages: As depicted in Figure 3, the overall training throughput efficiency for large-scale ML models depends on the throughput performance of both data ingestion and pre-processing and model training. Disaggregating the data ingestion and pre-processing stage of the machine learning pipeline from model training is the de-facto approach for industry-scale machine learning model training. This allows training accelerator, network and storage I/O bandwidth utilization to scale independently, thereby increasing the overall model training throughput by 56% [44]. Disaggregation with well-designed check-pointing support [110], [111] improves training fault tolerance as well. By doing so, failure on nodes that are responsible for data ingestion and pre-processing can be recovered efficiently without requiring re-runs of the entire training experiment. From a sustainability perspective, disaggregating the data storage and ingestion stage from model training maximizes infrastructure efficiency by using less system resources to achieve higher training throughput, resulting in lower embodied carbon footprint. By increasing fault tolerance, the operational carbon footprint is reduced at the same time.\n\nFault-Tolerant AI Systems and Hardware: One way to amortize the rising embodied carbon cost of AI infrastructures is to extend hardware lifetime. However, hardware ages — depending on the wear-out characteristics, increasingly more errors can surface over time and result in silent data corruption, leading to erroneous computation, model accuracy degradation, non-deterministic ML execution, or fatal system failure. In a large fleet of processors, silent data corruption can occur frequently enough to have disruptive impact on service productivity [112], [113]. Decommissioning an AI system entirely because of hardware faults is expensive from the perspective of resource and environmental footprints. System architects can design differential reliability levels for micro architectural components on an AI system depending on the ML model execution characteristics. Alternatively, algorithmic fault tolerance can be built into deep learning programming frameworks to provide a code execution path that is cognizant of hardware wear-out characteristics.", "original_types": ["text", "header"], "id": 1132}
{"type": "section", "content": "On-Device Learning: Federated learning and optimization can result in a non-negligible amount of carbon emissions at the edge, similar to the carbon footprint of training TransformerBig [21]. Figure 11 shows that the federated learning and optimization process emits non-negligible carbon at the edge due to both computation and wireless communication during the process. To estimate the carbon emission, we used a similar methodology to [114]. We collected the 90-day log data for federated learning production use cases at Facebook, which recorded the time spent on computation, data downloading, and data uploading per client device. We multiplied the computation time with the estimated device power and upload/download time with the estimated router power, and omitted other energy. We assumed a device power of 3W and a router power of 7.5W [115], [114]. Model training on client edge devices is inherently less energy-efficient because of the high wireless communication overheads, sub-optimal training data distribution in individual client devices, and highly-fragmented edge device architectures that make system-level optimization significantly more challenging [116]. Note, the wireless communication energy cost takes up a significant portion of the overall energy footprint of federated learning, making energy footprint optimization on communication important.\n\nEfficiency and Self-Supervised Learning\n\nSelf-supervised learning (SSL) have received much attention in the research community in recent years. SSL methods train deep neural networks without using explicit supervision in the form of human-annotated labels for each training sample. Having humans annotate data is a time-consuming, expensive, and typically noisy process. SSL methods are typically used to train foundation models — models that can readily be fine-tuned using a small amount of labeled data on a down-stream task [117]. SSL methods have been extremely successful for pre-training large language models, becoming the de-facto standard, and they have also attracted great interest in computer vision. When comparing supervised and self-supervised methods, there is a glaring trade-off between having labels and the amount of computational overhead involved in pre-training. For example, Chen et al. report achieving 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for", "doc_id": "wu2021a", "page": 15, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "On-Device Learning: Federated learning and optimization can result in a non-negligible amount of carbon emissions at the edge, similar to the carbon footprint of training TransformerBig [21]. Figure 11 shows that the federated learning and optimization process emits non-negligible carbon at the edge due to both computation and wireless communication during the process. To estimate the carbon emission, we used a similar methodology to [114]. We collected the 90-day log data for federated learning production use cases at Facebook, which recorded the time spent on computation, data downloading, and data uploading per client device. We multiplied the computation time with the estimated device power and upload/download time with the estimated router power, and omitted other energy. We assumed a device power of 3W and a router power of 7.5W [115], [114]. Model training on client edge devices is inherently less energy-efficient because of the high wireless communication overheads, sub-optimal training data distribution in individual client devices, and highly-fragmented edge device architectures that make system-level optimization significantly more challenging [116]. Note, the wireless communication energy cost takes up a significant portion of the overall energy footprint of federated learning, making energy footprint optimization on communication important.\n\nEfficiency and Self-Supervised Learning\n\nSelf-supervised learning (SSL) have received much attention in the research community in recent years. SSL methods train deep neural networks without using explicit supervision in the form of human-annotated labels for each training sample. Having humans annotate data is a time-consuming, expensive, and typically noisy process. SSL methods are typically used to train foundation models — models that can readily be fine-tuned using a small amount of labeled data on a down-stream task [117]. SSL methods have been extremely successful for pre-training large language models, becoming the de-facto standard, and they have also attracted great interest in computer vision. When comparing supervised and self-supervised methods, there is a glaring trade-off between having labels and the amount of computational overhead involved in pre-training. For example, Chen et al. report achieving 69.3% top-1 validation accuracy with a ResNet-50 model after SSL pre-training for", "original_types": ["text", "header"], "id": 1133}
{"type": "section", "content": "1000 epochs on the ImageNet dataset and using the linear evaluation protocol, freezing the pre-trained feature extractor, and fine-tuning a linear classifier on top for 60 epochs using the full ImageNet dataset with all labels [118]. In contrast, the same model typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training. Thus, in this example, using labels and supervised training is worth a roughly 10× reduction in training effort, measured in terms of number of passes over the dataset.\n\nRecent work suggests that incorporating even a small amount of labeled data can significantly bridge this gap. Assran et al. describe an approach called Predicting view Assignments With Support samples (PAWS) for semi-supervised pre-training inspired by SSL [119]. With access to labels for just 10% of the training images in ImageNet, a ResNet-50 achieves 75.5% top-1 accuracy after just 200 epochs of PAWS pre-training. Running on 64 V100 GPUs, this takes roughly 16 hours. Similar observations have recently been made for language model pre-training as well [120].\n\nSelf-supervised pre-training potentially has advantages in that a single foundation model can be trained (expensive) but then fine-tuned (inexpensive), amortizing the up front cost across many tasks [117]. Substantial additional research is needed to better understand the cost-benefit trade-offs for this paradigm.", "doc_id": "wu2021a", "page": 16, "url": "https://arxiv.org/pdf/2111.00364", "embedded_text": "1000 epochs on the ImageNet dataset and using the linear evaluation protocol, freezing the pre-trained feature extractor, and fine-tuning a linear classifier on top for 60 epochs using the full ImageNet dataset with all labels [118]. In contrast, the same model typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training. Thus, in this example, using labels and supervised training is worth a roughly 10× reduction in training effort, measured in terms of number of passes over the dataset.\n\nRecent work suggests that incorporating even a small amount of labeled data can significantly bridge this gap. Assran et al. describe an approach called Predicting view Assignments With Support samples (PAWS) for semi-supervised pre-training inspired by SSL [119]. With access to labels for just 10% of the training images in ImageNet, a ResNet-50 achieves 75.5% top-1 accuracy after just 200 epochs of PAWS pre-training. Running on 64 V100 GPUs, this takes roughly 16 hours. Similar observations have recently been made for language model pre-training as well [120].\n\nSelf-supervised pre-training potentially has advantages in that a single foundation model can be trained (expensive) but then fine-tuned (inexpensive), amortizing the up front cost across many tasks [117]. Substantial additional research is needed to better understand the cost-benefit trade-offs for this paradigm.", "original_types": ["text"], "id": 1134}
{"type": "section", "content": "Abstract\n\nLarge language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs—despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.", "doc_id": "samsi2024", "page": 1, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Abstract\n\nLarge language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs—despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.", "original_types": ["text", "header"], "id": 1135}
{"type": "section", "content": "II. Overview of Large Language Models\n\nThe landscape of large language models (LLMs) and large foundation models (LFMs) has seen explosive growth in both the speed of development as well as complexity of ever larger models. Over the past several years, competition has been fierce and the pace un-relenting as AI research groups across private companies and academic institutions have developed new models whose performance continues to improve on a wide suite of natural language benchmarks but still requires significant amounts of compute and energy. We provide a brief overview of LLMs and LFMs below along with details around the specific LLM we use for our analysis.", "doc_id": "samsi2024", "page": 2, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "II. Overview of Large Language Models\n\nThe landscape of large language models (LLMs) and large foundation models (LFMs) has seen explosive growth in both the speed of development as well as complexity of ever larger models. Over the past several years, competition has been fierce and the pace un-relenting as AI research groups across private companies and academic institutions have developed new models whose performance continues to improve on a wide suite of natural language benchmarks but still requires significant amounts of compute and energy. We provide a brief overview of LLMs and LFMs below along with details around the specific LLM we use for our analysis.", "original_types": ["text", "header"], "id": 1136}
{"type": "figure", "content": "Fig. 1: Development paths of LLMs: A tree diagram illustrating the development of language models and foundation models from 2017 to early 2023. Pink branches indicate encoder-type language models, green indicates encoder-decoder hybrid models, and the dark grey indicates decoder-style models. The bar-plot on the bottom right tallies the number of open/closed source models developed by different companies/institutions. We study LLaMA (outlined by the red arrow and red circle in the diagram above) as an example of one of the more recent, modern, and state-of-the-art LLMs whose size/complexity resemble Google’s Bard and OpenAI’s GPT-4, all three of which were released around the same time (spring 2023). Original figure from [15].", "doc_id": "samsi2024", "page": 2, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 1: Development paths of LLMs: A tree diagram illustrating the development of language models and foundation models from 2017 to early 2023. Pink branches indicate encoder-type language models, green indicates encoder-decoder hybrid models, and the dark grey indicates decoder-style models. The bar-plot on the bottom right tallies the number of open/closed source models developed by different companies/institutions. We study LLaMA (outlined by the red arrow and red circle in the diagram above) as an example of one of the more recent, modern, and state-of-the-art LLMs whose size/complexity resemble Google’s Bard and OpenAI’s GPT-4, all three of which were released around the same time (spring 2023). Original figure from [15].", "id": 1137}
{"type": "section", "content": "III. EXPERIMENTAL SETUP\n\nWe conducted our experiments on the MIT Supercloud high-performance computing (HPC) system [18]. This heterogeneous HPC cluster consists of 448 compute nodes with dual Intel Xeon Gold 6248 CPUs with 384 GB of RAM and two NVIDIA Volta V100 GPUs with 32 GB of memory per node. Each node on the system has two independent back-end fabrics: a 100 Gb/s Intel Omnipath as well as a 25 Gb/s Ethernet interconnect using Mellanox ConnectX-4 adapters with all servers connected to a single, non-blocking Arista DCS-7516 Ethernet core switch. The GPUs, Omnipath, and Ethernet cards are all connected to PCIe slots that route directly to the Xeon processors without any intermediary PCIe switches. All experiments in this paper exclusively used the 25 Gb/s Ethernet interconnect. The system also includes 480 CPU-only nodes with Intel Xeon Platinum 8260 processors. In addition, four nodes with NVIDIA A100 GPUs were also available for experiments described in this paper. A summary of the hardware is shown in Table I. All experiments described in this paper were run exclusively on NVIDIA GPUs.", "doc_id": "samsi2024", "page": 3, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "III. EXPERIMENTAL SETUP\n\nWe conducted our experiments on the MIT Supercloud high-performance computing (HPC) system [18]. This heterogeneous HPC cluster consists of 448 compute nodes with dual Intel Xeon Gold 6248 CPUs with 384 GB of RAM and two NVIDIA Volta V100 GPUs with 32 GB of memory per node. Each node on the system has two independent back-end fabrics: a 100 Gb/s Intel Omnipath as well as a 25 Gb/s Ethernet interconnect using Mellanox ConnectX-4 adapters with all servers connected to a single, non-blocking Arista DCS-7516 Ethernet core switch. The GPUs, Omnipath, and Ethernet cards are all connected to PCIe slots that route directly to the Xeon processors without any intermediary PCIe switches. All experiments in this paper exclusively used the 25 Gb/s Ethernet interconnect. The system also includes 480 CPU-only nodes with Intel Xeon Platinum 8260 processors. In addition, four nodes with NVIDIA A100 GPUs were also available for experiments described in this paper. A summary of the hardware is shown in Table I. All experiments described in this paper were run exclusively on NVIDIA GPUs.", "original_types": ["text", "header"], "id": 1138}
{"type": "table", "content": "TABLE I: Compute node configurations\nThis table lists the types of hardware used for inference evaluations in our experiments. Each node consists of 2 CPUs and 2 GPUs in the configuration listed below. All GPUs are from NVIDIA.", "doc_id": "samsi2024", "page": 3, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "TABLE I: Compute node configurations\nThis table lists the types of hardware used for inference evaluations in our experiments. Each node consists of 2 CPUs and 2 GPUs in the configuration listed below. All GPUs are from NVIDIA.", "id": 1139}
{"type": "section", "content": "IV. Results\n\nA. Baselines: LLaMA 7B, 13B, & 65B\n\n1) Inference Performance: We begin our analysis with a baseline comparison of LLaMA 65B with smaller-scale LLaMA models: LLaMA 7B and 13B. The goal is to understand the following: what do inference performance and energy trade-offs look like for the different sizes of LLaMA under the bare-minimum set of resources required to have them running inference? This question can be important for researchers and users who have may not have limitless computational resources and hardware acceleration or may be constrained in terms of GPU memory, etc.", "doc_id": "samsi2024", "page": 4, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "IV. Results\n\nA. Baselines: LLaMA 7B, 13B, & 65B\n\n1) Inference Performance: We begin our analysis with a baseline comparison of LLaMA 65B with smaller-scale LLaMA models: LLaMA 7B and 13B. The goal is to understand the following: what do inference performance and energy trade-offs look like for the different sizes of LLaMA under the bare-minimum set of resources required to have them running inference? This question can be important for researchers and users who have may not have limitless computational resources and hardware acceleration or may be constrained in terms of GPU memory, etc.", "original_types": ["text", "header"], "id": 1140}
{"type": "table", "content": "Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.", "doc_id": "samsi2024", "page": 4, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.", "id": 1141}
{"type": "table", "content": "Model Size V100 32GB A100 80GB\n\nCount Max. Batch size Count Max. Batch size\n\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n", "doc_id": "samsi2024", "page": 4, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Model Size V100 32GB A100 80GB\n\nCount Max. Batch size Count Max. Batch size\n\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n", "id": 1142}
{"type": "section", "content": "13B on two GPUs in each case whereas the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).\n\nAs expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. Faster response rates and inference are likely due to the fact that the number of computations, directly related to the number of parameters of said model, involved in the 7B and 13B models are significantly lower than the 65B model. We do note that for LLaMA 65B, we see a much smaller improvement in using the A100 over the V100; however, since the 65B model requires sharding across two (A100) or four (V100) compute nodes at the minimum, this could result in additional latency to each forward pass of the model, explaining the smaller improvements. We also observe that while LLaMA 7B exhibits a considerable improvement in inference throughput on both Alpaca and GSM8K with the A100, the improvement is much larger for Alpaca than GSM8K. This can also be attributed to the different complexities of inputs from each dataset.", "doc_id": "samsi2024", "page": 4, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "13B on two GPUs in each case whereas the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).\n\nAs expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second. Faster response rates and inference are likely due to the fact that the number of computations, directly related to the number of parameters of said model, involved in the 7B and 13B models are significantly lower than the 65B model. We do note that for LLaMA 65B, we see a much smaller improvement in using the A100 over the V100; however, since the 65B model requires sharding across two (A100) or four (V100) compute nodes at the minimum, this could result in additional latency to each forward pass of the model, explaining the smaller improvements. We also observe that while LLaMA 7B exhibits a considerable improvement in inference throughput on both Alpaca and GSM8K with the A100, the improvement is much larger for Alpaca than GSM8K. This can also be attributed to the different complexities of inputs from each dataset.", "original_types": ["text"], "id": 1143}
{"type": "figure", "content": "Fig. 2: Baseline comparison of inference performance/latency between LLaMA 7B, 13B and 65B: inference performance comparisons on the minimum set of hardware required to run inference (see Table II) across model sizes and between V100s and A100s.", "doc_id": "samsi2024", "page": 5, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 2: Baseline comparison of inference performance/latency between LLaMA 7B, 13B and 65B: inference performance comparisons on the minimum set of hardware required to run inference (see Table II) across model sizes and between V100s and A100s.", "id": 1144}
{"type": "figure", "content": "Fig. 3: Baseline energy per second (Watts) estimates of performing inference with LLaMA 7B, 13B, and 65B: inference energy comparisons on the minimum set of hardware/settings required (see Table II) with Alpaca and GSM8K on a log-scale. Color indicates device (V100/A100), bars indicate average quantities and lines indicate error bars. Energy is averaged over maximum generation lengths of 256, 512, and 1024 due to near-identical energy/size trends for each generation length.", "doc_id": "samsi2024", "page": 5, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 3: Baseline energy per second (Watts) estimates of performing inference with LLaMA 7B, 13B, and 65B: inference energy comparisons on the minimum set of hardware/settings required (see Table II) with Alpaca and GSM8K on a log-scale. Color indicates device (V100/A100), bars indicate average quantities and lines indicate error bars. Energy is averaged over maximum generation lengths of 256, 512, and 1024 due to near-identical energy/size trends for each generation length.", "id": 1145}
{"type": "figure", "content": "Fig. 4: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/256 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 5, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 4: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/256 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1146}
{"type": "figure", "content": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/256 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 5, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/256 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1147}
{"type": "section", "content": "B. Energy per Second: LLaMA 65B\n\nWe first take a look at the amount of energy inference costs per unit time in seconds. Figures 4 and 5 show a more in-depth look of the energy inference costs of LLaMA 65B across different batch sizes and degrees of sharding. Specifically, Figure 4 shows energy costs for maximum generation length 512 and Figure 5 shows energy costs for 1024.", "doc_id": "samsi2024", "page": 5, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "B. Energy per Second: LLaMA 65B\n\nWe first take a look at the amount of energy inference costs per unit time in seconds. Figures 4 and 5 show a more in-depth look of the energy inference costs of LLaMA 65B across different batch sizes and degrees of sharding. Specifically, Figure 4 shows energy costs for maximum generation length 512 and Figure 5 shows energy costs for 1024.", "original_types": ["text"], "id": 1148}
{"type": "figure", "content": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 6, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1149}
{"type": "figure", "content": "Fig. 6: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 6, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 6: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1150}
{"type": "figure", "content": "Fig. 7: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 6, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 7: Energy per output token estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 1024: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1151}
{"type": "figure", "content": "Fig. 8: Energy per response estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 7, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 8: Energy per response estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1152}
{"type": "figure", "content": "Fig. 9: Energy per response estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "doc_id": "samsi2024", "page": 7, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "Fig. 9: Energy per response estimates of LLaMA 65B across batch sizes of 64/128/256/512 and 8/16/32 shards for max generation length 512: inference energy estimates on Alpaca and GSM8K on log-scale. Color indicates batch size.", "id": 1153}
{"type": "table", "content": "TABLE III: Effects of GPU power capping on LLaMA 65B inference: This table shows the relative performance of the LLaMA 65B model on the GSM8k dataset with a batch size of 64 and output lengths of 256, 512, 1024 using NVIDIA A100 GPUs. The GPUs were power capped at 250W, 175W and 150W. Results shown here are relative to model performance at 250W to stay consistent with the settings in the rest of the experiments described here.", "doc_id": "samsi2024", "page": 7, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "TABLE III: Effects of GPU power capping on LLaMA 65B inference: This table shows the relative performance of the LLaMA 65B model on the GSM8k dataset with a batch size of 64 and output lengths of 256, 512, 1024 using NVIDIA A100 GPUs. The GPUs were power capped at 250W, 175W and 150W. Results shown here are relative to model performance at 250W to stay consistent with the settings in the rest of the experiments described here.", "id": 1154}
{"type": "section", "content": "generation length 512, increasing the batch size while keeping the number of shards fixed at 16 is associated with a decrease in energy per response, which is consistent with what we observed in energy per tokens in the same setting.\n\nEffects of GPU Power Capping on LLaMA 65B Power consumption in AI is an increasingly important concern. In prior work, we have shown [25] that power capping GPUs during training of language models such as BERT [26] is an effective way of reducing the energy consumed training these models. While the work in [25] focused on model training, in this paper, we focus on inference. In order to study the effect of power capping on inference using large language models, we ran a limited set of experiments using LLaMA 65B. We ran the 65B model on four 80GB A100 GPUs with the power cap set at 250W, 175W and 150W.", "doc_id": "samsi2024", "page": 7, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "generation length 512, increasing the batch size while keeping the number of shards fixed at 16 is associated with a decrease in energy per response, which is consistent with what we observed in energy per tokens in the same setting.\n\nEffects of GPU Power Capping on LLaMA 65B Power consumption in AI is an increasingly important concern. In prior work, we have shown [25] that power capping GPUs during training of language models such as BERT [26] is an effective way of reducing the energy consumed training these models. While the work in [25] focused on model training, in this paper, we focus on inference. In order to study the effect of power capping on inference using large language models, we ran a limited set of experiments using LLaMA 65B. We ran the 65B model on four 80GB A100 GPUs with the power cap set at 250W, 175W and 150W.", "original_types": ["text"], "id": 1155}
{"type": "section", "content": "V. Discussion\n\nIn this paper, we show the results of benchmarking a representative large language model on NVIDIA GPUs. We show baseline results from smaller models (7B, 13B) and compare them against the largest available version (65B) of LLaMA. We also examine the inference performance and energy across distributed settings and different configurations by varying model parameters, input data, and hardware configurations. By comparing a natural language instruction following dataset (Alpaca) and a mathematical question-answer dataset (GSM8K), we also find that the complexity of the input dataset can affect the model performance for a given set of hyperparameters and hardware configuration.", "doc_id": "samsi2024", "page": 8, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "V. Discussion\n\nIn this paper, we show the results of benchmarking a representative large language model on NVIDIA GPUs. We show baseline results from smaller models (7B, 13B) and compare them against the largest available version (65B) of LLaMA. We also examine the inference performance and energy across distributed settings and different configurations by varying model parameters, input data, and hardware configurations. By comparing a natural language instruction following dataset (Alpaca) and a mathematical question-answer dataset (GSM8K), we also find that the complexity of the input dataset can affect the model performance for a given set of hyperparameters and hardware configuration.", "original_types": ["text", "header"], "id": 1156}
{"type": "table", "content": "A100 Utilization\nThis table shows GPU utilization for 80GB A100 GPUs and LLaMA 65B with 4 shards, batch size of 64 averaged across both datasets used in this paper.", "doc_id": "samsi2024", "page": 8, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "A100 Utilization\nThis table shows GPU utilization for 80GB A100 GPUs and LLaMA 65B with 4 shards, batch size of 64 averaged across both datasets used in this paper.", "id": 1157}
{"type": "table", "content": "V100 Utilization\nThis table shows GPU utilization for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32 shards, a batch size of 64 and maximum generated output length of 256 averaged across both datasets used in this paper. We limit this result to an ouptut length of 256 because longer outputs on 8 V100 GPUs are not possible given memory limits of the GPU.", "doc_id": "samsi2024", "page": 8, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "V100 Utilization\nThis table shows GPU utilization for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32 shards, a batch size of 64 and maximum generated output length of 256 averaged across both datasets used in this paper. We limit this result to an ouptut length of 256 because longer outputs on 8 V100 GPUs are not possible given memory limits of the GPU.", "id": 1158}
{"type": "section", "content": "MIT SuperCloud team: William Arcand, William Bergeron, Chansup Byun, Michael Houle, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Albert Reuther, Antonio Rosa, and Charles Yee. The authors also wish to acknowledge the following individuals for their contributions and support: Bob Bond, Allan Vanterpool, Tucker Hamilton, Jeff Gottschalk, Mike Kanaan, Charles Leiserson, Dave Martinez, Steve Rejto, Marc Zissman.\n\nREFERENCES\n\n[1] Stability-AI, “Stable Diffusion,” https://github.com/Stability-AI/StableDiffusion, 2023.\n\n[2] D. Foster, Generative deep learning. ” O’Reilly Media, Inc.”, 2022.\n\n[3] Z. Epstein, A. Hertzmann, the Investigators of Human Creativity et al., “Art and the science of generative ai,” Science, vol. 380, no. 6650, pp. 1110–1111, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/science.adh4451\n\n[4] B. Meskó and E. J. Topol, “The imperative for regulatory oversight of large language models (or generative ai) in healthcare,” npj Digital Medicine, vol. 6, no. 1, p. 120, 2023.\n\n[5] H. Zohny, J. McMillan, and M. King, “Ethics of generative ai,” Journal of Medical Ethics, vol. 49, no. 2, pp. 79–80, 2023. [Online]. Available: https://jme.bmj.com/content/49/2/79\n\n[6] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in NLP,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 3645–3650.\n\n[7] M. Shoeybi, M. Patwary, R. Puri et al., “Megatron-lm: Training multi-billion parameter language models using model parallelism,” 2020.\n\n[8] D. Narayanan, M. Shoeybi, J. Casper et al., “Efficient large-scale language model training on gpu clusters using megatron-lm,” 2021.\n\n[9] “Nvidia/megatron-lm: Ongoing research training transformer models at scale,” https://github.com/NVIDIA/Megatron-LM, 2023.\n\n[10] J. Sevilla, L. Heim, A. Ho et al., “Compute trends across three eras of machine learning,” 2022.\n\n[11] D. Patel, “The ai brick wall – a practical limit for scaling dense transformer models, and how gpt 4 will break past it,” https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit, 2023.\n\n[12] R. Desislavov, F. Martínez-Plumed, and J. Hernández-Orallo, “Trends in ai inference energy consumption: Beyond the performance-vs-parameter laws of deep learning,” Sustainable Computing: Informatics and Systems, vol. 38, p. 100857, 2023.\n\n[13] D. Zhao, N. C. Frey, J. McDonald et al., “A green(er) world for a.i.” in 2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2022, pp. 742–750.\n\n[14] H. Touvron, T. Lavril, G. Izacard et al., “Llama: Open and efficient foundation language models,” 2023.\n\n[15] “Different development paths of llms,” https://www.interconnects.ai/p/llm-development-paths.\n\n[16] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,” 2017.", "doc_id": "samsi2024", "page": 9, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "MIT SuperCloud team: William Arcand, William Bergeron, Chansup Byun, Michael Houle, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Albert Reuther, Antonio Rosa, and Charles Yee. The authors also wish to acknowledge the following individuals for their contributions and support: Bob Bond, Allan Vanterpool, Tucker Hamilton, Jeff Gottschalk, Mike Kanaan, Charles Leiserson, Dave Martinez, Steve Rejto, Marc Zissman.\n\nREFERENCES\n\n[1] Stability-AI, “Stable Diffusion,” https://github.com/Stability-AI/StableDiffusion, 2023.\n\n[2] D. Foster, Generative deep learning. ” O’Reilly Media, Inc.”, 2022.\n\n[3] Z. Epstein, A. Hertzmann, the Investigators of Human Creativity et al., “Art and the science of generative ai,” Science, vol. 380, no. 6650, pp. 1110–1111, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/science.adh4451\n\n[4] B. Meskó and E. J. Topol, “The imperative for regulatory oversight of large language models (or generative ai) in healthcare,” npj Digital Medicine, vol. 6, no. 1, p. 120, 2023.\n\n[5] H. Zohny, J. McMillan, and M. King, “Ethics of generative ai,” Journal of Medical Ethics, vol. 49, no. 2, pp. 79–80, 2023. [Online]. Available: https://jme.bmj.com/content/49/2/79\n\n[6] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in NLP,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 3645–3650.\n\n[7] M. Shoeybi, M. Patwary, R. Puri et al., “Megatron-lm: Training multi-billion parameter language models using model parallelism,” 2020.\n\n[8] D. Narayanan, M. Shoeybi, J. Casper et al., “Efficient large-scale language model training on gpu clusters using megatron-lm,” 2021.\n\n[9] “Nvidia/megatron-lm: Ongoing research training transformer models at scale,” https://github.com/NVIDIA/Megatron-LM, 2023.\n\n[10] J. Sevilla, L. Heim, A. Ho et al., “Compute trends across three eras of machine learning,” 2022.\n\n[11] D. Patel, “The ai brick wall – a practical limit for scaling dense transformer models, and how gpt 4 will break past it,” https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit, 2023.\n\n[12] R. Desislavov, F. Martínez-Plumed, and J. Hernández-Orallo, “Trends in ai inference energy consumption: Beyond the performance-vs-parameter laws of deep learning,” Sustainable Computing: Informatics and Systems, vol. 38, p. 100857, 2023.\n\n[13] D. Zhao, N. C. Frey, J. McDonald et al., “A green(er) world for a.i.” in 2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), 2022, pp. 742–750.\n\n[14] H. Touvron, T. Lavril, G. Izacard et al., “Llama: Open and efficient foundation language models,” 2023.\n\n[15] “Different development paths of llms,” https://www.interconnects.ai/p/llm-development-paths.\n\n[16] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,” 2017.", "original_types": ["text", "header"], "id": 1159}
{"type": "section", "content": "[17] R. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not all you need. a state of the art review of large generative ai models,” 2023.\n\n[18] A. Reuther, J. Kepner, C. Byun et al., “Interactive supercomputing on 40,000 cores for machine learning and data analysis,” in 2018 IEEE High Performance extreme Computing Conference (HPEC). IEEE, 2018, pp. 1–6.\n\n[19] Facebook Research, online, 2023. [Online]. Available: https://github.com/facebookresearch/llama\n\n[20] FairScale authors, “Fairscale: A general purpose modular pytorch library for high performance and large scale training,” https://github.com/facebookresearch/fairscale, 2021.\n\n[21] R. Taori, I. Gulrajani, T. Zhang et al., “Stanford alpaca: An instruction-following llama model,” https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\n[22] K. Cobbe, V. Kosaraju, M. Bavarian et al., “Training verifiers to solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\n\n[23] NVIDIA. Nvidia-smi. [Online]. Available: http://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n\n[24] ——. Nvidia data center GPU manager (dcgm). [Online]. Available: https://developer.nvidia.com/dcgm\n\n[25] J. McDonald, B. Li, N. Frey et al., “Great power, great responsibility: Recommendations for reducing energy for training language models,” in Findings of the Association for Computational Linguistics: NAACL 2022, 2022, pp. 1962–1970.\n\n[26] J. Devlin, M.-W. Chang, K. Lee et al., “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n\n[27] NVIDIA, “Multi-Process Service,” https://docs.nvidia.com/deploy/mps/, 2023.\n\n[28] NVIDIA, “NVIDIA Multi Instance GPU User Guide,” https://docs.nvidia.com/datacenter/tesla/mig-user-guide/, 2023.\n\n[29] B. Li, T. Patel, S. Samsi et al., “Miso: exploiting multi-instance gpu capability on multi-tenant gpu clusters,” in Proceedings of the 13th Symposium on Cloud Computing, 2022, pp. 173–189.", "doc_id": "samsi2024", "page": 9, "url": "https://arxiv.org/pdf/2310.03003", "embedded_text": "[17] R. Gozalo-Brizuela and E. C. Garrido-Merchan, “Chatgpt is not all you need. a state of the art review of large generative ai models,” 2023.\n\n[18] A. Reuther, J. Kepner, C. Byun et al., “Interactive supercomputing on 40,000 cores for machine learning and data analysis,” in 2018 IEEE High Performance extreme Computing Conference (HPEC). IEEE, 2018, pp. 1–6.\n\n[19] Facebook Research, online, 2023. [Online]. Available: https://github.com/facebookresearch/llama\n\n[20] FairScale authors, “Fairscale: A general purpose modular pytorch library for high performance and large scale training,” https://github.com/facebookresearch/fairscale, 2021.\n\n[21] R. Taori, I. Gulrajani, T. Zhang et al., “Stanford alpaca: An instruction-following llama model,” https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\n[22] K. Cobbe, V. Kosaraju, M. Bavarian et al., “Training verifiers to solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\n\n[23] NVIDIA. Nvidia-smi. [Online]. Available: http://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n\n[24] ——. Nvidia data center GPU manager (dcgm). [Online]. Available: https://developer.nvidia.com/dcgm\n\n[25] J. McDonald, B. Li, N. Frey et al., “Great power, great responsibility: Recommendations for reducing energy for training language models,” in Findings of the Association for Computational Linguistics: NAACL 2022, 2022, pp. 1962–1970.\n\n[26] J. Devlin, M.-W. Chang, K. Lee et al., “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171–4186. [Online]. Available: https://aclanthology.org/N19-1423\n\n[27] NVIDIA, “Multi-Process Service,” https://docs.nvidia.com/deploy/mps/, 2023.\n\n[28] NVIDIA, “NVIDIA Multi Instance GPU User Guide,” https://docs.nvidia.com/datacenter/tesla/mig-user-guide/, 2023.\n\n[29] B. Li, T. Patel, S. Samsi et al., “Miso: exploiting multi-instance gpu capability on multi-tenant gpu clusters,” in Proceedings of the 13th Symposium on Cloud Computing, 2022, pp. 173–189.", "original_types": ["text"], "id": 1160}
{"type": "section", "content": "Abstract\n\nAs large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.", "doc_id": "fernandez2025", "page": 1, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Abstract\n\nAs large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.", "original_types": ["text", "header"], "id": 1161}
{"type": "figure", "content": "Figure 1: Proper application of efficiency methods with optimized vLLM (orange) approaches the ideal energy consumption (green) as compared with an unoptimized baseline PyTorch (purple) implementation.", "doc_id": "fernandez2025", "page": 1, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 1: Proper application of efficiency methods with optimized vLLM (orange) approaches the ideal energy consumption (green) as compared with an unoptimized baseline PyTorch (purple) implementation.", "id": 1162}
{"type": "section", "content": "Methodology\n\nIn the following section, we describe our experimental setup for evaluating inference efficiency.\n\nDecoding Strategies.\n\nDifferent decoding strategies used for generation have different computational profiles and can have a substantial impact on the generation efficiency (Kwon et al., 2023). In order to study the impact of sampling methods and auto-regressive decoding strategies, we investigate greedy decoding, beam search decoding, temperature sampling, top-p decoding affect the energy requirements and end-to-end latency (Holtzman et al., 2020).\n\nIn addition to auto-regressive decoding, we study the impact of speculative decoding. Speculative decoding is commonly used as a latency minimization inference optimization (Kwon et al., 2023). In speculative decoding, a lightweight draft model is used to predict multiple tokens (γ) which are then verified by the target model in parallel (Leviathan et al., 2023; Chen et al., 2023). Speculative decoding provides latency improvement by better utilizing GPUs over autoregressive decoding.\n\nPerformance Measures.\n\nWe evaluate the efficiency of inference by measuring the latency, throughput, GPU energy, and GPU power required for the inference of 1,024 examples  . Total energy use and GPU power metrics are measured using Nvidia Management Library (NVML) via the CodeCarbon library (Courty et al., 2024). Prior to evaluation, we conduct a warmup on up to 20 batches to allow for memory allocation, required CUDA graph capture, and JiT compilation  . Results are reported as the mean values energy use, latency, or power usage of three runs.\n\n3 Results\n\nIn the following section, we examine the effects of variations of data dimensionality, model architecture, decoding strategies, and software optimizations on inference energy use.\n\n3.1 Effects of Dataset and Sequence Length\n\nWe present results from our controlled sweep of sequence lengths and batch sizes in Figure 2. Prefill costs increase as a function of input sequence length, at the same rate regardless of batch sizes when scaling sequences larger than 128 tokens. At shorter sequence lengths and smaller batch sizes, the energy costs of prefill are constant regardless of the computational workload due to significant undersaturation of the accelerator. Although we fix output generation tokens to 64, we verify that at this convergence in rate of energy intensity increase occurs at the same point when instead fixing generation length to 8 tokens; see Figure 11 in Appendix E.", "doc_id": "fernandez2025", "page": "2-4", "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Methodology\n\nIn the following section, we describe our experimental setup for evaluating inference efficiency.\n\nDecoding Strategies.\n\nDifferent decoding strategies used for generation have different computational profiles and can have a substantial impact on the generation efficiency (Kwon et al., 2023). In order to study the impact of sampling methods and auto-regressive decoding strategies, we investigate greedy decoding, beam search decoding, temperature sampling, top-p decoding affect the energy requirements and end-to-end latency (Holtzman et al., 2020).\n\nIn addition to auto-regressive decoding, we study the impact of speculative decoding. Speculative decoding is commonly used as a latency minimization inference optimization (Kwon et al., 2023). In speculative decoding, a lightweight draft model is used to predict multiple tokens (γ) which are then verified by the target model in parallel (Leviathan et al., 2023; Chen et al., 2023). Speculative decoding provides latency improvement by better utilizing GPUs over autoregressive decoding.\n\nPerformance Measures.\n\nWe evaluate the efficiency of inference by measuring the latency, throughput, GPU energy, and GPU power required for the inference of 1,024 examples  . Total energy use and GPU power metrics are measured using Nvidia Management Library (NVML) via the CodeCarbon library (Courty et al., 2024). Prior to evaluation, we conduct a warmup on up to 20 batches to allow for memory allocation, required CUDA graph capture, and JiT compilation  . Results are reported as the mean values energy use, latency, or power usage of three runs.\n\n3 Results\n\nIn the following section, we examine the effects of variations of data dimensionality, model architecture, decoding strategies, and software optimizations on inference energy use.\n\n3.1 Effects of Dataset and Sequence Length\n\nWe present results from our controlled sweep of sequence lengths and batch sizes in Figure 2. Prefill costs increase as a function of input sequence length, at the same rate regardless of batch sizes when scaling sequences larger than 128 tokens. At shorter sequence lengths and smaller batch sizes, the energy costs of prefill are constant regardless of the computational workload due to significant undersaturation of the accelerator. Although we fix output generation tokens to 64, we verify that at this convergence in rate of energy intensity increase occurs at the same point when instead fixing generation length to 8 tokens; see Figure 11 in Appendix E.", "original_types": ["text", "header"], "id": 1163}
{"type": "figure", "content": "Figure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends of 1024 samples for 64 output tokens. For each GPU, we compare PyTorch with and without compilation, and vLLM with and without CUDA Graph serialization. The line in black represents the maximum allowable batch size for PyTorch. Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations can serve a larger batch size.", "doc_id": "fernandez2025", "page": 5, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 5: Energy consumption comparison across different GPUs for inference with PyTorch and vLLM backends of 1024 samples for 64 output tokens. For each GPU, we compare PyTorch with and without compilation, and vLLM with and without CUDA Graph serialization. The line in black represents the maximum allowable batch size for PyTorch. Relative savings are most apparent in the low batch size regime and that vLLM due to its optimizations can serve a larger batch size.", "id": 1164}
{"type": "figure", "content": "Figure 6: Energy Use of Llama-3.1 8B and Qwen 32B with varying degrees of Tensor Parallelism.", "doc_id": "fernandez2025", "page": 6, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 6: Energy Use of Llama-3.1 8B and Qwen 32B with varying degrees of Tensor Parallelism.", "id": 1165}
{"type": "figure", "content": "Azure Conversation Input Tokens - CDF", "doc_id": "fernandez2025", "page": 7, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Azure Conversation Input Tokens - CDF", "id": 1166}
{"type": "figure", "content": "Azure Conversation Output Tokens - CDF", "doc_id": "fernandez2025", "page": 7, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Azure Conversation Output Tokens - CDF", "id": 1167}
{"type": "section", "content": "Since \\( i_k \\) and \\( o_k \\) vary significantly across requests, we utilize dataset statistics—including the median and 99th percentile of input and output lengths (discussed in §4.3) to inform our binning strategy.", "doc_id": "fernandez2025", "page": 7, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Since \\( i_k \\) and \\( o_k \\) vary significantly across requests, we utilize dataset statistics—including the median and 99th percentile of input and output lengths (discussed in §4.3) to inform our binning strategy.", "original_types": ["text"], "id": 1168}
{"type": "section", "content": "4.3 Evaluations\n\nWe examine a suite of classical NLP tasks and LLM inference workloads, each characterized by a range of different input context and output generation sequences; with dataset statistics provided in Tables 3, 1, 2. We simulate a large-scale offline processing setting on the RTX A6000 GPUs, in which examples are binned by sequence lengths (as described in §4 and processed in parallel in the largest possible batches that fit in GPU memory. Utilizing the simulated workloads described in Sec 4.1, we estimate the effectiveness of the inference efficiency optimizations evaluated in Section 4.1. Based on these results, we select an inference framework with efficiency optimizations targeting large batch inference. Concretely, we consider inference with a dense model utilizing vLLM with CUDA graph serialization (eager mode off) on a single GPU and compare it to unoptimized inference native PyTorch as a lower bound on energy efficiency. In addition, we also model the idealized energy baseline based on the model and hardware configurations. Classical NLP Tasks. We benchmark the energy use in a set of classical natural language processing tasks in the English language: text classification (IMDB, Maas et al., 2011), machine translation (WMT-14, Bojar et al., 2014), summarization (CNN-DailyMail, Nallapati et al., 2016), and text generation (Wikitext-2 (Merity et al., 2016)). For each of these tasks, we sample a subset of 1024 examples with statistics of each dataset for the input and the output tokens provided in Table 3. Based on the Nvidia datasheet for the RTX A6000 GPU, we utilize consider FLOPS_HW of 309.7 TFLOPS and a 300W TDP power draw; and estimate theoretical inference FLOPs with the DeepSpeed profiler (Rasley et al., 2020).", "doc_id": "fernandez2025", "page": 8, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "4.3 Evaluations\n\nWe examine a suite of classical NLP tasks and LLM inference workloads, each characterized by a range of different input context and output generation sequences; with dataset statistics provided in Tables 3, 1, 2. We simulate a large-scale offline processing setting on the RTX A6000 GPUs, in which examples are binned by sequence lengths (as described in §4 and processed in parallel in the largest possible batches that fit in GPU memory. Utilizing the simulated workloads described in Sec 4.1, we estimate the effectiveness of the inference efficiency optimizations evaluated in Section 4.1. Based on these results, we select an inference framework with efficiency optimizations targeting large batch inference. Concretely, we consider inference with a dense model utilizing vLLM with CUDA graph serialization (eager mode off) on a single GPU and compare it to unoptimized inference native PyTorch as a lower bound on energy efficiency. In addition, we also model the idealized energy baseline based on the model and hardware configurations. Classical NLP Tasks. We benchmark the energy use in a set of classical natural language processing tasks in the English language: text classification (IMDB, Maas et al., 2011), machine translation (WMT-14, Bojar et al., 2014), summarization (CNN-DailyMail, Nallapati et al., 2016), and text generation (Wikitext-2 (Merity et al., 2016)). For each of these tasks, we sample a subset of 1024 examples with statistics of each dataset for the input and the output tokens provided in Table 3. Based on the Nvidia datasheet for the RTX A6000 GPU, we utilize consider FLOPS_HW of 309.7 TFLOPS and a 300W TDP power draw; and estimate theoretical inference FLOPs with the DeepSpeed profiler (Rasley et al., 2020).", "original_types": ["text", "header"], "id": 1169}
{"type": "table", "content": "Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads\nDataset & Mean ± Std & Median & 99th\n\nBurstGPT & 256.80 ± 242.27 & 215 & 1038\nAzure Chat & 1631.58 ± 1529.64 & 928 & 6683\nAzure Code & 2511.28 ± 2133.54 & 1930 & 7685", "doc_id": "fernandez2025", "page": 8, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Table 1: Input Sequence Length Statistics Across Real-World LLM Workloads\nDataset & Mean ± Std & Median & 99th\n\nBurstGPT & 256.80 ± 242.27 & 215 & 1038\nAzure Chat & 1631.58 ± 1529.64 & 928 & 6683\nAzure Code & 2511.28 ± 2133.54 & 1930 & 7685", "id": 1170}
{"type": "table", "content": "Table 2: Output Sequence Length Statistics Across Real-World LLM Workloads\nDataset & Mean ± Std & Median & 99th\n\nBurstGPT & 35.10 ± 108.59 & 7 & 478\nAzure Chat & 105.51 ± 158.25 & 41 & 694\nAzure Code & 22.69 ± 74.78 & 8 & 271", "doc_id": "fernandez2025", "page": 8, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Table 2: Output Sequence Length Statistics Across Real-World LLM Workloads\nDataset & Mean ± Std & Median & 99th\n\nBurstGPT & 35.10 ± 108.59 & 7 & 478\nAzure Chat & 105.51 ± 158.25 & 41 & 694\nAzure Code & 22.69 ± 74.78 & 8 & 271", "id": 1171}
{"type": "figure", "content": "Figure 8: Energy Comparison in doing inference over 1024 samples between PyTorch with Compilation off and vLLM with eager model off.", "doc_id": "fernandez2025", "page": 8, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 8: Energy Comparison in doing inference over 1024 samples between PyTorch with Compilation off and vLLM with eager model off.", "id": 1172}
{"type": "table", "content": "Table 4: Percentage differences of energy consumption relative to theoretical values for Various Tasks with Offline Inference.\nDataset PyTorch %Δ vLLM % Δ\nBurstGPT 506.52% 63.75%\nAzure Code 102.79% 26.59%\nAzure Conversation 490.23% 64.22%\n", "doc_id": "fernandez2025", "page": 9, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Table 4: Percentage differences of energy consumption relative to theoretical values for Various Tasks with Offline Inference.\nDataset PyTorch %Δ vLLM % Δ\nBurstGPT 506.52% 63.75%\nAzure Code 102.79% 26.59%\nAzure Conversation 490.23% 64.22%\n", "id": 1173}
{"type": "section", "content": "inference and operate at a higher TDP than other components, we do not account for the energy use by other other components of the hardware system such as power use from CPU, memory, or disk storage (McAllister et al., 2024; Patel et al., 2024); or estimate the energy requirements of other hardware accelerator architectures (e.g. TPUs, NPUs, etc.). Likewise, we conduct an investigation of commonly used inference software frameworks and standard efficiency optimizations. However, there remain other settings and computational optimizations that can be applied to LLM inference, such as utilizing: reduced or mixed precision, adaptive adjustment of GPU frequency, additional forms of model parallelism, or other forms of load management and workload scheduling; which remain out of the scope of this work (Stojkovic et al., 2024b). In this work, we primarily focus on the operation energy use of machine learning inference. Estimation of the embodied costs of inference; and the costs of machine learning training remain out of the scope of this work. Although improved characterization of the energy use of LLM inference can be used to design more efficient serving settings and reduce the energy needs of inference, it is possible that reductions in the cost of pretraining may then lead more individuals and organizations to pursue large model pretraining (i.e. Jevons Paradox).\n\nReferences\n\nAmey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming throughput-latency tradeoff in llm inference with sarathi-serve. Proceedings of 18th USENIX Symposium on Operating Systems Design and Implementation, 2024, Santa Clara.", "doc_id": "fernandez2025", "page": 10, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "inference and operate at a higher TDP than other components, we do not account for the energy use by other other components of the hardware system such as power use from CPU, memory, or disk storage (McAllister et al., 2024; Patel et al., 2024); or estimate the energy requirements of other hardware accelerator architectures (e.g. TPUs, NPUs, etc.). Likewise, we conduct an investigation of commonly used inference software frameworks and standard efficiency optimizations. However, there remain other settings and computational optimizations that can be applied to LLM inference, such as utilizing: reduced or mixed precision, adaptive adjustment of GPU frequency, additional forms of model parallelism, or other forms of load management and workload scheduling; which remain out of the scope of this work (Stojkovic et al., 2024b). In this work, we primarily focus on the operation energy use of machine learning inference. Estimation of the embodied costs of inference; and the costs of machine learning training remain out of the scope of this work. Although improved characterization of the energy use of LLM inference can be used to design more efficient serving settings and reduce the energy needs of inference, it is possible that reductions in the cost of pretraining may then lead more individuals and organizations to pursue large model pretraining (i.e. Jevons Paradox).\n\nReferences\n\nAmey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming throughput-latency tradeoff in llm inference with sarathi-serve. Proceedings of 18th USENIX Symposium on Operating Systems Design and Implementation, 2024, Santa Clara.", "original_types": ["text"], "id": 1174}
{"type": "section", "content": "Alistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. 2024. How data centers and the energy sector can sate ai’s hunger for power. McKinsey and Company.\n\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhatia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838.\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.\n\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji- quan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32.\n\nMike Isaac. 2025. Meta to increase spending to $65 billion this year in a.i. push. New York Times.\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\nGeorge Leopold. 2019. Aws to offer nvidia’s t4 gpus for ai inferencing. URL: https://web.archive.org/web/20220309000921/https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/(visited on 2022-04-19).\n\nYaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR.\n\nBaolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. Sprout: Green generative ai with carbon-efficient llm inference. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21799–21813.\n\nPengfei Li\n\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704.\n\nXiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024. Optimizing speculative decoding for serving large language models using goodput. arXiv preprint arXiv:2406.14066.\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24(253):1–15.", "doc_id": "fernandez2025", "page": 11, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Alistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. 2024. How data centers and the energy sector can sate ai’s hunger for power. McKinsey and Company.\n\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhatia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838.\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.\n\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Ji- quan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32.\n\nMike Isaac. 2025. Meta to increase spending to $65 billion this year in a.i. push. New York Times.\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.\n\nGeorge Leopold. 2019. Aws to offer nvidia’s t4 gpus for ai inferencing. URL: https://web.archive.org/web/20220309000921/https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/(visited on 2022-04-19).\n\nYaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR.\n\nBaolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. Sprout: Green generative ai with carbon-efficient llm inference. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21799–21813.\n\nPengfei Li\n\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704.\n\nXiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024. Optimizing speculative decoding for serving large language models using goodput. arXiv preprint arXiv:2406.14066.\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2023. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24(253):1–15.", "original_types": ["text"], "id": 1175}
{"type": "section", "content": "Sasha Luccioni, Boris Gamazaychikov, Sara Hooker, Régis Pierrard, Emma Strubell, Yacine Jernite, and Carole-Jean Wu. 2024a. Light bulbs have energy ratings—so why can’t ai chatbots? Nature, 632(8026):736–738.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. 2024b. Power hungry processing: Watts driving the cost of ai deployment? In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24, page 85–99, New York, NY, USA. Association for Computing Machinery.\n\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150.\n\nSara McAllister, Fiodar Kazhamiaka, Daniel S Berger, Rodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh Sah, Ricardo Bianchini, George Amvrosiadis, Nathan Beckmann, et al. 2024. A call for research on storage emissions. In Proceedings of the 3rd Workshop on Sustainable Computer Systems (HotCarbon).\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843.\n\nJacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, and Jesse Dodge. 2025. Holistically evaluating the environmental impact of creating language models. In The Thirteenth International Conference on Learning Representations.\n\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. 2024. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060.\n\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280–290, Berlin, Germany. Association for Computational Linguistics.", "doc_id": "fernandez2025", "page": 11, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Sasha Luccioni, Boris Gamazaychikov, Sara Hooker, Régis Pierrard, Emma Strubell, Yacine Jernite, and Carole-Jean Wu. 2024a. Light bulbs have energy ratings—so why can’t ai chatbots? Nature, 632(8026):736–738.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. 2024b. Power hungry processing: Watts driving the cost of ai deployment? In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24, page 85–99, New York, NY, USA. Association for Computing Machinery.\n\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142–150.\n\nSara McAllister, Fiodar Kazhamiaka, Daniel S Berger, Rodrigo Fonseca, Kali Frost, Aaron Ogus, Maneesh Sah, Ricardo Bianchini, George Amvrosiadis, Nathan Beckmann, et al. 2024. A call for research on storage emissions. In Proceedings of the 3rd Workshop on Sustainable Computer Systems (HotCarbon).\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843.\n\nJacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, and Jesse Dodge. 2025. Holistically evaluating the environmental impact of creating language models. In The Thirteenth International Conference on Learning Representations.\n\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. 2024. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060.\n\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pages 280–290, Berlin, Germany. Association for Computational Linguistics.", "original_types": ["text"], "id": 1176}
{"type": "section", "content": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–15.\n\nTeam OLMO, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.\n\nPratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. 2024. Characterizing power management opportunities for llms in the cloud. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 207–222.\n\nDavid Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The carbon footprint of machine learning training will plateau, then shrink. Preprint, arXiv:2204.05149.\n\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint.\n\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506.\n\nSiddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Berg-eron, Jeremy Kepner, Devesh Tiwari, and Vijay Gade-pally. 2023. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–9. IEEE.\n\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. Communications of the ACM, 63(12):54–63.\n\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024. 2024 united states data center energy usage report.\n\nTianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding. 2024. Greenllm: Disaggregating large language model serving on heterogeneous gpus for lower carbon emissions. arXiv preprint arXiv:2412.20322.\n\nBrad Smith. 2025. The golden opportunity for american ai.", "doc_id": "fernandez2025", "page": 12, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–15.\n\nTeam OLMO, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.\n\nPratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. 2024. Characterizing power management opportunities for llms in the cloud. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 207–222.\n\nDavid Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The carbon footprint of machine learning training will plateau, then shrink. Preprint, arXiv:2204.05149.\n\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint.\n\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506.\n\nSiddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Berg-eron, Jeremy Kepner, Devesh Tiwari, and Vijay Gade-pally. 2023. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pages 1–9. IEEE.\n\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. Communications of the ACM, 63(12):54–63.\n\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024. 2024 united states data center energy usage report.\n\nTianyao Shi, Yanran Wu, Sihang Liu, and Yi Ding. 2024. Greenllm: Disaggregating large language model serving on heterogeneous gpus for lower carbon emissions. arXiv preprint arXiv:2412.20322.\n\nBrad Smith. 2025. The golden opportunity for american ai.", "original_types": ["text"], "id": 1177}
{"type": "section", "content": "Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Íñigo Goiri, and Josep Torrellas. 2024a. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. arXiv preprint arXiv:2403.20306.\n\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Torrellas, and Esha Choukse. 2024b. Dynamollm: Designing llm inference clusters for performance and energy efficiency. arXiv preprint arXiv:2408.00741.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693–13696.\n\nXiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, and Sasha Luccioni. 2023. Energy and carbon considerations of fine-tuning BERT. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9058–9069, Singapore. Association for Computational Linguistics.\n\nYuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Burstgpt: A real-world workload dataset to optimize llm serving systems. Preprint, arXiv:2401.17644.\n\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline energy-optimal llm serving: Workload-based energy models for llm inference on heterogeneous systems. ACM SigEnergy newletter.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45.\n\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813.\n\nYanran Wu, Inez Hua, and Yi Ding. 2025. Unveiling environmental impacts of large language model serving: A functional unit view. arXiv preprint arXiv:2502.11256.\n\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.", "doc_id": "fernandez2025", "page": 12, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Íñigo Goiri, and Josep Torrellas. 2024a. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. arXiv preprint arXiv:2403.20306.\n\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Torrellas, and Esha Choukse. 2024b. Dynamollm: Designing llm inference clusters for performance and energy efficiency. arXiv preprint arXiv:2408.00741.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2020. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13693–13696.\n\nXiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, and Sasha Luccioni. 2023. Energy and carbon considerations of fine-tuning BERT. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9058–9069, Singapore. Association for Computational Linguistics.\n\nYuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Burstgpt: A real-world workload dataset to optimize llm serving systems. Preprint, arXiv:2401.17644.\n\nGrant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Offline energy-optimal llm serving: Workload-based energy models for llm inference on heterogeneous systems. ACM SigEnergy newletter.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45.\n\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813.\n\nYanran Wu, Inez Hua, and Yi Ding. 2025. Unveiling environmental impacts of large language model serving: A functional unit view. arXiv preprint arXiv:2502.11256.\n\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.", "original_types": ["text"], "id": 1178}
{"type": "section", "content": "Hardware Details\n\nIn Table 5, we provide additional details on the hardware configurations of the nodes used in our benchmarking experiments.\n\nDataset Licenses\n\nThe CNN-DailyMail dataset used for summarization is released under the Apache-2.0 License. The dataset Wikitext-2 dataset for text generation is available under the Creative Commons Attribution-ShareAlike License. The WMT-14 translation datasets are released for non-commercial use. The BurstGPT and Azure trace datasets are released under CC-BY-4.0 licenses.\n\nAcknowledgment of AI Assistance\n\nArtificial intelligence assistance was used to assist in literature review and for code completion assistance, specifically during the creation of visualizations.\n\nAdditional Optimzations: Continuous Batching\n\nIn Figure 9, we present additional results on the impact of vLLM’s continuous batching for online inference in which we observe that at large batch sizes continuous batching yields reductions in energy use.\n\nAdditional Sequence Length Results\n\nIn Figure 10, we present additional results on the effects of scaling input and output sequence lengths with the PyTorch framework.", "doc_id": "fernandez2025", "page": 13, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Hardware Details\n\nIn Table 5, we provide additional details on the hardware configurations of the nodes used in our benchmarking experiments.\n\nDataset Licenses\n\nThe CNN-DailyMail dataset used for summarization is released under the Apache-2.0 License. The dataset Wikitext-2 dataset for text generation is available under the Creative Commons Attribution-ShareAlike License. The WMT-14 translation datasets are released for non-commercial use. The BurstGPT and Azure trace datasets are released under CC-BY-4.0 licenses.\n\nAcknowledgment of AI Assistance\n\nArtificial intelligence assistance was used to assist in literature review and for code completion assistance, specifically during the creation of visualizations.\n\nAdditional Optimzations: Continuous Batching\n\nIn Figure 9, we present additional results on the impact of vLLM’s continuous batching for online inference in which we observe that at large batch sizes continuous batching yields reductions in energy use.\n\nAdditional Sequence Length Results\n\nIn Figure 10, we present additional results on the effects of scaling input and output sequence lengths with the PyTorch framework.", "original_types": ["text", "header"], "id": 1179}
{"type": "table", "content": "Table 5: Node Hardware Specifications\n```markdown\n| CPU | RAM | GPU | GPU TDP | FP32 TFLOPS | Bfloat16 TFLOPS |\n|-----|-----|-----|---------|-------------|--------------|\n| 256xAMD EPYC 7763 | 1TB | Nvidia RTX A6000 | 300W | 38.7 | -\n| 128xAMD EPYC 7513 | 500GB | Nvidia RTX A6000 Ada | 300W | 91.1 | -\n| 128xAMD EPYC 7763 | 1TB | Nvidia RTX A100-80 GB | 300W | 156 | 312\n```", "doc_id": "fernandez2025", "page": 14, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Table 5: Node Hardware Specifications\n```markdown\n| CPU | RAM | GPU | GPU TDP | FP32 TFLOPS | Bfloat16 TFLOPS |\n|-----|-----|-----|---------|-------------|--------------|\n| 256xAMD EPYC 7763 | 1TB | Nvidia RTX A6000 | 300W | 38.7 | -\n| 128xAMD EPYC 7513 | 500GB | Nvidia RTX A6000 Ada | 300W | 91.1 | -\n| 128xAMD EPYC 7763 | 1TB | Nvidia RTX A100-80 GB | 300W | 156 | 312\n```", "id": 1180}
{"type": "figure", "content": "Figure 9: Energy reduction comparison between online and offline serving modes across different GPUs (Eoffline − Eonline) * 100/Eoffline). The optimizations employed for online serving save up to 5% energy at larger batch sizes", "doc_id": "fernandez2025", "page": 14, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 9: Energy reduction comparison between online and offline serving modes across different GPUs (Eoffline − Eonline) * 100/Eoffline). The optimizations employed for online serving save up to 5% energy at larger batch sizes", "id": 1181}
{"type": "figure", "content": "Figure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference. Here, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other dimension of sequence length.", "doc_id": "fernandez2025", "page": 15, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 11: Controlled sweeps of input and output sequence lengths on A6000 GPUs, with vLLM offline inference. Here, we display multiple fixed sequence length sizes for comparison as we sweep across batch size and the other dimension of sequence length.", "id": 1182}
{"type": "figure", "content": "Figure 12: Classical NLP tasks and their energy intensities with vLLM backends. From top to bottom, the batch size varies from 1, 8, to 128", "doc_id": "fernandez2025", "page": 16, "url": "https://arxiv.org/pdf/2504.17674", "embedded_text": "Figure 12: Classical NLP tasks and their energy intensities with vLLM backends. From top to bottom, the batch size varies from 1, 8, to 128", "id": 1183}
{"type": "section", "content": "Socio-Technological Challenges and Opportunities: Paths Forward\n\nA Preprint\n\nCarole-Jean Wu Facebook\n\nSrilatha Manne Facebook*\n\nParthasarathy Ranganathan Google\n\nSarah Bird Microsoft\n\nShane Greenstein Harvard University\n\nAbstract\n\nAdvancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible – stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world’s physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world.\n\nThis article is intended to capture the ISCA panel on the Microprocessor 50: Societal Challenges (see https://www.iscaconf.org/isca2021/program/) from the lens of computer architects and the following discussions. This work represents the opinions of the authors and does not reflect the position of their respective companies.\n\n* Will be with Facebook; work started at Microsoft.\n\nIntroduction", "doc_id": "wu2021b", "page": 1, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Socio-Technological Challenges and Opportunities: Paths Forward\n\nA Preprint\n\nCarole-Jean Wu Facebook\n\nSrilatha Manne Facebook*\n\nParthasarathy Ranganathan Google\n\nSarah Bird Microsoft\n\nShane Greenstein Harvard University\n\nAbstract\n\nAdvancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible – stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world’s physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world.\n\nThis article is intended to capture the ISCA panel on the Microprocessor 50: Societal Challenges (see https://www.iscaconf.org/isca2021/program/) from the lens of computer architects and the following discussions. This work represents the opinions of the authors and does not reflect the position of their respective companies.\n\n* Will be with Facebook; work started at Microsoft.\n\nIntroduction", "original_types": ["text", "header"], "id": 1184}
{"type": "section", "content": "Digital technologies have had an undeniable influence on humanity’s well-being, transforming all aspects of our lives. Underpinned by advances in process technology, computer architecture, software engineering, and artificial intelligence (AI), the rapid technological development of the past five decades has altered the way we learn, work, commute, shop, socialize, eat, relax, and even sleep, both directly and indirectly. At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]. Digital technologies have also impacted major aspects of services that we regularly use and rely upon. Amazon warehouses are equipped with over 200,000 robots in 2020 to boost operational efficiency [Ackerman, 2021]. AI-powered robots are a growing presence in the farming industry [Sheikh, 2020]. Medicine has been transformed by technological advances leading to the decoding of the humane genome, resulting in genetically targeted therapies that help cancer patients survive longer or even enter full remission [Hum, 2021]. Looking ahead, AI is showing great promise in solving the grand challenge in biology – the protein structure prediction problem – which can once again lead to revolutionary changes in the field of biological sciences [Jumper et al., 2021]. Technology has also aided under-privileged and vulnerable groups in surprising ways. As an example, cell phones empower women in vulnerable situations to stay connected with the world, receive education and news, and establish businesses to support their families [Pathak, 2021]. Emerging technologies such as surveillance cameras enable authorities to respond to violence and curb crimes. Drones deliver life-saving medical supplies in rescues [Nyaaba and", "doc_id": "wu2021b", "page": 1, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Digital technologies have had an undeniable influence on humanity’s well-being, transforming all aspects of our lives. Underpinned by advances in process technology, computer architecture, software engineering, and artificial intelligence (AI), the rapid technological development of the past five decades has altered the way we learn, work, commute, shop, socialize, eat, relax, and even sleep, both directly and indirectly. At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021]. Digital technologies have also impacted major aspects of services that we regularly use and rely upon. Amazon warehouses are equipped with over 200,000 robots in 2020 to boost operational efficiency [Ackerman, 2021]. AI-powered robots are a growing presence in the farming industry [Sheikh, 2020]. Medicine has been transformed by technological advances leading to the decoding of the humane genome, resulting in genetically targeted therapies that help cancer patients survive longer or even enter full remission [Hum, 2021]. Looking ahead, AI is showing great promise in solving the grand challenge in biology – the protein structure prediction problem – which can once again lead to revolutionary changes in the field of biological sciences [Jumper et al., 2021]. Technology has also aided under-privileged and vulnerable groups in surprising ways. As an example, cell phones empower women in vulnerable situations to stay connected with the world, receive education and news, and establish businesses to support their families [Pathak, 2021]. Emerging technologies such as surveillance cameras enable authorities to respond to violence and curb crimes. Drones deliver life-saving medical supplies in rescues [Nyaaba and", "original_types": ["text"], "id": 1185}
{"type": "section", "content": "Microprocessors at 50\n\nDigital technologies have witnessed significant advancement over the past five decades. The first commercially-produced microprocessor – Intel 4004 – was manufactured in 10,000 nm process technology in 1971, and ran at 740kHZ with 2,250 transistors [Intel]. Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.\n\nMoore’s law scaling underpins the evolution of microprocessors [Moore, 1965]. The steady doubling of transistor density enables miniaturization of computing systems, from large mainframes to personal computers and from mobile/smartphones to Internet of Things (IoTs) and AR/VR wearables. The 1990s were the golden age of microprocessor innovations. Microarchitectural optimizations enabled impressive ILP scaling: most notably, in-order vs. out-of-order execution [Tomasulo, 1967, Smith, 1982a, Hwu and Patt, 1986], branch predictors [Smith, 1981, Lee and Smith, 1984, Pnevmatikatos et al., 1993, Yeh et al., 1993, Jimenez and Lin, 2001], caches [Smith, 1982b, Hill, 1988, Przybylski et al., 1989, Jouppi, 1990], prefetchers [Baer and Chen, 1991, Fu et al., 1992, Falsafi and Wenisch, 2014], single vs. simultaneous multithreading [Tullsen et al., 1995, 1996, Nemirovsky and Tullsen, 2013].", "doc_id": "wu2021b", "page": 2, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Microprocessors at 50\n\nDigital technologies have witnessed significant advancement over the past five decades. The first commercially-produced microprocessor – Intel 4004 – was manufactured in 10,000 nm process technology in 1971, and ran at 740kHZ with 2,250 transistors [Intel]. Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.\n\nMoore’s law scaling underpins the evolution of microprocessors [Moore, 1965]. The steady doubling of transistor density enables miniaturization of computing systems, from large mainframes to personal computers and from mobile/smartphones to Internet of Things (IoTs) and AR/VR wearables. The 1990s were the golden age of microprocessor innovations. Microarchitectural optimizations enabled impressive ILP scaling: most notably, in-order vs. out-of-order execution [Tomasulo, 1967, Smith, 1982a, Hwu and Patt, 1986], branch predictors [Smith, 1981, Lee and Smith, 1984, Pnevmatikatos et al., 1993, Yeh et al., 1993, Jimenez and Lin, 2001], caches [Smith, 1982b, Hill, 1988, Przybylski et al., 1989, Jouppi, 1990], prefetchers [Baer and Chen, 1991, Fu et al., 1992, Falsafi and Wenisch, 2014], single vs. simultaneous multithreading [Tullsen et al., 1995, 1996, Nemirovsky and Tullsen, 2013].", "original_types": ["text", "header"], "id": 1186}
{"type": "figure", "content": "Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].", "doc_id": "wu2021b", "page": 2, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].", "id": 1187}
{"type": "section", "content": "In the 2000s, microprocessors faced two significant challenges: the memory wall [Wulf and McKee, 1996] and the power wall [Dennard et al., 1974, Bohr, 2007]. While processor frequencies improved with Moore’s law scaling, memory latency did not and memory subsystems increasingly gated performance. Furthermore, Dennard scaling came to an end and fine-grained, high power density thermal hot spots limited the performance of microprocessors. The Memory and Power walls subsequently drove decades of innovations in multi-core scaling [Olukotun et al., 2007], memory consistency and cache coherence [Adve and Gharachorloo, 1996, Hill, 1998, Sorin et al., 2011], cache and memory hierarchy optimization [Qureshi et al., 2007, Jaleel et al., 2010, Wu et al., 2011, Balasubramonian et al., 2011, Sardashti et al., 2015, Balasubramonian, 2019, Jain and Lin, 2019], network-on-chip design and optimization [Dally and Towles, 2001, Wang et al., 2002, Enright and Peh, 2009, Jerger et al., 2017], and power- and thermal-aware design and management [Brooks et al., 2000, Brooks and Martonosi, 2001, Skadron et al., 2003, Kaxiras and Martonosi, 2008, Själander et al., 2014].\n\nDuring the same period, computations were migrating from client/personal devices to the cloud, demanding significant investment in large-scale data centers [Barroso and Hölzle, 2009]. The location of these data centers is dictated by a myriad of constraints – maximizing power and operational efficiency, proximity to population centers, weather conditions, local tax breaks – leading to some interesting tradeoffs in size, location, and ownership (on premise or cloud based) of the data centers. In 2019, urban data centers that optimize for service latency responsiveness are 26.7% smaller in size than the average data center operated by the major cloud providers [Greenstein and Fang, 2020]. Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1). Going forward, the demand", "doc_id": "wu2021b", "page": 2, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "In the 2000s, microprocessors faced two significant challenges: the memory wall [Wulf and McKee, 1996] and the power wall [Dennard et al., 1974, Bohr, 2007]. While processor frequencies improved with Moore’s law scaling, memory latency did not and memory subsystems increasingly gated performance. Furthermore, Dennard scaling came to an end and fine-grained, high power density thermal hot spots limited the performance of microprocessors. The Memory and Power walls subsequently drove decades of innovations in multi-core scaling [Olukotun et al., 2007], memory consistency and cache coherence [Adve and Gharachorloo, 1996, Hill, 1998, Sorin et al., 2011], cache and memory hierarchy optimization [Qureshi et al., 2007, Jaleel et al., 2010, Wu et al., 2011, Balasubramonian et al., 2011, Sardashti et al., 2015, Balasubramonian, 2019, Jain and Lin, 2019], network-on-chip design and optimization [Dally and Towles, 2001, Wang et al., 2002, Enright and Peh, 2009, Jerger et al., 2017], and power- and thermal-aware design and management [Brooks et al., 2000, Brooks and Martonosi, 2001, Skadron et al., 2003, Kaxiras and Martonosi, 2008, Själander et al., 2014].\n\nDuring the same period, computations were migrating from client/personal devices to the cloud, demanding significant investment in large-scale data centers [Barroso and Hölzle, 2009]. The location of these data centers is dictated by a myriad of constraints – maximizing power and operational efficiency, proximity to population centers, weather conditions, local tax breaks – leading to some interesting tradeoffs in size, location, and ownership (on premise or cloud based) of the data centers. In 2019, urban data centers that optimize for service latency responsiveness are 26.7% smaller in size than the average data center operated by the major cloud providers [Greenstein and Fang, 2020]. Furthermore, between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference – more than 40% higher efficiency for hyperscale data centers (Figure 1). Going forward, the demand", "original_types": ["text"], "id": 1188}
{"type": "section", "content": "Socio-Technological Challenges and Opportunities: Paths Forward\n\nA PREPRINT\n\non fast(er) service response and availability is playing an increasingly significant role on data center site selection and computing infrastructures connecting the edge and the cloud.\n\nThe 2010s is the golden age of domain-specific architectures and specialized hardware, fueled by the rise of big data and AI [Esmaeilzadeh et al., 2011, Jouppi et al., 2018]. The massive economic growth opportunities of AI have revolutionized the entire system stack design, resulting in hardware tailored to machine learning execution [Ovtcharov et al., 2015, Chen et al., 2016, Jouppi et al., 2017, Fowers et al., 2018, Shao et al., 2019, Mattson et al., 2020, Henry et al., 2020, Reddi et al., 2021, Anderson et al., 2021, Jang et al., 2021, Thompto et al., 2021, NVIDIA]from megawatt data center-scale infrastructures, to tens of watts inference engines, to micro-watt microcontrollers at the edge. Building on top of domain-specific characteristics, further system efficiency can be extracted with a rich array of application-specific accelerators at the cloud scale [Taylor et al., 2020, Ranganathan et al., 2021].", "doc_id": "wu2021b", "page": 3, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Socio-Technological Challenges and Opportunities: Paths Forward\n\nA PREPRINT\n\non fast(er) service response and availability is playing an increasingly significant role on data center site selection and computing infrastructures connecting the edge and the cloud.\n\nThe 2010s is the golden age of domain-specific architectures and specialized hardware, fueled by the rise of big data and AI [Esmaeilzadeh et al., 2011, Jouppi et al., 2018]. The massive economic growth opportunities of AI have revolutionized the entire system stack design, resulting in hardware tailored to machine learning execution [Ovtcharov et al., 2015, Chen et al., 2016, Jouppi et al., 2017, Fowers et al., 2018, Shao et al., 2019, Mattson et al., 2020, Henry et al., 2020, Reddi et al., 2021, Anderson et al., 2021, Jang et al., 2021, Thompto et al., 2021, NVIDIA]from megawatt data center-scale infrastructures, to tens of watts inference engines, to micro-watt microcontrollers at the edge. Building on top of domain-specific characteristics, further system efficiency can be extracted with a rich array of application-specific accelerators at the cloud scale [Taylor et al., 2020, Ranganathan et al., 2021].", "original_types": ["text", "header"], "id": 1189}
{"type": "figure", "content": "Figure 1: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].", "doc_id": "wu2021b", "page": 3, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Figure 1: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].", "id": 1190}
{"type": "section", "content": "The last fifty years of digital products have been driven by a combination of market innovation and user needs. In some cases, scientific curiosity, coupled with a market need for specialty medical drugs drove innovations such as the pursuit of the human genome. In other cases, innovations created a market such as the case for smartphones. The availability of smartphones led to further innovations and massive disruptions via sharing economy companies such as Uber and AirBnB [Hamari et al., 2016]. None of this would have been feasible without fundamental innovation in process technology, hardware and software design, and a laser focus on efficiency (Figure 1), optimization (Figure 2), and cost reduction. Altogether, digital technology advancement has led to efficiencies of scale propelling decades of economic growth.", "doc_id": "wu2021b", "page": 3, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "The last fifty years of digital products have been driven by a combination of market innovation and user needs. In some cases, scientific curiosity, coupled with a market need for specialty medical drugs drove innovations such as the pursuit of the human genome. In other cases, innovations created a market such as the case for smartphones. The availability of smartphones led to further innovations and massive disruptions via sharing economy companies such as Uber and AirBnB [Hamari et al., 2016]. None of this would have been feasible without fundamental innovation in process technology, hardware and software design, and a laser focus on efficiency (Figure 1), optimization (Figure 2), and cost reduction. Altogether, digital technology advancement has led to efficiencies of scale propelling decades of economic growth.", "original_types": ["text"], "id": 1191}
{"type": "figure", "content": "Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].", "doc_id": "wu2021b", "page": 3, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].", "id": 1192}
{"type": "section", "content": "The narrow focus on marketability, efficiency, and disruptive innovation has also resulted in many challenging, and sometimes unexpected, societal issues. Examples include widening disparity and inequity of access to digital technologies, spread of disinformation and misinformation in online platforms, privacy and security violations at both the personal and political level, and propagation of human bias into AI training and use cases at-scale [Simons and Jones, 2012, Abelson et al., 2015, Gervais et al., 2016, Ahmed et al., 2017, Whittaker et al., 2018, Speicher et al., 2018, Chouldechova et al., 2018, Ekstrand et al., 2018, Selbst et al., 2019, Ali et al., 2019, Babaei et al., 2019, Crawford et al., 2019, Papakyriakopoulos et al., 2020, Park et al., 2021, Bender et al., 2021, Kleinberg and Raghavan, 2021]. In addition, the information and computing technology sectors consume a significant amount of global electricity, water, and natural resources, leading to paramount carbon and environmental footprint [Jain and Wullert, 2002, Strubell et al., 2019, Manne, 2020, Wu and Gupta, 2021, Gupta et al., 2021, Crawford et al., 2021]. Digital technology is an integral part of human existence. As the field matures and as the world faces dire challenges from climate change to societal and political upheavals, a deliberate approach is required to technological innovation that centers on a positive societal impact while serving the needs of a market economy.\n\nLooking to the Future\n\nPredicting the future is always difficult. How many of us could have predicted that the technology depicted in futuristic TV shows from five decades ago such as Star Trek would be commonplace today (cell phones, AI, voice recognition to name a few)? However, what is possible to foretell are the following:", "doc_id": "wu2021b", "page": 3, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "The narrow focus on marketability, efficiency, and disruptive innovation has also resulted in many challenging, and sometimes unexpected, societal issues. Examples include widening disparity and inequity of access to digital technologies, spread of disinformation and misinformation in online platforms, privacy and security violations at both the personal and political level, and propagation of human bias into AI training and use cases at-scale [Simons and Jones, 2012, Abelson et al., 2015, Gervais et al., 2016, Ahmed et al., 2017, Whittaker et al., 2018, Speicher et al., 2018, Chouldechova et al., 2018, Ekstrand et al., 2018, Selbst et al., 2019, Ali et al., 2019, Babaei et al., 2019, Crawford et al., 2019, Papakyriakopoulos et al., 2020, Park et al., 2021, Bender et al., 2021, Kleinberg and Raghavan, 2021]. In addition, the information and computing technology sectors consume a significant amount of global electricity, water, and natural resources, leading to paramount carbon and environmental footprint [Jain and Wullert, 2002, Strubell et al., 2019, Manne, 2020, Wu and Gupta, 2021, Gupta et al., 2021, Crawford et al., 2021]. Digital technology is an integral part of human existence. As the field matures and as the world faces dire challenges from climate change to societal and political upheavals, a deliberate approach is required to technological innovation that centers on a positive societal impact while serving the needs of a market economy.\n\nLooking to the Future\n\nPredicting the future is always difficult. How many of us could have predicted that the technology depicted in futuristic TV shows from five decades ago such as Star Trek would be commonplace today (cell phones, AI, voice recognition to name a few)? However, what is possible to foretell are the following:", "original_types": ["text", "header"], "id": 1193}
{"type": "section", "content": "Pervasive Connectivity\n\nIn 2021, approximately 65% of the world’s population has access to the internet [Lin, 2021], and this is expected to improve moving forward. Seamless connectivity will be required for everyday activities from ordering groceries to driving your car. There are advantages to connectivity from more efficient driving and less food waste, easy access to educational resources, and the ability to work remotely. However, what this also implies is that our lives and livelihoods will be inexorably linked to the accessibility, privacy, security, and resilience of the IT infrastructure.\n\nReliable internet access for the world’s population is an essential requirement of pervasive connectivity. Despite the positive societal and economical impact, at-scale power delivery and networking infrastructure development has proven to be costly and highly geographically-constrained. According to an analysis from the FCC, at least 18 million Americans did not have stable broadband in 2020 [Wheeler, 2020], and many who have access may not be able to afford it. According to a recent UN report, two-thirds of school age children do not have access to the internet in their home [unicef, 2020]. Diverse technology innovations are particularly needed to increase information accessibility and to connect people of differing needs in the world together in a resilient manner. Microsoft Airband [Microsoft, a] aims to expand broadband access in rural parts of the United States while Google Loon [Loon] and Facebook Aquila [Wikipedia] provide affordable high-speed internet to under-connected communities by overcoming physical barriers with innovative technology solutions. The recent success of placing internet communication satellites in low Earth orbit opens the door to tremendous opportunities for providing internet access to populations in challenging geographic locations [Starlink]. While satellite communication removes the geographical and political boundaries, it can accelerate the spread of (mis)information. Thus, as technologies continue to enable pervasive connectivity at scale, we must take a deliberate approach to develop technology solutions responsibly.\n\nAlong with pervasive connectivity comes the requirements for security and privacy given the prevalence of ransomware attacks and ongoing privacy hacks from both private groups and nation states. Many aspects of our public and private lives will be online and accessible to bad actors, and the internet and the technology it enables will continue to be susceptible without strong security and privacy measures in place from the top of the software stack to the underlying hardware running the code [Lipp et al., 2018, Kocher et al., 2019].", "doc_id": "wu2021b", "page": 4, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Pervasive Connectivity\n\nIn 2021, approximately 65% of the world’s population has access to the internet [Lin, 2021], and this is expected to improve moving forward. Seamless connectivity will be required for everyday activities from ordering groceries to driving your car. There are advantages to connectivity from more efficient driving and less food waste, easy access to educational resources, and the ability to work remotely. However, what this also implies is that our lives and livelihoods will be inexorably linked to the accessibility, privacy, security, and resilience of the IT infrastructure.\n\nReliable internet access for the world’s population is an essential requirement of pervasive connectivity. Despite the positive societal and economical impact, at-scale power delivery and networking infrastructure development has proven to be costly and highly geographically-constrained. According to an analysis from the FCC, at least 18 million Americans did not have stable broadband in 2020 [Wheeler, 2020], and many who have access may not be able to afford it. According to a recent UN report, two-thirds of school age children do not have access to the internet in their home [unicef, 2020]. Diverse technology innovations are particularly needed to increase information accessibility and to connect people of differing needs in the world together in a resilient manner. Microsoft Airband [Microsoft, a] aims to expand broadband access in rural parts of the United States while Google Loon [Loon] and Facebook Aquila [Wikipedia] provide affordable high-speed internet to under-connected communities by overcoming physical barriers with innovative technology solutions. The recent success of placing internet communication satellites in low Earth orbit opens the door to tremendous opportunities for providing internet access to populations in challenging geographic locations [Starlink]. While satellite communication removes the geographical and political boundaries, it can accelerate the spread of (mis)information. Thus, as technologies continue to enable pervasive connectivity at scale, we must take a deliberate approach to develop technology solutions responsibly.\n\nAlong with pervasive connectivity comes the requirements for security and privacy given the prevalence of ransomware attacks and ongoing privacy hacks from both private groups and nation states. Many aspects of our public and private lives will be online and accessible to bad actors, and the internet and the technology it enables will continue to be susceptible without strong security and privacy measures in place from the top of the software stack to the underlying hardware running the code [Lipp et al., 2018, Kocher et al., 2019].", "original_types": ["text", "header"], "id": 1194}
{"type": "section", "content": "The IT infrastructure, just like in other critical infrastructure such as telecommunication and power delivery, will require backup and recovery mechanisms in order to minimize or eliminate downtime. These already exist in cloud data centers with examples such as redundant storage [Microsoft, 2021a] and network [Microsoft, 2021b], and backup batteries and generators. However, redundancy can require significant additional hardware and network bandwidth which inflates the cost of the overall cloud infrastructure.\n\nRecent climate events are pushing the issue of resiliency to the forefront. In the Texas power outages of 2021, many top-tier data centers were able to continue operations using diesel backup generators. However, there is a limit to how long backup systems can operate given the finite amount of fuel onsite and weather conditions making it difficult to transport more fuel to the data center [Sverdlik, 2021]. As catastrophic weather events become more common and last longer due to climate change, even the largest data centers may have trouble [Department of Energy, 2019]. For instance, the latest fires in Oregon brought down major power grid infrastructure that connects the grids of California to Oregon at the same time as a heat wave resulted in a need for additional power for cooling [Roth, 2021]. California narrowly escaped rolling blackouts this time, but these events will continue to increase in intensity and frequency as hot and dry conditions lengthen and strengthen the fire season, reduce hydro-electric power due to a lack of water, and increase the need for power for human comfort and survival.\n\nFor IT infrastructures of the future, a one-size-fits-all solution may not work either for the type of data center or for the services that it provides. Large data centers that require many megawatts to operate may not be viable during a blackout with limited power and where critical operations such as hospitals are given priority. Smaller data centers may be more resilient either with backup generators or using a local micro-grid to run operations when the main grid is down. On the service side, some applications may be more tolerant of outages than others. Hence, with a limited power budget, application availability should be tiered during critical times. Other solutions include making applications resilient by design or more amenable to migration to a non-impacted data center given enough warning before catastrophic events occur [Hornsby, 2018, Google Cloud Architecture Center].", "doc_id": "wu2021b", "page": 4, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "The IT infrastructure, just like in other critical infrastructure such as telecommunication and power delivery, will require backup and recovery mechanisms in order to minimize or eliminate downtime. These already exist in cloud data centers with examples such as redundant storage [Microsoft, 2021a] and network [Microsoft, 2021b], and backup batteries and generators. However, redundancy can require significant additional hardware and network bandwidth which inflates the cost of the overall cloud infrastructure.\n\nRecent climate events are pushing the issue of resiliency to the forefront. In the Texas power outages of 2021, many top-tier data centers were able to continue operations using diesel backup generators. However, there is a limit to how long backup systems can operate given the finite amount of fuel onsite and weather conditions making it difficult to transport more fuel to the data center [Sverdlik, 2021]. As catastrophic weather events become more common and last longer due to climate change, even the largest data centers may have trouble [Department of Energy, 2019]. For instance, the latest fires in Oregon brought down major power grid infrastructure that connects the grids of California to Oregon at the same time as a heat wave resulted in a need for additional power for cooling [Roth, 2021]. California narrowly escaped rolling blackouts this time, but these events will continue to increase in intensity and frequency as hot and dry conditions lengthen and strengthen the fire season, reduce hydro-electric power due to a lack of water, and increase the need for power for human comfort and survival.\n\nFor IT infrastructures of the future, a one-size-fits-all solution may not work either for the type of data center or for the services that it provides. Large data centers that require many megawatts to operate may not be viable during a blackout with limited power and where critical operations such as hospitals are given priority. Smaller data centers may be more resilient either with backup generators or using a local micro-grid to run operations when the main grid is down. On the service side, some applications may be more tolerant of outages than others. Hence, with a limited power budget, application availability should be tiered during critical times. Other solutions include making applications resilient by design or more amenable to migration to a non-impacted data center given enough warning before catastrophic events occur [Hornsby, 2018, Google Cloud Architecture Center].", "original_types": ["text"], "id": 1195}
{"type": "section", "content": "Designing for resilience addresses issues that are not just related to resilience under climate duress but also solutions that are tolerant of different geographies, power and networking infrastructures. In order to achieve pervasive connectivity across the world, IT infrastructure must be available across developed and developing regions. In Africa, for example, it is difficult and expensive to develop a power grid that can serve such a large continent. Renewables, however, have the benefit of being portable and scalable, and are powering much of Africa’s latest power advances, with off-grid and micro-grid renewable energy solutions serving poorer and/or non-centralized communities [The World Bank, 2020]. Data centers serving these regions may also have to rely on micro-grids running on renewable energy that fluctuate in their capacity based on the weather. Hence, rather than designing for a resilient and consistent power grid, IT", "doc_id": "wu2021b", "page": 4, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Designing for resilience addresses issues that are not just related to resilience under climate duress but also solutions that are tolerant of different geographies, power and networking infrastructures. In order to achieve pervasive connectivity across the world, IT infrastructure must be available across developed and developing regions. In Africa, for example, it is difficult and expensive to develop a power grid that can serve such a large continent. Renewables, however, have the benefit of being portable and scalable, and are powering much of Africa’s latest power advances, with off-grid and micro-grid renewable energy solutions serving poorer and/or non-centralized communities [The World Bank, 2020]. Data centers serving these regions may also have to rely on micro-grids running on renewable energy that fluctuate in their capacity based on the weather. Hence, rather than designing for a resilient and consistent power grid, IT", "original_types": ["text"], "id": 1196}
{"type": "section", "content": "infrastructure may be better served by (co-)designing agile applications and data centers, such as [Íñigo Goiri et al., 2015, Radovanovic et al., 2021, Lin et al., 2021, Zhang et al., 2021], that implicitly assume power fluctuations and variability.\n\nAchieving pervasive connectivity in the presence of resiliency, security, and privacy requirements can be challenging given the interrelated nature of the problems. For instance, current approaches to achieving resilient computing often rely on replication and redundancy which not only increases cost but also enlarges the surface of security and privacy preservation. Migrating applications to another region, which may be necessary due to power constraints, can expose data to further security threats. To retain physical security of data, applications may be hosted in limited geographical regions but this can increase application vulnerability to catastrophic events. In addition, secure and resilient computing infrastructures can often come with significant environmental implications. Hence, any computing infrastructure solutions must be cognizant of the multifaceted nature of the problems being addressed.\n\nSustainability. Resource limitations, climate change, water depletion, electronic waste, ecosystem damage, and environmental racism are just a few of the topics under the larger sustainability umbrella. There is increased focus on these topics from both industrial and political institutions. All major technology companies have pledged to reduce or eliminate their carbon footprint in the next decade by reducing the environmental impact associated with manufacturing and using their products. Examples of such commitments are Facebook achieving NetZero in operational emissions in 2020 and across its value chain by 2030 [Facebook], Apple’s pledge for 100% carbon neutral supply chain by 2030 [Apple], Microsoft’s goal of being carbon negative by 2030 [Smith], and Google’s aim of 24x7 carbon free data centers [Google, b]. In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021]. In addition, countries and trading zones are legislating carbon emission requirements. China has committed to be carbon free by 2060 [Myers, 2020] and the EU has committed to cut carbon emissions by 55% by 2030 [BBC, 2021].", "doc_id": "wu2021b", "page": 5, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "infrastructure may be better served by (co-)designing agile applications and data centers, such as [Íñigo Goiri et al., 2015, Radovanovic et al., 2021, Lin et al., 2021, Zhang et al., 2021], that implicitly assume power fluctuations and variability.\n\nAchieving pervasive connectivity in the presence of resiliency, security, and privacy requirements can be challenging given the interrelated nature of the problems. For instance, current approaches to achieving resilient computing often rely on replication and redundancy which not only increases cost but also enlarges the surface of security and privacy preservation. Migrating applications to another region, which may be necessary due to power constraints, can expose data to further security threats. To retain physical security of data, applications may be hosted in limited geographical regions but this can increase application vulnerability to catastrophic events. In addition, secure and resilient computing infrastructures can often come with significant environmental implications. Hence, any computing infrastructure solutions must be cognizant of the multifaceted nature of the problems being addressed.\n\nSustainability. Resource limitations, climate change, water depletion, electronic waste, ecosystem damage, and environmental racism are just a few of the topics under the larger sustainability umbrella. There is increased focus on these topics from both industrial and political institutions. All major technology companies have pledged to reduce or eliminate their carbon footprint in the next decade by reducing the environmental impact associated with manufacturing and using their products. Examples of such commitments are Facebook achieving NetZero in operational emissions in 2020 and across its value chain by 2030 [Facebook], Apple’s pledge for 100% carbon neutral supply chain by 2030 [Apple], Microsoft’s goal of being carbon negative by 2030 [Smith], and Google’s aim of 24x7 carbon free data centers [Google, b]. In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased significant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021]. In addition, countries and trading zones are legislating carbon emission requirements. China has committed to be carbon free by 2060 [Myers, 2020] and the EU has committed to cut carbon emissions by 55% by 2030 [BBC, 2021].", "original_types": ["text"], "id": 1197}
{"type": "section", "content": "Sustainability targets and the associated regulations will continue to increase, and hardware must be manufactured with less planetary impact, use less energy while in operation, and produce less e-waste at the end of life [Orcuttarchive, 2015, Chang et al., 2017]. Existing practices such as the move to hyperscale data centers have already reduced IT’s carbon footprint by consolidating and sharing computing resources, and operating those resources more efficiently (Figure 1) – AWS [Amazon, 2019], Azure [Microsoft, 2020], Google cloud [Evans and Gao, 2016], and Facebook datacenter infrastructures [Lee and Rowe, 2020]. In fact, data center electricity consumption has slowed down significantly. The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]. Furthermore, despite the increase in the global data center energy use, the number of users benefiting from the cloud infrastructure have increased even more—from 2010-18, the global data center compute instances increased by 5.5 times with an estimated 6% increase in the global data center energy use.", "doc_id": "wu2021b", "page": 5, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Sustainability targets and the associated regulations will continue to increase, and hardware must be manufactured with less planetary impact, use less energy while in operation, and produce less e-waste at the end of life [Orcuttarchive, 2015, Chang et al., 2017]. Existing practices such as the move to hyperscale data centers have already reduced IT’s carbon footprint by consolidating and sharing computing resources, and operating those resources more efficiently (Figure 1) – AWS [Amazon, 2019], Azure [Microsoft, 2020], Google cloud [Evans and Gao, 2016], and Facebook datacenter infrastructures [Lee and Rowe, 2020]. In fact, data center electricity consumption has slowed down significantly. The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]. Furthermore, despite the increase in the global data center energy use, the number of users benefiting from the cloud infrastructure have increased even more—from 2010-18, the global data center compute instances increased by 5.5 times with an estimated 6% increase in the global data center energy use.", "original_types": ["text"], "id": 1198}
{"type": "section", "content": "However, more can be done – we must go beyond efficiency optimization and build a sustainable ecosystem to achieve environmentally-sustainable computing. For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020]. Modular system design will enable component-level upgrades without having to decommission the system at its entirety, reducing overall electronic waste and the environmental footprint [Fairphone]. Other actions being discussed or implemented require manufacturers to make their systems more repairable, resulting in increased product lifetime [Wiens and Gordon-Byrne, 2017, Alsever, 2021]. Another option is to reduce the number of devices in an average household. In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021]. In many cases, smartphones might be powerful enough for the task, but a tablet or laptop is needed for a viable keyboard or a larger viewing platform. Some of these additional devices could be replaced with a virtual reality or augmented reality solution that is both portable and can provide virtual keyboards and visual clarity without requiring additional hardware. To achieve an environmentally sustainable computing future, we will have to build a circular economy for computing that supports the design principle of reduce, reuse, repair, and recycle. These and other potential solutions likely require a complete redesign of the software and hardware stacks both at the edge, within the cloud, and in the edge-cloud collaborative execution environment, in order to provide resilient, long lasting, innovative solutions.\n\nMaking technology more sustainable is only one part of the technical challenge. There is another side to the story—there are significant sustainability benefits resulting from computing technology. Programs, such as Farm Beats [Microsoft, 2015], address how to optimize operations on a farm using technology that is inexpensive and readily available to people in rural areas. Food production accounts for 19% of the world’s carbon emissions, and producing food more efficiently and with less toxicity has long term benefits for the world [Gates, 2021]. For the residential sector in the US, space heating and cooling contributed to over 40% of the total electricity consumption [U.S. Energy Information", "doc_id": "wu2021b", "page": 5, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "However, more can be done – we must go beyond efficiency optimization and build a sustainable ecosystem to achieve environmentally-sustainable computing. For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020]. Modular system design will enable component-level upgrades without having to decommission the system at its entirety, reducing overall electronic waste and the environmental footprint [Fairphone]. Other actions being discussed or implemented require manufacturers to make their systems more repairable, resulting in increased product lifetime [Wiens and Gordon-Byrne, 2017, Alsever, 2021]. Another option is to reduce the number of devices in an average household. In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021]. In many cases, smartphones might be powerful enough for the task, but a tablet or laptop is needed for a viable keyboard or a larger viewing platform. Some of these additional devices could be replaced with a virtual reality or augmented reality solution that is both portable and can provide virtual keyboards and visual clarity without requiring additional hardware. To achieve an environmentally sustainable computing future, we will have to build a circular economy for computing that supports the design principle of reduce, reuse, repair, and recycle. These and other potential solutions likely require a complete redesign of the software and hardware stacks both at the edge, within the cloud, and in the edge-cloud collaborative execution environment, in order to provide resilient, long lasting, innovative solutions.\n\nMaking technology more sustainable is only one part of the technical challenge. There is another side to the story—there are significant sustainability benefits resulting from computing technology. Programs, such as Farm Beats [Microsoft, 2015], address how to optimize operations on a farm using technology that is inexpensive and readily available to people in rural areas. Food production accounts for 19% of the world’s carbon emissions, and producing food more efficiently and with less toxicity has long term benefits for the world [Gates, 2021]. For the residential sector in the US, space heating and cooling contributed to over 40% of the total electricity consumption [U.S. Energy Information", "original_types": ["text"], "id": 1199}
{"type": "section", "content": "Adminstration]. This is where smart home IoT devices, such as Nest, can have an impact. AI is used to discover new electrocatalysts for more efficient and scalable ways to store and use renewable energy [Zitnick et al., 2020] while also being used to predict renewable energy availability ahead of actual generation to better utilize the energy [Elkin and Witherspoon]. Another example is the current Covid outbreak. As horrendous as the outbreak has been and continues to be, technology has enabled a portion of the economy to continue to operate even as employees work remotely. In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021]. Looking forward, information technology can improve efficiencies in practically every sector, from manufacturing to food production to transportation to controlling the climate in our homes and offices. Although there is a carbon cost associated with manufacturing and operating the IT ecosystem, this cost must be evaluated holistically [Chang et al., 2010, Bardon et al., 2020, Gupta et al., 2021, Patterson et al., 2021] in light of the benefits such an ecosystem can provide in other domains [Hager et al., 2019, Tomasev et al., 2020, Mulhern, 2021, United Nation, 2021].\n\nDemographic Inclusion. Recent data has pointed to significant demographic changes that will be occurring by the year 2100 [Vollset et al., 2020]. These include a radically declining and/or aging population in Europe, North America and Asia, and an increasing population of younger workers in Sub-Saharan Africa. Figure 3 shows one population projection using data from the United Nations [Roser, 2019], indicating flattening or decreasing populations in most of the world except for Africa. The population decline is strongly correlated with an improved quality of life and educational achievement of women – much of which is being facilitated by technologically engendered developments such as portable devices and the internet – along with access to reproductive services. This trend holds regardless of the country, culture, or religion. These demographic changes will have a seismic impact on all societies, and will also dominate what and where technology is designed and deployed, respectively, in the future.\n\nFor technology to be truly inclusive in a fully-connected world, it must be available and usable for everyone – regardless of physical capabilities, geographic restrictions, or economic constraints. With the projected demographic changes, assistive technologies to address the needs of an aging population with physical restrictions will be essential for enhancing an individual’s ability to be a viable and contributing member of society.", "doc_id": "wu2021b", "page": 6, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Adminstration]. This is where smart home IoT devices, such as Nest, can have an impact. AI is used to discover new electrocatalysts for more efficient and scalable ways to store and use renewable energy [Zitnick et al., 2020] while also being used to predict renewable energy availability ahead of actual generation to better utilize the energy [Elkin and Witherspoon]. Another example is the current Covid outbreak. As horrendous as the outbreak has been and continues to be, technology has enabled a portion of the economy to continue to operate even as employees work remotely. In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021]. Looking forward, information technology can improve efficiencies in practically every sector, from manufacturing to food production to transportation to controlling the climate in our homes and offices. Although there is a carbon cost associated with manufacturing and operating the IT ecosystem, this cost must be evaluated holistically [Chang et al., 2010, Bardon et al., 2020, Gupta et al., 2021, Patterson et al., 2021] in light of the benefits such an ecosystem can provide in other domains [Hager et al., 2019, Tomasev et al., 2020, Mulhern, 2021, United Nation, 2021].\n\nDemographic Inclusion. Recent data has pointed to significant demographic changes that will be occurring by the year 2100 [Vollset et al., 2020]. These include a radically declining and/or aging population in Europe, North America and Asia, and an increasing population of younger workers in Sub-Saharan Africa. Figure 3 shows one population projection using data from the United Nations [Roser, 2019], indicating flattening or decreasing populations in most of the world except for Africa. The population decline is strongly correlated with an improved quality of life and educational achievement of women – much of which is being facilitated by technologically engendered developments such as portable devices and the internet – along with access to reproductive services. This trend holds regardless of the country, culture, or religion. These demographic changes will have a seismic impact on all societies, and will also dominate what and where technology is designed and deployed, respectively, in the future.\n\nFor technology to be truly inclusive in a fully-connected world, it must be available and usable for everyone – regardless of physical capabilities, geographic restrictions, or economic constraints. With the projected demographic changes, assistive technologies to address the needs of an aging population with physical restrictions will be essential for enhancing an individual’s ability to be a viable and contributing member of society.", "original_types": ["text", "header"], "id": 1200}
{"type": "figure", "content": "Figure 3: Population growth estimation based on United Nations medium growth projections [Roser, 2019].", "doc_id": "wu2021b", "page": 6, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Figure 3: Population growth estimation based on United Nations medium growth projections [Roser, 2019].", "id": 1201}
{"type": "section", "content": "and infrastructure, whether it be availability of the latest computing systems, broadband access, cell towers or power sources. However, much of the current world, including parts of developed countries, do not currently have unfettered access to these facilities. Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].\n\nIn order to meet the upcoming needs of the world, technological growth and resources must focus on under-served areas. This includes developing data centers for regions that may not have a resilient power infrastructure or an infrastructure that is based on local renewable energy sources and does not tap into a regional grid. IT devices must be operable under adverse conditions where the local network and/or power may not be available 24/7 or varies based on external conditions [Isaacman and Martonosi, 2011, Hester et al., 2013, Lucia et al., 2017, Saleem et al., 2020]. For instance, devices need to operate for days without a recharge or must operate locally until wireless or broadband is available [Yetim and Martonosi, 2015] to ensure delay tolerance while maintaining user experiences. Energy storage technology plays a crucial role to smooth the intermittent nature of renewable energy generation but the cost must be significantly improved for practical deployment [Plumer, 2021]. And, for technology to be truly inclusive, the way AI technologies are developed and used must be human-centered, driven by the cultural and demographic differences in the population and with pro-social goals [Stray, 2021]. Furthermore, given its increasingly large impact on the society, AI must be developed with deliberation to construct fairer and more inclusive decisions and, at the same time, it must be adopted responsibly [Askell et al., 2019, NIST, 2021]. AI-powered products must be transparent with users on how data is gathered, used, and stored, and with controls to disable it [IBM, Microsoft, b, Google, c, Facebook, DOD].", "doc_id": "wu2021b", "page": 7, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "and infrastructure, whether it be availability of the latest computing systems, broadband access, cell towers or power sources. However, much of the current world, including parts of developed countries, do not currently have unfettered access to these facilities. Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].\n\nIn order to meet the upcoming needs of the world, technological growth and resources must focus on under-served areas. This includes developing data centers for regions that may not have a resilient power infrastructure or an infrastructure that is based on local renewable energy sources and does not tap into a regional grid. IT devices must be operable under adverse conditions where the local network and/or power may not be available 24/7 or varies based on external conditions [Isaacman and Martonosi, 2011, Hester et al., 2013, Lucia et al., 2017, Saleem et al., 2020]. For instance, devices need to operate for days without a recharge or must operate locally until wireless or broadband is available [Yetim and Martonosi, 2015] to ensure delay tolerance while maintaining user experiences. Energy storage technology plays a crucial role to smooth the intermittent nature of renewable energy generation but the cost must be significantly improved for practical deployment [Plumer, 2021]. And, for technology to be truly inclusive, the way AI technologies are developed and used must be human-centered, driven by the cultural and demographic differences in the population and with pro-social goals [Stray, 2021]. Furthermore, given its increasingly large impact on the society, AI must be developed with deliberation to construct fairer and more inclusive decisions and, at the same time, it must be adopted responsibly [Askell et al., 2019, NIST, 2021]. AI-powered products must be transparent with users on how data is gathered, used, and stored, and with controls to disable it [IBM, Microsoft, b, Google, c, Facebook, DOD].", "original_types": ["text"], "id": 1202}
{"type": "section", "content": "Finally, any IT technology must be economically-accessible to most of the world’s population. Unfortunately, even the most basic form of computing devices, a cell phone, is prohibitively expensive for much of the world. According to the Alliance for Affordable Internet, nearly 2.5 billion people live in countries where a basic cell phone would cost nearly a quarter or more of the average monthly income [Roser, 2020]. Making the internet available to everyone will not solve the problem if the devices commonly used to access the internet are a luxury item for the world’s poor [Naseem et al., 2020]. In addition, many of the devices currently being used are inexpensive basic phones. Hence, solutions must be designed for “dumb” phones in order to reach the poorest communities. Even if users eventually have smartphones, data rates and lack of free wifi will limit the utility of data-heavy applications. Without reasonable device and application availability for the poorest communities, reaching the goal of pervasive connectivity will be difficult if not impossible and the digital divide will continue to widen between the rich and the poor. And, all technological solutions come with environmental impact that will inadvertently impact the marginalized communities the most. Thus, when developing technological solutions, we must keep in mind why products are created and for whom, and where technology is designed and deployed, such that we consciously build environmentally-sustainable, socially-responsible, inclusive technologies for the next decades to come.", "doc_id": "wu2021b", "page": 7, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Finally, any IT technology must be economically-accessible to most of the world’s population. Unfortunately, even the most basic form of computing devices, a cell phone, is prohibitively expensive for much of the world. According to the Alliance for Affordable Internet, nearly 2.5 billion people live in countries where a basic cell phone would cost nearly a quarter or more of the average monthly income [Roser, 2020]. Making the internet available to everyone will not solve the problem if the devices commonly used to access the internet are a luxury item for the world’s poor [Naseem et al., 2020]. In addition, many of the devices currently being used are inexpensive basic phones. Hence, solutions must be designed for “dumb” phones in order to reach the poorest communities. Even if users eventually have smartphones, data rates and lack of free wifi will limit the utility of data-heavy applications. Without reasonable device and application availability for the poorest communities, reaching the goal of pervasive connectivity will be difficult if not impossible and the digital divide will continue to widen between the rich and the poor. And, all technological solutions come with environmental impact that will inadvertently impact the marginalized communities the most. Thus, when developing technological solutions, we must keep in mind why products are created and for whom, and where technology is designed and deployed, such that we consciously build environmentally-sustainable, socially-responsible, inclusive technologies for the next decades to come.", "original_types": ["text"], "id": 1203}
{"type": "section", "content": "Acknowledgement\n\nWe would like to thank Doug Carmean, David Brooks, and Lizy John for their insightful feedback on this work.", "doc_id": "wu2021b", "page": 8, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Acknowledgement\n\nWe would like to thank Doug Carmean, David Brooks, and Lizy John for their insightful feedback on this work.", "original_types": ["text", "header"], "id": 1204}
{"type": "section", "content": "R. M. Tomasulo. An efficient algorithm for exploiting multiple arithmetic units. IBM Journal of Research and Development, 11(1), 1967.\n\nJames E. Smith. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture, page 112–119, 1982a.\n\nW. Hwu and Y. N. Patt. Hpsm, a high performance restricted data flow architecture having minimal functionality. In Proceedings of the 13th Annual International Symposium on Computer Architecture, page 297–306, 1986.\n\nJames E. Smith. A study of branch prediction strategies. In Proceedings of the 8th Annual Symposium on Computer Architecture, page 135–148, 1981.\n\nLee and Smith. Branch prediction strategies and branch target buffer design. Computer, 17(1):6–22, 1984.\n\nDionisios N. Pnevmatikatos, Manoj Franklin, and Gurindar S. Sohi. Control flow prediction for dynamic ilp processors. In Proceedings of the 26th Annual International Symposium on Microarchitecture, page 153–163, 1993.\n\nTse-Yu Yeh, Deborah T. Marr, and Yale N. Patt. Increasing the instruction fetch rate via multiple branch prediction and a branch address cache. In Proceedings of the 7th International Conference on Supercomputing, page 67–76, 1993.\n\nD.A. Jimenez and C. Lin. Dynamic branch prediction with perceptrons. In Proceedings of the Seventh International Symposium on High-Performance Computer Architecture, 2001.\n\nAlan Jay Smith. Cache memories. ACM Computing Surveys, 14:473–530, 1982b.\n\nM.D. Hill. A case for direct-mapped caches. Computer, 21(12), 1988.\n\nS. Przybylski, M. Horowitz, and J. Hennessy. Characteristics of performance-optimal multi-level cache hierarchies. Proceedings of the 16th Annual International Symposium on Computer Architecture, pages 114–121, 1989.\n\nNorman P. Jouppi. Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. In Proceedings of the 17th Annual International Symposium on Computer Architecture, page 364–373, 1990.\n\nJean-Loup Baer and Tien-Fu Chen. An effective on-chip preloading scheme to reduce data access penalty. In Proceedings of the 1991 ACM/IEEE Conference on Supercomputing, pages 176–186, 1991.\n\nJohn Fu, Janak Patel, and Bob Janssens. Stride directed prefetching in scalar processors. Proceedings the 25th Annual International Symposium on Microarchitecture, pages 102–110, 1992.\n\nBabak Falsafi and Thomas F. Wenisch. Morgan and Claypool Publishers, 2014.\n\nDean M. Tullsen, Susan J. Eggers, and Henry M. Levy. Simultaneous multithreading: Maximizing on-chip parallelism. In Proceedings of the 22nd Annual International Symposium on Computer Architecture, page 392–403, 1995.\n\nDean M. Tullsen, Susan J. Eggers, Joel S. Emer, Henry M. Levy, Jack L. Lo, and Rebecca L. Stamm. Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. In Proceedings of the 23rd Annual International Symposium on Computer Architecture, page 191–202, 1996.", "doc_id": "wu2021b", "page": 9, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "R. M. Tomasulo. An efficient algorithm for exploiting multiple arithmetic units. IBM Journal of Research and Development, 11(1), 1967.\n\nJames E. Smith. Decoupled access/execute computer architectures. In Proceedings of the 9th Annual Symposium on Computer Architecture, page 112–119, 1982a.\n\nW. Hwu and Y. N. Patt. Hpsm, a high performance restricted data flow architecture having minimal functionality. In Proceedings of the 13th Annual International Symposium on Computer Architecture, page 297–306, 1986.\n\nJames E. Smith. A study of branch prediction strategies. In Proceedings of the 8th Annual Symposium on Computer Architecture, page 135–148, 1981.\n\nLee and Smith. Branch prediction strategies and branch target buffer design. Computer, 17(1):6–22, 1984.\n\nDionisios N. Pnevmatikatos, Manoj Franklin, and Gurindar S. Sohi. Control flow prediction for dynamic ilp processors. In Proceedings of the 26th Annual International Symposium on Microarchitecture, page 153–163, 1993.\n\nTse-Yu Yeh, Deborah T. Marr, and Yale N. Patt. Increasing the instruction fetch rate via multiple branch prediction and a branch address cache. In Proceedings of the 7th International Conference on Supercomputing, page 67–76, 1993.\n\nD.A. Jimenez and C. Lin. Dynamic branch prediction with perceptrons. In Proceedings of the Seventh International Symposium on High-Performance Computer Architecture, 2001.\n\nAlan Jay Smith. Cache memories. ACM Computing Surveys, 14:473–530, 1982b.\n\nM.D. Hill. A case for direct-mapped caches. Computer, 21(12), 1988.\n\nS. Przybylski, M. Horowitz, and J. Hennessy. Characteristics of performance-optimal multi-level cache hierarchies. Proceedings of the 16th Annual International Symposium on Computer Architecture, pages 114–121, 1989.\n\nNorman P. Jouppi. Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. In Proceedings of the 17th Annual International Symposium on Computer Architecture, page 364–373, 1990.\n\nJean-Loup Baer and Tien-Fu Chen. An effective on-chip preloading scheme to reduce data access penalty. In Proceedings of the 1991 ACM/IEEE Conference on Supercomputing, pages 176–186, 1991.\n\nJohn Fu, Janak Patel, and Bob Janssens. Stride directed prefetching in scalar processors. Proceedings the 25th Annual International Symposium on Microarchitecture, pages 102–110, 1992.\n\nBabak Falsafi and Thomas F. Wenisch. Morgan and Claypool Publishers, 2014.\n\nDean M. Tullsen, Susan J. Eggers, and Henry M. Levy. Simultaneous multithreading: Maximizing on-chip parallelism. In Proceedings of the 22nd Annual International Symposium on Computer Architecture, page 392–403, 1995.\n\nDean M. Tullsen, Susan J. Eggers, Joel S. Emer, Henry M. Levy, Jack L. Lo, and Rebecca L. Stamm. Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. In Proceedings of the 23rd Annual International Symposium on Computer Architecture, page 191–202, 1996.", "original_types": ["text"], "id": 1205}
{"type": "section", "content": "Mario Nemirovsky and Dean Tullsen. Morgan and Claypool Publishers, 2013.\n\nGoogle. Google Data Centers Efficiency. https://www.google.com/about/datacenters/efficiency/, a.\n\nFacebook. Facebook Sustainability – Data Centers. https://sustainability.fb.com/report-page/data-centers/.", "doc_id": "wu2021b", "page": 9, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Mario Nemirovsky and Dean Tullsen. Morgan and Claypool Publishers, 2013.\n\nGoogle. Google Data Centers Efficiency. https://www.google.com/about/datacenters/efficiency/, a.\n\nFacebook. Facebook Sustainability – Data Centers. https://sustainability.fb.com/report-page/data-centers/.", "original_types": ["text"], "id": 1206}
{"type": "section", "content": "Daniel J. Sorin, Mark D. Hill, and David A. Wood. A Primer on Memory Consistency and Cache Coherence. Morgan and Claypool Publishers, 2011. ISBN 1608455645.\n\nMoinuddin K. Qureshi, Aamer Jaleel, Yale N. Patt, Simon C. Steely, and Joel Emer. Adaptive insertion policies for high performance caching. In Proceedings of the 34th Annual International Symposium on Computer Architecture, page 381–391, 2007.\n\nAamer Jaleel, Kevin B. Theobald, Simon C. Steely, and Joel Emer. High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP). In Proceedings of the 37th Annual International Symposium on Computer Architecture, page 60–71, 2010.\n\nCarole-Jean Wu, Aamer Jaleel, Will Hasenplaugh, Margaret Martonosi, Simon C. Steely, and Joel Emer. SHiP: Signature-Based Hit Predictor for High Performance Caching. In Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, page 430–441, 2011.\n\nRajeev Balasubramonian, Norman P. Jouppi, and Naveen Muralimanohar. Morgan and Claypool Publishers, 2011.\n\nSomayeh Sardashti, Angelos Arelakis, Per Stenstom, and David A. Wood. Morgan and Claypool Publishers, 2015.\n\nRajeev Balasubramonian. Morgan and Claypool Publishers, 2019.\n\nAkanksha Jain and Calvin Lin. Morgan and Claypool Publishers, 2019.\n\nWilliam J. Dally and Brian Towles. Route packets, not wires: On-chip inteconnection networks. In Proceedings of the 38th Annual Design Automation Conference, 2001.\n\nHang-Sheng Wang, Xinping Zhu, Li-Shiuan Peh, and S. Malik. Orion: a power-performance simulator for interconnection networks. In Proceedings of the 35th Annual IEEE/ACM International Symposium on Microarchitecture, pages 294–305, 2002.\n\nNatalie Enright and Li-shiuan Peh. Morgan and Claypool Publishers, 2009.\n\nNatalie Enright Jerger, Tushar Krishna, Li-Shiuan Peh, and Margaret Martonosi. Morgan and Claypool Publishers, 2017.\n\nDavid Brooks, Vivek Tiwari, and Margaret Martonosi. Wattch: A framework for architectural-level power analysis and optimizations. In Proceedings of the 27th Annual International Symposium on Computer Architecture, pages 83–94, 2000.\n\nD. Brooks and M. Martonosi. Dynamic thermal management for high-performance microprocessors. In Proceedings of the 7th International Symposium on High-Performance Computer Architecture, pages 171–182, 2001.\n\nK. Skadron, M.R. Stan, W. Huang, Sivakumar Velusamy, Karthik Sankaranarayanan, and D. Tarjan. Temperature-aware microarchitecture. In Proceedings of the 30th Annual International Symposium on Computer Architecture, pages 2–13, 2003.\n\nStefanos Kaxiras and Margaret Martonosi. Computer Architecture Techniques for Power-Efficiency. Morgan and Claypool Publishers, 1st edition, 2008. ISBN 1598292080.\n\nMagnus Själander, Margaret Martonosi, and Stefanos Kaxiras. Morgan and Claypool Publishers, 2014.\n\nLuiz André Barroso and Urs Hölzle. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. 2009.", "doc_id": "wu2021b", "page": 10, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Daniel J. Sorin, Mark D. Hill, and David A. Wood. A Primer on Memory Consistency and Cache Coherence. Morgan and Claypool Publishers, 2011. ISBN 1608455645.\n\nMoinuddin K. Qureshi, Aamer Jaleel, Yale N. Patt, Simon C. Steely, and Joel Emer. Adaptive insertion policies for high performance caching. In Proceedings of the 34th Annual International Symposium on Computer Architecture, page 381–391, 2007.\n\nAamer Jaleel, Kevin B. Theobald, Simon C. Steely, and Joel Emer. High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP). In Proceedings of the 37th Annual International Symposium on Computer Architecture, page 60–71, 2010.\n\nCarole-Jean Wu, Aamer Jaleel, Will Hasenplaugh, Margaret Martonosi, Simon C. Steely, and Joel Emer. SHiP: Signature-Based Hit Predictor for High Performance Caching. In Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, page 430–441, 2011.\n\nRajeev Balasubramonian, Norman P. Jouppi, and Naveen Muralimanohar. Morgan and Claypool Publishers, 2011.\n\nSomayeh Sardashti, Angelos Arelakis, Per Stenstom, and David A. Wood. Morgan and Claypool Publishers, 2015.\n\nRajeev Balasubramonian. Morgan and Claypool Publishers, 2019.\n\nAkanksha Jain and Calvin Lin. Morgan and Claypool Publishers, 2019.\n\nWilliam J. Dally and Brian Towles. Route packets, not wires: On-chip inteconnection networks. In Proceedings of the 38th Annual Design Automation Conference, 2001.\n\nHang-Sheng Wang, Xinping Zhu, Li-Shiuan Peh, and S. Malik. Orion: a power-performance simulator for interconnection networks. In Proceedings of the 35th Annual IEEE/ACM International Symposium on Microarchitecture, pages 294–305, 2002.\n\nNatalie Enright and Li-shiuan Peh. Morgan and Claypool Publishers, 2009.\n\nNatalie Enright Jerger, Tushar Krishna, Li-Shiuan Peh, and Margaret Martonosi. Morgan and Claypool Publishers, 2017.\n\nDavid Brooks, Vivek Tiwari, and Margaret Martonosi. Wattch: A framework for architectural-level power analysis and optimizations. In Proceedings of the 27th Annual International Symposium on Computer Architecture, pages 83–94, 2000.\n\nD. Brooks and M. Martonosi. Dynamic thermal management for high-performance microprocessors. In Proceedings of the 7th International Symposium on High-Performance Computer Architecture, pages 171–182, 2001.\n\nK. Skadron, M.R. Stan, W. Huang, Sivakumar Velusamy, Karthik Sankaranarayanan, and D. Tarjan. Temperature-aware microarchitecture. In Proceedings of the 30th Annual International Symposium on Computer Architecture, pages 2–13, 2003.\n\nStefanos Kaxiras and Margaret Martonosi. Computer Architecture Techniques for Power-Efficiency. Morgan and Claypool Publishers, 1st edition, 2008. ISBN 1598292080.\n\nMagnus Själander, Margaret Martonosi, and Stefanos Kaxiras. Morgan and Claypool Publishers, 2014.\n\nLuiz André Barroso and Urs Hölzle. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. 2009.", "original_types": ["text"], "id": 1207}
{"type": "section", "content": "S. Greenstein and Tommy Pan Fang. Where the cloud rests: The economic geography of data centers. 2020.\n\nHadi Esmaeilzadeh, Emily Blem, Renée St. Amant, Karthikeyan Sankaralingam, and Doug Burger. Dark silicon and the end of multicore scaling. In Proceedings of the 38th Annual International Symposium on Computer Architecture, pages 365–376, 2011.\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, and David Patterson. A domain-specific architecture for deep neural networks. Communication of ACM, 61(9):50–59, 2018.\n\nKalin Ovtcharov, Olatunji Ruwase, Joo-Young Kim, Jeremy Fowers, Karin Strauss, and Eric Chung. Toward accelerating deep learning at scale using specialized hardware in the datacenter. In Proceedings of the 27th IEEE HotChips Symposium on High-Performance Chips, 2015.\n\nYu-Hsin Chen, Joel Emer, and Vivienne Sze. Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks. In Proceedings of the ACM/IEEE 43rd Annual International Symposium on Computer Architecture, pages 367–379, 2016.\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert", "doc_id": "wu2021b", "page": 10, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "S. Greenstein and Tommy Pan Fang. Where the cloud rests: The economic geography of data centers. 2020.\n\nHadi Esmaeilzadeh, Emily Blem, Renée St. Amant, Karthikeyan Sankaralingam, and Doug Burger. Dark silicon and the end of multicore scaling. In Proceedings of the 38th Annual International Symposium on Computer Architecture, pages 365–376, 2011.\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, and David Patterson. A domain-specific architecture for deep neural networks. Communication of ACM, 61(9):50–59, 2018.\n\nKalin Ovtcharov, Olatunji Ruwase, Joo-Young Kim, Jeremy Fowers, Karin Strauss, and Eric Chung. Toward accelerating deep learning at scale using specialized hardware in the datacenter. In Proceedings of the 27th IEEE HotChips Symposium on High-Performance Chips, 2015.\n\nYu-Hsin Chen, Joel Emer, and Vivienne Sze. Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks. In Proceedings of the ACM/IEEE 43rd Annual International Symposium on Computer Architecture, pages 367–379, 2016.\n\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert", "original_types": ["text"], "id": 1208}
{"type": "section", "content": "Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture, 2017.\n\nJeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz, Lisa Woods, Sitaram Lanka, Steve Reinhardt, Adrian Caulfield, Eric Chung, and Doug Burger. A configurable cloud-scale dnn processor for real-time ai. In Proceedings of the 45th International Symposium on Computer Architecture, 2018, 2018.\n\nYakun Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J. Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. Simba: Scaling deep-learning inference with multi-chip-module-based architecture. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, page 14–27, 2019.\n\nPeter Mattson, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, David Patterson, Guenther Schmuelling, Hanlin Tang, Gu-Yeon Wei, and Carole-Jean Wu. Mlperf: An industry standard benchmark suite for machine learning performance. IEEE Micro, 40(2), 2020.\n\nGlenn Henry, Parviz Palangpour, Michael Thomson, J Scott Gardner, Bryce Arden, Jim Donahue, Kimble Houck, Jonathan Johnson, Kyle O’Brien, Scott Petersen, Benjamin Seroussi, and Tyler Walker. High-Performance Deep-Learning Coprocessor Integrated into x86 SoC with Server-Class CPUs Industrial Product. In Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, 2020.\n\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, and Carole-Jean Wu. The vision behind mlperf: Understanding ai inference performance. IEEE Micro, 41(3):10–18, 2021.", "doc_id": "wu2021b", "page": 11, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture, 2017.\n\nJeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz, Lisa Woods, Sitaram Lanka, Steve Reinhardt, Adrian Caulfield, Eric Chung, and Doug Burger. A configurable cloud-scale dnn processor for real-time ai. In Proceedings of the 45th International Symposium on Computer Architecture, 2018, 2018.\n\nYakun Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, Stephen G. Tell, Yanqing Zhang, William J. Dally, Joel Emer, C. Thomas Gray, Brucek Khailany, and Stephen W. Keckler. Simba: Scaling deep-learning inference with multi-chip-module-based architecture. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, page 14–27, 2019.\n\nPeter Mattson, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, David Patterson, Guenther Schmuelling, Hanlin Tang, Gu-Yeon Wei, and Carole-Jean Wu. Mlperf: An industry standard benchmark suite for machine learning performance. IEEE Micro, 40(2), 2020.\n\nGlenn Henry, Parviz Palangpour, Michael Thomson, J Scott Gardner, Bryce Arden, Jim Donahue, Kimble Houck, Jonathan Johnson, Kyle O’Brien, Scott Petersen, Benjamin Seroussi, and Tyler Walker. High-Performance Deep-Learning Coprocessor Integrated into x86 SoC with Server-Class CPUs Industrial Product. In Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, 2020.\n\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, and Carole-Jean Wu. The vision behind mlperf: Understanding ai inference performance. IEEE Micro, 41(3):10–18, 2021.", "original_types": ["text"], "id": 1209}
{"type": "section", "content": "Michael Anderson, Benny Chen, Stephen Chen, Summer Deng, Jordan Fix, Michael Gschwind, Aravind Kalaiah, Changkyu Kim, Jaewon Lee, Jason Liang, Haixin Liu, Yinghai Lu, Jack Montgomery, Arun Moorthy, Satish Nadathur, Sam Naghshineh, Avinash Nayak, Jongsoo Park, Chris Petersen, Martin Schatz, Narayanan Sundaram, Bangsheng Tang, Peter Tang, Amy Yang, Jiecao Yu, Hector Yuen, Ying Zhang, Aravind Anbudurai, Vandana Balan, Harsha Bojja, Joe Boyd, Matthew Breitbach, Claudio Caldato, Anna Calvo, Garret Catron, Sneh Chandwani, Panos Christeas, Brad Cottel, Brian Coutinho, Arun Dalli, Abhishek Dhanotia, Oniel Duncan, Roman Dzhabarov, Simon Elmir, Chunli Fu, Wenyin Fu, Michael Fulthorp, Adi Gangidi, Nick Gibson, Sean Gordon, Beatriz Padilla Hernandez, Daniel Ho, Yu-Cheng Huang, Olof Johansson, Shishir Juluri, Shobhit Kanaujia, Manali Kesarkar, Jonathan Killinger, Ben Kim, Rohan Kulkarni, Meghan Lele, Huayu Li, Huamin Li, Yueming Li, Cynthia Liu, Jerry Liu, Bert Maher, Chandra Mallipedi, Seema Mangla, Kiran Kumar Matam, Jubin Mehta, Shobhit Mehta, Christopher Mitchell, Bharath Muthiah, Nitin Nagarkatte, Ashwin Narasimha, Bernard Nguyen, Thiara Ortiz, Soumya Padmanabha, Deng Pan, Ashwin Poojary, Ye, Qi, Olivier Raginel, Dwarak Rajagopal, Tristan Rice, Craig Ross, Nadav Rotem, Scott Russ, Kushal Shah, Baohua Shan, Hao Shen, Pavan Shetty, Krish Skandakumaran, Kutta Srinivasan, Roshan Sumbaly, Michael Tauberg, Mor Tzur, Hao Wang, Man Wang, Ben Wei, Alex Xia, Chenyu Xu, Martin Yang, Kai Zhang, Ruoxi Zhang, Ming Zhao, Whitney Zhao, Rui Zhu, Lin Qiao, Misha Smelyanskiy, Bill Jia, and Vijay Rao. First-generation inference accelerator deployment at Facebook, 2021.\n\nJun-Woo Jang, Sehwan Lee, Dongyoung Kim, Hyunsung Park, Ali Shafiee Ardestani, Yeongjae Choi, Channoh Kim, Yoojin Kim, Hyeongseok Yu, Hamzah Abdel-Aziz, Jun-Seok Park, H. Lee, Dongwoo, Lee, Myeong Woo Kim, Hanwoong Jung, Hee-Young Nam, Dong-Hyuk Lim, Seungwon Lee, Joonho Song, Suk-Chon Kwon, Joseph Hassoun, SukHwan Lim, and Changkyu Choi. Sparsity-aware and re-configurable npu architecture for samsung flagship mobile soc. In Proceedings of the ACM/IEEE 48th Annual International Symposium on Computer Architecture, 2021.\n\nBrian W. Thompto, Dung Q. Nguyen, J. Moreira, Ramon Bertran, H. Jacobson, R. Eickemeyer, R. Rao, M. Goulet, Marcy Byers, Christopher J. Gonzalez, Karthik Swaminathan, N. Dhanwada, Silvia M. Müller, Andreas Wagner, S. Sadasivam, R. Montoye, William J. Starke, Christian G. Zoellin, M. Floyd, Jeffrey Stuecheli, N. Chandramoorthy, J. Wellman, A. Buyuktosunoglu, M. Pflanz, B. Sinharoy, and P. Bose. Energy efficiency boost in the ai-infused power 10 processor: Industrial product. In Proceedings of the ACM/IEEE 48th Annual International Symposium on Computer Architecture, 2021.", "doc_id": "wu2021b", "page": 11, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Michael Anderson, Benny Chen, Stephen Chen, Summer Deng, Jordan Fix, Michael Gschwind, Aravind Kalaiah, Changkyu Kim, Jaewon Lee, Jason Liang, Haixin Liu, Yinghai Lu, Jack Montgomery, Arun Moorthy, Satish Nadathur, Sam Naghshineh, Avinash Nayak, Jongsoo Park, Chris Petersen, Martin Schatz, Narayanan Sundaram, Bangsheng Tang, Peter Tang, Amy Yang, Jiecao Yu, Hector Yuen, Ying Zhang, Aravind Anbudurai, Vandana Balan, Harsha Bojja, Joe Boyd, Matthew Breitbach, Claudio Caldato, Anna Calvo, Garret Catron, Sneh Chandwani, Panos Christeas, Brad Cottel, Brian Coutinho, Arun Dalli, Abhishek Dhanotia, Oniel Duncan, Roman Dzhabarov, Simon Elmir, Chunli Fu, Wenyin Fu, Michael Fulthorp, Adi Gangidi, Nick Gibson, Sean Gordon, Beatriz Padilla Hernandez, Daniel Ho, Yu-Cheng Huang, Olof Johansson, Shishir Juluri, Shobhit Kanaujia, Manali Kesarkar, Jonathan Killinger, Ben Kim, Rohan Kulkarni, Meghan Lele, Huayu Li, Huamin Li, Yueming Li, Cynthia Liu, Jerry Liu, Bert Maher, Chandra Mallipedi, Seema Mangla, Kiran Kumar Matam, Jubin Mehta, Shobhit Mehta, Christopher Mitchell, Bharath Muthiah, Nitin Nagarkatte, Ashwin Narasimha, Bernard Nguyen, Thiara Ortiz, Soumya Padmanabha, Deng Pan, Ashwin Poojary, Ye, Qi, Olivier Raginel, Dwarak Rajagopal, Tristan Rice, Craig Ross, Nadav Rotem, Scott Russ, Kushal Shah, Baohua Shan, Hao Shen, Pavan Shetty, Krish Skandakumaran, Kutta Srinivasan, Roshan Sumbaly, Michael Tauberg, Mor Tzur, Hao Wang, Man Wang, Ben Wei, Alex Xia, Chenyu Xu, Martin Yang, Kai Zhang, Ruoxi Zhang, Ming Zhao, Whitney Zhao, Rui Zhu, Lin Qiao, Misha Smelyanskiy, Bill Jia, and Vijay Rao. First-generation inference accelerator deployment at Facebook, 2021.\n\nJun-Woo Jang, Sehwan Lee, Dongyoung Kim, Hyunsung Park, Ali Shafiee Ardestani, Yeongjae Choi, Channoh Kim, Yoojin Kim, Hyeongseok Yu, Hamzah Abdel-Aziz, Jun-Seok Park, H. Lee, Dongwoo, Lee, Myeong Woo Kim, Hanwoong Jung, Hee-Young Nam, Dong-Hyuk Lim, Seungwon Lee, Joonho Song, Suk-Chon Kwon, Joseph Hassoun, SukHwan Lim, and Changkyu Choi. Sparsity-aware and re-configurable npu architecture for samsung flagship mobile soc. In Proceedings of the ACM/IEEE 48th Annual International Symposium on Computer Architecture, 2021.\n\nBrian W. Thompto, Dung Q. Nguyen, J. Moreira, Ramon Bertran, H. Jacobson, R. Eickemeyer, R. Rao, M. Goulet, Marcy Byers, Christopher J. Gonzalez, Karthik Swaminathan, N. Dhanwada, Silvia M. Müller, Andreas Wagner, S. Sadasivam, R. Montoye, William J. Starke, Christian G. Zoellin, M. Floyd, Jeffrey Stuecheli, N. Chandramoorthy, J. Wellman, A. Buyuktosunoglu, M. Pflanz, B. Sinharoy, and P. Bose. Energy efficiency boost in the ai-infused power 10 processor: Industrial product. In Proceedings of the ACM/IEEE 48th Annual International Symposium on Computer Architecture, 2021.", "original_types": ["text"], "id": 1210}
{"type": "section", "content": "NVIDIA. Tensor Cores: Unprecedented Acceleration for HPC and AI.\n\nhttps://www.nvidia.com/en-us/data-center/tensor-cores/\n\nMichael Taylor, Luis Vega, Moein Khazraee, Ikuo Magaki, Scott Davidson, and Dustin Richmond. Asic clouds: specializing the datacenter for planet-scale applications. Communications of the ACM, 63:103–109, 06 2020. doi:10.1145/3399734.\n\nParthasarathy Ranganathan, Daniel Stodolsky, Jeff Calow, Jeremy Dorfman, Marisabel Guevara, Clinton Wills Smullen IV, Aki Kuusela, Raghu Balasubramanian, Sandeep Bhatia, Prakash Chauhan, Anna Cheung, In Suk Chong, Niranjani Dasharathi, Jia Feng, Brian Fosco, Samuel Foss, Ben Gelb, Sara J. Gwin, Yoshiaki Hase, Da-ke He, C. Richard Ho, Roy W. Huffman Jr., Elisha Indupalli, Indira Jayaram, Poonacha Kongetira, Cho Mon Kyaw, Aaron Laursen, Yuan Li, Fong Lou, Kyle A. Lucke, JP Maaninen, Ramon Macias, Maire Mahony, David Alexander Munday, Srikanth Muroor, Narayana Penukonda, Eric Perkins-Argueta, Devin Persaud, Alex Ramirez, Ville-Mikko Rautio, Yolanda Ripley, Amir Salek, Sathish Sekar, Sergey N. Sokolov, Rob Springer, Don Stark, Mercedes Tan, Mark S. Wachsler, Andrew C. Walton, David A. Wickeraad, Alvin Wijaya, and Hon Kwan Wu. Warehouse-scale video acceleration: Co-design and deployment in the wild. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, page 600–615, 2021.\n\nJuho Hamari, Mimmi Sjöklint, and Antti Ukkonen. The sharing economy: Why people participate in collaborative consumption. Journal of the Association for Information Science and Technology, 67:2047–2059, 09 2016.\n\nYifan Sun, Nicolas Bohm Agostini, Shi Dong, and D. Kaeli. Summarizing cpu and gpu design trends with product data. ArXiv, abs/1911.11313, 2019.\n\nBarbara Simons and Douglas W. Jones. Internet voting in the u.s. Communication of ACM, 55(10):68–77, 2012.\n\nH. Abelson, Ross J. Anderson, S. Bellovin, Josh Benaloh, M. Blaze, W. Diffie, J. Gilmore, Matthew Green, S. Landau, P. Neumann, R. Rivest, J. Schiller, B. Schneier, Michael A. Specter, and D. Weitzner. Keys under doormats: mandating insecurity by requiring government access to all data and communications. Journal of Cybersecurity, 1: 69–79, 2015.\n\nArthur Gervais, Ghassan O. Karame, Karl Wüst, Vasileios Glykantzis, Hubert Ritzdorf, and Srdjan Capkun. On the security and performance of proof of work blockchains. In Proceedings of the ACM Conference on Computer and Communications Security, page 3–16, 2016.\n\nSyed Ishtiaque Ahmed, Md. Romael Haque, Jay Chen, and Nicola Dell. Digital privacy challenges with shared mobile phone use in bangladesh. Proceedings of the ACM Human-Computer Interaction, 1(CSCW), 2017.\n\nMeredith Whittaker, Kate Crawford, Roel Dobbe, Genevieve Fried, Elizabeth Kaziunas, Varoon Mathur, Sarah Mysers West, Rashida Richardson, Jason Schultz, and Oscar Schwartz. AI Now 2018 Report. https://ec.europa.eu/futurium/en/system/files/ged/ai_now_2018_report.pdf, 2018.", "doc_id": "wu2021b", "page": 12, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "NVIDIA. Tensor Cores: Unprecedented Acceleration for HPC and AI.\n\nhttps://www.nvidia.com/en-us/data-center/tensor-cores/\n\nMichael Taylor, Luis Vega, Moein Khazraee, Ikuo Magaki, Scott Davidson, and Dustin Richmond. Asic clouds: specializing the datacenter for planet-scale applications. Communications of the ACM, 63:103–109, 06 2020. doi:10.1145/3399734.\n\nParthasarathy Ranganathan, Daniel Stodolsky, Jeff Calow, Jeremy Dorfman, Marisabel Guevara, Clinton Wills Smullen IV, Aki Kuusela, Raghu Balasubramanian, Sandeep Bhatia, Prakash Chauhan, Anna Cheung, In Suk Chong, Niranjani Dasharathi, Jia Feng, Brian Fosco, Samuel Foss, Ben Gelb, Sara J. Gwin, Yoshiaki Hase, Da-ke He, C. Richard Ho, Roy W. Huffman Jr., Elisha Indupalli, Indira Jayaram, Poonacha Kongetira, Cho Mon Kyaw, Aaron Laursen, Yuan Li, Fong Lou, Kyle A. Lucke, JP Maaninen, Ramon Macias, Maire Mahony, David Alexander Munday, Srikanth Muroor, Narayana Penukonda, Eric Perkins-Argueta, Devin Persaud, Alex Ramirez, Ville-Mikko Rautio, Yolanda Ripley, Amir Salek, Sathish Sekar, Sergey N. Sokolov, Rob Springer, Don Stark, Mercedes Tan, Mark S. Wachsler, Andrew C. Walton, David A. Wickeraad, Alvin Wijaya, and Hon Kwan Wu. Warehouse-scale video acceleration: Co-design and deployment in the wild. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, page 600–615, 2021.\n\nJuho Hamari, Mimmi Sjöklint, and Antti Ukkonen. The sharing economy: Why people participate in collaborative consumption. Journal of the Association for Information Science and Technology, 67:2047–2059, 09 2016.\n\nYifan Sun, Nicolas Bohm Agostini, Shi Dong, and D. Kaeli. Summarizing cpu and gpu design trends with product data. ArXiv, abs/1911.11313, 2019.\n\nBarbara Simons and Douglas W. Jones. Internet voting in the u.s. Communication of ACM, 55(10):68–77, 2012.\n\nH. Abelson, Ross J. Anderson, S. Bellovin, Josh Benaloh, M. Blaze, W. Diffie, J. Gilmore, Matthew Green, S. Landau, P. Neumann, R. Rivest, J. Schiller, B. Schneier, Michael A. Specter, and D. Weitzner. Keys under doormats: mandating insecurity by requiring government access to all data and communications. Journal of Cybersecurity, 1: 69–79, 2015.\n\nArthur Gervais, Ghassan O. Karame, Karl Wüst, Vasileios Glykantzis, Hubert Ritzdorf, and Srdjan Capkun. On the security and performance of proof of work blockchains. In Proceedings of the ACM Conference on Computer and Communications Security, page 3–16, 2016.\n\nSyed Ishtiaque Ahmed, Md. Romael Haque, Jay Chen, and Nicola Dell. Digital privacy challenges with shared mobile phone use in bangladesh. Proceedings of the ACM Human-Computer Interaction, 1(CSCW), 2017.\n\nMeredith Whittaker, Kate Crawford, Roel Dobbe, Genevieve Fried, Elizabeth Kaziunas, Varoon Mathur, Sarah Mysers West, Rashida Richardson, Jason Schultz, and Oscar Schwartz. AI Now 2018 Report. https://ec.europa.eu/futurium/en/system/files/ged/ai_now_2018_report.pdf, 2018.", "original_types": ["text"], "id": 1211}
{"type": "section", "content": "Till Speicher, Muhammad Ali, Giridhari Venkatadri, Filipe Nunes Ribeiro, George Arvanitakis, Fabrício Benevenuto, Krishna P. Gummadi, Patrick Loiseau, and Alan Mislove. Potential for discrimination in online targeted advertising. In Proceedings of the Conference on Fairness, Accountability and Transparency, pages 5–19, 2018.\n\nAlexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proceedings of the Conference on Fairness, Accountability and Transparency, pages 134–148, 2018.\n\nMichael Ekstrand, Mucun Tian, Ion Azpiazu, Jennifer Ekstrand, Ogenemaro Anuyah, David McNeill, and Maria Pera. All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2018.\n\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nMuhammad Ali, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. Discrimination through optimization: How facebook’s ad delivery can lead to biased outcomes. 2019.\n\nMahmoudreza Babaei, Abhijnan Chakraborty, Juhi Kulshrestha, Elissa M. Redmiles, Meeyoung Cha, and Krishna P. Gummadi. Analyzing biases in perception of truth in news stories and their implications for fact checking. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nKate Crawford, Roel Dobbe, Theodora Dryer, Genevieve Fried, Ben Green, Elizabeth Kaziunas, Amba Kak, Varoon Mathur, Erin McElroy, Andrea Nill Sánchez, Deborah Raji, Joy Lisi Rankin, Rashida Richardson, Jason Schultz, Sarah Myers West, and Meredith Whittaker. AI Now 2019 Report. https://ainowinstitute.org/AI_Now_2019_Report.pdf, 2019.", "doc_id": "wu2021b", "page": 12, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Till Speicher, Muhammad Ali, Giridhari Venkatadri, Filipe Nunes Ribeiro, George Arvanitakis, Fabrício Benevenuto, Krishna P. Gummadi, Patrick Loiseau, and Alan Mislove. Potential for discrimination in online targeted advertising. In Proceedings of the Conference on Fairness, Accountability and Transparency, pages 5–19, 2018.\n\nAlexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proceedings of the Conference on Fairness, Accountability and Transparency, pages 134–148, 2018.\n\nMichael Ekstrand, Mucun Tian, Ion Azpiazu, Jennifer Ekstrand, Ogenemaro Anuyah, David McNeill, and Maria Pera. All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2018.\n\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nMuhammad Ali, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. Discrimination through optimization: How facebook’s ad delivery can lead to biased outcomes. 2019.\n\nMahmoudreza Babaei, Abhijnan Chakraborty, Juhi Kulshrestha, Elissa M. Redmiles, Meeyoung Cha, and Krishna P. Gummadi. Analyzing biases in perception of truth in news stories and their implications for fact checking. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2019.\n\nKate Crawford, Roel Dobbe, Theodora Dryer, Genevieve Fried, Ben Green, Elizabeth Kaziunas, Amba Kak, Varoon Mathur, Erin McElroy, Andrea Nill Sánchez, Deborah Raji, Joy Lisi Rankin, Rashida Richardson, Jason Schultz, Sarah Myers West, and Meredith Whittaker. AI Now 2019 Report. https://ainowinstitute.org/AI_Now_2019_Report.pdf, 2019.", "original_types": ["text"], "id": 1212}
{"type": "section", "content": "Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco. Bias in word embeddings. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, page 446–457, 2020.\n\nSunoo Park, Michael A. Specter, Neha Narula, and R. Rivest. Going from bad to worse: from internet voting to blockchain voting. Journal of Cybersecurity, 7, 2021.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, page 610–623, 2021.\n\nJon Kleinberg and Manish Raghavan. Algorithmic monoculture and social welfare. Proceedings of the National Academy of Sciences, 118(22), 2021.\n\nRavi Jain and John Wullert. Challenges: Environmental design for pervasive computing systems. In Proceedings of the 8th Annual International Conference on Mobile Computing and Networking, page 263–270, 2002.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp, 2019.\n\nSrilatha Manne. Examining the Carbon Footprint of Devices. https://devblogs.microsoft.com/sustainable-software/examining-the-carbon-footprint-of-devices/, 2020.\n\nCarole-Jean Wu and Udit Gupta. Most of computing’s carbon emissions are coming from manufacturing and infrastructure. https://tech.fb.com/sustainable-computing/, 2021.\n\nUdit Gupta, Young Geun Kim, S. Lee, J. Tse, Hsien-Hsin S. Lee, Gu-Yeon Wei, D. Brooks, and Carole-Jean Wu. Chasing carbon: The elusive environmental footprint of computing. Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, pages 854–867, 2021.\n\nAlan Crawford, Ian King, and Debby Wu. The Chip Industry Has a Problem With Its Giant Carbon Footprint. https://www.bloomberg.com/news/articles/2021-04-08/the-chip-industry-has-a-problem-with-its-giant-carbon-footprint, 2021.\n\nYing Lin. 10 Internet Statistics Every Marketer Should Know in 2021. https://www.oberlo.com/blog/internet-statistics, May 2021.\n\nTom Wheeler. 5 steps to get the internet to all americans. https://www.brookings.edu/research/5-steps-to-get-the-internet-to-all-americans/, May 2020.\n\nunicef. Two thirds of the world’s school-age children have no internet access at home. https://www.unicef.org/press-releases/two-thirds-worlds-school-age-children-have-no-internet-access-home-new-unicef-itu, 2020.\n\nMicrosoft. Airband: The initiative to bring the internet to everyone. https://news.microsoft.com/on-the-issues/2020/09/01/airband-initiative-rural-broadband-digital-divide/, a.\n\nLoon. Loon: Expanding internet connectivity with stratospheric balloons. https://x.company/projects/loon/. Wikipedia. Facebook Aquila. https://en.wikipedia.org/wiki/Facebook_Aquila.", "doc_id": "wu2021b", "page": 13, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Orestis Papakyriakopoulos, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco. Bias in word embeddings. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, page 446–457, 2020.\n\nSunoo Park, Michael A. Specter, Neha Narula, and R. Rivest. Going from bad to worse: from internet voting to blockchain voting. Journal of Cybersecurity, 7, 2021.\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, page 610–623, 2021.\n\nJon Kleinberg and Manish Raghavan. Algorithmic monoculture and social welfare. Proceedings of the National Academy of Sciences, 118(22), 2021.\n\nRavi Jain and John Wullert. Challenges: Environmental design for pervasive computing systems. In Proceedings of the 8th Annual International Conference on Mobile Computing and Networking, page 263–270, 2002.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp, 2019.\n\nSrilatha Manne. Examining the Carbon Footprint of Devices. https://devblogs.microsoft.com/sustainable-software/examining-the-carbon-footprint-of-devices/, 2020.\n\nCarole-Jean Wu and Udit Gupta. Most of computing’s carbon emissions are coming from manufacturing and infrastructure. https://tech.fb.com/sustainable-computing/, 2021.\n\nUdit Gupta, Young Geun Kim, S. Lee, J. Tse, Hsien-Hsin S. Lee, Gu-Yeon Wei, D. Brooks, and Carole-Jean Wu. Chasing carbon: The elusive environmental footprint of computing. Proceedings of the IEEE International Symposium on High-Performance Computer Architecture, pages 854–867, 2021.\n\nAlan Crawford, Ian King, and Debby Wu. The Chip Industry Has a Problem With Its Giant Carbon Footprint. https://www.bloomberg.com/news/articles/2021-04-08/the-chip-industry-has-a-problem-with-its-giant-carbon-footprint, 2021.\n\nYing Lin. 10 Internet Statistics Every Marketer Should Know in 2021. https://www.oberlo.com/blog/internet-statistics, May 2021.\n\nTom Wheeler. 5 steps to get the internet to all americans. https://www.brookings.edu/research/5-steps-to-get-the-internet-to-all-americans/, May 2020.\n\nunicef. Two thirds of the world’s school-age children have no internet access at home. https://www.unicef.org/press-releases/two-thirds-worlds-school-age-children-have-no-internet-access-home-new-unicef-itu, 2020.\n\nMicrosoft. Airband: The initiative to bring the internet to everyone. https://news.microsoft.com/on-the-issues/2020/09/01/airband-initiative-rural-broadband-digital-divide/, a.\n\nLoon. Loon: Expanding internet connectivity with stratospheric balloons. https://x.company/projects/loon/. Wikipedia. Facebook Aquila. https://en.wikipedia.org/wiki/Facebook_Aquila.", "original_types": ["text"], "id": 1213}
{"type": "section", "content": "Starlink. High-speed, low latency broadband internet. https://www.starlink.com/. Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, and Mike Hamburg. Meltdown: Reading kernel memory from user space. In Proceedings of the 27th USENIX Security Symposium, 2018.\n\nPaul Kocher, Jann Horn, Anders Fogh, , Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom. Spectre attacks: Exploiting speculative execution. In Proceedings of the 40th IEEE Symposium on Security and Privacy, 2019.\n\nMicrosoft. Azure Storage redundancy. https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy, July 2021a.\n\nMicrosoft. Designing for disaster recovery with ExpressRoute private peering. https://docs.microsoft.com/en-us/azure/expressroute/designing-for-disaster-recovery-with-expressroute-privatepeering, March 2021b.\n\nYevegeniy Sverdlik. Most Texas Data Centers Weathered the Storm, But Things Did Not Go Smoothly. https://www.datacenterknowledge.com/uptime/most-texas-data-centers-weathered-storm-things-did-not-go-smoothly, March 2021.", "doc_id": "wu2021b", "page": 13, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Starlink. High-speed, low latency broadband internet. https://www.starlink.com/. Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, and Mike Hamburg. Meltdown: Reading kernel memory from user space. In Proceedings of the 27th USENIX Security Symposium, 2018.\n\nPaul Kocher, Jann Horn, Anders Fogh, , Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom. Spectre attacks: Exploiting speculative execution. In Proceedings of the 40th IEEE Symposium on Security and Privacy, 2019.\n\nMicrosoft. Azure Storage redundancy. https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy, July 2021a.\n\nMicrosoft. Designing for disaster recovery with ExpressRoute private peering. https://docs.microsoft.com/en-us/azure/expressroute/designing-for-disaster-recovery-with-expressroute-privatepeering, March 2021b.\n\nYevegeniy Sverdlik. Most Texas Data Centers Weathered the Storm, But Things Did Not Go Smoothly. https://www.datacenterknowledge.com/uptime/most-texas-data-centers-weathered-storm-things-did-not-go-smoothly, March 2021.", "original_types": ["text"], "id": 1214}
{"type": "section", "content": "Department of Energy. Designing And Managing Data Centers For Resilience: Demand Response And Microgrids. https://www.wbdg.org/continuing-education/femp-courses/fempodw034, 2019.\n\nSammy Roth. How an Oregon wildfire almost derailed California’s power grid. https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy, July 2021.\n\nAdrian Hornsby. Patterns for resilient architecture — part 1: The story of embracing failure at scale. https://medium.com/the-cloud-architect/patterns-for-resilient-architecture-part-1-d3b60cd8d2b6, 2018.\n\nGoogle Cloud Architecture Center. Patterns for scalable and resilient apps. https://cloud.google.com/architecture/scalable-and-resilient-apps.\n\nThe World Bank. Lighting Up Africa: Bringing Renewable, Off-Grid Energy to Communities. https://www.worldbank.org/en/news/feature/2020/08/13/lighting-up-africa-bringing-renewable-off-grid-energy-to-communities, August 2020.\n\nÍñigo Goiri, Md E. Haque, Kien Le, Ryan Beauchea, Thu D. Nguyen, Jordi Guitart, Jordi Torres, and Ricardo Bianchini. Matching renewable energy supply and demand in green datacenters. Ad Hoc Networks, 25:520–534, 2015.\n\nAna Radovanovic, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne. Carbon-aware computing for datacenters. CoRR, abs/2106.11750, 2021. URL https://arxiv.org/abs/2106.11750.\n\nLiuzixuan Lin, Victor M. Zavala, and Andrew A. Chien. Evaluating coupling models for cloud datacenters and power grids. In Proceedings of the 12th ACM International Conference on Future Energy Systems, 2021.\n\nChaojie Zhang, Alok Kumbhare, Ioannis Manousakis, Deli Zhang, Pulkit Misra, Rod Assis, Kyle Woolcock, Nithish Mahalingam, Brijesh Warrier, David Gauthier, Lalu Kunnath, Steve Solomon, Osvaldo Morales, Marcus Fontoura, and Ricardo Bianchini. Flex: High-availability datacenters with zero reserved power. In Proceedings of the International Symposium on Computer Architecture, 2021.\n\nFacebook. Facebook is committed to reaching net zero emissions across our value chain in 2030, aligning our efforts with the latest science on what is needed to transition to a zero-carbon future. https://sustainability.fb.com/report-page/climate/.\n\nApple. Apple commits to be 100 percent carbon neutral for its supply chain and products by 2030. https://www.apple.com/newsroom/2020/07/apple-commits-to-be-100-percent-carbon-neutral-for-its-supply-chain-and-products-by-2030/.\n\nBrad Smith. Microsoft will be carbon negative by 2030. https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/.\n\nGoogle. The Internet is 24x7—carbon-free energy should be too. https://sustainability.google/progress/projects/24x7/, b.\n\nSam Schechner. Amazon and Other Tech Giants Race to Buy Up Renewable Energy. https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894, 2021.", "doc_id": "wu2021b", "page": 14, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Department of Energy. Designing And Managing Data Centers For Resilience: Demand Response And Microgrids. https://www.wbdg.org/continuing-education/femp-courses/fempodw034, 2019.\n\nSammy Roth. How an Oregon wildfire almost derailed California’s power grid. https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy, July 2021.\n\nAdrian Hornsby. Patterns for resilient architecture — part 1: The story of embracing failure at scale. https://medium.com/the-cloud-architect/patterns-for-resilient-architecture-part-1-d3b60cd8d2b6, 2018.\n\nGoogle Cloud Architecture Center. Patterns for scalable and resilient apps. https://cloud.google.com/architecture/scalable-and-resilient-apps.\n\nThe World Bank. Lighting Up Africa: Bringing Renewable, Off-Grid Energy to Communities. https://www.worldbank.org/en/news/feature/2020/08/13/lighting-up-africa-bringing-renewable-off-grid-energy-to-communities, August 2020.\n\nÍñigo Goiri, Md E. Haque, Kien Le, Ryan Beauchea, Thu D. Nguyen, Jordi Guitart, Jordi Torres, and Ricardo Bianchini. Matching renewable energy supply and demand in green datacenters. Ad Hoc Networks, 25:520–534, 2015.\n\nAna Radovanovic, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne. Carbon-aware computing for datacenters. CoRR, abs/2106.11750, 2021. URL https://arxiv.org/abs/2106.11750.\n\nLiuzixuan Lin, Victor M. Zavala, and Andrew A. Chien. Evaluating coupling models for cloud datacenters and power grids. In Proceedings of the 12th ACM International Conference on Future Energy Systems, 2021.\n\nChaojie Zhang, Alok Kumbhare, Ioannis Manousakis, Deli Zhang, Pulkit Misra, Rod Assis, Kyle Woolcock, Nithish Mahalingam, Brijesh Warrier, David Gauthier, Lalu Kunnath, Steve Solomon, Osvaldo Morales, Marcus Fontoura, and Ricardo Bianchini. Flex: High-availability datacenters with zero reserved power. In Proceedings of the International Symposium on Computer Architecture, 2021.\n\nFacebook. Facebook is committed to reaching net zero emissions across our value chain in 2030, aligning our efforts with the latest science on what is needed to transition to a zero-carbon future. https://sustainability.fb.com/report-page/climate/.\n\nApple. Apple commits to be 100 percent carbon neutral for its supply chain and products by 2030. https://www.apple.com/newsroom/2020/07/apple-commits-to-be-100-percent-carbon-neutral-for-its-supply-chain-and-products-by-2030/.\n\nBrad Smith. Microsoft will be carbon negative by 2030. https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/.\n\nGoogle. The Internet is 24x7—carbon-free energy should be too. https://sustainability.google/progress/projects/24x7/, b.\n\nSam Schechner. Amazon and Other Tech Giants Race to Buy Up Renewable Energy. https://www.wsj.com/articles/amazon-and-other-tech-giants-race-to-buy-up-renewable-energy-11624438894, 2021.", "original_types": ["text"], "id": 1215}
{"type": "section", "content": "Steven Lee Myers. China’s Pledge to Be Carbon Neutral by 2060: What It Means. https://www.nytimes.com/2020/09/23/world/asia/china-climate-change.html, September 2020.\n\nBBC. Climate change: EU to cut CO2 emissions by 55% by 2030. https://www.bbc.com/news/world-europe-56828383, April 2021.\n\nMike Orcuttarchive. A biodegradable computer chip that performs surprisingly well. https://www.technologyreview.com/2015/07/14/167161/a-biodegradable-computer-chip-that-performs-surprisingly-well/, 2015.\n\nTing-Jung Chang, Zhuozhi Yao, Paul J. Jackson, Barry P. Rand, and David Wentzlaff. Architectural tradeoffs for biodegradable computing. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture, page 706–717, 2017.\n\nAmazon. The Carbon Reduction Opportunity of Moving to Amazon Web Services. https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf, 2019.\n\nMicrosoft. The Carbon Benefits of Cloud Computing: a Study of the Microsoft Cloud. https://www.microsoft.com/en-us/download/details.aspx?id=56950, 2020.\n\nRichard Evans and Jim Gao. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%. https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40, 2016.", "doc_id": "wu2021b", "page": 14, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Steven Lee Myers. China’s Pledge to Be Carbon Neutral by 2060: What It Means. https://www.nytimes.com/2020/09/23/world/asia/china-climate-change.html, September 2020.\n\nBBC. Climate change: EU to cut CO2 emissions by 55% by 2030. https://www.bbc.com/news/world-europe-56828383, April 2021.\n\nMike Orcuttarchive. A biodegradable computer chip that performs surprisingly well. https://www.technologyreview.com/2015/07/14/167161/a-biodegradable-computer-chip-that-performs-surprisingly-well/, 2015.\n\nTing-Jung Chang, Zhuozhi Yao, Paul J. Jackson, Barry P. Rand, and David Wentzlaff. Architectural tradeoffs for biodegradable computing. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture, page 706–717, 2017.\n\nAmazon. The Carbon Reduction Opportunity of Moving to Amazon Web Services. https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf, 2019.\n\nMicrosoft. The Carbon Benefits of Cloud Computing: a Study of the Microsoft Cloud. https://www.microsoft.com/en-us/download/details.aspx?id=56950, 2020.\n\nRichard Evans and Jim Gao. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%. https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40, 2016.", "original_types": ["text"], "id": 1216}
{"type": "section", "content": "Dan Lee and Jonathan Rowe. Software, servers, systems, sensors, and science: Facebook’s recipe for hyperefficient data centers. https://tech.fb.com/hyperefficient-data-centers/, 2020.\n\nEric Masanet, Arman Shehabi, Nuo Lei, Sarah Smith, and Jonathan Koomey. Recalibrating global data center energy-use estimates. Science, 367(6481):984–986, 2020.\n\nMauro Cordella, Felice Alfieri, Christian Clemm, and Anton Berwald. Durability of smartphones: A technical analysis of reliability and repairability aspects. Journal of Cleaner Production, 286:125388, 12 2020. doi:10.1016/j.jclepro.2020.125388.\n\nRhonda Ascierto and Andy Lawrence. Uptime Institute global data center survey 2020. https://uptimeinstitute.com/2020-data-center-industry-survey-results, 2020.\n\nFairphone. The world’s most sustainable smartphone now at a lower price. https://www.fairphone.com/en/.\n\nKyle Wiens and Gay Gordon-Byrne. Why We Must Fight for the Right to Repair Our Electronics. https://spectrum.ieee.org/green-tech/conservation/why-we-must-fight-for-the-right-to-repair-our-electronics, 2017.\n\nJennifer Alsever. What Biden’s 'right-to-repair' order could mean for Apple and Tesla. https://fortune.com/2021/07/09/right-to-repair-order-biden-apple-tesla-hacks/, July 2021.\n\nMicrosoft. FarmBeats: Democratizing AI for farmers around the world. https://www.microsoft.com/en-us/garage/wall-of-fame/farmbeats/, 2015.\n\nBill Gates. How to Avoid a Climate Disaster: The Solutions We Have and the Breakthroughs We Need. Knopf, 2021.\n\nU.S. Energy Information Adminstration. Electricity explained: Use of electricity. https://www.eia.gov/energyexplained/electricity/use-of-electricity.php.\n\nC. Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere, Muhammed Shuaibi, Anuroop Sriram, Kevin Tran, Brandon Wood, Junwoong Yoon, Devi Parikh, and Zachary Ulissi. An introduction to electrocatalyst design using machine learning for renewable energy storage, 2020.\n\nCarl Elkin and Sims Witherspoon. Machine learning can boost the value of wind energy. https://deepmind.com/blog/article/machine-learning-can-boost-value-wind-energy.\n\nJeff Tollefson. COVID curbed carbon emissions in 2020 — but not by much. https://www.nature.com/articles/d41586-021-00090-3, 2021.\n\nJichuan Chang, Justin Meza, P. Ranganathan, C. Bash, and Amip Shah. Green server design: beyond operational energy to sustainability. In Proceedings of the 2010 international conference on Power aware computing and systems, 2010.\n\nM. Bardon, P. Wuytens, L.-Å. Ragnarsson, G. Mirabelli, D. Jang, G. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais. Dtco including sustainability: Power-performance-area-cost-environmental score (ppace) analysis for logic technologies. In Proceedings of the IEEE International Electron Devices Meeting, 2020.", "doc_id": "wu2021b", "page": 15, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Dan Lee and Jonathan Rowe. Software, servers, systems, sensors, and science: Facebook’s recipe for hyperefficient data centers. https://tech.fb.com/hyperefficient-data-centers/, 2020.\n\nEric Masanet, Arman Shehabi, Nuo Lei, Sarah Smith, and Jonathan Koomey. Recalibrating global data center energy-use estimates. Science, 367(6481):984–986, 2020.\n\nMauro Cordella, Felice Alfieri, Christian Clemm, and Anton Berwald. Durability of smartphones: A technical analysis of reliability and repairability aspects. Journal of Cleaner Production, 286:125388, 12 2020. doi:10.1016/j.jclepro.2020.125388.\n\nRhonda Ascierto and Andy Lawrence. Uptime Institute global data center survey 2020. https://uptimeinstitute.com/2020-data-center-industry-survey-results, 2020.\n\nFairphone. The world’s most sustainable smartphone now at a lower price. https://www.fairphone.com/en/.\n\nKyle Wiens and Gay Gordon-Byrne. Why We Must Fight for the Right to Repair Our Electronics. https://spectrum.ieee.org/green-tech/conservation/why-we-must-fight-for-the-right-to-repair-our-electronics, 2017.\n\nJennifer Alsever. What Biden’s 'right-to-repair' order could mean for Apple and Tesla. https://fortune.com/2021/07/09/right-to-repair-order-biden-apple-tesla-hacks/, July 2021.\n\nMicrosoft. FarmBeats: Democratizing AI for farmers around the world. https://www.microsoft.com/en-us/garage/wall-of-fame/farmbeats/, 2015.\n\nBill Gates. How to Avoid a Climate Disaster: The Solutions We Have and the Breakthroughs We Need. Knopf, 2021.\n\nU.S. Energy Information Adminstration. Electricity explained: Use of electricity. https://www.eia.gov/energyexplained/electricity/use-of-electricity.php.\n\nC. Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere, Muhammed Shuaibi, Anuroop Sriram, Kevin Tran, Brandon Wood, Junwoong Yoon, Devi Parikh, and Zachary Ulissi. An introduction to electrocatalyst design using machine learning for renewable energy storage, 2020.\n\nCarl Elkin and Sims Witherspoon. Machine learning can boost the value of wind energy. https://deepmind.com/blog/article/machine-learning-can-boost-value-wind-energy.\n\nJeff Tollefson. COVID curbed carbon emissions in 2020 — but not by much. https://www.nature.com/articles/d41586-021-00090-3, 2021.\n\nJichuan Chang, Justin Meza, P. Ranganathan, C. Bash, and Amip Shah. Green server design: beyond operational energy to sustainability. In Proceedings of the 2010 international conference on Power aware computing and systems, 2010.\n\nM. Bardon, P. Wuytens, L.-Å. Ragnarsson, G. Mirabelli, D. Jang, G. Willems, A. Mallik, A. Spessot, J. Ryckaert, and B. Parvais. Dtco including sustainability: Power-performance-area-cost-environmental score (ppace) analysis for logic technologies. In Proceedings of the IEEE International Electron Devices Meeting, 2020.", "original_types": ["text"], "id": 1217}
{"type": "section", "content": "David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training, 2021.\n\nGregory D. Hager, Ann Drobnis, Fei Fang, Rayid Ghani, Amy Greenwald, Terah Lyons, David C. Parkes, Jason Schultz, Suchi Saria, Stephen F. Smith, and Milind Tambe. Artificial intelligence for social good, 2019.\n\nNenad Tomasev, Julien Cornebise, F. Hutter, S. Mohamed, Angela Picciariello, Bec Connelly, D. Belgrave, Daphne Ezer, Fanny Cachat van der Haert, Frank Mugisha, G. Abila, Hiromi Arai, Hisham Almiraat, Julia Proskurnia, Kyle Snyder, M. Otake-Matsuura, M. Othman, T. Glasmachers, W. D. Wever, Y. Teh, M. E. Khan, Ruben De Winne, Tom Schaul, and C. Clopath. AI for social good: unlocking the opportunity for positive impact. Nature Communications, 11, 2020.\n\nOwen Mulhern. Artificial intelligence: Can it help achieve environmental sustainability? https://earth.org/data_visualization/ai-can-it-help-achieve-environmental-sustainable/, 2021.\n\nUnited Nation. The Good, The Bad And The Blockchain. https://unfccc.int/blog/the-good-the-bad-and-the-blockchain, 2021.\n\nSE Vollset, E. Goren, CW Yuan, J Cao, AE Smith, T Hsiao, C Bisignano, GS Azhar, E Castro, J Chalek, AJ Dolgert, T Frank, K Fukutaki, SI Hay, R Lozano, AH Mokdad, V Nandakumar, M Pierce, M Pletcher, T Robalik, KM Steuben, HY Wunrow, BS Zlavog, and CJL Murray. Fertility, mortality, migration, and population scenarios for 195 countries and territories from 2017 to 2100: a forecasting analysis for the Global Burden of Disease Study. Lancet, 396, October 2020.", "doc_id": "wu2021b", "page": 15, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training, 2021.\n\nGregory D. Hager, Ann Drobnis, Fei Fang, Rayid Ghani, Amy Greenwald, Terah Lyons, David C. Parkes, Jason Schultz, Suchi Saria, Stephen F. Smith, and Milind Tambe. Artificial intelligence for social good, 2019.\n\nNenad Tomasev, Julien Cornebise, F. Hutter, S. Mohamed, Angela Picciariello, Bec Connelly, D. Belgrave, Daphne Ezer, Fanny Cachat van der Haert, Frank Mugisha, G. Abila, Hiromi Arai, Hisham Almiraat, Julia Proskurnia, Kyle Snyder, M. Otake-Matsuura, M. Othman, T. Glasmachers, W. D. Wever, Y. Teh, M. E. Khan, Ruben De Winne, Tom Schaul, and C. Clopath. AI for social good: unlocking the opportunity for positive impact. Nature Communications, 11, 2020.\n\nOwen Mulhern. Artificial intelligence: Can it help achieve environmental sustainability? https://earth.org/data_visualization/ai-can-it-help-achieve-environmental-sustainable/, 2021.\n\nUnited Nation. The Good, The Bad And The Blockchain. https://unfccc.int/blog/the-good-the-bad-and-the-blockchain, 2021.\n\nSE Vollset, E. Goren, CW Yuan, J Cao, AE Smith, T Hsiao, C Bisignano, GS Azhar, E Castro, J Chalek, AJ Dolgert, T Frank, K Fukutaki, SI Hay, R Lozano, AH Mokdad, V Nandakumar, M Pierce, M Pletcher, T Robalik, KM Steuben, HY Wunrow, BS Zlavog, and CJL Murray. Fertility, mortality, migration, and population scenarios for 195 countries and territories from 2017 to 2100: a forecasting analysis for the Global Burden of Disease Study. Lancet, 396, October 2020.", "original_types": ["text"], "id": 1218}
{"type": "section", "content": "Max Roser. Future Population Growth. https://ourworldindata.org/future-population-growth, November 2019.\n\nM. Jacobs. The effectiveness of word prediction software wordq: \"...predict it, hear it, choose it, review it, correct it, write it now...\". 2015.\n\nTodd Haselton. This hidden AirPods feature helps you hear better. https://www.cnbc.com/2019/01/20/how-to-turn-airpods-into-hearing-aids.html, 2019.\n\nEliya Nachmani, Yossi Adi, and Lior Wolf. Voice separation with an unknown number of multiple speakers, 2020.\n\nNicholas Fearn. How VR Is Helping Visually Impaired Patients Regain Close To Normal Levels Of Sight. https://www.forbes.com/sites/nicholasfearn/2020/01/08/how-vr-is-helping-visually-impaired-patients-regain-close-to-normal-levels-of-sight/?sh=15e1a9c60277, 2020.\n\nAmalkumar Ghosh, Evan P. Donoghue, Ilyas Khayrullin, Tariq Ali, Ihor Wacyk, Kerry Tice, Fridrich Vazan, Laurie Sziklas, David Fellowes, and Russell Draper. Invited paper: Directly pattereded 2645 ppi full color oled microdisplay for head mounted wearables. Society for Information Display Symposium Digest of Technical Papers, 47(1), 2016.\n\nTaylor Kubota. Stanford materials scientists borrow solar panel tech to create new ultrahigh-res OLED display. https://news.stanford.edu/2020/10/22/future-vr-employ-new-ultrahigh-res-display/, 2020.\n\nVenkatesh Kodukula, Saad Katrawala, Britton Jones, Carole-Jean Wu, and Robert LiKamWa. Dynamic temperature management of near-sensor processing for energy-efficient high-fidelity imaging. Sensors, 21(3), 2021.\n\nJ. Devin MacKenzie and Christine Ho. Perspectives on energy storage for flexible electronic systems. Proceedings of the IEEE, 103(4):535–553, 2015.\n\nP Brunner, L Bianchi, C Guger, F Cincotti, and G. Schalk. Current trends in hardware and software for brain-computer interfaces (bcis). In Journal of Neural Engineering, 2011.\n\nIoannis Karageorgos, Karthik Sriram, Ján Veselý, Michael Wu, Marc Powell, David Borton, Rajit Manohar, and Abhishek Bhattacharjee. Hardware-software co-design for brain-computer interfaces. In Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\nInternational Energy Agency. Access to electricity. https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity.\n\nSibren Isaacman and Margaret Martonosi. Low-infrastructure methods to improve internet access for mobile users in emerging regions. In Proceedings of the 20th International Conference Companion on World Wide Web, page 473–482, 2011.\n\nJosiah Hester, Trae King, Alex Propst, Kalyan Piratla, and Jacob Sorber. Enabling sustainable sensing in adverse environments. In Proceedings of the IEEE International Conference on Sensing, Communications and Networking, pages 249–251, 2013.\n\nBrandon Lucia, Vignesh Balaji, A. Colin, Kiwan Maeng, and E. Ruppel. Intermittent computing: Challenges and opportunities. In SNAPL, 2017.", "doc_id": "wu2021b", "page": 16, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Max Roser. Future Population Growth. https://ourworldindata.org/future-population-growth, November 2019.\n\nM. Jacobs. The effectiveness of word prediction software wordq: \"...predict it, hear it, choose it, review it, correct it, write it now...\". 2015.\n\nTodd Haselton. This hidden AirPods feature helps you hear better. https://www.cnbc.com/2019/01/20/how-to-turn-airpods-into-hearing-aids.html, 2019.\n\nEliya Nachmani, Yossi Adi, and Lior Wolf. Voice separation with an unknown number of multiple speakers, 2020.\n\nNicholas Fearn. How VR Is Helping Visually Impaired Patients Regain Close To Normal Levels Of Sight. https://www.forbes.com/sites/nicholasfearn/2020/01/08/how-vr-is-helping-visually-impaired-patients-regain-close-to-normal-levels-of-sight/?sh=15e1a9c60277, 2020.\n\nAmalkumar Ghosh, Evan P. Donoghue, Ilyas Khayrullin, Tariq Ali, Ihor Wacyk, Kerry Tice, Fridrich Vazan, Laurie Sziklas, David Fellowes, and Russell Draper. Invited paper: Directly pattereded 2645 ppi full color oled microdisplay for head mounted wearables. Society for Information Display Symposium Digest of Technical Papers, 47(1), 2016.\n\nTaylor Kubota. Stanford materials scientists borrow solar panel tech to create new ultrahigh-res OLED display. https://news.stanford.edu/2020/10/22/future-vr-employ-new-ultrahigh-res-display/, 2020.\n\nVenkatesh Kodukula, Saad Katrawala, Britton Jones, Carole-Jean Wu, and Robert LiKamWa. Dynamic temperature management of near-sensor processing for energy-efficient high-fidelity imaging. Sensors, 21(3), 2021.\n\nJ. Devin MacKenzie and Christine Ho. Perspectives on energy storage for flexible electronic systems. Proceedings of the IEEE, 103(4):535–553, 2015.\n\nP Brunner, L Bianchi, C Guger, F Cincotti, and G. Schalk. Current trends in hardware and software for brain-computer interfaces (bcis). In Journal of Neural Engineering, 2011.\n\nIoannis Karageorgos, Karthik Sriram, Ján Veselý, Michael Wu, Marc Powell, David Borton, Rajit Manohar, and Abhishek Bhattacharjee. Hardware-software co-design for brain-computer interfaces. In Proceedings of the ACM/IEEE Annual International Symposium on Computer Architecture, 2020.\n\nInternational Energy Agency. Access to electricity. https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity.\n\nSibren Isaacman and Margaret Martonosi. Low-infrastructure methods to improve internet access for mobile users in emerging regions. In Proceedings of the 20th International Conference Companion on World Wide Web, page 473–482, 2011.\n\nJosiah Hester, Trae King, Alex Propst, Kalyan Piratla, and Jacob Sorber. Enabling sustainable sensing in adverse environments. In Proceedings of the IEEE International Conference on Sensing, Communications and Networking, pages 249–251, 2013.\n\nBrandon Lucia, Vignesh Balaji, A. Colin, Kiwan Maeng, and E. Ruppel. Intermittent computing: Challenges and opportunities. In SNAPL, 2017.", "original_types": ["text"], "id": 1219}
{"type": "section", "content": "Bilal Saleem, Paul Schmitt, Jay Chen, and Barath Raghavan. Beyond the trees: Resilient multipath for last-mile WISP networks. CoRR, abs/2002.12473, 2020.\n\nOzlem Bilgir Yetim and Margaret Martonosi. Dynamic adaptive techniques for learning application delay tolerance for mobile data offloading. In 2015 IEEE Conference on Computer Communications (INFOCOM), pages 1885–1893, 2015.\n\nBrad Plumer. Energy Department Targets Vastly Cheaper Batteries to Clean Up the Grid. https://www.nytimes.com/2021/07/14/climate/renewable-energy-batteries.html, 2021.\n\nJonathan Stray. Beyond Engagement: Aligning Algorithmic Recommendations With Prosocial Goals. https://www.partnershiponai.org/beyond-engagement-aligning-algorithmic-recommendations-with-prosocial-goals/, 2021.\n\nAmanda Askell, Miles Brundage, and Gillian Hadfield. The role of cooperation in responsible ai development, 2019.\n\nNIST. Nist proposes approach for reducing risk of bias in artificial intelligence. https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence, 2021.\n\nIBM. Ai ethics. https://www.ibm.com/artificial-intelligence/ethics.\n\nMicrosoft. Trustworthy AI. https://www.microsoft.com/en-us/research/project/trustworthy-ai/, b.", "doc_id": "wu2021b", "page": 16, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Bilal Saleem, Paul Schmitt, Jay Chen, and Barath Raghavan. Beyond the trees: Resilient multipath for last-mile WISP networks. CoRR, abs/2002.12473, 2020.\n\nOzlem Bilgir Yetim and Margaret Martonosi. Dynamic adaptive techniques for learning application delay tolerance for mobile data offloading. In 2015 IEEE Conference on Computer Communications (INFOCOM), pages 1885–1893, 2015.\n\nBrad Plumer. Energy Department Targets Vastly Cheaper Batteries to Clean Up the Grid. https://www.nytimes.com/2021/07/14/climate/renewable-energy-batteries.html, 2021.\n\nJonathan Stray. Beyond Engagement: Aligning Algorithmic Recommendations With Prosocial Goals. https://www.partnershiponai.org/beyond-engagement-aligning-algorithmic-recommendations-with-prosocial-goals/, 2021.\n\nAmanda Askell, Miles Brundage, and Gillian Hadfield. The role of cooperation in responsible ai development, 2019.\n\nNIST. Nist proposes approach for reducing risk of bias in artificial intelligence. https://www.nist.gov/news-events/news/2021/06/nist-proposes-approach-reducing-risk-bias-artificial-intelligence, 2021.\n\nIBM. Ai ethics. https://www.ibm.com/artificial-intelligence/ethics.\n\nMicrosoft. Trustworthy AI. https://www.microsoft.com/en-us/research/project/trustworthy-ai/, b.", "original_types": ["text"], "id": 1220}
{"type": "section", "content": "Google. Building responsible ai for everyone. https://ai.google/responsibilities/, c.\n\nFacebook. Facebook’s five pillars of Responsible AI. https://ai.facebook.com/blog/facebook-five-pillars-of-responsible-ai/.\n\nDOD. AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense. https://media.defense.gov/2019/Oct/31/2002204458/-1/-1/0/DIB_AI_PRINCIPLES_PRIMARY_DOCUMENT.PDF.\n\nMax Roser. Mobile devices are too expensive for billions of people – and it’s keeping them offline. https://a4ai.org/mobile-devices-are-too-expensive-for-billions-of-people-and-its-keeping-them-offline/, August 2020.\n\nMustafa Naseem, Bilal Saleem, Sacha St-Onge Ahmad, Jay Chen, and Agha Ali Raza. An Empirical Comparison of Technologically Mediated Advertising in Under-Connected Populations, page 1–13. 2020.\n\nDaniel W. O’Neill, Andrew L. Fanning, William F. Lamb, and J. Steinberger. A good life for all within planetary boundaries. Nature Sustainability, 1:88–95, 2018.", "doc_id": "wu2021b", "page": 17, "url": "https://arxiv.org/pdf/2108.06738", "embedded_text": "Google. Building responsible ai for everyone. https://ai.google/responsibilities/, c.\n\nFacebook. Facebook’s five pillars of Responsible AI. https://ai.facebook.com/blog/facebook-five-pillars-of-responsible-ai/.\n\nDOD. AI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense. https://media.defense.gov/2019/Oct/31/2002204458/-1/-1/0/DIB_AI_PRINCIPLES_PRIMARY_DOCUMENT.PDF.\n\nMax Roser. Mobile devices are too expensive for billions of people – and it’s keeping them offline. https://a4ai.org/mobile-devices-are-too-expensive-for-billions-of-people-and-its-keeping-them-offline/, August 2020.\n\nMustafa Naseem, Bilal Saleem, Sacha St-Onge Ahmad, Jay Chen, and Agha Ali Raza. An Empirical Comparison of Technologically Mediated Advertising in Under-Connected Populations, page 1–13. 2020.\n\nDaniel W. O’Neill, Andrew L. Fanning, William F. Lamb, and J. Steinberger. A good life for all within planetary boundaries. Nature Sustainability, 1:88–95, 2018.", "original_types": ["text"], "id": 1221}
{"type": "section", "content": "ABSTRACT\n\nThis paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV, NLP and ASR models. To expand the current training options further, we compare the scalability potential for hybrid-cloud scenarios by adding cloud resources to on-premise hardware to improve training throughput. Finally, we show how leveraging spot instance pricing enables a new cost-efficient way to train models with multiple cheap VMs, trumping both more centralized and powerful hardware and even on-demand cloud offerings at competitive prices.", "doc_id": "erben2023", "page": 1, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "ABSTRACT\n\nThis paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV, NLP and ASR models. To expand the current training options further, we compare the scalability potential for hybrid-cloud scenarios by adding cloud resources to on-premise hardware to improve training throughput. Finally, we show how leveraging spot instance pricing enables a new cost-efficient way to train models with multiple cheap VMs, trumping both more centralized and powerful hardware and even on-demand cloud offerings at competitive prices.", "original_types": ["text", "header"], "id": 1222}
{"type": "table", "content": "Table 1: Average us-west cloud pricing in April '23.\nMarkdown representation of the table", "doc_id": "erben2023", "page": 1, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Table 1: Average us-west cloud pricing in April '23.\nMarkdown representation of the table", "id": 1223}
{"type": "figure", "content": "Figure 1: Cost to throughput tradeoff for ConvNextLarge at different instance types. Our training setups (circled) are cheaper (8xT4) and faster (8xA10) than centralized offerings (DGX-2.)", "doc_id": "erben2023", "page": 1, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 1: Cost to throughput tradeoff for ConvNextLarge at different instance types. Our training setups (circled) are cheaper (8xT4) and faster (8xA10) than centralized offerings (DGX-2.)", "id": 1224}
{"type": "section", "content": "1. Introduction\n\nWe analyze the impact of multi-cloud training with spot and on-demand instances from Google Cloud (GC), Microsoft Azure, Amazon Web Services (AWS), and LambdaLabs on cost-efficiency. While we find performance penalties due to remote versus on-premise compute resources, the throughput still scales with increased computing power. By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.\n\n2. Deep Learning on Spot Instances\n\nIn this section, we describe how the Hivemind framework works and how it can enable distributed spot training.\n\n2.1 Hivemind\n\nHivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training. It does so with two features: a distributed hash table [31] (DHT) which spans over all participating peers for metadata storage, such as training progress and peer health, and a gradient averaging algorithm that is designed to reduce the impact of lost gradients. A key difference to other distributed training frameworks is the definition of a hivemind epoch, which is the number of samples that must be aggregated before an averaging step is performed. This sample count is called the target batch size (TBS), which corresponds to the minibatch size in standard DL training. The DHT is used for coordination, and shortly before the TBS is predicted to be reached, the peers start to form the initial groups for averaging. The time allocated for group forming is called matchmaking time and typically runs asynchronously to the training (cf. Section 3). The individual peer gradients are accumulated locally and sent to the other peers via an adaptive all-reduce algorithm (MoshpitSGD [38]). The next hivemind epoch starts after each peer applies the accumulated gradients to the local model. The advantage of Hivemind for geo-distributed training comes from cumulating different techniques, such as Delayed Parameter Updates [36], big-batch training [44] and aggressive communication quantization [16]. All of these combined reduce time and frequency of the communication rounds, which in turn makes training on heterogeneous devices and low-bandwidth networks possible.\n\n2.2 Distributed Spot Training", "doc_id": "erben2023", "page": 2, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "1. Introduction\n\nWe analyze the impact of multi-cloud training with spot and on-demand instances from Google Cloud (GC), Microsoft Azure, Amazon Web Services (AWS), and LambdaLabs on cost-efficiency. While we find performance penalties due to remote versus on-premise compute resources, the throughput still scales with increased computing power. By leveraging multiple spot instances with one T4 GPU each, we can be more cost-efficient than a DGX-2 node or the very competitively priced A10 offerings from LambdaLabs.\n\n2. Deep Learning on Spot Instances\n\nIn this section, we describe how the Hivemind framework works and how it can enable distributed spot training.\n\n2.1 Hivemind\n\nHivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training. It does so with two features: a distributed hash table [31] (DHT) which spans over all participating peers for metadata storage, such as training progress and peer health, and a gradient averaging algorithm that is designed to reduce the impact of lost gradients. A key difference to other distributed training frameworks is the definition of a hivemind epoch, which is the number of samples that must be aggregated before an averaging step is performed. This sample count is called the target batch size (TBS), which corresponds to the minibatch size in standard DL training. The DHT is used for coordination, and shortly before the TBS is predicted to be reached, the peers start to form the initial groups for averaging. The time allocated for group forming is called matchmaking time and typically runs asynchronously to the training (cf. Section 3). The individual peer gradients are accumulated locally and sent to the other peers via an adaptive all-reduce algorithm (MoshpitSGD [38]). The next hivemind epoch starts after each peer applies the accumulated gradients to the local model. The advantage of Hivemind for geo-distributed training comes from cumulating different techniques, such as Delayed Parameter Updates [36], big-batch training [44] and aggressive communication quantization [16]. All of these combined reduce time and frequency of the communication rounds, which in turn makes training on heterogeneous devices and low-bandwidth networks possible.\n\n2.2 Distributed Spot Training", "original_types": ["text", "header"], "id": 1225}
{"type": "section", "content": "In this paper, we focus only on models that fit into the memory of a single GPU, as we are interested in utilizing data parallelism on cheaper and more readily available hardware. However, our insights are applicable to larger models with techniques such as ZeRO offloading [36], more aggressive quantization [41] and even model parallelism [37]. The current options for data parallelism are either using multiple GPUs on the same node (e.g., a DGX system with eight GPUs) or having multiple nodes with a GPU each in the same high-bandwidth network (>25 Gb/s) to minimize communication time. The latter does not work on cheap but interruptable instances,", "doc_id": "erben2023", "page": 2, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "In this paper, we focus only on models that fit into the memory of a single GPU, as we are interested in utilizing data parallelism on cheaper and more readily available hardware. However, our insights are applicable to larger models with techniques such as ZeRO offloading [36], more aggressive quantization [41] and even model parallelism [37]. The current options for data parallelism are either using multiple GPUs on the same node (e.g., a DGX system with eight GPUs) or having multiple nodes with a GPU each in the same high-bandwidth network (>25 Gb/s) to minimize communication time. The latter does not work on cheap but interruptable instances,", "original_types": ["text"], "id": 1226}
{"type": "section", "content": "Model Suitability\n\nSelecting suitable models with a big enough parallel workload is essential to ensure successful distributed spot training. To cover a wide range of established models, we drew from MLCommons' comprehensive DL training benchmark [30]. We used models from the CV and NLP domains and gradually increased their size and TBS to increase the parallel compute amount. As discussed in Section 2, the TBS may be exclusively responsible for the success of distributed training and was chosen to cover both medium and large batches (8K, 16K and 32K). These minibatch sizes start to become more common due to the LAMB optimizer [44], which works well enough for both smaller (512) and huge batches (64K) and should be representative of state-of-the-art workloads. For a representatative experimental study with a minibatch size of 256 on the automatic speech recognition model (Whisper [34]), please refer to Section 11. All experiments were run with FP16 precision, as the target T4 GPUs have a considerable improvement in FLOPs compared to FP32 (8:1).\n\nFor CV, we take five models from the extended ResNet family, starting with the smallest one, ResNet18 [21] (RN18), ResNet50 (RN50), ResNet152 (RN152), WideResNet101_2 [46] (WRN101) and ConvNextLarge [29] (CONV), which is almost 20 times larger than RN18. The paramter count is 11.7M, 25.6M, 60.2M, 126.9M, and 197.8M, respectively. These models were popularized due to their ability to help with the vanishing gradient problem by using residual connections between layers. Currently, they are not only used for classification, but can serve as an embedding of images by removing the classification head [18, 40]. For the dataset, we use Imagenet1K [15] and train the classification task, which tries to assign one of 1000 classes to each image.", "doc_id": "erben2023", "page": 3, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Model Suitability\n\nSelecting suitable models with a big enough parallel workload is essential to ensure successful distributed spot training. To cover a wide range of established models, we drew from MLCommons' comprehensive DL training benchmark [30]. We used models from the CV and NLP domains and gradually increased their size and TBS to increase the parallel compute amount. As discussed in Section 2, the TBS may be exclusively responsible for the success of distributed training and was chosen to cover both medium and large batches (8K, 16K and 32K). These minibatch sizes start to become more common due to the LAMB optimizer [44], which works well enough for both smaller (512) and huge batches (64K) and should be representative of state-of-the-art workloads. For a representatative experimental study with a minibatch size of 256 on the automatic speech recognition model (Whisper [34]), please refer to Section 11. All experiments were run with FP16 precision, as the target T4 GPUs have a considerable improvement in FLOPs compared to FP32 (8:1).\n\nFor CV, we take five models from the extended ResNet family, starting with the smallest one, ResNet18 [21] (RN18), ResNet50 (RN50), ResNet152 (RN152), WideResNet101_2 [46] (WRN101) and ConvNextLarge [29] (CONV), which is almost 20 times larger than RN18. The paramter count is 11.7M, 25.6M, 60.2M, 126.9M, and 197.8M, respectively. These models were popularized due to their ability to help with the vanishing gradient problem by using residual connections between layers. Currently, they are not only used for classification, but can serve as an embedding of images by removing the classification head [18, 40]. For the dataset, we use Imagenet1K [15] and train the classification task, which tries to assign one of 1000 classes to each image.", "original_types": ["text", "header"], "id": 1227}
{"type": "figure", "content": "Figure 2: Hivemind penalty on normalized throughputs.", "doc_id": "erben2023", "page": 4, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 2: Hivemind penalty on normalized throughputs.", "id": 1228}
{"type": "figure", "content": "Figure 3: Throughput comparison between single GPU baselines and the Hivemind runs with two GPUs.", "doc_id": "erben2023", "page": 4, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 3: Throughput comparison between single GPU baselines and the Hivemind runs with two GPUs.", "id": 1229}
{"type": "figure", "content": "Figure 4: TBS vs. total training time on 2xA10s. Granularity is shown above each bar. Dotted lines separate different models.", "doc_id": "erben2023", "page": 4, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 4: TBS vs. total training time on 2xA10s. Granularity is shown above each bar. Dotted lines separate different models.", "id": 1230}
{"type": "section", "content": "Our experimental results in Figure 3 show the practical implications of this observation. For the 2xGPU experiments in Figures 3b and 3d, we can see the effect of a TBS increase which improves the total throughput. Doubling the TBS equals cutting down the per-sample communication cost by two, which leads to the slight increase in performance visible in both CV and NLP experiments. However, the smallest models, RN18 and RBase, fluctuate significantly at a TBS of 8K due to a minimum matchmaking time of 5 seconds. Whenever all peers accumulate the TBS in less than 5 seconds, the asynchronous thread that matches the peers in groups to perform the all-reduce may still need to finish. This results in an unstable averaging time, which limits the scalability of small models with a small TBS.", "doc_id": "erben2023", "page": 4, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Our experimental results in Figure 3 show the practical implications of this observation. For the 2xGPU experiments in Figures 3b and 3d, we can see the effect of a TBS increase which improves the total throughput. Doubling the TBS equals cutting down the per-sample communication cost by two, which leads to the slight increase in performance visible in both CV and NLP experiments. However, the smallest models, RN18 and RBase, fluctuate significantly at a TBS of 8K due to a minimum matchmaking time of 5 seconds. Whenever all peers accumulate the TBS in less than 5 seconds, the asynchronous thread that matches the peers in groups to perform the all-reduce may still need to finish. This results in an unstable averaging time, which limits the scalability of small models with a small TBS.", "original_types": ["text"], "id": 1231}
{"type": "section", "content": "GEO-DISTRIBUTED PERFORMANCE\n\nAs spot prices for the same hardware differ depending on the region, zone, and time of day [23], it might be a good idea to use VMs across different data centers. However, is the connectivity between regions and continents good enough to enable distributed deep learning? To explore this question, we decided to conduct three types of experiments (Table 2):", "doc_id": "erben2023", "page": 5, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "GEO-DISTRIBUTED PERFORMANCE\n\nAs spot prices for the same hardware differ depending on the region, zone, and time of day [23], it might be a good idea to use VMs across different data centers. However, is the connectivity between regions and continents good enough to enable distributed deep learning? To explore this question, we decided to conduct three types of experiments (Table 2):", "original_types": ["text", "header"], "id": 1232}
{"type": "table", "content": "Table 2: Geo-distributed experiments on GC with T4 VMs.\nMarkdown representation of the table", "doc_id": "erben2023", "page": 5, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Table 2: Geo-distributed experiments on GC with T4 VMs.\nMarkdown representation of the table", "id": 1233}
{"type": "figure", "content": "Figure 5: Throughput comparison from 1 to 8 A10 GPUs.", "doc_id": "erben2023", "page": 5, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 5: Throughput comparison from 1 to 8 A10 GPUs.", "id": 1234}
{"type": "figure", "content": "Figure 6: Multi-GPU scalability at 32K TBS. Granularity is shown above each bar. Dotted lines separate different models.", "doc_id": "erben2023", "page": 5, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 6: Multi-GPU scalability at 32K TBS. Granularity is shown above each bar. Dotted lines separate different models.", "id": 1235}
{"type": "section", "content": "Experimental design. Based on the insights from Section 3, we decided to use the largest models (CONV, RXLM) for all further cloud experiments in Sections 4 to 6, with the TBS of 32K as a baseline with good scaling properties. We abbreviate them with their respective domain names (CV, NLP). We used Google Cloud [5] for all experiments in this section, as they were the first to give us access to all necessary zones. The default networking solution in GC is the \"Premium Tier\", which tries to use a Google-owned network instead of the public internet. We measured the throughput and latency between all zones via iperf and ping and report the average of 5 consecutive runs in Table 3. Unsurprisingly, the diagonal shows that the local connectivity between zones runs at almost 7 Gb/s with a latency of 0.7ms, probably due to the hypervisors being in the same data center. While the up- and download were perfectly symmetrical in all setups, the throughput dropped to <210 Mb/s for all non-local connections. The US-based data center is located in Iowa and is best connected with at least 120 Mb/s to the remaining regions, namely Belgium in the EU (6,911km), Taiwan in ASIA (11,853km), and Sydney in Australia (AUS, 14,555km), presumably due to the physical distance. The lowest bandwidth and highest latency connections are between the EU region and ASIA and AUS, reaching around 80 Mb/s and 270ms. We decided to use the n1-standard-8 template with eight cores, 30 GB memory, and a T4 GPU, as the smaller image with 15 GB was insufficient to meet the memory requirements for gradient application on the CPU with the biggest models. The experiment naming in this section is prefixed with the type of location (A), (B) or (C) and the number of VMs, e.g., A-4 is the intra-zone experiment with 4 VMs. The full experimental description is specified in Table 2.", "doc_id": "erben2023", "page": 5, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Experimental design. Based on the insights from Section 3, we decided to use the largest models (CONV, RXLM) for all further cloud experiments in Sections 4 to 6, with the TBS of 32K as a baseline with good scaling properties. We abbreviate them with their respective domain names (CV, NLP). We used Google Cloud [5] for all experiments in this section, as they were the first to give us access to all necessary zones. The default networking solution in GC is the \"Premium Tier\", which tries to use a Google-owned network instead of the public internet. We measured the throughput and latency between all zones via iperf and ping and report the average of 5 consecutive runs in Table 3. Unsurprisingly, the diagonal shows that the local connectivity between zones runs at almost 7 Gb/s with a latency of 0.7ms, probably due to the hypervisors being in the same data center. While the up- and download were perfectly symmetrical in all setups, the throughput dropped to <210 Mb/s for all non-local connections. The US-based data center is located in Iowa and is best connected with at least 120 Mb/s to the remaining regions, namely Belgium in the EU (6,911km), Taiwan in ASIA (11,853km), and Sydney in Australia (AUS, 14,555km), presumably due to the physical distance. The lowest bandwidth and highest latency connections are between the EU region and ASIA and AUS, reaching around 80 Mb/s and 270ms. We decided to use the n1-standard-8 template with eight cores, 30 GB memory, and a T4 GPU, as the smaller image with 15 GB was insufficient to meet the memory requirements for gradient application on the CPU with the biggest models. The experiment naming in this section is prefixed with the type of location (A), (B) or (C) and the number of VMs, e.g., A-4 is the intra-zone experiment with 4 VMs. The full experimental description is specified in Table 2.", "original_types": ["text"], "id": 1236}
{"type": "figure", "content": "(A) Intra-zone performance for CV and NLP.", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(A) Intra-zone performance for CV and NLP.", "id": 1237}
{"type": "figure", "content": "(B) Transatlantic performance for CV and NLP.", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(B) Transatlantic performance for CV and NLP.", "id": 1238}
{"type": "figure", "content": "(C) Intercontinental performance for CV and NLP.", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(C) Intercontinental performance for CV and NLP.", "id": 1239}
{"type": "section", "content": "in Section 3. However, starting with three GPUs, we see an increase in throughput with a maximum speedup of up to 3.2x CV and 2.75x for NLP at eight GPUs. CV’s per-GPU speedup (speedup / GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively. The reason for this is the NLP granularity of 1.15 with 8 GPUs indicating an almost equal part in communication and calculation (Figure 7b) due to the much longer averaging round related to the model size (198M vs. 560M parameters). The peak network bandwidth utilization between peers was at most a symmetric 1.1 Gb/s while averaging and 33 Mb/s ingress while training due to data loading. This means that the network bandwidth of 7 Gb/s was not a limiting factor.\n\n(B) Transatlantic scalability. We scale when computing hardware is local. However, what happens when there is cheap capacity in another region? In this case, we study the throughput of experiments with resources in the us-west and eu-central regions (B-2,4,6,8). The B-2 experiment has one VM in the US and one in the EU, achieving a virtually identical throughput of 68.4 (US-EU) versus 70.1 (US) at CV (Figure 8a). Our maximum peak egress rate of 250 Mb/s does not affect the CV experiments, while the US experiments peaked at 1.1 Gb/s. The reduction in bandwidth penalizes NLP harder, where we are 16% slower with 177.3 SPS (US-EU) compared to the intra-zone experiment with 211.4 SPS (US). The resulting increased communication can be easily seen in the granularity analysis in Figure 8b (NLP A-2,4,6,8 vs. B-2,4,6,8). As only communication time increases in the NLP (B) experiments compared to (A), a granularity of ≫ 1 indicates good scalability: Adding two more GPUs to the B-6 experiment with a granularity of 1.03 results in a throughput increase of 15% (B-8) relative to the baseline. Meanwhile, adding two more GPUs to the B-2 experiment with a granularity of 2.21 results in a throughput increase of 77% (B-4) relative to the baseline.", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "in Section 3. However, starting with three GPUs, we see an increase in throughput with a maximum speedup of up to 3.2x CV and 2.75x for NLP at eight GPUs. CV’s per-GPU speedup (speedup / GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively. The reason for this is the NLP granularity of 1.15 with 8 GPUs indicating an almost equal part in communication and calculation (Figure 7b) due to the much longer averaging round related to the model size (198M vs. 560M parameters). The peak network bandwidth utilization between peers was at most a symmetric 1.1 Gb/s while averaging and 33 Mb/s ingress while training due to data loading. This means that the network bandwidth of 7 Gb/s was not a limiting factor.\n\n(B) Transatlantic scalability. We scale when computing hardware is local. However, what happens when there is cheap capacity in another region? In this case, we study the throughput of experiments with resources in the us-west and eu-central regions (B-2,4,6,8). The B-2 experiment has one VM in the US and one in the EU, achieving a virtually identical throughput of 68.4 (US-EU) versus 70.1 (US) at CV (Figure 8a). Our maximum peak egress rate of 250 Mb/s does not affect the CV experiments, while the US experiments peaked at 1.1 Gb/s. The reduction in bandwidth penalizes NLP harder, where we are 16% slower with 177.3 SPS (US-EU) compared to the intra-zone experiment with 211.4 SPS (US). The resulting increased communication can be easily seen in the granularity analysis in Figure 8b (NLP A-2,4,6,8 vs. B-2,4,6,8). As only communication time increases in the NLP (B) experiments compared to (A), a granularity of ≫ 1 indicates good scalability: Adding two more GPUs to the B-6 experiment with a granularity of 1.03 results in a throughput increase of 15% (B-8) relative to the baseline. Meanwhile, adding two more GPUs to the B-2 experiment with a granularity of 2.21 results in a throughput increase of 77% (B-4) relative to the baseline.", "original_types": ["text"], "id": 1240}
{"type": "section", "content": "In the B-4 experiment, we look at what happens when we increase the number of VMs to four, with two in the US and two in the EU. Nothing surprising happens with CV, as the workload continues to be mostly computation, with a throughput of 135.8 (B-4), only 3% slower than the intra-zone experiment with 140.4 SPS (A-4). However, at NLP, things get more interesting as we now have more overall communication with four peers, but they can average locally first and only later transmit across the Atlantic. However, compared to their A-counterparts, we do not see a difference in relative scalability with either B-4, B-6, or B-8. This means that training across regions (B) is slower, but the contribution per GPU decreases at the same rate as in training within a zone (A). The per-GPU speedup with additional hardware reduces at the same rate for either setup (between 0.05 and 0.06). This results in two observations: First, communication overhead scales linearly with the number of peers. Second, we only have to pay the penalty for transatlantic training once. However, we cannot expect a significant improvement in communication efficiency when we increase the amount of available local resources.\n\nSummarizing, with an transatlantic setup, CV achieves a virtually identical maximum speedup of 3.2x with 8 GPUs compared to A-1 (B-8 is 2% slower than A-8), while NLP is more affected by lower network bandwidth and only achieves a speedup of 2.15x (B-8 is 22% slower than A-8). The transatlantic training penalty is applied once; however, it does not affect the relative scaling with additional compute resources.", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "In the B-4 experiment, we look at what happens when we increase the number of VMs to four, with two in the US and two in the EU. Nothing surprising happens with CV, as the workload continues to be mostly computation, with a throughput of 135.8 (B-4), only 3% slower than the intra-zone experiment with 140.4 SPS (A-4). However, at NLP, things get more interesting as we now have more overall communication with four peers, but they can average locally first and only later transmit across the Atlantic. However, compared to their A-counterparts, we do not see a difference in relative scalability with either B-4, B-6, or B-8. This means that training across regions (B) is slower, but the contribution per GPU decreases at the same rate as in training within a zone (A). The per-GPU speedup with additional hardware reduces at the same rate for either setup (between 0.05 and 0.06). This results in two observations: First, communication overhead scales linearly with the number of peers. Second, we only have to pay the penalty for transatlantic training once. However, we cannot expect a significant improvement in communication efficiency when we increase the amount of available local resources.\n\nSummarizing, with an transatlantic setup, CV achieves a virtually identical maximum speedup of 3.2x with 8 GPUs compared to A-1 (B-8 is 2% slower than A-8), while NLP is more affected by lower network bandwidth and only achieves a speedup of 2.15x (B-8 is 22% slower than A-8). The transatlantic training penalty is applied once; however, it does not affect the relative scaling with additional compute resources.", "original_types": ["text"], "id": 1241}
{"type": "section", "content": "(C) Intercontinental scalability. To take geo-distribution to the extreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS, to see how much worse bandwidth affects the training throughput (C-3,4,6,8 in Table 2). How does the intercontinental penalty investigated in (B) affect deployments with a single GPU on each continent? Comparing the A-3 and C-3 experiments with three local versus three fully remote GPUs, CV is only 5% slower, while NLP suffers a 34% drop in throughput (Figure 9a) and does not even reach the baseline single GPU performance (A-1). The peak egress for each region was 318, 258, and 237 Mb/s for the US, EU, and ASIA, respectively. Since our bandwidth measurements were 210 and 130 Mb/s from the US to the EU and ASIA, respectively (Table 3), this suggests that the averaging was done over the US node and not an N-to-N all-reduce (a detailed analysis of how averaging affects bandwidths is discussed in Section 6). Thus, the limiting factor was the US-ASIA connection at 130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend continues with the C-4 run, which adds AUS as a continent with one additional VM. As we know from the transatlantic experiments (B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a). Again, the US VM is used as an averaging intermediary with a peak egress of 365 Mb/s, while the other continents are between 318 and 330 Mb/s. When comparing the two continents (B-4) versus four continents (C-4) experiments, one GPU on each continent (C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs on two continents (B-4). This reinforces that local hardware should be preferred whenever possible. However, we are always faster than the baseline (A-1), starting from 4 GPUs in both the transatlantic and intercontinental settings. While these experiments were specifically designed to be a worst-case scenario, what about a more balanced GPU distribution with at least two GPUs in each region?", "doc_id": "erben2023", "page": 6, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(C) Intercontinental scalability. To take geo-distribution to the extreme, we spawn VMs in up to 4 regions: USA, EU, ASIA, and AUS, to see how much worse bandwidth affects the training throughput (C-3,4,6,8 in Table 2). How does the intercontinental penalty investigated in (B) affect deployments with a single GPU on each continent? Comparing the A-3 and C-3 experiments with three local versus three fully remote GPUs, CV is only 5% slower, while NLP suffers a 34% drop in throughput (Figure 9a) and does not even reach the baseline single GPU performance (A-1). The peak egress for each region was 318, 258, and 237 Mb/s for the US, EU, and ASIA, respectively. Since our bandwidth measurements were 210 and 130 Mb/s from the US to the EU and ASIA, respectively (Table 3), this suggests that the averaging was done over the US node and not an N-to-N all-reduce (a detailed analysis of how averaging affects bandwidths is discussed in Section 6). Thus, the limiting factor was the US-ASIA connection at 130 Mb/s rather than the 80 Mb/s from EU-ASIA. The same trend continues with the C-4 run, which adds AUS as a continent with one additional VM. As we know from the transatlantic experiments (B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a). Again, the US VM is used as an averaging intermediary with a peak egress of 365 Mb/s, while the other continents are between 318 and 330 Mb/s. When comparing the two continents (B-4) versus four continents (C-4) experiments, one GPU on each continent (C-4) is slower by 6% for CV and 20% for NLP compared to two GPUs on two continents (B-4). This reinforces that local hardware should be preferred whenever possible. However, we are always faster than the baseline (A-1), starting from 4 GPUs in both the transatlantic and intercontinental settings. While these experiments were specifically designed to be a worst-case scenario, what about a more balanced GPU distribution with at least two GPUs in each region?", "original_types": ["text"], "id": 1242}
{"type": "section", "content": "5. MULTI-CLOUD PERFORMANCE\n\nUsing multiple cloud providers makes sense if we want to use resources cost-effectively and have additional reliability. In our scenario, we are interested in what throughput per $ can be expected and if any barriers prevent multi-cloud training. However, one can also consider the data center’s carbon footprint, which can change depending on the season and time of day [6].", "doc_id": "erben2023", "page": 7, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "5. MULTI-CLOUD PERFORMANCE\n\nUsing multiple cloud providers makes sense if we want to use resources cost-effectively and have additional reliability. In our scenario, we are interested in what throughput per $ can be expected and if any barriers prevent multi-cloud training. However, one can also consider the data center’s carbon footprint, which can change depending on the season and time of day [6].", "original_types": ["text", "header"], "id": 1243}
{"type": "table", "content": "Table 4: Average multi-cloud throughput and latency.\nMarkdown representation of the table", "doc_id": "erben2023", "page": 7, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Table 4: Average multi-cloud throughput and latency.\nMarkdown representation of the table", "id": 1244}
{"type": "figure", "content": "Figure 10: Multi-cloud performance for CV and NLP.", "doc_id": "erben2023", "page": 7, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 10: Multi-cloud performance for CV and NLP.", "id": 1245}
{"type": "figure", "content": "(a) Intra- and inter-zone in the US region (D-2/3).", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(a) Intra- and inter-zone in the US region (D-2/3).", "id": 1246}
{"type": "figure", "content": "(b) Intercontinental in the US, EU, ASIA and AUS (C-8).", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(b) Intercontinental in the US, EU, ASIA and AUS (C-8).", "id": 1247}
{"type": "figure", "content": "Figure 11: Costs breakdown for D-2/3 and C-8 experiments.", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 11: Costs breakdown for D-2/3 and C-8 experiments.", "id": 1248}
{"type": "section", "content": "each peer sends its gradients to every other peer. This means that 1/3 of the egress was internal to the partner VM in the same cloud, and the remaining 2/3 went to the remaining two peers in the other cloud. First, loading data from Backblaze costs $0.01/GB from anywhere in the world, which gives us a rate of $0.144/h for the CV and $0.083/h for the NLP experiments. Even when CV throughput is less than half of the NLP model (Figure 10a), images are much larger than text, resulting in a higher data rate. While this is close to the spot instance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time costs until the entire dataset is downloaded and retrieved from the disk cache, assuming large enough local storage. A more detailed comparison of cloud provider storage offerings is beyond our scope, but current prices range from $0.02/GB to $0.14/GB in various GC regions, making our setting (B2) competitive. Second, the external egress costs for the NLP experiments are very high compared to the other costs. They are 2.2x higher than the spot instance for GC and 5.7x higher for Azure, as the traffic costs in the US zone are $0.01/GB and $0.02/GB, respectively. The Azure cost is even higher ($0.763/h) than the on-demand instance price of $0.489/h. The CV experiments are much less affected due to the smaller model size, but Azure still manages to almost match its spot instance price of $0.134/h with the external egress cost of $0.115/h. Finally, the total compute cost, including egress and data loading in this multi-cloud constellation, is the sum of all the cloud providers' prices times the number of VMs used. For the CV experiments, GC, AWS, and Azure cost $0.762/h, $1.192/h, and $0.363/h, respectively, making the combination of GC with Azure 42% cheaper than GC with AWS. For the NLP experiments, GC, AWS, and Azure cost $0.835/h, $1.05/h, and $0.973/h, respectively, and GC combined with Azure is better than GC with AWS by a smaller margin of 3.9%. However, the intercontinental network egress prices for both GC and Azure are up to 15 times higher than the inter-zone prices, so what about the cost-effectiveness compared to geo-distributed experiments? (3) Geo-distributed egress can incur most of the cost. To illustrate the cost of intercontinental training, we use our C-8 experiment with two VMs in four continents from Section 4 to plug the cost for each cloud provider. The egress costs are calculated slightly differently than in the D-2 and D-3 experiments because four groups of two VMs average locally and then distribute the gradients across the other groups. This results in 8/20 internal egress calls (two calls between each group), 6/20 intercontinental egress calls (two calls between three regions), and 6/20 AUS egress calls (three regions share their gradients with AUS and vice versa). Figure 11b shows the resulting egress traffic cost per VM. The high cost between continents scales to a multiple of the remaining cost for CV and NLP with GC and Azure. For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h). Even with Azure having a more moderate rate of $0.02/GB for intercontinental communication and only $0.08/GB for OCE traffic, it still results in $1.882/h external egress cost ($2.101/h total). This is in contrast to AWS, which has a cap of $0.02/GB to any location, resulting in the best total cost of $1.376/h per VM. The relatively high AWS instance cost compares favorably to the other cloud providers regarding geo-distributed training. Keeping egress traffic in mind when deciding to scale to other continents is essential, as it can be the most significant part of the total cost. This raises another question: If egress traffic matters so much, how does model size affect it? (4) Small models have lower egress rates than larger models. Model size affects two parts of the distributed training time. First, larger models tend to have slower averaging rates, but more data movement costs due to their size. However, larger models are also averaged less frequently because they take longer to perform a step. To analyze this, we review the experiments in Section 3, where we evaluate different model sizes and GPUs counts. Figure 12 shows the average egress rate over each experiment's runtime for both CV and NLP from two to eight A10 GPUs. The trend is clear: the smaller the model, the lower the egress rate for all GPUs (e.g., RN18 vs. RN50). This is surprising, as the \"square-cube\" law [37] states that with a decrease in parameters, the calculation time will decrease quadratically while the communication time decreases linearly. This means that with a sufficiently small model, most of the training will consist of communication time, and the egress rate would increase, as it is defined through parameter count calculation time. However, we find that even with our smallest model, RN18, with 11.7M parameters and eight A10 GPUs, we are still not at the point where the communication time takes up most of the time. In summary, multi-cloud training is generally possible and can be cost-effective when keeping the egress costs and granularity in mind. Regardless of the cloud provider, staying in the same region is preferred, with the US having the most favorable egress price offers. A significant portion of the cost may be hidden in egress costs, accounting for more than 90% of the total cost in our NLP experiments in GC and Azure. Based on the additional egress costs alone, renting on-demand hardware may be more advantageous than using spot instances between different regions. CV training is generally more calculation- than communication-heavy, resulting in slightly higher data-loading but fewer egress costs. However, from our experiments, this is a favorable trade-off because data-loading is much cheaper than egress costs.", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "each peer sends its gradients to every other peer. This means that 1/3 of the egress was internal to the partner VM in the same cloud, and the remaining 2/3 went to the remaining two peers in the other cloud. First, loading data from Backblaze costs $0.01/GB from anywhere in the world, which gives us a rate of $0.144/h for the CV and $0.083/h for the NLP experiments. Even when CV throughput is less than half of the NLP model (Figure 10a), images are much larger than text, resulting in a higher data rate. While this is close to the spot instance costs of GC ($0.18/h) and Azure ($0.134/h), these are one-time costs until the entire dataset is downloaded and retrieved from the disk cache, assuming large enough local storage. A more detailed comparison of cloud provider storage offerings is beyond our scope, but current prices range from $0.02/GB to $0.14/GB in various GC regions, making our setting (B2) competitive. Second, the external egress costs for the NLP experiments are very high compared to the other costs. They are 2.2x higher than the spot instance for GC and 5.7x higher for Azure, as the traffic costs in the US zone are $0.01/GB and $0.02/GB, respectively. The Azure cost is even higher ($0.763/h) than the on-demand instance price of $0.489/h. The CV experiments are much less affected due to the smaller model size, but Azure still manages to almost match its spot instance price of $0.134/h with the external egress cost of $0.115/h. Finally, the total compute cost, including egress and data loading in this multi-cloud constellation, is the sum of all the cloud providers' prices times the number of VMs used. For the CV experiments, GC, AWS, and Azure cost $0.762/h, $1.192/h, and $0.363/h, respectively, making the combination of GC with Azure 42% cheaper than GC with AWS. For the NLP experiments, GC, AWS, and Azure cost $0.835/h, $1.05/h, and $0.973/h, respectively, and GC combined with Azure is better than GC with AWS by a smaller margin of 3.9%. However, the intercontinental network egress prices for both GC and Azure are up to 15 times higher than the inter-zone prices, so what about the cost-effectiveness compared to geo-distributed experiments? (3) Geo-distributed egress can incur most of the cost. To illustrate the cost of intercontinental training, we use our C-8 experiment with two VMs in four continents from Section 4 to plug the cost for each cloud provider. The egress costs are calculated slightly differently than in the D-2 and D-3 experiments because four groups of two VMs average locally and then distribute the gradients across the other groups. This results in 8/20 internal egress calls (two calls between each group), 6/20 intercontinental egress calls (two calls between three regions), and 6/20 AUS egress calls (three regions share their gradients with AUS and vice versa). Figure 11b shows the resulting egress traffic cost per VM. The high cost between continents scales to a multiple of the remaining cost for CV and NLP with GC and Azure. For NLP, the external egress cost for GC is $4.329/h, more than 90% of the total cost per VM ($4.804/h). Even with Azure having a more moderate rate of $0.02/GB for intercontinental communication and only $0.08/GB for OCE traffic, it still results in $1.882/h external egress cost ($2.101/h total). This is in contrast to AWS, which has a cap of $0.02/GB to any location, resulting in the best total cost of $1.376/h per VM. The relatively high AWS instance cost compares favorably to the other cloud providers regarding geo-distributed training. Keeping egress traffic in mind when deciding to scale to other continents is essential, as it can be the most significant part of the total cost. This raises another question: If egress traffic matters so much, how does model size affect it? (4) Small models have lower egress rates than larger models. Model size affects two parts of the distributed training time. First, larger models tend to have slower averaging rates, but more data movement costs due to their size. However, larger models are also averaged less frequently because they take longer to perform a step. To analyze this, we review the experiments in Section 3, where we evaluate different model sizes and GPUs counts. Figure 12 shows the average egress rate over each experiment's runtime for both CV and NLP from two to eight A10 GPUs. The trend is clear: the smaller the model, the lower the egress rate for all GPUs (e.g., RN18 vs. RN50). This is surprising, as the \"square-cube\" law [37] states that with a decrease in parameters, the calculation time will decrease quadratically while the communication time decreases linearly. This means that with a sufficiently small model, most of the training will consist of communication time, and the egress rate would increase, as it is defined through parameter count calculation time. However, we find that even with our smallest model, RN18, with 11.7M parameters and eight A10 GPUs, we are still not at the point where the communication time takes up most of the time. In summary, multi-cloud training is generally possible and can be cost-effective when keeping the egress costs and granularity in mind. Regardless of the cloud provider, staying in the same region is preferred, with the US having the most favorable egress price offers. A significant portion of the cost may be hidden in egress costs, accounting for more than 90% of the total cost in our NLP experiments in GC and Azure. Based on the additional egress costs alone, renting on-demand hardware may be more advantageous than using spot instances between different regions. CV training is generally more calculation- than communication-heavy, resulting in slightly higher data-loading but fewer egress costs. However, from our experiments, this is a favorable trade-off because data-loading is much cheaper than egress costs.", "id": 1249}
{"type": "figure", "content": "(a) CV", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(a) CV", "id": 1250}
{"type": "figure", "content": "(b) NLP", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "(b) NLP", "id": 1251}
{"type": "figure", "content": "Figure 12: Baseline egress rate on 2-8 A10 GPUs.", "doc_id": "erben2023", "page": 8, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 12: Baseline egress rate on 2-8 A10 GPUs.", "id": 1252}
{"type": "table", "content": "Table 5: Average hybrid-cloud throughput and latency.\nMarkdown representation of the table", "doc_id": "erben2023", "page": 9, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Table 5: Average hybrid-cloud throughput and latency.\nMarkdown representation of the table", "id": 1253}
{"type": "table", "content": "Table 6: Hybrid- vs. cloud-only throughput for the (E) setting.\nMarkdown representation of the table", "doc_id": "erben2023", "page": 9, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Table 6: Hybrid- vs. cloud-only throughput for the (E) setting.\nMarkdown representation of the table", "id": 1254}
{"type": "figure", "content": "Figure 13: Hybrid-cloud experiments for the (E) setting.", "doc_id": "erben2023", "page": 9, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 13: Hybrid-cloud experiments for the (E) setting.", "id": 1255}
{"type": "section", "content": "However, is combining on-premise and remote cloud resources better than using the cloud without paying the intercontinental bandwidth tax? To analyze this, we compare the (E) experiments with the 8xA10 experiment from Section 3 and 8xT4 experiment from Section 4 in Section 6. First, the 8xA10 experiments are the fastest for both CV and NLP, which removes the respective hybrid-cloud combination from contention (E-C-8). Second, the 8xT4 experiments for NLP are faster than any other hybrid-cloud setup, making the cloud-only solution favorable. Finally, while we always beat the baseline 8xT4 CV throughput (261.9 SPS), but in the case of E-B-8 (283.5 SPS), just barely. The throughput of E-A-8 (316.8 SPS) makes the hybrid-cloud setup the most favorable in terms of relative GPU scaling (32.5 SPS per GPU), but it does not come close to the best cloud-only throughput of 8xA10 with 620.6 SPS.\n\nSummarizing, the cloud-only experiments are the fastest overall due to their single-GPU throughput and locality. Adding cloud resources to on-premise hardware leads to a high communication time, which is not compensated by the additional processing speed of the GPUs. Proximity to the on-premise hardware is essential, as the more local cloud resources (E-A-8) consistently resulted in a better throughput than the same remote cloud resources (E-B-8).\n\n(F) Server-grade setting. The baseline throughput is significantly higher compared to the RTX8000, with a much more powerful 8xV100 DGX node to 413 SPS for CV and 1811 SPS for NLP (Figures 14a and 14c) via PyTorch data parallelism [26]. This increases the penalties from Section 3, leading to the only speedup from baseline for CV in experiments F-A-8 (507 SPS) and F-C-8 (510 SPS). This is surprising, as the older T4 GPUs in the EU perform similarly to the much newer A10 GPUs in the US, showcasing the trade-off between slower, local compute and faster, remote compute. The granularity of 2.46 for F-A-8 shows that there is enough calculation time to distribute, while the F-C-8 experiments spend ≈62% of the total training time on communication with a granularity of 0.57 (Figure 14b). The NLP experiments never reach the baseline throughput of the 8xV100 due to using most of the time for communication. The NLP F-B and F-C experiments mainly consist of communication (Figure 14d) with a granularity of up to 0.02, which results in a nonlinear, unstable training time due to the minimum matchmaking time issue (2) from Section 3.\n\nIn summary, the hybrid-cloud experiments conclude that while on-premise hardware can be augmented with cloud resources, it will likely be cost-efficient if all resources are on the same continent. Using only cloud resources is more advantageous if the on-premises hardware is not co-located.", "doc_id": "erben2023", "page": 9, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "However, is combining on-premise and remote cloud resources better than using the cloud without paying the intercontinental bandwidth tax? To analyze this, we compare the (E) experiments with the 8xA10 experiment from Section 3 and 8xT4 experiment from Section 4 in Section 6. First, the 8xA10 experiments are the fastest for both CV and NLP, which removes the respective hybrid-cloud combination from contention (E-C-8). Second, the 8xT4 experiments for NLP are faster than any other hybrid-cloud setup, making the cloud-only solution favorable. Finally, while we always beat the baseline 8xT4 CV throughput (261.9 SPS), but in the case of E-B-8 (283.5 SPS), just barely. The throughput of E-A-8 (316.8 SPS) makes the hybrid-cloud setup the most favorable in terms of relative GPU scaling (32.5 SPS per GPU), but it does not come close to the best cloud-only throughput of 8xA10 with 620.6 SPS.\n\nSummarizing, the cloud-only experiments are the fastest overall due to their single-GPU throughput and locality. Adding cloud resources to on-premise hardware leads to a high communication time, which is not compensated by the additional processing speed of the GPUs. Proximity to the on-premise hardware is essential, as the more local cloud resources (E-A-8) consistently resulted in a better throughput than the same remote cloud resources (E-B-8).\n\n(F) Server-grade setting. The baseline throughput is significantly higher compared to the RTX8000, with a much more powerful 8xV100 DGX node to 413 SPS for CV and 1811 SPS for NLP (Figures 14a and 14c) via PyTorch data parallelism [26]. This increases the penalties from Section 3, leading to the only speedup from baseline for CV in experiments F-A-8 (507 SPS) and F-C-8 (510 SPS). This is surprising, as the older T4 GPUs in the EU perform similarly to the much newer A10 GPUs in the US, showcasing the trade-off between slower, local compute and faster, remote compute. The granularity of 2.46 for F-A-8 shows that there is enough calculation time to distribute, while the F-C-8 experiments spend ≈62% of the total training time on communication with a granularity of 0.57 (Figure 14b). The NLP experiments never reach the baseline throughput of the 8xV100 due to using most of the time for communication. The NLP F-B and F-C experiments mainly consist of communication (Figure 14d) with a granularity of up to 0.02, which results in a nonlinear, unstable training time due to the minimum matchmaking time issue (2) from Section 3.\n\nIn summary, the hybrid-cloud experiments conclude that while on-premise hardware can be augmented with cloud resources, it will likely be cost-efficient if all resources are on the same continent. Using only cloud resources is more advantageous if the on-premises hardware is not co-located.", "original_types": ["text"], "id": 1256}
{"type": "section", "content": "FURTHER INSIGHTS\n\nCommunication time can decrease with more peers. Let us compare the granularity of the experiments for E-B (Figure 13b), which uses T4 GPUs in the US as an additional cloud resource. Both the computation and communication time decrease with the number of GPUs, even increasing the granularity from 1.98 at E-B-2 to 2.15 at E-B-4. This is surprising since, usually, with more peers, the communication time should increase, and the US-EU communication bottleneck should slow us down to the same extent as the E-B-1 experiment. This reduction is a Hivemind-specific anomaly, as it uses a single TCP stream per peer. With TCP, there needs to be an acknowledgment (ACK) of each packet by the receiving peer, which is impacted by the connection’s latency. In our high latency network between continents, the round trip time (RTT) of 300-318ms limits the maximum bandwidth a single TCP stream to 50-80 Mb/s. However, a way to improve link utilization is to use multiple streams, one for each peer, which we encounter in experiments E-(B|C)-2,4,8. To verify the potential gains, we perform a microbenchmark of the multi-stream bandwidth from the RTX8000 to the EU and US data centers.Although there is wide variation, likely due to network utilization, with 80 clients, we achieve a maximum bandwidth of 6 Gb/s within the EU and up to 4 Gb/s to the US. While larger peer groups and, consequently, larger models benefit from multi-peer communication by default and do not see significant changes in communication time, small models in unevenly distributed VMs setups can be disproportionately affected. The same trend can be observed in all high latency experiments (i.e., between the EU and the US), e.g., E-B, E-C for CV and NLP (Figures 13b and 13d, and F-B and F-C for CV (Figure 14b). In summary, uneven distribution of computational resources in high-latency networks (e.g., intercontinental) can reduce communication time with Hivemind due to more parallelism, lessening the impact of low bandwidth for a single data stream.", "doc_id": "erben2023", "page": 10, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "FURTHER INSIGHTS\n\nCommunication time can decrease with more peers. Let us compare the granularity of the experiments for E-B (Figure 13b), which uses T4 GPUs in the US as an additional cloud resource. Both the computation and communication time decrease with the number of GPUs, even increasing the granularity from 1.98 at E-B-2 to 2.15 at E-B-4. This is surprising since, usually, with more peers, the communication time should increase, and the US-EU communication bottleneck should slow us down to the same extent as the E-B-1 experiment. This reduction is a Hivemind-specific anomaly, as it uses a single TCP stream per peer. With TCP, there needs to be an acknowledgment (ACK) of each packet by the receiving peer, which is impacted by the connection’s latency. In our high latency network between continents, the round trip time (RTT) of 300-318ms limits the maximum bandwidth a single TCP stream to 50-80 Mb/s. However, a way to improve link utilization is to use multiple streams, one for each peer, which we encounter in experiments E-(B|C)-2,4,8. To verify the potential gains, we perform a microbenchmark of the multi-stream bandwidth from the RTX8000 to the EU and US data centers.Although there is wide variation, likely due to network utilization, with 80 clients, we achieve a maximum bandwidth of 6 Gb/s within the EU and up to 4 Gb/s to the US. While larger peer groups and, consequently, larger models benefit from multi-peer communication by default and do not see significant changes in communication time, small models in unevenly distributed VMs setups can be disproportionately affected. The same trend can be observed in all high latency experiments (i.e., between the EU and the US), e.g., E-B, E-C for CV and NLP (Figures 13b and 13d, and F-B and F-C for CV (Figure 14b). In summary, uneven distribution of computational resources in high-latency networks (e.g., intercontinental) can reduce communication time with Hivemind due to more parallelism, lessening the impact of low bandwidth for a single data stream.", "original_types": ["text", "header"], "id": 1257}
{"type": "figure", "content": "Figure 15: Cost to throughput tradeoff for RoBERTaXLM at different instance types. Our training setups (circled), that are due the low granularity of the NLP model, neither cheaper, nor faster than the centralized offering (DGX-2)", "doc_id": "erben2023", "page": 10, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Figure 15: Cost to throughput tradeoff for RoBERTaXLM at different instance types. Our training setups (circled), that are due the low granularity of the NLP model, neither cheaper, nor faster than the centralized offering (DGX-2)", "id": 1258}
{"type": "section", "content": "Cost analysis. The DGX-2 (8xV100) node from Section 6 represents server-grade hardware that could be used to train models. However, how does it compare in throughput per $ to all of our distributed cloud experiments? The Figure 1 (CV) and Figure 15 (NLP) show the complete cost analysis of the DGX-2, the 8xT4 experiments, and the 8xA10 experiments for spot and on-demand pricing. We use the internal egress costs from Figure 11a as a reference for the 8xT4 setup. For simplicity, we compare the spot pricing without interruptions, as we assume that a new VM can be spun up fast enough not to affect the training throughput in the long run. We mark the centralized baseline (DGX-2) cost per 1M samples and the throughput in samples per second with a horizontal and vertical line. This means that we are cheaper to the left to the vertical line, and above the horizontal line, we are faster (and vice versa). We circle the new value propositions that we enable in both figures. Our hardware setups have additional key characteristics: They are resilient by default to interruptions due to running in a decentralized fashion and they enable the combination of more GPUs than cloud providers offer in a single node. Currently, common hardware configurations (DGX) allow up to eight GPUs connected via NVLink, and with older hardware, only up to 4xT4s connected via PCIe at 10 GB/s between GPUs (with GC). We were able to combine eight single GPU nodes from GC and LambdaLabs to create competing performance and price setups without dedicated GPU interconnects. A spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-demand) in GC US, which makes it the best value proposition for the low granularity NLP task. It is followed by the 8xA10, which are 41% slower and 30% more expensive than the DGX-2 (Figure 15). The 8xT4 experiments are even more expensive, as the internal egress costs take up more than half of the costs, making them the worst value proposition. However, for CV, we manage to provide two new offerings: First, the 8xA10, which is both 50% faster and 49% cheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-2, while being 37% slower (Figure 1). The CV model can be scaled more easily due to its initially high granularity, which makes the very competitive offering of $0.6/h per A10 from LambdaLabs an excellent value proposition. However, while we only evaluated eight T4 GPUs for our GC-based experiments, with a granularity of 5.19 (CV A-8 in Figure 7b), there is ample space to scale even further. It is important to note that LambdaLabs does not charge for any data egress, but GC does with $0.01/GB, and the 8xT4 experiment is still cheaper. While LambdaLabs is often at capacity, Google Cloud positions itself as a hyperscaler with the advantage of rarely being at max occupancy. We also evaluated the performance of the 4xT4 PyTorch DDP [26] for CV with the best available multi-T4 node on GC (4xT4). The NLP experiments ran OOM. Since the DDP 4xT4 runs on a single node, it causes no interconnect costs and is priced at $0.96 per 1M samples at spot pricing, while our 8xT4 setup costs $1.77 per 1M samples (84% more expensive). However, the 8xT4 setup has a higher throughput of 262 SPS (26% faster) compared to the 4xT4 node (207 SPS). This higher speed is not available at the price point of the 4xT4 node. Moreover, the 8xT4 setup has the potential for further scaling, which we discussed in detail in Section 4. In summary, the lower spot prices for older GPUs allow us to train models more cost-efficiently when task granularity allows it and get more value per $ when training on the 8xT4 or 8xA10 compared to an DGX-2 node. Combining multiple nodes with single GPUs with lower bandwidths enables scaling that was previously impossible to achieve without resorting to much more powerful GPUs. Distributed spot instance pricing opens up a new value proposition compared to on-demand offerings that can even compete with the competitive pricing of smaller cloud providers. Spot VM Interruption Frequency. While we used low spot prices as a cost-saving argument in our experiments, we did not elaborate on the most significant drawback - the possibility of being terminated by the cloud provider at any time. There is already some research on how different cloud providers track the interruption frequency and can be used for varying workloads to achieve a positive $-per-throughput effect [24, 42, 43].", "doc_id": "erben2023", "page": 10, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "Cost analysis. The DGX-2 (8xV100) node from Section 6 represents server-grade hardware that could be used to train models. However, how does it compare in throughput per $ to all of our distributed cloud experiments? The Figure 1 (CV) and Figure 15 (NLP) show the complete cost analysis of the DGX-2, the 8xT4 experiments, and the 8xA10 experiments for spot and on-demand pricing. We use the internal egress costs from Figure 11a as a reference for the 8xT4 setup. For simplicity, we compare the spot pricing without interruptions, as we assume that a new VM can be spun up fast enough not to affect the training throughput in the long run. We mark the centralized baseline (DGX-2) cost per 1M samples and the throughput in samples per second with a horizontal and vertical line. This means that we are cheaper to the left to the vertical line, and above the horizontal line, we are faster (and vice versa). We circle the new value propositions that we enable in both figures. Our hardware setups have additional key characteristics: They are resilient by default to interruptions due to running in a decentralized fashion and they enable the combination of more GPUs than cloud providers offer in a single node. Currently, common hardware configurations (DGX) allow up to eight GPUs connected via NVLink, and with older hardware, only up to 4xT4s connected via PCIe at 10 GB/s between GPUs (with GC). We were able to combine eight single GPU nodes from GC and LambdaLabs to create competing performance and price setups without dedicated GPU interconnects. A spot DGX-2 costs at the time of writing $6.30/h ($14.60/h on-demand) in GC US, which makes it the best value proposition for the low granularity NLP task. It is followed by the 8xA10, which are 41% slower and 30% more expensive than the DGX-2 (Figure 15). The 8xT4 experiments are even more expensive, as the internal egress costs take up more than half of the costs, making them the worst value proposition. However, for CV, we manage to provide two new offerings: First, the 8xA10, which is both 50% faster and 49% cheaper than the DGX-2, and 8xT4, which is 58% cheaper than DGX-2, while being 37% slower (Figure 1). The CV model can be scaled more easily due to its initially high granularity, which makes the very competitive offering of $0.6/h per A10 from LambdaLabs an excellent value proposition. However, while we only evaluated eight T4 GPUs for our GC-based experiments, with a granularity of 5.19 (CV A-8 in Figure 7b), there is ample space to scale even further. It is important to note that LambdaLabs does not charge for any data egress, but GC does with $0.01/GB, and the 8xT4 experiment is still cheaper. While LambdaLabs is often at capacity, Google Cloud positions itself as a hyperscaler with the advantage of rarely being at max occupancy. We also evaluated the performance of the 4xT4 PyTorch DDP [26] for CV with the best available multi-T4 node on GC (4xT4). The NLP experiments ran OOM. Since the DDP 4xT4 runs on a single node, it causes no interconnect costs and is priced at $0.96 per 1M samples at spot pricing, while our 8xT4 setup costs $1.77 per 1M samples (84% more expensive). However, the 8xT4 setup has a higher throughput of 262 SPS (26% faster) compared to the 4xT4 node (207 SPS). This higher speed is not available at the price point of the 4xT4 node. Moreover, the 8xT4 setup has the potential for further scaling, which we discussed in detail in Section 4. In summary, the lower spot prices for older GPUs allow us to train models more cost-efficiently when task granularity allows it and get more value per $ when training on the 8xT4 or 8xA10 compared to an DGX-2 node. Combining multiple nodes with single GPUs with lower bandwidths enables scaling that was previously impossible to achieve without resorting to much more powerful GPUs. Distributed spot instance pricing opens up a new value proposition compared to on-demand offerings that can even compete with the competitive pricing of smaller cloud providers. Spot VM Interruption Frequency. While we used low spot prices as a cost-saving argument in our experiments, we did not elaborate on the most significant drawback - the possibility of being terminated by the cloud provider at any time. There is already some research on how different cloud providers track the interruption frequency and can be used for varying workloads to achieve a positive $-per-throughput effect [24, 42, 43].", "id": 1259}
{"type": "section", "content": "8 LESSONS LEARNED\n\nWe find it important to summarize our findings more generically to provide guidance for DL practitioners that want to perform distributed spot training. These lessons are based on the Sections 3 to 6.\n\nSmall model training still scales. We have shown that models between 12M-560M parameters can be trained in a decentralized, distributed fashion achieving a speedup of up to 4.37x on eight Ampere-GPUs. The limiting factor as to when a model is suitable for (geo-)distributed training is the target batch size which all peers need to accumulate until synchronization happens. We found a TBS of 32K suitable to not only train in a single zone, but even see a speedup when using VMs in four different continents. As long as the optimizer can handle big-batch training and the dataset is big enough to accommodate large batches, the remaining issue to find the base granularity of the model to decide how to scale it cost-effectively. Finally, we found that small models induce less traffic over larger models over time, even at a much higher averaging rate, making them better suited for cost-efficient training than large models.\n\nEgress costs can take up most of the total cost. Egress pricing for the NLP experiments overtook the spot and the on-demand costs of T4 GPUs when training on four continents or even in two zones. For example, RoBERTaXLM’s high throughput and parameter count require more data to be sent between peers during averaging due to smaller granularity. Under the current pricing models, AWS has the best value for geo-distributed training, while GC and Azure are best at training in a single zone. The biggest cost-saving potential lies in cloud providers that do not charge for egress at all, like LambdaLabs.", "doc_id": "erben2023", "page": 11, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "8 LESSONS LEARNED\n\nWe find it important to summarize our findings more generically to provide guidance for DL practitioners that want to perform distributed spot training. These lessons are based on the Sections 3 to 6.\n\nSmall model training still scales. We have shown that models between 12M-560M parameters can be trained in a decentralized, distributed fashion achieving a speedup of up to 4.37x on eight Ampere-GPUs. The limiting factor as to when a model is suitable for (geo-)distributed training is the target batch size which all peers need to accumulate until synchronization happens. We found a TBS of 32K suitable to not only train in a single zone, but even see a speedup when using VMs in four different continents. As long as the optimizer can handle big-batch training and the dataset is big enough to accommodate large batches, the remaining issue to find the base granularity of the model to decide how to scale it cost-effectively. Finally, we found that small models induce less traffic over larger models over time, even at a much higher averaging rate, making them better suited for cost-efficient training than large models.\n\nEgress costs can take up most of the total cost. Egress pricing for the NLP experiments overtook the spot and the on-demand costs of T4 GPUs when training on four continents or even in two zones. For example, RoBERTaXLM’s high throughput and parameter count require more data to be sent between peers during averaging due to smaller granularity. Under the current pricing models, AWS has the best value for geo-distributed training, while GC and Azure are best at training in a single zone. The biggest cost-saving potential lies in cloud providers that do not charge for egress at all, like LambdaLabs.", "original_types": ["text", "header"], "id": 1260}
{"type": "section", "content": "CONCLUSION\n\nThis paper analyzes multi- and hybrid-cloud training in a decentralized fashion on spot instances. We define the lower bounds of model sizes that can be scaled cost-efficiently using the granularity metric to estimate their suitability for distributed training in low-bandwidth, high-latency situations. We show that training on multiple cloud providers and four continents still scales with additional compute resources. Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs. Finally, we provide an intuition about where costs in such a training scenario come from and how different model sizes from CV and NLP affect throughput and costs. Our work empowers practitioners to utilize spot-priced instances for distributed deep learning with relatively small models. Our insights show some potential that can further improve distributed training performance, such as optimizers with higher minibatch sizes and improvements regarding the communication time with, e.g., better compression.\n\nAPPENDIX: ASR CASE STUDY", "doc_id": "erben2023", "page": 12, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "CONCLUSION\n\nThis paper analyzes multi- and hybrid-cloud training in a decentralized fashion on spot instances. We define the lower bounds of model sizes that can be scaled cost-efficiently using the granularity metric to estimate their suitability for distributed training in low-bandwidth, high-latency situations. We show that training on multiple cloud providers and four continents still scales with additional compute resources. Alternatively to the current use of spot instances in DL, we show the potential of using spot instances in a distributed, decentralized way by being more cost-efficient with eight T4 instances over a DGX-2 from the same cloud provider while paying additional egress costs. Finally, we provide an intuition about where costs in such a training scenario come from and how different model sizes from CV and NLP affect throughput and costs. Our work empowers practitioners to utilize spot-priced instances for distributed deep learning with relatively small models. Our insights show some potential that can further improve distributed training performance, such as optimizers with higher minibatch sizes and improvements regarding the communication time with, e.g., better compression.\n\nAPPENDIX: ASR CASE STUDY", "original_types": ["text", "header"], "id": 1261}
{"type": "section", "content": "We perform a case study on Automatic Speech Recognition (ASR) to showcase spot training on weaker GPUs. Whisper [34] is a state-of-the-art ASR model trained on 680,000 hours of labeled data to transcribe audio. It features different sizes, from 37.8M to 1.5B parameters, and was trained with a minibatch size of 256. We use the Commonvoice [11] dataset, preprocessed to Log-Mel spectrograms. In our distributed experiments, we start with a TBS of 256 and increase to 512 and 1024 to combat potential granularity issues. Due to memory constraints, only three model sizes (Tiny, Base, Small) were trainable on the T4 GPU. Unfortunately, the original TBS of 256 was not large enough to train the relatively small models due to their small granularity (0.04, 0.14 and 0.57 at 8xT4, respectively) with no performance benefits. The only model showing scaling potential is WhisperSmall, with a granularity of 1.8 with 2xT4. However, when scaling the target batch size to 512 and 1024, we see some benefit over the single GPU runs for the WhisperSmall model (Figure 16). By effectively increasing the amount of computation by the factors of 2 and 4, we can generate a speedup of 1.27× and 2.2× with 8xT4’s for the TBS 512 and 1024, respectively. When compared to other hardware setups, our A100 80GB GPU and the best multi-T4 GPU on GC (4xT4) with Pytorch DDP (Figure 17) have almost double the throughput at 46 SPS and are slightly slower at 24 SPS, respectively, compared to our 8xT4 setup which runs at 28 SPS. This outcome is not surprising due to the generational leap in architecture for the A100 and the slower interconnect with our 8xT4 experiments compared to a single 4xT4 node (see Section 3 for a detailed throughput analysis). The proposed cost-throughput ratio is mixed: the A100 is at $12.19/1M samples, the DDP 4xT4 is at $8.41/1M, and our 8xT4 is at $14.53/1M. Our proposed setup is slightly more expensive than the A100, and it will not scale beyond eight T4 GPUs due a granularity at 1.17, leaving the A100 as the fastest and the DDP 4xT4 setup as the cheaper but slower alternative. Despite these results, our proposed setup has several benefits, including resilience for spot interruptions, interruption-free migration to the lowest cloud prices, and the possibility to scale the GPU count up as long as granularity permits it.", "doc_id": "erben2023", "page": 12, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "We perform a case study on Automatic Speech Recognition (ASR) to showcase spot training on weaker GPUs. Whisper [34] is a state-of-the-art ASR model trained on 680,000 hours of labeled data to transcribe audio. It features different sizes, from 37.8M to 1.5B parameters, and was trained with a minibatch size of 256. We use the Commonvoice [11] dataset, preprocessed to Log-Mel spectrograms. In our distributed experiments, we start with a TBS of 256 and increase to 512 and 1024 to combat potential granularity issues. Due to memory constraints, only three model sizes (Tiny, Base, Small) were trainable on the T4 GPU. Unfortunately, the original TBS of 256 was not large enough to train the relatively small models due to their small granularity (0.04, 0.14 and 0.57 at 8xT4, respectively) with no performance benefits. The only model showing scaling potential is WhisperSmall, with a granularity of 1.8 with 2xT4. However, when scaling the target batch size to 512 and 1024, we see some benefit over the single GPU runs for the WhisperSmall model (Figure 16). By effectively increasing the amount of computation by the factors of 2 and 4, we can generate a speedup of 1.27× and 2.2× with 8xT4’s for the TBS 512 and 1024, respectively. When compared to other hardware setups, our A100 80GB GPU and the best multi-T4 GPU on GC (4xT4) with Pytorch DDP (Figure 17) have almost double the throughput at 46 SPS and are slightly slower at 24 SPS, respectively, compared to our 8xT4 setup which runs at 28 SPS. This outcome is not surprising due to the generational leap in architecture for the A100 and the slower interconnect with our 8xT4 experiments compared to a single 4xT4 node (see Section 3 for a detailed throughput analysis). The proposed cost-throughput ratio is mixed: the A100 is at $12.19/1M samples, the DDP 4xT4 is at $8.41/1M, and our 8xT4 is at $14.53/1M. Our proposed setup is slightly more expensive than the A100, and it will not scale beyond eight T4 GPUs due a granularity at 1.17, leaving the A100 as the fastest and the DDP 4xT4 setup as the cheaper but slower alternative. Despite these results, our proposed setup has several benefits, including resilience for spot interruptions, interruption-free migration to the lowest cloud prices, and the possibility to scale the GPU count up as long as granularity permits it.", "original_types": ["text"], "id": 1262}
{"type": "section", "content": "REFERENCES\n\n[1] [n.d.]. Horovod: fast and easy distributed deep learning in TensorFlow, author=Sergeev, Alexander and Del Balso, Mike, journal=arXiv preprint arXiv:1802.05799, year=2018. ([n. d.]).\n\n[2] 2023. Amazon AWS. Accessed: 19 May 2023, aws.amazon.com.\n\n[3] 2023. Amazon AWS Spot Pricing. https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/. Accessed: 2023-09-27.\n\n[4] 2023. Backblaze. https://backblaze.com/. Accessed: 2023-10-05.\n\n[5] 2023. Google Cloud. Accessed: 19 May 2023, cloud.google.com.\n\n[6] 2023. Google Cloud Region Picker. https://cloud.withgoogle.com/region-picker/. Accessed: 2023-10-05.\n\n[7] 2023. Hivemind GAC Issue. https://github.com/learning-at-home/hivemind/issues/566. Accessed: 2023-10-05.\n\n[8] 2023. LambdaLabs. Accessed: 19 May 2023, lambdalabs.com.\n\n[9] 2023. Microsoft Azure. Accessed: 19 May 2023, portal.azure.com.\n\n[10] Alex Aizman, Gavin Maltby, and Thomas Breuel. 2019. High Performance I/O For Large Scale Deep Learning. In 2019 IEEE International Conference on Big Data (Big Data). 5965–5967. https://doi.org/10.1109/BigData47090.2019.9005703\n\n[11] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. 2019. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670 (2019).\n\n[12] Alexander Borzunov, Max Ryabinin, Tim Dettmers, Quentin Lhoest, Lucile Saulnier, Michael Diskin, and Yacine Jernite. 2022. Training Transformers Together. In NeurIPS 2021 Competitions and Demonstrations Track. PMLR, 335–342.\n\n[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. arXiv:1911.02116 [cs.CL]\n\n[14] Piali Das, Nikita Ivkin, Tanya Bansal, Laurence Rouesnel, Philip Gautier, Zohar Karnin, Leo Dirac, Lakshmi Ramakrishnan, Andre Perunicic, Iaroslav Shcherbatyi, Wilton Wu, Aida Zolic, Huibin Shen, Amr Ahmed, Fela Winkelmolen, Miroslav Miladinovic, Cedric Archambeau, Alex Tang, Bhaskar Dutt, Patricia Grao, and Kumar Venkateswar. 2020. Amazon SageMaker Autopilot: A White Box AutoML Solution at Scale. In Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning (Portland, OR, USA) (DEEM'20). Association for Computing Machinery, New York, NY, USA, Article 2, 7 pages. https://doi.org/10.1145/3399579.3399870\n\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[16] Tim Dettmers. 2016. 8-Bit Approximations for Parallelism in Deep Learning. arXiv:1511.04561 [cs.NE]", "doc_id": "erben2023", "page": 13, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "REFERENCES\n\n[1] [n.d.]. Horovod: fast and easy distributed deep learning in TensorFlow, author=Sergeev, Alexander and Del Balso, Mike, journal=arXiv preprint arXiv:1802.05799, year=2018. ([n. d.]).\n\n[2] 2023. Amazon AWS. Accessed: 19 May 2023, aws.amazon.com.\n\n[3] 2023. Amazon AWS Spot Pricing. https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/. Accessed: 2023-09-27.\n\n[4] 2023. Backblaze. https://backblaze.com/. Accessed: 2023-10-05.\n\n[5] 2023. Google Cloud. Accessed: 19 May 2023, cloud.google.com.\n\n[6] 2023. Google Cloud Region Picker. https://cloud.withgoogle.com/region-picker/. Accessed: 2023-10-05.\n\n[7] 2023. Hivemind GAC Issue. https://github.com/learning-at-home/hivemind/issues/566. Accessed: 2023-10-05.\n\n[8] 2023. LambdaLabs. Accessed: 19 May 2023, lambdalabs.com.\n\n[9] 2023. Microsoft Azure. Accessed: 19 May 2023, portal.azure.com.\n\n[10] Alex Aizman, Gavin Maltby, and Thomas Breuel. 2019. High Performance I/O For Large Scale Deep Learning. In 2019 IEEE International Conference on Big Data (Big Data). 5965–5967. https://doi.org/10.1109/BigData47090.2019.9005703\n\n[11] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. 2019. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670 (2019).\n\n[12] Alexander Borzunov, Max Ryabinin, Tim Dettmers, Quentin Lhoest, Lucile Saulnier, Michael Diskin, and Yacine Jernite. 2022. Training Transformers Together. In NeurIPS 2021 Competitions and Demonstrations Track. PMLR, 335–342.\n\n[13] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. arXiv:1911.02116 [cs.CL]\n\n[14] Piali Das, Nikita Ivkin, Tanya Bansal, Laurence Rouesnel, Philip Gautier, Zohar Karnin, Leo Dirac, Lakshmi Ramakrishnan, Andre Perunicic, Iaroslav Shcherbatyi, Wilton Wu, Aida Zolic, Huibin Shen, Amr Ahmed, Fela Winkelmolen, Miroslav Miladinovic, Cedric Archambeau, Alex Tang, Bhaskar Dutt, Patricia Grao, and Kumar Venkateswar. 2020. Amazon SageMaker Autopilot: A White Box AutoML Solution at Scale. In Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning (Portland, OR, USA) (DEEM'20). Association for Computing Machinery, New York, NY, USA, Article 2, 7 pages. https://doi.org/10.1145/3399579.3399870\n\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n\n[16] Tim Dettmers. 2016. 8-Bit Approximations for Parallelism in Deep Learning. arXiv:1511.04561 [cs.NE]", "original_types": ["text", "header"], "id": 1263}
{"type": "section", "content": "[17] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, et al. 2021. Distributed Deep Learning In Open Collaborations. Advances in Neural Information Processing Systems 34 (2021), 7879–7897.\n\n[18] O Elharrouss, Y Akbari, N Almaadeed, and S Al-Maadeed. [n.d.]. Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches. arXiv 2022. arXiv preprint arXiv:2206.08016 ([n. d.]).\n\n[19] Anne C Elster and Tor A Haugdahl. 2022. NVIDIA Hopper GPU and Grace CPU Highlights. Computing in Science & Engineering 24, 2 (2022), 95–100.\n\n[20] Wikimedia Foundation. 2023. \"Wikimedia Downloads\". https://dumps.wikimedia.org\n\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.\n\n[22] Kai Hwang. 1992. Advanced Computer Architecture: Parallelism,Scalability,Programmability (1st ed.). McGraw-Hill Higher Education.\n\n[23] Kyungyong Lee and Myungjun Son. 2017. DeepSpotCloud: Leveraging Cross-Region GPU Spot Instances for Deep Learning. In 2017 IEEE 10th International Conference on Cloud Computing (CLOUD). 98–105. https://doi.org/10.1109/CLOUD.2017.21\n\n[24] Sungjae Lee, Jael Hwang, and Kyungyong Lee. 2022. SpotLake: Diverse Spot Instance Dataset Archive Service. In 2022 IEEE International Symposium on Workload Characterization (IISWC). 242–255. https://doi.org/10.1109/IISWC55918.2022.00029\n\n[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 583–598.\n\n[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704 (2020).\n\n[27] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. 2017. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Advances in neural information processing systems 30 (2017).\n\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11976–11986.", "doc_id": "erben2023", "page": 13, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "[17] Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del Moral, et al. 2021. Distributed Deep Learning In Open Collaborations. Advances in Neural Information Processing Systems 34 (2021), 7879–7897.\n\n[18] O Elharrouss, Y Akbari, N Almaadeed, and S Al-Maadeed. [n.d.]. Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches. arXiv 2022. arXiv preprint arXiv:2206.08016 ([n. d.]).\n\n[19] Anne C Elster and Tor A Haugdahl. 2022. NVIDIA Hopper GPU and Grace CPU Highlights. Computing in Science & Engineering 24, 2 (2022), 95–100.\n\n[20] Wikimedia Foundation. 2023. \"Wikimedia Downloads\". https://dumps.wikimedia.org\n\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.\n\n[22] Kai Hwang. 1992. Advanced Computer Architecture: Parallelism,Scalability,Programmability (1st ed.). McGraw-Hill Higher Education.\n\n[23] Kyungyong Lee and Myungjun Son. 2017. DeepSpotCloud: Leveraging Cross-Region GPU Spot Instances for Deep Learning. In 2017 IEEE 10th International Conference on Cloud Computing (CLOUD). 98–105. https://doi.org/10.1109/CLOUD.2017.21\n\n[24] Sungjae Lee, Jael Hwang, and Kyungyong Lee. 2022. SpotLake: Diverse Spot Instance Dataset Archive Service. In 2022 IEEE International Symposium on Workload Characterization (IISWC). 242–255. https://doi.org/10.1109/IISWC55918.2022.00029\n\n[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 583–598.\n\n[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704 (2020).\n\n[27] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. 2017. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Advances in neural information processing systems 30 (2017).\n\n[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11976–11986.", "original_types": ["text"], "id": 1264}
{"type": "section", "content": "[30] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. 2020. MLPerf Training Benchmark. Proceedings of Machine Learning and Systems 2 (2020), 336–349.\n\n[31] Petar Maymounkov and David Mazieres. 2002. Kademia: A peer-to-peer information system based on the xor metric. In Peer-to-Peer Systems: First International Workshop, IPTPS 2002 Cambridge, MA, USA, March 7–8, 2002 Revised Papers. Springer, 53–65.\n\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).\n\n[33] Gustavo Portella, Genaina N Rodrigues, Eduardo Nakano, and Alba CMA Melo. 2019. Statistical analysis of Amazon EC2 cloud pricing models. Concurrency and Computation: Practice and Experience 31, 18 (2019), e4451.\n\n[34] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202. PMLR, 28492–28518. https://proceedings.mlr.press/v202/radford23a.html\n\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 3505–3506.\n\n[36] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training. arXiv:2101.06840 [cs.DC]\n\n[37] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov. 2023. SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. arXiv preprint arXiv:2301.11913 (2023).\n\n[38] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. 2021. Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices. In Advances in Neural Information Processing Systems, Vol. 34. https://proceedings.neurips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf\n\n[39] Learning@home team. 2020. Hivemind: a Library for Decentralized Deep Learning. https://github.com/learning-at-home/hivemind.\n\n[40] Chathurika S. Wickramasinghe, Daniel L. Marino, and Milos Manic. 2021. ResNet Autoencoders for Unsupervised Feature Learning From High-Dimensional Data: Deep Models Resistant to Performance Degradation. IEEE Access 9 (2021), 40511–40520. https://doi.org/10.1109/ACCESS.2021.3064819", "doc_id": "erben2023", "page": 13, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "[30] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. 2020. MLPerf Training Benchmark. Proceedings of Machine Learning and Systems 2 (2020), 336–349.\n\n[31] Petar Maymounkov and David Mazieres. 2002. Kademia: A peer-to-peer information system based on the xor metric. In Peer-to-Peer Systems: First International Workshop, IPTPS 2002 Cambridge, MA, USA, March 7–8, 2002 Revised Papers. Springer, 53–65.\n\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).\n\n[33] Gustavo Portella, Genaina N Rodrigues, Eduardo Nakano, and Alba CMA Melo. 2019. Statistical analysis of Amazon EC2 cloud pricing models. Concurrency and Computation: Practice and Experience 31, 18 (2019), e4451.\n\n[34] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.), Vol. 202. PMLR, 28492–28518. https://proceedings.mlr.press/v202/radford23a.html\n\n[35] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 3505–3506.\n\n[36] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training. arXiv:2101.06840 [cs.DC]\n\n[37] Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov. 2023. SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient. arXiv preprint arXiv:2301.11913 (2023).\n\n[38] Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko. 2021. Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices. In Advances in Neural Information Processing Systems, Vol. 34. https://proceedings.neurips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf\n\n[39] Learning@home team. 2020. Hivemind: a Library for Decentralized Deep Learning. https://github.com/learning-at-home/hivemind.\n\n[40] Chathurika S. Wickramasinghe, Daniel L. Marino, and Milos Manic. 2021. ResNet Autoencoders for Unsupervised Feature Learning From High-Dimensional Data: Deep Models Resistant to Performance Degradation. IEEE Access 9 (2021), 40511–40520. https://doi.org/10.1109/ACCESS.2021.3064819", "original_types": ["text"], "id": 1265}
{"type": "section", "content": "[41] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. 2023. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 (2023).\n\n[42] Sheng Yang, Samir Khuller, Sunav Choudhary, Subrata Mitra, and Kanak Mahadik. 2022. Scheduling ML Training on Unreliable Spot Instances. In Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion (Leicester, United Kingdom) (UCC '21). Association for Computing Machinery, New York, NY, USA, Article 29, 8 pages. https://doi.org/10.1145/3492323.3495594\n\n[43] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, and Ion Stoica. 2023. SkyPilot: An Intercloud Broker for Sky Computing. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). USENIX Association, Boston, MA, 437–455. https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng\n\n[44] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2019. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962 (2019).\n\n[45] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. 2022. Decentralized training of foundation models in heterogeneous environments. Advances in Neural Information Processing Systems 35 (2022), 25464–25477.", "doc_id": "erben2023", "page": 13, "url": "https://arxiv.org/pdf/2306.03163", "embedded_text": "[41] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. 2023. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 (2023).\n\n[42] Sheng Yang, Samir Khuller, Sunav Choudhary, Subrata Mitra, and Kanak Mahadik. 2022. Scheduling ML Training on Unreliable Spot Instances. In Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion (Leicester, United Kingdom) (UCC '21). Association for Computing Machinery, New York, NY, USA, Article 29, 8 pages. https://doi.org/10.1145/3492323.3495594\n\n[43] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, and Ion Stoica. 2023. SkyPilot: An Intercloud Broker for Sky Computing. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). USENIX Association, Boston, MA, 437–455. https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng\n\n[44] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2019. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962 (2019).\n\n[45] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. 2022. Decentralized training of foundation models in heterogeneous environments. Advances in Neural Information Processing Systems 35 (2022), 25464–25477.", "original_types": ["text"], "id": 1266}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nHolistically Evaluating the Environmental Impact of Creating Language Models\n\nJacob Morrison1 Clara Na2 Jared Fernandez2 Tim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n\n1Allen Institute for AI 2Carnegie Mellon University\n\njacobm@allenai.org\n\nAbstract\n\nAs the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 13 billion active parameters, trained on up to 5.6 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 493 metric tons of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed 2.769 million liters of water, equivalent to about 24.5 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to ~50% of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ~15% and ~85% of our hardware’s maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.", "doc_id": "morrison2025", "page": 1, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nHolistically Evaluating the Environmental Impact of Creating Language Models\n\nJacob Morrison1 Clara Na2 Jared Fernandez2 Tim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n\n1Allen Institute for AI 2Carnegie Mellon University\n\njacobm@allenai.org\n\nAbstract\n\nAs the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 13 billion active parameters, trained on up to 5.6 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 493 metric tons of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed 2.769 million liters of water, equivalent to about 24.5 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to ~50% of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ~15% and ~85% of our hardware’s maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.", "original_types": ["text", "header"], "id": 1267}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is imperative to carefully characterize the true cost of building and deploying state-of-the-art models, to inform effective strategies for mitigating potential harms and planning for future demand.", "doc_id": "morrison2025", "page": 2, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is imperative to carefully characterize the true cost of building and deploying state-of-the-art models, to inform effective strategies for mitigating potential harms and planning for future demand.", "original_types": ["text", "header"], "id": 1268}
{"type": "figure", "content": "Figure 1: Environmental impact for a selection of the final training runs described in Section 4.1, where we rank each model by both its total water consumption and its CO2 emissions. Our small models (<1B parameters) were trained on 1.7 trillion tokens, OLMo 1B was trained on 3 trillion, OLMo 2 7B was trained on 4 trillion, OLMoE was trained on 5 trillion, and OLMo 2 13B was trained on 5.6 trillion. We see that the total environmental impact for larger training runs is quite high, and increases quickly with model and dataset size.", "doc_id": "morrison2025", "page": 2, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Figure 1: Environmental impact for a selection of the final training runs described in Section 4.1, where we rank each model by both its total water consumption and its CO2 emissions. Our small models (<1B parameters) were trained on 1.7 trillion tokens, OLMo 1B was trained on 3 trillion, OLMo 2 7B was trained on 4 trillion, OLMoE was trained on 5 trillion, and OLMo 2 13B was trained on 5.6 trillion. We see that the total environmental impact for larger training runs is quite high, and increases quickly with model and dataset size.", "id": 1269}
{"type": "section", "content": "Importantly, we calculate (i) electricity consumption, (ii) carbon emissions, and (iii) water consumption at three points in the machine learning pipeline: early model development (e.g., hyperparameter tuning and experiments before the final training run), training of the main model, and inference. To the best of our knowledge, we are the first to report this information for model development of large language models, and we find the environmental impact of developing even our relatively small models (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or the amount of water consumed by one average person in the United States in about 7.5 years. We encourage the reader to consider larger models released by other organizations to have equivalently larger environmental impacts.", "doc_id": "morrison2025", "page": 2, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Importantly, we calculate (i) electricity consumption, (ii) carbon emissions, and (iii) water consumption at three points in the machine learning pipeline: early model development (e.g., hyperparameter tuning and experiments before the final training run), training of the main model, and inference. To the best of our knowledge, we are the first to report this information for model development of large language models, and we find the environmental impact of developing even our relatively small models (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or the amount of water consumed by one average person in the United States in about 7.5 years. We encourage the reader to consider larger models released by other organizations to have equivalently larger environmental impacts.", "original_types": ["text"], "id": 1270}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied emissions), from electricity consumption during training, and from electricity consumption of the cluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consumption and carbon emissions for training language models and computer vision models with granular timesteps with region-specific carbon intensity, but did not measure development costs, water consumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey et al., 2024) reported electricity consumption and carbon emissions estimates of training their final models; they did not estimate development cost or water consumption, and their approach to carbon intensity varied. Gemma developers (Gemma Team et al., 2024) only report a single number: the total emissions from pretraining their models, not broken down by model or by different stages of training, or by electricity consumption and carbon intensity. The OLMO report (Groeneveld et al., 2024) documents electricity consumption per model, and uses region-specific carbon intensity to estimate emissions for two regions, but does not estimate other environmental impacts. The OLMO 2 report (OLMO et al., 2025) again documents electricity consumption per model and uses region- and datacenter-specific intensity factors to estimate emissions and also water consumption, but does not measure development costs or potential inference costs. Energy use and environmental impacts are not typically documented for proprietary models.\n\nComparably little transparency has been provided on the water consumption of AI systems. Li et al. (2023) estimate the water consumption of some closed models like GPT-3, but these estimates are based on speculation about location of training, energy consumption, etc., as there is very little public information about GPT-3’s training. Similarly, there are few estimates of embodied carbon for AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting of environmental impact is based on training of the final model that is released. Instead of only focusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems. To the best of our knowledge our work provides the first public estimates of environmental impact of development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n\n3. METHODOLOGY", "doc_id": "morrison2025", "page": 3, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied emissions), from electricity consumption during training, and from electricity consumption of the cluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consumption and carbon emissions for training language models and computer vision models with granular timesteps with region-specific carbon intensity, but did not measure development costs, water consumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey et al., 2024) reported electricity consumption and carbon emissions estimates of training their final models; they did not estimate development cost or water consumption, and their approach to carbon intensity varied. Gemma developers (Gemma Team et al., 2024) only report a single number: the total emissions from pretraining their models, not broken down by model or by different stages of training, or by electricity consumption and carbon intensity. The OLMO report (Groeneveld et al., 2024) documents electricity consumption per model, and uses region-specific carbon intensity to estimate emissions for two regions, but does not estimate other environmental impacts. The OLMO 2 report (OLMO et al., 2025) again documents electricity consumption per model and uses region- and datacenter-specific intensity factors to estimate emissions and also water consumption, but does not measure development costs or potential inference costs. Energy use and environmental impacts are not typically documented for proprietary models.\n\nComparably little transparency has been provided on the water consumption of AI systems. Li et al. (2023) estimate the water consumption of some closed models like GPT-3, but these estimates are based on speculation about location of training, energy consumption, etc., as there is very little public information about GPT-3’s training. Similarly, there are few estimates of embodied carbon for AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting of environmental impact is based on training of the final model that is released. Instead of only focusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems. To the best of our knowledge our work provides the first public estimates of environmental impact of development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n\n3. METHODOLOGY", "original_types": ["text", "header"], "id": 1271}
{"type": "section", "content": "Our goal in this work is to characterize the holistic environmental impacts of large language models in as much detail as possible, enabling assessment of key challenges and future directions towards reducing those impacts. Typically, studies documenting language model training and development methodology will address this concern by reporting the cost to train the final, deployed model measured in GPU hours, kWh energy, and/or CO2 emissions. However, this calculation provides an incomplete characterization of the factors leading to environmental degradation due to LLMs that under-estimates impacts and provides insufficient information to inform strategies for developing and deploying LLMs in a more environmentally conscious way.\n\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023), we expand our measurement to include both operational GHG emissions arising from the energy required for the development, training, and inference phases of the ML model lifecycle, as well as embodied emissions attributed to manufacturing of the hardware supporting those operations. We also go beyond previous work to report non-GHG externalities such as water use, and finer-grained data such as variance in energy use throughout training. We describe our methodology for measuring and estimating these impacts in more detail below.\n\n3.1 OPERATIONAL IMPACTS\n\nOperational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. We base our analysis of operational emissions around the following equation introduced by Schwartz et al. (2020) to describe the amount of computation required to produce a machine learning artifact, such as an LLM:", "doc_id": "morrison2025", "page": 3, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Our goal in this work is to characterize the holistic environmental impacts of large language models in as much detail as possible, enabling assessment of key challenges and future directions towards reducing those impacts. Typically, studies documenting language model training and development methodology will address this concern by reporting the cost to train the final, deployed model measured in GPU hours, kWh energy, and/or CO2 emissions. However, this calculation provides an incomplete characterization of the factors leading to environmental degradation due to LLMs that under-estimates impacts and provides insufficient information to inform strategies for developing and deploying LLMs in a more environmentally conscious way.\n\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023), we expand our measurement to include both operational GHG emissions arising from the energy required for the development, training, and inference phases of the ML model lifecycle, as well as embodied emissions attributed to manufacturing of the hardware supporting those operations. We also go beyond previous work to report non-GHG externalities such as water use, and finer-grained data such as variance in energy use throughout training. We describe our methodology for measuring and estimating these impacts in more detail below.\n\n3.1 OPERATIONAL IMPACTS\n\nOperational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. We base our analysis of operational emissions around the following equation introduced by Schwartz et al. (2020) to describe the amount of computation required to produce a machine learning artifact, such as an LLM:", "original_types": ["text", "header"], "id": 1272}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nwhere the cost of a scientific result \\( R \\) (e.g. a claim that a particular training setup reaches \\( X \\) accuracy on benchmark \\( Y \\)) is proportional to the product of the cost of processing a single example \\( E \\), the size of the training dataset \\( D \\), and the number of hyperparameter experiments \\( H \\). In previous work, \\( E \\cdot D \\), the cost of training on the training dataset, is what is most commonly reported, and \\( H \\), the total number of experiments, is most often excluded.\n\nIn our analysis, we calculate the total power consumption during model training, development, and inference, and use this to estimate the total carbon emissions and water consumption during each stage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al., 2024) to calculate CO2 emissions (CO2e) from power consumption:\n\nCO2e = P \\cdot PUE \\cdot CI\n\nwhere the total carbon emissions is equal to the power usage \\( P \\), multiplied by the power usage effectiveness (PUE)6 of the data center, multiplied by the carbon intensity \\( CI \\) of the local power grid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in Texas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was trained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\n\nOur data center providers informed us that Jupiter’s PUE is between 1.1 and 1.2 depending on the current total utilization (we conservatively assume 1.2 for our calculations), and that Augusta’s trailing twelve-month average was 1.12. Jupiter is powered by Austin Energy, which most recently reported a carbon intensity of 0.332 kg CO2 per kWh.7 Augusta is located in Iowa, and the state of Iowa has an average carbon intensity of 0.352 kg CO2 per kWh,8 which we use for our calculations.\n\nWe follow Li et al. (2023) to calculate water consumed onsite and through power generation:\n\nConsumption = P \\cdot PUE \\cdot (WUE onsite + WUE offsite)\n\nwhere WUE onsite is the water usage effectiveness of the data center, dictated by the cooling hardware used, and WUE offsite is the water usage effectiveness of the local power provider, dictated by the precise mixture of sources of power generation, as thermo- and hydro-electric power plants lead to evaporated water that is lost and will not re-enter circulation in the local environment.\n\nAs our data center uses an efficient closed-loop cooling system with no evaporative cooling, we assume a WUE onsite of 0 liters per kWh. Following Reig et al. (2020), we assume a WUE offsite of 1.29 L per kWh for our Jupiter cluster and 3.10 L per kWh for our Augusta cluster.", "doc_id": "morrison2025", "page": 4, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nwhere the cost of a scientific result \\( R \\) (e.g. a claim that a particular training setup reaches \\( X \\) accuracy on benchmark \\( Y \\)) is proportional to the product of the cost of processing a single example \\( E \\), the size of the training dataset \\( D \\), and the number of hyperparameter experiments \\( H \\). In previous work, \\( E \\cdot D \\), the cost of training on the training dataset, is what is most commonly reported, and \\( H \\), the total number of experiments, is most often excluded.\n\nIn our analysis, we calculate the total power consumption during model training, development, and inference, and use this to estimate the total carbon emissions and water consumption during each stage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al., 2024) to calculate CO2 emissions (CO2e) from power consumption:\n\nCO2e = P \\cdot PUE \\cdot CI\n\nwhere the total carbon emissions is equal to the power usage \\( P \\), multiplied by the power usage effectiveness (PUE)6 of the data center, multiplied by the carbon intensity \\( CI \\) of the local power grid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in Texas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was trained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\n\nOur data center providers informed us that Jupiter’s PUE is between 1.1 and 1.2 depending on the current total utilization (we conservatively assume 1.2 for our calculations), and that Augusta’s trailing twelve-month average was 1.12. Jupiter is powered by Austin Energy, which most recently reported a carbon intensity of 0.332 kg CO2 per kWh.7 Augusta is located in Iowa, and the state of Iowa has an average carbon intensity of 0.352 kg CO2 per kWh,8 which we use for our calculations.\n\nWe follow Li et al. (2023) to calculate water consumed onsite and through power generation:\n\nConsumption = P \\cdot PUE \\cdot (WUE onsite + WUE offsite)\n\nwhere WUE onsite is the water usage effectiveness of the data center, dictated by the cooling hardware used, and WUE offsite is the water usage effectiveness of the local power provider, dictated by the precise mixture of sources of power generation, as thermo- and hydro-electric power plants lead to evaporated water that is lost and will not re-enter circulation in the local environment.\n\nAs our data center uses an efficient closed-loop cooling system with no evaporative cooling, we assume a WUE onsite of 0 liters per kWh. Following Reig et al. (2020), we assume a WUE offsite of 1.29 L per kWh for our Jupiter cluster and 3.10 L per kWh for our Augusta cluster.", "original_types": ["text", "header", "equation"], "id": 1273}
{"type": "section", "content": "Both calculations rely on total power usage. To calculate power usage during development and training, we analyze detailed time series data for a single node throughout each run, logging power data at sub-second intervals, and extrapolate to the total number of nodes. As we only measure GPU power consumption, our estimates should be viewed as a lower bound on the true amount of power consumed during development and training.\n\n3.2 Embodied Impacts\n\nEmbodied impacts are those arising from the production of physical elements required to support LLM development and use, such as hardware manufacturing and data center construction. To calculate embodied emissions, we follow Luccioni et al. (2023) by amortizing the carbon emissions from manufacturing over the lifetime of the hardware to get an estimate of the per hour cost, and multiplying by the number of GPU hours used throughout model development and training. We extend this to include water consumption as well, by amortizing estimates of water consumption during manufacturing over the lifetime of the hardware.", "doc_id": "morrison2025", "page": 4, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Both calculations rely on total power usage. To calculate power usage during development and training, we analyze detailed time series data for a single node throughout each run, logging power data at sub-second intervals, and extrapolate to the total number of nodes. As we only measure GPU power consumption, our estimates should be viewed as a lower bound on the true amount of power consumed during development and training.\n\n3.2 Embodied Impacts\n\nEmbodied impacts are those arising from the production of physical elements required to support LLM development and use, such as hardware manufacturing and data center construction. To calculate embodied emissions, we follow Luccioni et al. (2023) by amortizing the carbon emissions from manufacturing over the lifetime of the hardware to get an estimate of the per hour cost, and multiplying by the number of GPU hours used throughout model development and training. We extend this to include water consumption as well, by amortizing estimates of water consumption during manufacturing over the lifetime of the hardware.", "original_types": ["text", "header"], "id": 1274}
{"type": "section", "content": "3.3 MODELS, DATA, AND HARDWARE\n\nMost of the models we evaluate are standard dense transformers, with an architecture similar to Llama (Touvron et al., 2023a;b; Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other recent popular models, ranging in size from 20 million to 13 billion active parameters. Each of the sub-billion parameter models was trained on 1.7 trillion tokens, the 1 billion parameter model was trained to 3 trillion tokens, the 7 billion parameter models were trained to 2, 3 and 4 trillion tokens, and the 13 billion parameter model to 5.6 trillion tokens. We additionally evaluate a mixture-of-experts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high speed interconnect between each node, and between 2 and 128 nodes concurrently per training run. All models except the 13B were trained in the same data center. See OLMo et al. (2025) for more information on our technical infrastructure.\n\n3.4 SIMULATING INFERENCE", "doc_id": "morrison2025", "page": 5, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "3.3 MODELS, DATA, AND HARDWARE\n\nMost of the models we evaluate are standard dense transformers, with an architecture similar to Llama (Touvron et al., 2023a;b; Dubey et al., 2024), OLMo (Groeneveld et al., 2024), and other recent popular models, ranging in size from 20 million to 13 billion active parameters. Each of the sub-billion parameter models was trained on 1.7 trillion tokens, the 1 billion parameter model was trained to 3 trillion tokens, the 7 billion parameter models were trained to 2, 3 and 4 trillion tokens, and the 13 billion parameter model to 5.6 trillion tokens. We additionally evaluate a mixture-of-experts (MoE) model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. Each model was trained on standard HGX servers with 8 NVIDIA H100 GPUs per server, with high speed interconnect between each node, and between 2 and 128 nodes concurrently per training run. All models except the 13B were trained in the same data center. See OLMo et al. (2025) for more information on our technical infrastructure.\n\n3.4 SIMULATING INFERENCE", "original_types": ["text", "header"], "id": 1275}
{"type": "section", "content": "Because we do not deploy our models, we do not collect or report data about real usage of our models. We instead report estimated costs associated with deployment of a subset of our models, along with comparison models, with varying inference configurations. In reality, causal language models can have a variety of use cases and be deployed on a variety of hardware infrastructure. As a representative deployment setting, we assume a setting in which users interact with the models via chat; we collect measurements assuming models are served on a single H100 GPU via SGLang (Zheng et al., 2024). All three inference configurations used can be mapped to a previously proposed realistic online inference scenario (Reddi et al., 2020; Peng et al., 2023). Specifically, other than the “batching” scenario where all requests are sent instantaneously, the requests follow a Poisson distribution, albeit at different rates that influence different batch sizes. The requests themselves come from the ShareGPT dataset,9 and each inference scenario involves the same sample of 2400 prompts (same random seed). Input and output lengths, therefore, are the same in theory for a given model, but due to differences in tokenization and model context length, there are slight variations in mean input/output lengths across models, 225-250 and 190-230 tokens respectively. In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking, which was verified against the same time series monitoring used throughout training. Notably, we measure total power and energy consumption associated with only the relevant processes, excluding the overhead associated with, for example, holding the model in memory or listening for requests. We ran our inference simulations on our Jupiter cluster, used to train almost all of our models, but we use only a single H100 GPU at a time. See Appendix A.1 for details about our inference methodology and assumptions.", "doc_id": "morrison2025", "page": 5, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Because we do not deploy our models, we do not collect or report data about real usage of our models. We instead report estimated costs associated with deployment of a subset of our models, along with comparison models, with varying inference configurations. In reality, causal language models can have a variety of use cases and be deployed on a variety of hardware infrastructure. As a representative deployment setting, we assume a setting in which users interact with the models via chat; we collect measurements assuming models are served on a single H100 GPU via SGLang (Zheng et al., 2024). All three inference configurations used can be mapped to a previously proposed realistic online inference scenario (Reddi et al., 2020; Peng et al., 2023). Specifically, other than the “batching” scenario where all requests are sent instantaneously, the requests follow a Poisson distribution, albeit at different rates that influence different batch sizes. The requests themselves come from the ShareGPT dataset,9 and each inference scenario involves the same sample of 2400 prompts (same random seed). Input and output lengths, therefore, are the same in theory for a given model, but due to differences in tokenization and model context length, there are slight variations in mean input/output lengths across models, 225-250 and 190-230 tokens respectively. In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking, which was verified against the same time series monitoring used throughout training. Notably, we measure total power and energy consumption associated with only the relevant processes, excluding the overhead associated with, for example, holding the model in memory or listening for requests. We ran our inference simulations on our Jupiter cluster, used to train almost all of our models, but we use only a single H100 GPU at a time. See Appendix A.1 for details about our inference methodology and assumptions.", "original_types": ["text"], "id": 1276}
{"type": "section", "content": "Table 1: We developed our models in five groups, based on parameter count and architecture: less than 1 billion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active and 7 billion total parameters. We found that ~70% of our developmental environmental impact came from developing the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth of gasoline, and equal to about 7 and a half years of water used by the average person in the United States.", "doc_id": "morrison2025", "page": 6, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 1: We developed our models in five groups, based on parameter count and architecture: less than 1 billion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active and 7 billion total parameters. We found that ~70% of our developmental environmental impact came from developing the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks’ worth of gasoline, and equal to about 7 and a half years of water used by the average person in the United States.", "original_types": ["header"], "id": 1277}
{"type": "table", "content": "Table 1\n| | GPU Hours | Total MWh | # Runs | Carbon Emissions (tCO₂eq) | Equivalent to... (energy usage, 1 home, U.S.) | Water Consumption (kL) | Equivalent to... (water usage, 1 person) \n\n<1B | 29k | 19 | 20 | 6 | 1 yr, 4 mo | 24 | 3 mo \n\n7B | 269k | 196 | 375 | 65 | 13 yrs, 6 mo | 252 | 2 yrs, 7 mo \n\n13B | 191k | 116 | 156 | 46 | 9 yrs, 7 mo | 402 | 3 yrs, 7 mo \n\nMoE | 27k | 19 | 35 | 6 | 1 yr, 4 mo | 24 | 3 mo \n\nTotal | 680k | 459 | 813 | 159 | 33 yrs, 1 mo | 843 | 7 yrs, 5 mo", "doc_id": "morrison2025", "page": 6, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 1\n| | GPU Hours | Total MWh | # Runs | Carbon Emissions (tCO₂eq) | Equivalent to... (energy usage, 1 home, U.S.) | Water Consumption (kL) | Equivalent to... (water usage, 1 person) \n\n<1B | 29k | 19 | 20 | 6 | 1 yr, 4 mo | 24 | 3 mo \n\n7B | 269k | 196 | 375 | 65 | 13 yrs, 6 mo | 252 | 2 yrs, 7 mo \n\n13B | 191k | 116 | 156 | 46 | 9 yrs, 7 mo | 402 | 3 yrs, 7 mo \n\nMoE | 27k | 19 | 35 | 6 | 1 yr, 4 mo | 24 | 3 mo \n\nTotal | 680k | 459 | 813 | 159 | 33 yrs, 1 mo | 843 | 7 yrs, 5 mo", "id": 1278}
{"type": "section", "content": "Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO₂eq per 8x server node, equal 463 kg per GPU. There is little public information on how much water is required to produce a single GPU, though chip manufacturing facilities require millions of liters per day.10 Some estimates11 place TSMC water usage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which we use for our analysis.\n\nWe additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO₂eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams12 and produces about 63 H100s.13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO₂eq per GPU.\n\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO₂eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in total, leading to a total of 22 tCO₂eq emitted and 4.8 kL of water consumed during manufacturing.", "doc_id": "morrison2025", "page": 6, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO₂eq per 8x server node, equal 463 kg per GPU. There is little public information on how much water is required to produce a single GPU, though chip manufacturing facilities require millions of liters per day.10 Some estimates11 place TSMC water usage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which we use for our analysis.\n\nWe additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO₂eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams12 and produces about 63 H100s.13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO₂eq per GPU.\n\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO₂eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in total, leading to a total of 22 tCO₂eq emitted and 4.8 kL of water consumed during manufacturing.", "original_types": ["text"], "id": 1279}
{"type": "section", "content": "Table 2:\n\nWe list the estimated power usage, carbon emissions, and water consumption from training our dense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and a mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We find that the environmental impact is quite high, even for our relatively small models. Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See Groeneveld et al. (2024) for more information.\n† denotes unreleased models that were trained for various internal experiments.", "doc_id": "morrison2025", "page": 7, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 2:\n\nWe list the estimated power usage, carbon emissions, and water consumption from training our dense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and a mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We find that the environmental impact is quite high, even for our relatively small models. Training our series of models emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and consumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See Groeneveld et al. (2024) for more information.\n† denotes unreleased models that were trained for various internal experiments.", "original_types": ["text", "header"], "id": 1280}
{"type": "table", "content": "Table 2: Estimated Power Usage, Carbon Emissions, and Water Consumption from Training Various Models\n| Model | Power Usage (MWh) | Carbon Emissions (tCO₂eq) | Equiv. to... (energy usage, 1 home, U.S.) | Water Consumption (kL) | Equiv. to... (water usage, 1 person, U.S.) |\n|-------|------------------|--------------------------|------------------------------------------|------------------------|------------------------------------------|\n| Gemma 2B & 9B | - | 131 | 25 yrs, 11 mo | - | - |\n| Llama 2 7B | 81 | 31 | 6 yrs, 1 mo | - | - |\n| Llama 2 13B | 162 | 62 | 12 yrs, 2 mo | - | - |\n| Llama 3.1 8B | - | 420 | 83 years | - | - |\n| Llama 3.2 1B | - | 107 | 14 years | - | - |\n\n| OLMo 20M† | 0.8 | 0.3 | 3 weeks | 1 | 3 days |\n| OLMo 60M† | 1.2 | 0.4 | 1 month | 1.6 | 5 days |\n| OLMo 150M† | 2.4 | 1 | 2 mo, 1 wk | 3.6 | 12 days |\n| OLMo 300M† | 5 | 2 | 5 months | 5.9 | 19 days |\n| OLMo 700M† | 8 | 3 | 7 months | 10 | 33 days |\n| OLMo 7B† | 67 | 22 | 4 yrs, 4 mo | 87 | 9 months |\n| OLMo 1B (3T) | 30 | 10 | 2 years | 39 | 4 months |\n| OLMo 7B | 149 | 0* | - | 0* | - |\n| OLMo 7B (Twin) | 114 | 70 | 13 yrs, 10 mo | 487 | 4 yrs, 4 mo |\n| OLMo (04|07)24 7B | 95 | 32 | 6 yrs, 4 mo | 122 | 1 yr, 1 mo |\n| OLMo 2 7B | 157 | 52 | 10 yrs, 4 mo | 202 | 1 yr, 9 mo |\n| OLMo 2 13B | 230 | 101 | 21 years | 892 | 7 yrs, 10 mo |\n| OLMoE 0924 | 54 | 18 | 3 yrs, 7 mo | 70 | 7 months |\n\n| Total (Ours) | 913 | 312 | 65 years | 1,921 | 17 yrs, 1 mo |\n", "doc_id": "morrison2025", "page": 7, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 2: Estimated Power Usage, Carbon Emissions, and Water Consumption from Training Various Models\n| Model | Power Usage (MWh) | Carbon Emissions (tCO₂eq) | Equiv. to... (energy usage, 1 home, U.S.) | Water Consumption (kL) | Equiv. to... (water usage, 1 person, U.S.) |\n|-------|------------------|--------------------------|------------------------------------------|------------------------|------------------------------------------|\n| Gemma 2B & 9B | - | 131 | 25 yrs, 11 mo | - | - |\n| Llama 2 7B | 81 | 31 | 6 yrs, 1 mo | - | - |\n| Llama 2 13B | 162 | 62 | 12 yrs, 2 mo | - | - |\n| Llama 3.1 8B | - | 420 | 83 years | - | - |\n| Llama 3.2 1B | - | 107 | 14 years | - | - |\n\n| OLMo 20M† | 0.8 | 0.3 | 3 weeks | 1 | 3 days |\n| OLMo 60M† | 1.2 | 0.4 | 1 month | 1.6 | 5 days |\n| OLMo 150M† | 2.4 | 1 | 2 mo, 1 wk | 3.6 | 12 days |\n| OLMo 300M† | 5 | 2 | 5 months | 5.9 | 19 days |\n| OLMo 700M† | 8 | 3 | 7 months | 10 | 33 days |\n| OLMo 7B† | 67 | 22 | 4 yrs, 4 mo | 87 | 9 months |\n| OLMo 1B (3T) | 30 | 10 | 2 years | 39 | 4 months |\n| OLMo 7B | 149 | 0* | - | 0* | - |\n| OLMo 7B (Twin) | 114 | 70 | 13 yrs, 10 mo | 487 | 4 yrs, 4 mo |\n| OLMo (04|07)24 7B | 95 | 32 | 6 yrs, 4 mo | 122 | 1 yr, 1 mo |\n| OLMo 2 7B | 157 | 52 | 10 yrs, 4 mo | 202 | 1 yr, 9 mo |\n| OLMo 2 13B | 230 | 101 | 21 years | 892 | 7 yrs, 10 mo |\n| OLMoE 0924 | 54 | 18 | 3 yrs, 7 mo | 70 | 7 months |\n\n| Total (Ours) | 913 | 312 | 65 years | 1,921 | 17 yrs, 1 mo |\n", "id": 1281}
{"type": "section", "content": "Putting it in perspective\n\nIn total, our series of models led to at least 493 tCO₂eq emitted. Using the U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator15, this is equivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy use for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in one year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to about 24 and a half years of water consumption by the average person in the U.S.16\n\nOther Costs\n\nIn this work, we strive to provide a thorough accounting of the total cost of developing our models. However, there remain a number of sources of emissions and water consumption that are difficult, if not impossible to comprehensively measure without access to proprietary information across a range of industries, such as transportation and end of life hardware disposal. While the costs we report above represent a large portion of the total development process, more transparency is needed to understand the full impact of model training.", "doc_id": "morrison2025", "page": 7, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Putting it in perspective\n\nIn total, our series of models led to at least 493 tCO₂eq emitted. Using the U.S. Environmental Protection Agency’s Greenhouse Gas Equivalencies Calculator15, this is equivalent to 6.5 tanker trucks’ worth of gasoline burned, emissions from the average yearly energy use for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in one year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to about 24 and a half years of water consumption by the average person in the U.S.16\n\nOther Costs\n\nIn this work, we strive to provide a thorough accounting of the total cost of developing our models. However, there remain a number of sources of emissions and water consumption that are difficult, if not impossible to comprehensively measure without access to proprietary information across a range of industries, such as transportation and end of life hardware disposal. While the costs we report above represent a large portion of the total development process, more transparency is needed to understand the full impact of model training.", "original_types": ["text", "header"], "id": 1282}
{"type": "section", "content": "Published as a conference paper at ICLR 2025", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025", "original_types": ["header"], "id": 1283}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1284}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1285}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1286}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1287}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1288}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1289}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1290}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1291}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1292}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1293}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1294}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1295}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1296}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1297}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. Also of note is the relatively small variability in carbon emissions and water consumption across different model sizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated; greater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report \"break-even\" points for Qwen 2.5 because its training costs are not public.", "id": 1298}
{"type": "table", "content": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.", "doc_id": "morrison2025", "page": 8, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. Since the models were served on machines from the same cluster that our OLMo 2 models were trained on, we use the same WUE and PUE coefficients of 1.29 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption and carbon emissions, namely MWh → kWh, tons → grams CO2eq, and kL → L. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.", "id": 1299}
{"type": "section", "content": "5 Discussion\n\n5.1 More Transparency is (Still) Needed\n\nWhile many model developers—including some of the largest for-profit entities operating in this space—make best efforts to report at least part of the cost of building their AI systems (Dubey et al., 2024; Gemma Team et al., 2024), more transparency is still needed throughout the development pipeline. The EU AI Act,18 and some proposed legislation, such as the Artificial Intelligence Environmental Impacts Act19 in the United States, would start the process for defining voluntary environmental impact reporting standards for model developers, but until such standards are widespread in the community, improved transparency can only come through voluntary efforts by companies and research organizations. Policy action is needed to ensure there is public visibility into environmental impacts across the entire supply chain, from hardware manufacturing, data center construction, and energy production, all the way through to model deployment and inference.\n\nEmbodied emissions are still an enigma Though a vital piece of all model development pipelines, the environmental impact of manufacturing the GPUs used is essentially unknown. In previous work, Wu et al. (2022) and Luccioni et al. (2023) highlighted the fact that researchers focused on AI’s environmental impact are forced to use unreliable estimates of the cost of manufacturing state-of-the-art computational hardware, and the situation is no better now, nearly two years later. Many companies that manufacture other pieces of data center hardware disclose estimates of the lifetime environmental impact,20 and until GPU manufacturers release similar information—on a voluntary or compulsory basis—this will not improve.\n\nDevelopment costs are substantial, and unreported As reported in Section 4.1, we present detailed information on the cost of developing our training pipeline, in contrast with previous work. We found that development costs—associated with failed runs, hyperparameter searches, testing architecture changes, and more—are responsible for a substantial portion of the total environmental impact of creating our systems, highlighting a need for more transparency from developers. This is especially important in light of AutoML tools, where many models may be automatically trained while searching for a solution, and scaling law experiments, where smaller models are trained to predict the performance of larger models, and then discarded (Li et al., 2024; Bhagia et al., 2024).", "doc_id": "morrison2025", "page": 9, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "5 Discussion\n\n5.1 More Transparency is (Still) Needed\n\nWhile many model developers—including some of the largest for-profit entities operating in this space—make best efforts to report at least part of the cost of building their AI systems (Dubey et al., 2024; Gemma Team et al., 2024), more transparency is still needed throughout the development pipeline. The EU AI Act,18 and some proposed legislation, such as the Artificial Intelligence Environmental Impacts Act19 in the United States, would start the process for defining voluntary environmental impact reporting standards for model developers, but until such standards are widespread in the community, improved transparency can only come through voluntary efforts by companies and research organizations. Policy action is needed to ensure there is public visibility into environmental impacts across the entire supply chain, from hardware manufacturing, data center construction, and energy production, all the way through to model deployment and inference.\n\nEmbodied emissions are still an enigma Though a vital piece of all model development pipelines, the environmental impact of manufacturing the GPUs used is essentially unknown. In previous work, Wu et al. (2022) and Luccioni et al. (2023) highlighted the fact that researchers focused on AI’s environmental impact are forced to use unreliable estimates of the cost of manufacturing state-of-the-art computational hardware, and the situation is no better now, nearly two years later. Many companies that manufacture other pieces of data center hardware disclose estimates of the lifetime environmental impact,20 and until GPU manufacturers release similar information—on a voluntary or compulsory basis—this will not improve.\n\nDevelopment costs are substantial, and unreported As reported in Section 4.1, we present detailed information on the cost of developing our training pipeline, in contrast with previous work. We found that development costs—associated with failed runs, hyperparameter searches, testing architecture changes, and more—are responsible for a substantial portion of the total environmental impact of creating our systems, highlighting a need for more transparency from developers. This is especially important in light of AutoML tools, where many models may be automatically trained while searching for a solution, and scaling law experiments, where smaller models are trained to predict the performance of larger models, and then discarded (Li et al., 2024; Bhagia et al., 2024).", "original_types": ["text", "header"], "id": 1300}
{"type": "section", "content": "Water costs are real, and under-explored While under-explored in previous work, AI’s growing water consumption is beginning to receive more and more attention21 (Li et al., 2023), though not as much as it may deserve. As shown in Section 4.1, even training a series of comparatively small models uses a large amount of water, the amount of which is also drastically impacted by both the cooling systems used in data centers as well as the power generation methods used. Without more transparency from developers on when, where, and how they are training their models, it will continue to be difficult to quantify the scale of the issue, stymieing efforts to address it.\n\n5.2 Small Choices During Training Can Have Large Impacts\n\nWhile many issues relating to transparency require action from corporations and large research groups, choices made during training have a large effect downstream.", "doc_id": "morrison2025", "page": 9, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Water costs are real, and under-explored While under-explored in previous work, AI’s growing water consumption is beginning to receive more and more attention21 (Li et al., 2023), though not as much as it may deserve. As shown in Section 4.1, even training a series of comparatively small models uses a large amount of water, the amount of which is also drastically impacted by both the cooling systems used in data centers as well as the power generation methods used. Without more transparency from developers on when, where, and how they are training their models, it will continue to be difficult to quantify the scale of the issue, stymieing efforts to address it.\n\n5.2 Small Choices During Training Can Have Large Impacts\n\nWhile many issues relating to transparency require action from corporations and large research groups, choices made during training have a large effect downstream.", "original_types": ["text", "header"], "id": 1301}
{"type": "section", "content": "Smaller models are cheaper to train and use, but at what cost?\n\nUntil recently, to achieve high model performance, a large model was needed. Compute-optimal scaling laws for neural network training (Hoffmann et al., 2022; Kaplan et al., 2020) imply that it is more efficient to put more data into a larger model, because of diminishing returns from “over-training” a small model. This meant that models were expensive to both train and deploy, limiting how widespread they could become, and how financially feasible they were to be used in a variety of scenarios.", "doc_id": "morrison2025", "page": 10, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Smaller models are cheaper to train and use, but at what cost?\n\nUntil recently, to achieve high model performance, a large model was needed. Compute-optimal scaling laws for neural network training (Hoffmann et al., 2022; Kaplan et al., 2020) imply that it is more efficient to put more data into a larger model, because of diminishing returns from “over-training” a small model. This meant that models were expensive to both train and deploy, limiting how widespread they could become, and how financially feasible they were to be used in a variety of scenarios.", "original_types": ["text", "header"], "id": 1302}
{"type": "figure", "content": "Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training. The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.", "doc_id": "morrison2025", "page": 10, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training. The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.", "id": 1303}
{"type": "section", "content": "References\n\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojgan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219.\n\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 4895–4901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298.\n\nAkshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Hannaneh Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL https://arxiv.org/abs/2412.04403.", "doc_id": "morrison2025", "page": 11, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "References\n\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojgan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219.\n\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 4895–4901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298.\n\nAkshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Hannaneh Hajishirzi. Establishing task scaling laws via compute-efficient model ladders, 2024. URL https://arxiv.org/abs/2412.04403.", "original_types": ["text", "header"], "id": 1304}
{"type": "section", "content": "Callum Browning, Stephen Northey, Nawshad Haque, Warren Bruckard, and Mark Cooksey. Life Cycle Assessment of Rare Earth Production from Monazite, pp. 83–88. Springer International Publishing, Cham, 2016. ISBN 978-3-319-48768-7. doi: 10.1007/978-3-319-48768-7_12. URL https://doi.org/10.1007/978-3-319-48768-7_12.\n\nBenoit Courty, Victor Schmidt, Goyal-Kamal, MarionCoutarel, Luis Blanche, Boris Feld, inimaz, Jérémy Lecourt, LiamConnell, SabAmine, supatomic, Mathilde Léval, Alexis Cruveiller, oumi-nasara, Franklin Zhao, Aditya Joshi, Christian Bauer, Amine Saboni, Patrick LLORET, Alexis Bogroff, Niko Laskaris, Hugues de Lavoreille, Alexandre Phiev, Edoardo Abati, rosekelly6400, Douglas Blank, Ziyao Wang, Lucas Otávio, and Armin Catovic. mlco2/codecarbon: v2.7.1, September 2024. URL https://doi.org/10.5281/zenodo.13744486.\n\nJesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Measuring the carbon intensity of ai in cloud instances, 2022. URL https://arxiv.org/abs/2206.05229.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex", "doc_id": "morrison2025", "page": 11, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Callum Browning, Stephen Northey, Nawshad Haque, Warren Bruckard, and Mark Cooksey. Life Cycle Assessment of Rare Earth Production from Monazite, pp. 83–88. Springer International Publishing, Cham, 2016. ISBN 978-3-319-48768-7. doi: 10.1007/978-3-319-48768-7_12. URL https://doi.org/10.1007/978-3-319-48768-7_12.\n\nBenoit Courty, Victor Schmidt, Goyal-Kamal, MarionCoutarel, Luis Blanche, Boris Feld, inimaz, Jérémy Lecourt, LiamConnell, SabAmine, supatomic, Mathilde Léval, Alexis Cruveiller, oumi-nasara, Franklin Zhao, Aditya Joshi, Christian Bauer, Amine Saboni, Patrick LLORET, Alexis Bogroff, Niko Laskaris, Hugues de Lavoreille, Alexandre Phiev, Edoardo Abati, rosekelly6400, Douglas Blank, Ziyao Wang, Lucas Otávio, and Armin Catovic. mlco2/codecarbon: v2.7.1, September 2024. URL https://doi.org/10.5281/zenodo.13744486.\n\nJesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Measuring the carbon intensity of ai in cloud instances, 2022. URL https://arxiv.org/abs/2206.05229.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex", "original_types": ["text"], "id": 1305}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nBotev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295.\n\nAlistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. How data centers and the energy sector can sate ai’s hunger for power. McKinsey and Company, 2024.\n\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Arthur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15789–15809, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.841. URL https://aclanthology.org/2024.acl-long.841.", "doc_id": "morrison2025", "page": 12, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nBotev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295.\n\nAlistair Green, Humayun Tai, Jesse Noffsinger, and Pankaj Sachdeva. How data centers and the energy sector can sate ai’s hunger for power. McKinsey and Company, 2024.\n\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Arthur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15789–15809, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.841. URL https://aclanthology.org/2024.acl-long.841.", "original_types": ["text"], "id": 1306}
{"type": "section", "content": "Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, 2024. URL https://arxiv.org/abs/2407.21075.", "doc_id": "morrison2025", "page": 12, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, 2024. URL https://arxiv.org/abs/2407.21075.", "original_types": ["text"], "id": 1307}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api.semanticscholar.org/CorpusID:247778764.\n\nWilliam Stanley Jevons. The Coal Question; An Inquiry Concerning the Progress of the Nation, and the Probable Exhaustion of Our Coal Mines. London: Macmillan and Co, 1865.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. URL https://arxiv.org/abs/2406.11794.\n\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making ai less ”thirsty”: Uncovering and addressing the secret water footprint of ai models, 2023. URL https://arxiv.org/abs/2304.03271.\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24 (253):1–15, 2023. URL http://jmlr.org/papers/v24/23-0069.html.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the cost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 85–99, 2024.\n\nSachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. OpenELM: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II @ ICML2024, 2024. URL https://openreview.net/forum?id=XNMbTkxroF.", "doc_id": "morrison2025", "page": 13, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https://api.semanticscholar.org/CorpusID:247778764.\n\nWilliam Stanley Jevons. The Coal Question; An Inquiry Concerning the Progress of the Nation, and the Probable Exhaustion of Our Coal Mines. London: Macmillan and Co, 1865.\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. URL https://arxiv.org/abs/2406.11794.\n\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making ai less ”thirsty”: Uncovering and addressing the secret water footprint of ai models, 2023. URL https://arxiv.org/abs/2304.03271.\n\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model. Journal of Machine Learning Research, 24 (253):1–15, 2023. URL http://jmlr.org/papers/v24/23-0069.html.\n\nSasha Luccioni, Yacine Jernite, and Emma Strubell. Power hungry processing: Watts driving the cost of ai deployment? In The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 85–99, 2024.\n\nSachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. OpenELM: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II @ ICML2024, 2024. URL https://openreview.net/forum?id=XNMbTkxroF.", "original_types": ["text"], "id": 1308}
{"type": "section", "content": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bha gia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py atkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656.\n\nHao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith, and Hannaneh Hajishirzi. Efficiency pentathlon: A standardized arena for efficiency evaluation, 2023. URL https://arxiv.org/abs/2307.09701.\n\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, CaroleJean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott", "doc_id": "morrison2025", "page": 13, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bha gia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py atkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656.\n\nHao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith, and Hannaneh Hajishirzi. Efficiency pentathlon: A standardized arena for efficiency evaluation, 2023. URL https://arxiv.org/abs/2307.09701.\n\nVijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, CaroleJean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott", "original_types": ["text"], "id": 1309}
{"type": "section", "content": "Published as a conference paper at ICLR 2025\n\nGardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius, Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pp. 446–459, 2020. doi: 10.1109/ISCA45697.2020.00045.\n\nPaul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water use embedded in purchased electricity, 2020. URL https://www.wri.org/research/guidance-calculating-water-use-embedded-purchased-electricity.\n\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12): 54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https://doi.org/10.1145/3381831.\n\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage report. 2024.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 13693–13696, 2020.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. URL https://arxiv.org/abs/2302.13971.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.\n\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104.", "doc_id": "morrison2025", "page": 14, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Published as a conference paper at ICLR 2025\n\nGardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius, Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pp. 446–459, 2020. doi: 10.1109/ISCA45697.2020.00045.\n\nPaul Reig, Tianyi Luo, Eric Christensen, and Julie Sinistore. Guidance for calculating water use embedded in purchased electricity, 2020. URL https://www.wri.org/research/guidance-calculating-water-use-embedded-purchased-electricity.\n\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Commun. ACM, 63(12): 54–63, November 2020. ISSN 0001-0782. doi: 10.1145/3381831. URL https://doi.org/10.1145/3381831.\n\nArman Shehabi, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakkar Siddik, Billie Holecek, Jonathan Koomey, Eric Masanet, Dale Sartor, et al. 2024 united states data center energy usage report. 2024.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 13693–13696, 2020.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. URL https://arxiv.org/abs/2302.13971.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.\n\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104.", "original_types": ["text"], "id": 1310}
{"type": "section", "content": "Appendix\n\nWe benchmark models using the ShareGPT dataset, assuming an online inference chat setting. In practice, with much longer inference examples, OLMo models may have an “unfair” advantage in that they were generally trained with context lengths shorter than the other models we benchmark. However, we do not believe that to be a significant factor in our results. In fact, we observe that Llama 3.1 8B is actually measured to be faster and less energy intensive than OLMo 7b models, likely due to the use of grouped-query attention (GQA; Ainslie et al. (2023)) in Llama 8b, vs not in OLMo models.\n\nWe report additional inference simulation results on a larger set of models in Table 4,\n\nLimitations\n\nOur main limitations are discussed throughout the main body of this work – in particular, we make various assumptions about embodied impacts due to lack of real data, and our inference and deployment numbers were benchmarked in a controlled, limited setting, as we do not in reality serve our models in the same sense, and we do not have access to data about most other models’ real usage.\n\nWe present only a limited set of inference simulations following a number of simplistic assumptions. Specifically, we simulate only settings where a deployed model is ingesting input tokens and generating output tokens following default parameters defined in SGLang (Zheng et al., 2024) – as opposed to, for instance, evaluating only the likelihood of a given text. Additionally, we note that practitioners frequently employ different inference-time optimizations such as quantization; perform generation with different decoding algorithms; and/or deploy to and run inference on edge devices, sometimes even without GPUs. We do not account for this variety of scenarios in our experiments.\n\nWe observe linear trends in training costs relative to parameter count across four orders of magnitude and eight model sizes. However, we do not necessarily expect that this trend would hold tightly in all training settings across all possible scales – for instance, decentralized training or training across multiple datacenters might be expected to incur significantly greater communication overhead throughout training. Though we have not trained these models ourselves, our hope is that our work will encourage others working in a broad range of settings to provide their own holistic reports of environmental resource consumption.", "doc_id": "morrison2025", "page": 15, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Appendix\n\nWe benchmark models using the ShareGPT dataset, assuming an online inference chat setting. In practice, with much longer inference examples, OLMo models may have an “unfair” advantage in that they were generally trained with context lengths shorter than the other models we benchmark. However, we do not believe that to be a significant factor in our results. In fact, we observe that Llama 3.1 8B is actually measured to be faster and less energy intensive than OLMo 7b models, likely due to the use of grouped-query attention (GQA; Ainslie et al. (2023)) in Llama 8b, vs not in OLMo models.\n\nWe report additional inference simulation results on a larger set of models in Table 4,\n\nLimitations\n\nOur main limitations are discussed throughout the main body of this work – in particular, we make various assumptions about embodied impacts due to lack of real data, and our inference and deployment numbers were benchmarked in a controlled, limited setting, as we do not in reality serve our models in the same sense, and we do not have access to data about most other models’ real usage.\n\nWe present only a limited set of inference simulations following a number of simplistic assumptions. Specifically, we simulate only settings where a deployed model is ingesting input tokens and generating output tokens following default parameters defined in SGLang (Zheng et al., 2024) – as opposed to, for instance, evaluating only the likelihood of a given text. Additionally, we note that practitioners frequently employ different inference-time optimizations such as quantization; perform generation with different decoding algorithms; and/or deploy to and run inference on edge devices, sometimes even without GPUs. We do not account for this variety of scenarios in our experiments.\n\nWe observe linear trends in training costs relative to parameter count across four orders of magnitude and eight model sizes. However, we do not necessarily expect that this trend would hold tightly in all training settings across all possible scales – for instance, decentralized training or training across multiple datacenters might be expected to incur significantly greater communication overhead throughout training. Though we have not trained these models ourselves, our hope is that our work will encourage others working in a broad range of settings to provide their own holistic reports of environmental resource consumption.", "original_types": ["text", "header"], "id": 1311}
{"type": "table", "content": "Table 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served on machines from the same cluster that our models were trained on, so we use the same WUE and PUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. We do not report “break-even” points for Qwen models since the training costs are not public.\nRequest freq. (req / s) | GPU Power Usage (kWh) | Carbon Emissions (g CO2eq) | Water consump. (L) | Seconds per 100 req. | # Inf. for CO2 equiv. w/ training\n\nLlama 3.2 1B | ∞ | 0.003 | 1.0 | 0.004 | 1.38 | 258 bil.\n8 | 0.036 | 12.0 | 0.054 | 12.64 | 21.5 bil.\n1 | 0.16 | 53.1 | 0.238 | 100.58 | 4.83 bil.\n\nLlama 2 7B | ∞ | 0.019 | 6.3 | 0.028 | 3.58 | 11.9 bil.\n8 | 0.054 | 17.9 | 0.08 | 12.83 | 4.18 bil.\n1 | 0.349 | 115.9 | 0.52 | 100.62 | 647 mil.\n\nLlama 3 8B | ∞ | 0.01 | 3.3 | 0.015 | 1.93 | 282 bil.\n8 | 0.052 | 17.3 | 0.077 | 12.78 | 54.2 bil.\n1 | 0.337 | 111.9 | 0.502 | 100.63 | 8.37 bil.\n\nLlama 3.1 8B | ∞ | 0.011 | 3.7 | 0.016 | 2.13 | 276 bil.\n8 | 0.051 | 16.9 | 0.076 | 12.79 | 59.5 bil.\n1 | 0.333 | 110.6 | 0.496 | 100.64 | 9.12 bil.\n\nLlama 2 13B | ∞ | 0.034 | 11.3 | 0.051 | 6.53 | 13.3 bil.\n8 | 0.06 | 19.9 | 0.089 | 13.09 | 7.52 bil.\n1 | 0.401 | 133.1 | 0.597 | 100.73 | 1.13 bil.\n\n\nQwen 2.5 1.5B | ∞ | 0.003 | 1.0 | 0.004 | 0.86 | –\n8 | 0.033 | 11.0 | 0.049 | 12.65 | –\n1 | 0.163 | 54.1 | 0.243 | 100.57 | –\n\nQwen 2.5 7B | ∞ | 0.009 | 3.0 | 0.013 | 1.79 | –\n8 | 0.053 | 17.6 | 0.079 | 12.77 | –\n1 | 0.308 | 102.3 | 0.459 | 100.58 | –\n\nQwen 2.5 14B | ∞ | 0.018 | 6.0 | 0.027 | 3.45 | –\n8 | 0.058 | 19.3 | 0.086 | 13.02 | –\n1 | 0.387 | 128.5 | 0.577 | 100.64 | –\n\nQwen 1.5 MoE (2.7BA, 14BT) | ∞ | 0.01 | 3.3 | 0.015 | 2.64 | –\n8 | 0.043 | 14.3 | 0.064 | 13.11 | –\n1 | 0.165 | 54.8 | 0.246 | 100.68 | –\n\n\nOLMo 1 1B | ∞ | 0.004 | 1.3 | 0.006 | 0.99 | 18.2 bil.\n8 | 0.038 | 12.6 | 0.057 | 12.63 | 1.91 bil.\n1 | 0.165 | 54.8 | 0.246 | 100.58 | 441 mil.\n\nOLMo 0724 7B | ∞ | 0.017 | 5.6 | 0.025 | 3.33 | 29.8 bil.\n8 | 0.052 | 17.3 | 0.077 | 12.77 | 9.73 bil.\n1 | 0.339 | 112.5 | 0.505 | 100.59 | 1.49 bil.\n\nOLMo 2 7B | ∞ | 0.018 | 6.0 | 0.027 | 3.68 | 20.9 bil.\n8 | 0.049 | 16.3 | 0.073 | 12.88 | 7.68 bil.\n1 | 0.358 | 118.9 | 0.533 | 100.54 | 1.05 bil.\n\nOLMo 2 13B | ∞ | 0.033 | 11.0 | 0.049 | 6.6 | 22.1 bil.\n8 | 0.057 | 18.9 | 0.085 | 13.05 | 12.8 bil.\n1 | 0.386 | 128.2 | 0.575 | 100.57 | 1.89 bil.\n\nOLMoE 0924 (1BA, 7BT) | ∞ | 0.006 | 2.0 | 0.009 | 1.7 | 21.7 bil.\n8 | 0.037 | 12.3 | 0.055 | 12.82 | 3.51 bil.\n1 | 0.151 | 50.1 | 0.225 | 100.6 | 861 mil.", "doc_id": "morrison2025", "page": 16, "url": "https://arxiv.org/pdf/2503.05804", "embedded_text": "Table 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served on machines from the same cluster that our models were trained on, so we use the same WUE and PUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO2e / kWh. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. We do not report “break-even” points for Qwen models since the training costs are not public.\nRequest freq. (req / s) | GPU Power Usage (kWh) | Carbon Emissions (g CO2eq) | Water consump. (L) | Seconds per 100 req. | # Inf. for CO2 equiv. w/ training\n\nLlama 3.2 1B | ∞ | 0.003 | 1.0 | 0.004 | 1.38 | 258 bil.\n8 | 0.036 | 12.0 | 0.054 | 12.64 | 21.5 bil.\n1 | 0.16 | 53.1 | 0.238 | 100.58 | 4.83 bil.\n\nLlama 2 7B | ∞ | 0.019 | 6.3 | 0.028 | 3.58 | 11.9 bil.\n8 | 0.054 | 17.9 | 0.08 | 12.83 | 4.18 bil.\n1 | 0.349 | 115.9 | 0.52 | 100.62 | 647 mil.\n\nLlama 3 8B | ∞ | 0.01 | 3.3 | 0.015 | 1.93 | 282 bil.\n8 | 0.052 | 17.3 | 0.077 | 12.78 | 54.2 bil.\n1 | 0.337 | 111.9 | 0.502 | 100.63 | 8.37 bil.\n\nLlama 3.1 8B | ∞ | 0.011 | 3.7 | 0.016 | 2.13 | 276 bil.\n8 | 0.051 | 16.9 | 0.076 | 12.79 | 59.5 bil.\n1 | 0.333 | 110.6 | 0.496 | 100.64 | 9.12 bil.\n\nLlama 2 13B | ∞ | 0.034 | 11.3 | 0.051 | 6.53 | 13.3 bil.\n8 | 0.06 | 19.9 | 0.089 | 13.09 | 7.52 bil.\n1 | 0.401 | 133.1 | 0.597 | 100.73 | 1.13 bil.\n\n\nQwen 2.5 1.5B | ∞ | 0.003 | 1.0 | 0.004 | 0.86 | –\n8 | 0.033 | 11.0 | 0.049 | 12.65 | –\n1 | 0.163 | 54.1 | 0.243 | 100.57 | –\n\nQwen 2.5 7B | ∞ | 0.009 | 3.0 | 0.013 | 1.79 | –\n8 | 0.053 | 17.6 | 0.079 | 12.77 | –\n1 | 0.308 | 102.3 | 0.459 | 100.58 | –\n\nQwen 2.5 14B | ∞ | 0.018 | 6.0 | 0.027 | 3.45 | –\n8 | 0.058 | 19.3 | 0.086 | 13.02 | –\n1 | 0.387 | 128.5 | 0.577 | 100.64 | –\n\nQwen 1.5 MoE (2.7BA, 14BT) | ∞ | 0.01 | 3.3 | 0.015 | 2.64 | –\n8 | 0.043 | 14.3 | 0.064 | 13.11 | –\n1 | 0.165 | 54.8 | 0.246 | 100.68 | –\n\n\nOLMo 1 1B | ∞ | 0.004 | 1.3 | 0.006 | 0.99 | 18.2 bil.\n8 | 0.038 | 12.6 | 0.057 | 12.63 | 1.91 bil.\n1 | 0.165 | 54.8 | 0.246 | 100.58 | 441 mil.\n\nOLMo 0724 7B | ∞ | 0.017 | 5.6 | 0.025 | 3.33 | 29.8 bil.\n8 | 0.052 | 17.3 | 0.077 | 12.77 | 9.73 bil.\n1 | 0.339 | 112.5 | 0.505 | 100.59 | 1.49 bil.\n\nOLMo 2 7B | ∞ | 0.018 | 6.0 | 0.027 | 3.68 | 20.9 bil.\n8 | 0.049 | 16.3 | 0.073 | 12.88 | 7.68 bil.\n1 | 0.358 | 118.9 | 0.533 | 100.54 | 1.05 bil.\n\nOLMo 2 13B | ∞ | 0.033 | 11.0 | 0.049 | 6.6 | 22.1 bil.\n8 | 0.057 | 18.9 | 0.085 | 13.05 | 12.8 bil.\n1 | 0.386 | 128.2 | 0.575 | 100.57 | 1.89 bil.\n\nOLMoE 0924 (1BA, 7BT) | ∞ | 0.006 | 2.0 | 0.009 | 1.7 | 21.7 bil.\n8 | 0.037 | 12.3 | 0.055 | 12.82 | 3.51 bil.\n1 | 0.151 | 50.1 | 0.225 | 100.6 | 861 mil.", "id": 1312}
{"type": "section", "content": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\nALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France\nRAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho\n\nAs the possibilities for Artificial Intelligence (AI) have grown, so have concerns regarding its impacts on society and the environment. However, these issues are often raised separately; i.e. carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment. On the other hand, model audits that aim to evaluate model performance and disparate impacts mostly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In this separation, both research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential solutions for making informed choices that can be developed at their convergence. In this essay, we build upon work carried out in AI and in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such as generalizability, transparency, evaluation and equity across AI research and practice. We argue that the efforts aiming to study AI’s ethical ramifications should be made in tandem with those evaluating its impacts on the environment, and we conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and practice.", "doc_id": "luccioni2025b", "page": 1, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\nALEXANDRA SASHA LUCCIONI and GIADA PISTILLI, Hugging Face, Canada/France\nRAESETJE SEFALA and NYALLENG MOOROSI, Distributed AI Research Institute, Canada/Lesotho\n\nAs the possibilities for Artificial Intelligence (AI) have grown, so have concerns regarding its impacts on society and the environment. However, these issues are often raised separately; i.e. carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment. On the other hand, model audits that aim to evaluate model performance and disparate impacts mostly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In this separation, both research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential solutions for making informed choices that can be developed at their convergence. In this essay, we build upon work carried out in AI and in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such as generalizability, transparency, evaluation and equity across AI research and practice. We argue that the efforts aiming to study AI’s ethical ramifications should be made in tandem with those evaluating its impacts on the environment, and we conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and practice.", "original_types": ["text", "header"], "id": 1313}
{"type": "section", "content": "2 KEY CONCEPTS AND DEFINITIONS\n\nSustainability\nWhen it comes to the concept of sustainability, one of its first and most widely-accepted definitions originates from the 1987 Brundtland Report, which defines sustainable development as “development that meets the needs of the present without compromising the ability of future generations to meet their own needs” [1987][p.41]. This definition remains central to the field of sustainability write large, informing frameworks such as the UN Sustainable Development Goals (SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [199]. However, also in 1987, environmental economist Edward Barbier proposed an alternative definition to sustainability, structuring it around three pillars: environmental, societal and economic, arguing that sustainable development can only be truly achieved when both environmental stewardship, social equity and economic viability coexist and are inter-connected [1987].\nIn the context of AI, the term ‘sustainability’ is most often used to refer solely to environmental sustainability [41, 61]. The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203]. This proposal would then encompass the vast variety of work being done at the nexus of machine learning and fields such as biodiversity monitoring, agriculture, transportation, etc. (for a review, see [206] and [168]). AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable AI Workshop, that have put the emphasis on efficient methods and the application of AI to sustainability-related problems, as well as the Tackling Climate Change with Machine Learning workshop, which aims to demonstrate that AI can be an invaluable tool in helping society adapt to and mitigate the effects of climate change.\n\nEthics\nInherently characterized by ongoing perplexity, ethics aspires for certainty and consensus, yet also remains dynamic and evolving. It has its origins in the work of philosophers such as Aristotle, who posited that ethics connects theory with praxis, with the goal of guiding human actions towards eudaimonia (i.e. the highest human good) [8]. In the millennia since Aristotle, the philosophical sub-domain of applied ethics has sought to establish normative principles for a variety of human activities and domains, which inexorably depend on the context of application and the individuals", "doc_id": "luccioni2025b", "page": 2, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "2 KEY CONCEPTS AND DEFINITIONS\n\nSustainability\nWhen it comes to the concept of sustainability, one of its first and most widely-accepted definitions originates from the 1987 Brundtland Report, which defines sustainable development as “development that meets the needs of the present without compromising the ability of future generations to meet their own needs” [1987][p.41]. This definition remains central to the field of sustainability write large, informing frameworks such as the UN Sustainable Development Goals (SDGs), which were developed in 2015 as a blueprint for achieving peace and prosperity for people and the planet [199]. However, also in 1987, environmental economist Edward Barbier proposed an alternative definition to sustainability, structuring it around three pillars: environmental, societal and economic, arguing that sustainable development can only be truly achieved when both environmental stewardship, social equity and economic viability coexist and are inter-connected [1987].\nIn the context of AI, the term ‘sustainability’ is most often used to refer solely to environmental sustainability [41, 61]. The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203]. This proposal would then encompass the vast variety of work being done at the nexus of machine learning and fields such as biodiversity monitoring, agriculture, transportation, etc. (for a review, see [206] and [168]). AI and sustainability has also been central to workshops such as SustaiNLP and the International Sustainable AI Workshop, that have put the emphasis on efficient methods and the application of AI to sustainability-related problems, as well as the Tackling Climate Change with Machine Learning workshop, which aims to demonstrate that AI can be an invaluable tool in helping society adapt to and mitigate the effects of climate change.\n\nEthics\nInherently characterized by ongoing perplexity, ethics aspires for certainty and consensus, yet also remains dynamic and evolving. It has its origins in the work of philosophers such as Aristotle, who posited that ethics connects theory with praxis, with the goal of guiding human actions towards eudaimonia (i.e. the highest human good) [8]. In the millennia since Aristotle, the philosophical sub-domain of applied ethics has sought to establish normative principles for a variety of human activities and domains, which inexorably depend on the context of application and the individuals", "original_types": ["text", "header"], "id": 1314}
{"type": "section", "content": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\ninvolved, leading to much debate regarding which norms should be applicable in which contexts, as well as the definition of key ethical concepts such as fairness [89, 175], transparency [60, 147] and, indeed, the very definition of ethics itself [21, 195].\n\nLacking consensus, the field of AI ethics often applies Western moral theories ranging from utilitarianism [18, 134] to egalitarianism [209] and virtue ethics [7, 8] to propose ways of assessing the ethicality of AI systems. However, the application of these moral theories faces challenges given the difficulty of, e.g. quantifying the concept of utility in utilitarianism, assessing and comparing who is worse off in egalitarianism, or evaluating cultural variability in defining virtues in virtue ethics. This is also the case in terms of the application of these theories in modern-day contexts involving new types of AI-driven technologies such as robots or autonomous vehicles, which can be limited without a comprehensive understanding of both AI’s technical capabilities (e.g. the limitations of the underlying models) as well as the diversity of life experiences of the people using these tools, who can interact with them in ways that are hard to predict or design for [102]. Given that these concepts encompass work from a multitude of domains that espouse different objectives, values and methods [23], their definition can have major consequences on the way in which these concepts are operationalized in AI models and systems [10, 65, 183]. Work that addresses the ethical aspects of AI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and Transparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a cross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.", "doc_id": "luccioni2025b", "page": 3, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\ninvolved, leading to much debate regarding which norms should be applicable in which contexts, as well as the definition of key ethical concepts such as fairness [89, 175], transparency [60, 147] and, indeed, the very definition of ethics itself [21, 195].\n\nLacking consensus, the field of AI ethics often applies Western moral theories ranging from utilitarianism [18, 134] to egalitarianism [209] and virtue ethics [7, 8] to propose ways of assessing the ethicality of AI systems. However, the application of these moral theories faces challenges given the difficulty of, e.g. quantifying the concept of utility in utilitarianism, assessing and comparing who is worse off in egalitarianism, or evaluating cultural variability in defining virtues in virtue ethics. This is also the case in terms of the application of these theories in modern-day contexts involving new types of AI-driven technologies such as robots or autonomous vehicles, which can be limited without a comprehensive understanding of both AI’s technical capabilities (e.g. the limitations of the underlying models) as well as the diversity of life experiences of the people using these tools, who can interact with them in ways that are hard to predict or design for [102]. Given that these concepts encompass work from a multitude of domains that espouse different objectives, values and methods [23], their definition can have major consequences on the way in which these concepts are operationalized in AI models and systems [10, 65, 183]. Work that addresses the ethical aspects of AI systems is discussed and published in conferences such as the ACM Conference on Fairness, Accountability, and Transparency (FAccT), as well as the AAAI/ACM Conference on AI, Ethics, and Society (AIES), which both have a cross-disciplinary focus and cover a multitude of topics in terms of the societal and ethical aspects of AI.", "original_types": ["text", "header"], "id": 1315}
{"type": "section", "content": "3.2 Research and Practice\n\nGiven that AI is a distributed field consisting of a multitude of practitioners and organizations, the practical application of the principles and frameworks described in the previous section can differ immensely. In a 2022 study of papers submitted to conferences such as ICML and NeurIPS, Birhane et al. analyzed the values that were highlighted by their authors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered explicitly [2022a]. From the 59 values they identified, the most emphasis was put on aspects such as technical progress, quantitative evidence, and novelty, whereas ethical considerations around values such as beneficence, interpretability and respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a fraction of papers. Also, not a single one of the values Birhane et al. identified was explicitly linked to environmental sustainability, highlighting once again the lack of connection in the research community with sustainability writ large.", "doc_id": "luccioni2025b", "page": 4, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "3.2 Research and Practice\n\nGiven that AI is a distributed field consisting of a multitude of practitioners and organizations, the practical application of the principles and frameworks described in the previous section can differ immensely. In a 2022 study of papers submitted to conferences such as ICML and NeurIPS, Birhane et al. analyzed the values that were highlighted by their authors – i.e. the positive attributes of their project that they emphasized and the negative impacts they considered explicitly [2022a]. From the 59 values they identified, the most emphasis was put on aspects such as technical progress, quantitative evidence, and novelty, whereas ethical considerations around values such as beneficence, interpretability and respect for privacy (which are core to many AI principles and frameworks cited above) were present only in a fraction of papers. Also, not a single one of the values Birhane et al. identified was explicitly linked to environmental sustainability, highlighting once again the lack of connection in the research community with sustainability writ large.", "original_types": ["text", "header"], "id": 1316}
{"type": "section", "content": "3.3 Governance and Regulation", "doc_id": "luccioni2025b", "page": 5, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "3.3 Governance and Regulation", "original_types": ["header"], "id": 1317}
{"type": "section", "content": "Governance and regulation aim to establish mechanisms for decision-making, guiding the development and deployment of AI systems, and outlining the roles and responsibilities of each party involved [219]. There are many distributed efforts for governance in AI whose aim is to develop ethical guardrails, with some highlighting the importance of international institutions [85], and others focusing on the public sector [101] – reflecting that both bottom-up and top-down endeavors are useful to establish functional mechanisms for governing AI. If we take a look at recent community endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that established mechanisms for various ethical aspects of the project such as data governance, quality metrics, and fostering stakeholder collaboration and transparency [92], as well as drafting a consensus-driven ethical framework for governing the resulting artifacts that encompasses both legal and technical dimensions [154]. Interestingly, Big Science was one of the few projects that also considered and documented the carbon footprint of model training, evaluation and deployment, proposing a holistic, life cycle approach to estimating emissions [121]. There have also been proposals arguing for putting sustainability in the center of AI development and deployment [203], as well as frameworks for certifying the sustainability of AI systems [25], across all the different pillars of sustainability (i.e. social, environmental and economic) [70]. However, we are still at the beginning of building governance mechanisms that offer a more comprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability. \n\nA pivotal example of top-down governance is the European Union’s AI Act, which draws from broadly defined ethical principles to inform its regulations [39]. In fact, the text of the Act demonstrates considerable progress following the recent European dialogues, seeking to regulate AI applications that may infringe on human rights, adhering, among others, to the ethical principles of human oversight, human agency and transparency. Moreover, the AI Act’s foundation on the premise that risk equates to potential human rights harms [152] echoes one of the longstanding traditions in AI ethics of prioritizing human rights [9, 216]. This approach embodies the ethical commitment to safeguard fundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these important principles. However, while both EU AI Act [2022], as well as similar regulatory initiatives in China [2023] and Canada [2023] point to the need to protect both fundamental human rights as well as to limit damage to environment, there are no official provisions regarding sustainability in any of their texts, and it remains to be seen how existing standards for environmental impacts in all of these jurisdictions will apply to AI systems. Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.", "doc_id": "luccioni2025b", "page": 5, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Governance and regulation aim to establish mechanisms for decision-making, guiding the development and deployment of AI systems, and outlining the roles and responsibilities of each party involved [219]. There are many distributed efforts for governance in AI whose aim is to develop ethical guardrails, with some highlighting the importance of international institutions [85], and others focusing on the public sector [101] – reflecting that both bottom-up and top-down endeavors are useful to establish functional mechanisms for governing AI. If we take a look at recent community endeavors for AI governance, the 2022 Big Science workshop proposed a bottom-up approach that established mechanisms for various ethical aspects of the project such as data governance, quality metrics, and fostering stakeholder collaboration and transparency [92], as well as drafting a consensus-driven ethical framework for governing the resulting artifacts that encompasses both legal and technical dimensions [154]. Interestingly, Big Science was one of the few projects that also considered and documented the carbon footprint of model training, evaluation and deployment, proposing a holistic, life cycle approach to estimating emissions [121]. There have also been proposals arguing for putting sustainability in the center of AI development and deployment [203], as well as frameworks for certifying the sustainability of AI systems [25], across all the different pillars of sustainability (i.e. social, environmental and economic) [70]. However, we are still at the beginning of building governance mechanisms that offer a more comprehensive analysis of the impacts of AI systems from the perspective of ethics and sustainability. \n\nA pivotal example of top-down governance is the European Union’s AI Act, which draws from broadly defined ethical principles to inform its regulations [39]. In fact, the text of the Act demonstrates considerable progress following the recent European dialogues, seeking to regulate AI applications that may infringe on human rights, adhering, among others, to the ethical principles of human oversight, human agency and transparency. Moreover, the AI Act’s foundation on the premise that risk equates to potential human rights harms [152] echoes one of the longstanding traditions in AI ethics of prioritizing human rights [9, 216]. This approach embodies the ethical commitment to safeguard fundamental freedoms and human rights in the digital era, ensuring that AI technologies do not infringe upon these important principles. However, while both EU AI Act [2022], as well as similar regulatory initiatives in China [2023] and Canada [2023] point to the need to protect both fundamental human rights as well as to limit damage to environment, there are no official provisions regarding sustainability in any of their texts, and it remains to be seen how existing standards for environmental impacts in all of these jurisdictions will apply to AI systems. Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention AI’s greenhouse gas emissions nor energy usage, as well as multi-nation declarations such as the Bletchley Declaration [2023], illustrating the disconnect between sustainability and ethics in recent approaches to AI regulation.", "id": 1318}
{"type": "section", "content": "4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY\n\nIs it fair … that the residents of the Maldives (likely to be underwater by 2100) or the 800,000 people in Sudan affected by drastic floods pay the environmental price of training and deploying ever larger English LMs, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic?\n\nBender, Gebru et al. [2021]\n\nIn the current section, we define four recurring issues that we have found to be particularly salient to discussions around both AI ethics and sustainability. These issues are inspired by previous carried out by critical scholars such as Dobbe and Whittaker [2019], Birhane [2022b], van Wynsberghe [2021] as well as Bender and Gebru [2021], as cited above. We start, in Section 4.1 with a discussion of the perils of assumptions of the generalizability of data and models in both ethics and sustainability. We follow, in Section 4.2 with a reflection upon the current state of evaluation of AI systems, what is measured, what is missing, and why that matters. Next, we remark upon the current lack of transparency with regards to information relevant to both ethics and sustainabilityin Section 4.3. Finally, in Section 4.4, we discuss the balance of power and the allocation of justice, and how existing inequalities can be further amplified by AI systems.\n\n4.1 Generalizability\n\nAI technologies function based on assumptions of generalizability and representativeness – i.e. that given sufficient data, an AI model can learn to accurately represent (any) given process and even adapt to previously unseen data [74]. For instance, the concept of pre-training AI models on large datasets such as ImageNet [47] dates back to the early 1990s [178] and has since become the dominant training paradigm in both computer vision [162, 193] and natural language processing [50, 114]. In fact, pre-training is heavily dependent upon the assumption of representativeness - i.e. that the huge amounts of training data used for pre-training represent the world as a whole, or at least a sufficient part of it to be useful for any kind of downstream application (i.e. fine-tuning, transfer learning, etc). While the limitations of such claims for both AI models and datasets have been previously shown (see Chasalow and Levy [36], Koch et al. [98], Raji et al. [157], Smith et al. [188]), the theory of generalizability, and the perception of certain types of AI models, ie. LLMs, as “general purpose technologies” continues to persist [57]. This can come with both ethical and environmental ramifications when systems trained under the pretense of generalizability are applied in contexts that differ from the ones represented in their training data – we discuss some of these below.", "doc_id": "luccioni2025b", "page": 6, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "4 TRANSVERSAL ISSUES IN AI ETHICS AND SUSTAINABILITY\n\nIs it fair … that the residents of the Maldives (likely to be underwater by 2100) or the 800,000 people in Sudan affected by drastic floods pay the environmental price of training and deploying ever larger English LMs, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic?\n\nBender, Gebru et al. [2021]\n\nIn the current section, we define four recurring issues that we have found to be particularly salient to discussions around both AI ethics and sustainability. These issues are inspired by previous carried out by critical scholars such as Dobbe and Whittaker [2019], Birhane [2022b], van Wynsberghe [2021] as well as Bender and Gebru [2021], as cited above. We start, in Section 4.1 with a discussion of the perils of assumptions of the generalizability of data and models in both ethics and sustainability. We follow, in Section 4.2 with a reflection upon the current state of evaluation of AI systems, what is measured, what is missing, and why that matters. Next, we remark upon the current lack of transparency with regards to information relevant to both ethics and sustainabilityin Section 4.3. Finally, in Section 4.4, we discuss the balance of power and the allocation of justice, and how existing inequalities can be further amplified by AI systems.\n\n4.1 Generalizability\n\nAI technologies function based on assumptions of generalizability and representativeness – i.e. that given sufficient data, an AI model can learn to accurately represent (any) given process and even adapt to previously unseen data [74]. For instance, the concept of pre-training AI models on large datasets such as ImageNet [47] dates back to the early 1990s [178] and has since become the dominant training paradigm in both computer vision [162, 193] and natural language processing [50, 114]. In fact, pre-training is heavily dependent upon the assumption of representativeness - i.e. that the huge amounts of training data used for pre-training represent the world as a whole, or at least a sufficient part of it to be useful for any kind of downstream application (i.e. fine-tuning, transfer learning, etc). While the limitations of such claims for both AI models and datasets have been previously shown (see Chasalow and Levy [36], Koch et al. [98], Raji et al. [157], Smith et al. [188]), the theory of generalizability, and the perception of certain types of AI models, ie. LLMs, as “general purpose technologies” continues to persist [57]. This can come with both ethical and environmental ramifications when systems trained under the pretense of generalizability are applied in contexts that differ from the ones represented in their training data – we discuss some of these below.", "original_types": ["text", "header"], "id": 1319}
{"type": "section", "content": "Given the data that fuels AI models is produced by humans, it is intrinsically laden with subjective judgments [132] and representative of specific worldviews [46]. Numerous studies have shown that both the data used for training AI models [54, 157, 166] and the models themselves [16, 73, 213] are not, in fact, representative of the world at large and that the biases contained in training data persist even if further fine-tuning is carried out [103], which can have ‘cascade’ effects when models are deployed in production [173], which can contribute to perpetuating negative biases [68]. Similarly, off-the-shelf, proprietary technologies that are marketed as generic can fail at the tasks when applied in differing contexts, e.g. in applications such as facial recognition (when they fail to recognize people from populations under-represented in training datasets [33]) and crime prediction (where they have dismal accuracy rates across different locations [174]) – and yet, out-of-the-box AI systems for these tasks and many others continue to be built and deployed under the assumptions that they will work no matter the context of their application. This can have devastating effects on already marginalized communities when applied for tasks such as criminal sentencing [6], and facial recognition [33].", "doc_id": "luccioni2025b", "page": 6, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Given the data that fuels AI models is produced by humans, it is intrinsically laden with subjective judgments [132] and representative of specific worldviews [46]. Numerous studies have shown that both the data used for training AI models [54, 157, 166] and the models themselves [16, 73, 213] are not, in fact, representative of the world at large and that the biases contained in training data persist even if further fine-tuning is carried out [103], which can have ‘cascade’ effects when models are deployed in production [173], which can contribute to perpetuating negative biases [68]. Similarly, off-the-shelf, proprietary technologies that are marketed as generic can fail at the tasks when applied in differing contexts, e.g. in applications such as facial recognition (when they fail to recognize people from populations under-represented in training datasets [33]) and crime prediction (where they have dismal accuracy rates across different locations [174]) – and yet, out-of-the-box AI systems for these tasks and many others continue to be built and deployed under the assumptions that they will work no matter the context of their application. This can have devastating effects on already marginalized communities when applied for tasks such as criminal sentencing [6], and facial recognition [33].", "original_types": ["text"], "id": 1320}
{"type": "section", "content": "4.2 Evaluation\n\nA popular adage states that “you can’t improve what you don’t measure” 4; in the context of AI systems, this can be translated into the fact that the criteria that we use to evaluate AI systems and the way in which this evaluation are carried out are important – i.e. the metrics we choose help us embed our values as communities about outcomes we wish to see and those that we put less emphasis on [136]. While leaderboards such as Papers With Code tend to only measure performance-based metrics such as accuracy or precision, factoring in other metrics can make comparisons between different models more meaningful and actionable. This is due to the fact that real-world constraints on model deployment often result in trade-offs being made between different factors that include accuracy and efficiency [31], but also robustness [217], inclusion [88] and data quality [11]. This means that in order to meaningfully assess the utility of AI systems in different practical contexts, other measures must be considered in parallel to performance and accuracy [122]; and often the best people to do the assessments for the trade-offs are the populations who will use these tools. For example, when it comes to the evaluation of generative technologies such as large language models, these do not have a single well-established evaluation approach [139, 159]. Approaches that are used for evaluating their ethical limitations include red-teaming [66], external audits [139, 158] as well as more holistic model evaluations that reflect different aspects of model performance [26, 67]. However, critiques of the approaches described above also include their non-inclusivity of marginalized communities [48] as well as a lack of formalized approaches and standards for model evaluation, making apples-to-apples comparisons between models difficult [40]. Also, as many of the most widely deployed AI systems are currently proprietary and direct access to models is not possible, it is hard to exhaustively evaluate many popular commercial models for any meaningful evaluation to take place [118, 190].", "doc_id": "luccioni2025b", "page": 7, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "4.2 Evaluation\n\nA popular adage states that “you can’t improve what you don’t measure” 4; in the context of AI systems, this can be translated into the fact that the criteria that we use to evaluate AI systems and the way in which this evaluation are carried out are important – i.e. the metrics we choose help us embed our values as communities about outcomes we wish to see and those that we put less emphasis on [136]. While leaderboards such as Papers With Code tend to only measure performance-based metrics such as accuracy or precision, factoring in other metrics can make comparisons between different models more meaningful and actionable. This is due to the fact that real-world constraints on model deployment often result in trade-offs being made between different factors that include accuracy and efficiency [31], but also robustness [217], inclusion [88] and data quality [11]. This means that in order to meaningfully assess the utility of AI systems in different practical contexts, other measures must be considered in parallel to performance and accuracy [122]; and often the best people to do the assessments for the trade-offs are the populations who will use these tools. For example, when it comes to the evaluation of generative technologies such as large language models, these do not have a single well-established evaluation approach [139, 159]. Approaches that are used for evaluating their ethical limitations include red-teaming [66], external audits [139, 158] as well as more holistic model evaluations that reflect different aspects of model performance [26, 67]. However, critiques of the approaches described above also include their non-inclusivity of marginalized communities [48] as well as a lack of formalized approaches and standards for model evaluation, making apples-to-apples comparisons between models difficult [40]. Also, as many of the most widely deployed AI systems are currently proprietary and direct access to models is not possible, it is hard to exhaustively evaluate many popular commercial models for any meaningful evaluation to take place [118, 190].", "original_types": ["text", "header"], "id": 1321}
{"type": "section", "content": "4.3 Transparency\n\nTransparency is widely recognized as a fundamental principle in science in general and AI in particular [63, 107, 208, 212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based systems are not inherently transparent or interpretable, given the complexity of the neural network architectures they espouse and the number of parameters they contain [105, 113, 140]. Efforts such as interpretability approaches are useful and can help interpret the predictions of models posthoc [104, 164], whereas artifacts such as data sheets and model cards [69, 137] can contribute towards making AI systems more understandable for users, providing essential information about AI models in a user-friendly format. These artifacts allow users to understand not just how an AI system functions, but also its limitations, potential biases, implications, and environmental impacts. However, even though model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face) and provide important information about models, they are not sufficient to guarantee, for instance, the reproducibility of reported results, which is a core tenet of scientific practice [146].\nIndeed, several studies of transparency found that an overwhelming amount of results published at technical AI conferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation highlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility of AI models and their findings. Such an approach would acknowledge the connection between transparency and reproducibility: transparent research practices enable reproducibility, which in turn facilitates independent scrutiny, validation, further development of research findings by other scientists [80]. The absence of transparency, especially in sharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing AI models’ scientific impact and impeding their adoption within the wider scientific community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental impacts of AI models and systems, with most work in this field being done post-hoc by researchers who did not do the initial model training and deployment (e.g. [116, 192]). The most common environmental sustainability metric for AI", "doc_id": "luccioni2025b", "page": 8, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "4.3 Transparency\n\nTransparency is widely recognized as a fundamental principle in science in general and AI in particular [63, 107, 208, 212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based systems are not inherently transparent or interpretable, given the complexity of the neural network architectures they espouse and the number of parameters they contain [105, 113, 140]. Efforts such as interpretability approaches are useful and can help interpret the predictions of models posthoc [104, 164], whereas artifacts such as data sheets and model cards [69, 137] can contribute towards making AI systems more understandable for users, providing essential information about AI models in a user-friendly format. These artifacts allow users to understand not just how an AI system functions, but also its limitations, potential biases, implications, and environmental impacts. However, even though model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face) and provide important information about models, they are not sufficient to guarantee, for instance, the reproducibility of reported results, which is a core tenet of scientific practice [146].\nIndeed, several studies of transparency found that an overwhelming amount of results published at technical AI conferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation highlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility of AI models and their findings. Such an approach would acknowledge the connection between transparency and reproducibility: transparent research practices enable reproducibility, which in turn facilitates independent scrutiny, validation, further development of research findings by other scientists [80]. The absence of transparency, especially in sharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing AI models’ scientific impact and impeding their adoption within the wider scientific community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental impacts of AI models and systems, with most work in this field being done post-hoc by researchers who did not do the initial model training and deployment (e.g. [116, 192]). The most common environmental sustainability metric for AI", "original_types": ["text", "header"], "id": 1322}
{"type": "section", "content": "4.4 Power and Equity\n\nModern AI research and practice are not equitable by design: their cost in terms of computer hardware as well as human skills means that only a small percentage of both academic and industrial organizations can contribute to many aspects of model development. With the recent advent of AI models of ever-increasing scale and complexity, the digital divide in AI is only increasing, as it takes more compute and human skill to train and deploy AI models and systems [1, 19, 35, 118]. This means that the future wide-sweeping benefits that AI technologies are promised to have for humanity as a whole [149] are contingent upon access to technologies that are fundamentally unequally distributed. Despite explicit proposals to pursue more equitable and explicitly de-colonialist approaches [124, 138], the ‘bigger-is-better’ paradigm continues to be central to AI research and practice [204]. In a similar fashion, the places where AI research is being carried out are also skewed towards institutions from a handful of countries mostly located in the Global North [2], which inexorably impacts the choices made during the AI development and deployment process, introducing many biases (which we have already addressed in previous sections). In fact, recent work has proposed that the very pursuit of sustainable AI has the opposite effect, contributing towards maintaining the status quo and “securing the dominant socio-economic interests of neo-liberal capitalism” [180]. For instance, major technological corporations have dedicated significant resources towards solutions such as improving the efficiency of their data centers, proposing numerous initiatives towards technological sustainability [4, 75], including research at the nexus of AI and the climate [44, 128]. However, both Microsoft and Google announced that they would miss their 2024 sustainability targets, due in part to the energy demands of the AI tools that they have been developing and deploying [130, 161]. The impacts of these computation-intensive data centers can further be expanded to include the mining of rare metals and the disposal of e-waste, both of which predominantly affect countries from the Global South which profiting technology companies from the Global North [86, 194].\n\nSimilarly to AI, issues of power, equity and justice are also central in the field of sustainability, since climate change is an inherently inequitable phenomenon – with a handful of countries and regions in North America, East Asia and Europe responsible for a disproportionate portion of global emissions, while the impacts of sea level rise and extreme weather events being felt most strongly in countries with very minimal carbon footprints, raising questions of equity and justice and how to address them [49, 126, 151, 177]. Similarly, the majority of climate-focused AI solutions overlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who", "doc_id": "luccioni2025b", "page": 9, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "4.4 Power and Equity\n\nModern AI research and practice are not equitable by design: their cost in terms of computer hardware as well as human skills means that only a small percentage of both academic and industrial organizations can contribute to many aspects of model development. With the recent advent of AI models of ever-increasing scale and complexity, the digital divide in AI is only increasing, as it takes more compute and human skill to train and deploy AI models and systems [1, 19, 35, 118]. This means that the future wide-sweeping benefits that AI technologies are promised to have for humanity as a whole [149] are contingent upon access to technologies that are fundamentally unequally distributed. Despite explicit proposals to pursue more equitable and explicitly de-colonialist approaches [124, 138], the ‘bigger-is-better’ paradigm continues to be central to AI research and practice [204]. In a similar fashion, the places where AI research is being carried out are also skewed towards institutions from a handful of countries mostly located in the Global North [2], which inexorably impacts the choices made during the AI development and deployment process, introducing many biases (which we have already addressed in previous sections). In fact, recent work has proposed that the very pursuit of sustainable AI has the opposite effect, contributing towards maintaining the status quo and “securing the dominant socio-economic interests of neo-liberal capitalism” [180]. For instance, major technological corporations have dedicated significant resources towards solutions such as improving the efficiency of their data centers, proposing numerous initiatives towards technological sustainability [4, 75], including research at the nexus of AI and the climate [44, 128]. However, both Microsoft and Google announced that they would miss their 2024 sustainability targets, due in part to the energy demands of the AI tools that they have been developing and deploying [130, 161]. The impacts of these computation-intensive data centers can further be expanded to include the mining of rare metals and the disposal of e-waste, both of which predominantly affect countries from the Global South which profiting technology companies from the Global North [86, 194].\n\nSimilarly to AI, issues of power, equity and justice are also central in the field of sustainability, since climate change is an inherently inequitable phenomenon – with a handful of countries and regions in North America, East Asia and Europe responsible for a disproportionate portion of global emissions, while the impacts of sea level rise and extreme weather events being felt most strongly in countries with very minimal carbon footprints, raising questions of equity and justice and how to address them [49, 126, 151, 177]. Similarly, the majority of climate-focused AI solutions overlook issues of justice and power, focusing predominantly on the climate-positive aspects of technologies and not who", "original_types": ["text", "header"], "id": 1323}
{"type": "section", "content": "Section Title (e.g., 1. Introduction)\n\nFull text of the paragraph...", "doc_id": "luccioni2025b", "page": 10, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Section Title (e.g., 1. Introduction)\n\nFull text of the paragraph...", "original_types": ["text", "header"], "id": 1324}
{"type": "table", "content": "Table 1: Title...\nMarkdown representation of the table", "doc_id": "luccioni2025b", "page": 10, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Table 1: Title...\nMarkdown representation of the table", "id": 1325}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "luccioni2025b", "page": 10, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Figure 1: Title...", "id": 1326}
{"type": "section", "content": "Generalizability\n\nGiven the observed disconnect between ethics and sustainability in the context of AI principles and frameworks, we find that, while these offer a valuable starting point, they often fall short in addressing AI’s complex ethical issues due to their lack of contextual sensitivity [142]. We also believe that improving upon this necessitates a more nuanced and context-specific approach to AI ethics, one that embraces the varied ethical dimensions presented by AI, including its environmental implications. Current ethical charters in AI, often detached from this perplexity, represent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [5]. By recognizing these limitations, we intend not to discard these definitions and principles but to improve upon them. In this context, environmental and sustainability challenges related to AI development and deployment are integral to the broader ethical reflections within the field. This integration between ethics and sustainability in AI calls for a holistic approach, where ethical considerations are not viewed in isolation but are intrinsically linked with environmental and sustainability oversight. For instance, shifting from the dominant Western moral philosophies to include perspectives from non-Western traditions such as relational ethics [131], Ubuntu ethics [145], and Confucian ethics [111] can offer valuable insights into community, social harmony, and the interconnectedness of beings, emphasizing the impact of AI on society and interpersonal relationships.\n\nEvaluation\n\nThere is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable. Recent work has begun bridging the gap; for instance, work by Lynch et al. is inspired by the concept of urgent governance in environmental studies, which distinguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure and technologies [123]. Raji et al. use a similar approach for their proposed end-to-end framework for internal algorithmic auditing of AI models, which takes into account both technical and ethical assessments [159]. In a similar vein, a recent model evaluation framework by Rakova et al., proposes an environmental justice-oriented lens to carry out algorithmic audits [160], that of Genovesi and Mönig places sustainability at the center of Ethical AI certification [70], while that of Metcalf et al. uses environmental impact assessments as an example of a formal mechanism that can be used to inspire the assessment of AI technologies [129]. All of these approaches acknowledge that ethical decisions in AI have environmental consequences and vice versa, thus necessitating a framing that accommodates both ethical and environmental responsibility.\n\nTransparency", "doc_id": "luccioni2025b", "page": 11, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Generalizability\n\nGiven the observed disconnect between ethics and sustainability in the context of AI principles and frameworks, we find that, while these offer a valuable starting point, they often fall short in addressing AI’s complex ethical issues due to their lack of contextual sensitivity [142]. We also believe that improving upon this necessitates a more nuanced and context-specific approach to AI ethics, one that embraces the varied ethical dimensions presented by AI, including its environmental implications. Current ethical charters in AI, often detached from this perplexity, represent preliminary thoughts on the ethical landscape but lack the depth required for many practical applications [5]. By recognizing these limitations, we intend not to discard these definitions and principles but to improve upon them. In this context, environmental and sustainability challenges related to AI development and deployment are integral to the broader ethical reflections within the field. This integration between ethics and sustainability in AI calls for a holistic approach, where ethical considerations are not viewed in isolation but are intrinsically linked with environmental and sustainability oversight. For instance, shifting from the dominant Western moral philosophies to include perspectives from non-Western traditions such as relational ethics [131], Ubuntu ethics [145], and Confucian ethics [111] can offer valuable insights into community, social harmony, and the interconnectedness of beings, emphasizing the impact of AI on society and interpersonal relationships.\n\nEvaluation\n\nThere is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable. Recent work has begun bridging the gap; for instance, work by Lynch et al. is inspired by the concept of urgent governance in environmental studies, which distinguishes system reliability and societal harm and advocates for the consideration of both when auditing infrastructure and technologies [123]. Raji et al. use a similar approach for their proposed end-to-end framework for internal algorithmic auditing of AI models, which takes into account both technical and ethical assessments [159]. In a similar vein, a recent model evaluation framework by Rakova et al., proposes an environmental justice-oriented lens to carry out algorithmic audits [160], that of Genovesi and Mönig places sustainability at the center of Ethical AI certification [70], while that of Metcalf et al. uses environmental impact assessments as an example of a formal mechanism that can be used to inspire the assessment of AI technologies [129]. All of these approaches acknowledge that ethical decisions in AI have environmental consequences and vice versa, thus necessitating a framing that accommodates both ethical and environmental responsibility.\n\nTransparency", "original_types": ["text", "header"], "id": 1327}
{"type": "section", "content": "When viewed as a means to foster greater accountability, transparency takes on a central role: it becomes a principle that enhances ethical compliance and promotes environmental responsibility, contributing to sustainability. For instance, integrating social transparency and sustainability can be exemplified by an AI system designed for urban planning, such as an AI tool developed to optimize city layouts for efficiency. In this example, to include sustainability approaches, developers would also provide information on the environmental footprint of running the AI system, such as energy consumption during data processing and potential environmental benefits of the proposed urban layouts, like reduced carbon emissions from optimized traffic flows or green spaces. In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have and making more informed decisions based on the information at our disposal [51].\n\nEquity and Power\n\nEquity is about ensuring fair access and participation in the benefits and governance of technology across different communities, especially those historically marginalized. Philosophical perspectives on equity, drawing", "doc_id": "luccioni2025b", "page": 11, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "When viewed as a means to foster greater accountability, transparency takes on a central role: it becomes a principle that enhances ethical compliance and promotes environmental responsibility, contributing to sustainability. For instance, integrating social transparency and sustainability can be exemplified by an AI system designed for urban planning, such as an AI tool developed to optimize city layouts for efficiency. In this example, to include sustainability approaches, developers would also provide information on the environmental footprint of running the AI system, such as energy consumption during data processing and potential environmental benefits of the proposed urban layouts, like reduced carbon emissions from optimized traffic flows or green spaces. In this way, by deepening the concept of transparency to include social and environmental aspects, we would move towards creating AI systems that are more robust, socially responsible and ultimately more accountable about the environmental impacts they have and making more informed decisions based on the information at our disposal [51].\n\nEquity and Power\n\nEquity is about ensuring fair access and participation in the benefits and governance of technology across different communities, especially those historically marginalized. Philosophical perspectives on equity, drawing", "original_types": ["text", "header"], "id": 1328}
{"type": "section", "content": "5.2 Research\n\nDespite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainability and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given their socio-technicality and the consequences this can have on the human and non-human species residing in these regions.", "doc_id": "luccioni2025b", "page": 12, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "5.2 Research\n\nDespite a lack of common terminology, similar issues arise both in terms of considerations of AI ethics and sustainability and considering the inter-connectedness of the two when designing and deploying AI systems is paramount given their socio-technicality and the consequences this can have on the human and non-human species residing in these regions.", "original_types": ["text", "header"], "id": 1329}
{"type": "section", "content": "5.3 Governance\n\nAs the field of AI ethics increasingly intersects with regulation, including law and policy, it showcases its interdisciplinary nature, meaning that in order to be successful, governance initiatives must incorporate efforts from different domains, depending on the context of the application.\n\nGeneralizability. While there is no single solution to complex questions involving governance over AI systems, various bottom-up governance approaches have been proposed based on the cultural, societal and geographical constraints of AI system deployment. Some of these follow the tenets of the Māori culture, which is based on principles that consider both impacts on nature as well as on fellow human beings, bridging the gap between ethics and sustainability [82, 143]; others espouse those established by the Indigenous AI community, which is based on both values and practices of social and environmental sustainability, both core to many Indigenous epistemologies [110]. Also, regulating the deployment of out-of-the-box AI solutions that operate on the premise of generalizability without taking", "doc_id": "luccioni2025b", "page": 13, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "5.3 Governance\n\nAs the field of AI ethics increasingly intersects with regulation, including law and policy, it showcases its interdisciplinary nature, meaning that in order to be successful, governance initiatives must incorporate efforts from different domains, depending on the context of the application.\n\nGeneralizability. While there is no single solution to complex questions involving governance over AI systems, various bottom-up governance approaches have been proposed based on the cultural, societal and geographical constraints of AI system deployment. Some of these follow the tenets of the Māori culture, which is based on principles that consider both impacts on nature as well as on fellow human beings, bridging the gap between ethics and sustainability [82, 143]; others espouse those established by the Indigenous AI community, which is based on both values and practices of social and environmental sustainability, both core to many Indigenous epistemologies [110]. Also, regulating the deployment of out-of-the-box AI solutions that operate on the premise of generalizability without taking", "original_types": ["text", "header"], "id": 1330}
{"type": "section", "content": "Evaluation\n\nAs noted by Metcalf et al. [2021], there is a parallel between environmental impact assessments and AI ethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such as energy consumption and carbon emissions. Requiring audits of commercial AI systems before their deployment in practice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but also in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental impacts, will require the development of new governance approaches. For instance, attempting to evaluate the wider rebound effects of AI tools and their impacts on consumption and human behavior is important to represent their broader impacts on both society and the environment [86, 96, 194]. Finally, integrating both social and environmental assessments into existing and in-progress regulation and developing new approaches to evaluate these impacts can help ensure that the deployment of AI systems is carried out in a way that is ethically sound and sustainable across multiple dimensions.\n\nTransparency\n\nRecent years have seen less transparency in AI research and practice, especially in terms of generative AI models [189]. However, as these systems are increasingly being deployed in society, having more information regarding how these systems were created and deployed remains paramount. Ensuring that enough details are provided both regarding the energy consumed and greenhouse gasses emitted during model training and deployment can help track how the environmental impacts of AI are evolving over time. Mandating transparency for already deployed AI systems can help establish audits, red teaming efforts and AI energy score ratings to raise users’ awareness around the impacts of the systems they use [29], contributing to what can be termed \"usable transparency\" [144]. By ensuring that the processes and results of these practices are well-documented and publicly accessible, stakeholders would be better equipped to understand and evaluate the ethical and sustainable aspects of AI systems. Such policies would promote a culture of openness in the AI industry, encouraging developers to prioritize ethical considerations and sustainability alongside technical advancements.\n\nEquity", "doc_id": "luccioni2025b", "page": 14, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Evaluation\n\nAs noted by Metcalf et al. [2021], there is a parallel between environmental impact assessments and AI ethics audits, which can be extended beyond ethical compliance to include assessments of environmental impacts, such as energy consumption and carbon emissions. Requiring audits of commercial AI systems before their deployment in practice, both in contexts such as education and healthcare that come with high stakes in terms of societal impacts, but also in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental impacts, will require the development of new governance approaches. For instance, attempting to evaluate the wider rebound effects of AI tools and their impacts on consumption and human behavior is important to represent their broader impacts on both society and the environment [86, 96, 194]. Finally, integrating both social and environmental assessments into existing and in-progress regulation and developing new approaches to evaluate these impacts can help ensure that the deployment of AI systems is carried out in a way that is ethically sound and sustainable across multiple dimensions.\n\nTransparency\n\nRecent years have seen less transparency in AI research and practice, especially in terms of generative AI models [189]. However, as these systems are increasingly being deployed in society, having more information regarding how these systems were created and deployed remains paramount. Ensuring that enough details are provided both regarding the energy consumed and greenhouse gasses emitted during model training and deployment can help track how the environmental impacts of AI are evolving over time. Mandating transparency for already deployed AI systems can help establish audits, red teaming efforts and AI energy score ratings to raise users’ awareness around the impacts of the systems they use [29], contributing to what can be termed \"usable transparency\" [144]. By ensuring that the processes and results of these practices are well-documented and publicly accessible, stakeholders would be better equipped to understand and evaluate the ethical and sustainable aspects of AI systems. Such policies would promote a culture of openness in the AI industry, encouraging developers to prioritize ethical considerations and sustainability alongside technical advancements.\n\nEquity", "original_types": ["text", "header"], "id": 1331}
{"type": "section", "content": "Involving multiple stakeholders, especially ones from the concerned communities and domains, in this process is important to ensure that different perspectives and lived experiences are reflected in the development and deployment of AI systems [170, 171] as well as the different communities and can influence existing and future practices [45, 126]. From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133], whereas the White House Office of Management and Budget (OMB)’s first government-wide policy around the usage of AI includes, inter alia, clauses that stipulate that government agencies should take both environmental impacts and bias and fairness into account when procuring AI-enabled services [59] - exhibiting thought leadership that will hopefully have wider repercussions.\n\nCONCLUSION\n\nWe recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies like AI, it can be difficult to define what progress looks like and how it can be achieved. We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) – but by adopting a multitude of endeavors such as the ones described in the paragraphs above can help involve different actors and", "doc_id": "luccioni2025b", "page": 14, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Involving multiple stakeholders, especially ones from the concerned communities and domains, in this process is important to ensure that different perspectives and lived experiences are reflected in the development and deployment of AI systems [170, 171] as well as the different communities and can influence existing and future practices [45, 126]. From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133], whereas the White House Office of Management and Budget (OMB)’s first government-wide policy around the usage of AI includes, inter alia, clauses that stipulate that government agencies should take both environmental impacts and bias and fairness into account when procuring AI-enabled services [59] - exhibiting thought leadership that will hopefully have wider repercussions.\n\nCONCLUSION\n\nWe recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies like AI, it can be difficult to define what progress looks like and how it can be achieved. We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) – but by adopting a multitude of endeavors such as the ones described in the paragraphs above can help involve different actors and", "original_types": ["text", "header"], "id": 1332}
{"type": "section", "content": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\nhopefully build momentum across the AI community. The beauty of making transversal connections that go above and beyond the silos in which many AI technologists tend to operate is that we can also be inspired by the multitude of rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and climate science, to propose ways forward that would allow the AI community to improve systems from the perspective of both ethics and sustainability. Looking forward, the field of AI ethics is rapidly evolving, with insights racing to keep up with the rapid pace of technological advancements in AI. This dynamic landscape presents an ongoing challenge: to develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts. Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is not just beneficial but necessary, ensuring that AI advancements are not only technologically innovative and ethically sound but also maximizing their potential to engender sustainable advancement.", "doc_id": "luccioni2025b", "page": 15, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice\n\nhopefully build momentum across the AI community. The beauty of making transversal connections that go above and beyond the silos in which many AI technologists tend to operate is that we can also be inspired by the multitude of rich and relevant work that has been done in other domains – from ecology to philosophy, as well as governance and climate science, to propose ways forward that would allow the AI community to improve systems from the perspective of both ethics and sustainability. Looking forward, the field of AI ethics is rapidly evolving, with insights racing to keep up with the rapid pace of technological advancements in AI. This dynamic landscape presents an ongoing challenge: to develop AI in a way that is inclusive, just, and cognizant of its environmental and societal impacts. Furthermore, it is becoming increasingly clear that AI ethics and sustainability are interdependent: they must go hand in hand to ensure a holistic societal impact. The absence of either aspect leads to an incomplete perspective, potentially overlooking critical societal and environmental consequences. Therefore, integrating AI ethics with sustainability is not just beneficial but necessary, ensuring that AI advancements are not only technologically innovative and ethically sound but also maximizing their potential to engender sustainable advancement.", "original_types": ["text", "header"], "id": 1333}
{"type": "section", "content": "REFERENCES\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névél, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. The elephant in the room: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n\n[3] AI Safety Summit. 2023. The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023. (2023). https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit\n\n[4] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud.\n\n[5] Daniel Andler. 2023. Intelligence artificielle, intelligence humaine: le double énigme. Gallimard, Paris.\n\n[6] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. Machine bias. In Ethics of data and analytics. Auerbach Publications, 254–264.\n\n[7] Thomas Aquinas. 1702. Summa theologica. J. Mentelin.\n\n[8] Aristotle. 350. Nicomachean Ethics. Original work published in 350 B.C.E.\n\n[9] Hutan Ashrafian. 2015. Intelligent robots must uphold human rights. Nature 519 (2015), 391. https://doi.org/10.1038/519391a\n\n[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and Deborah Raji. 2022. Disentangling the Components of Ethical Research in Machine Learning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 2057–2068. https://doi.org/10.1145/3531146.3533781\n\n[11] Ricardo Baeza-Yates and Zeinab Liaghat. 2017. Quality-efficiency trade-offs in machine learning for text processing. In 2017 IEEE international conference on big data (big data). IEEE, 897–904.\n\n[12] Edward B Barbier. 1987. The concept of sustainable economic development. Environmental conservation 14, 2 (1987), 101–110.\n\n[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Joyce, and Margaret Odero. 2023. Topology Estimation from Voltage Edge Sensing for Resource-Constrained Grids. Tackling Climate Change with Machine Learning Workshop - ICLR 2023 (2023).\n\n[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante, and Azzurra Ragone. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering. 358–367.\n\n[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfgang Schwanghart. 2014. Spatial bias in the GBIF database and its effect on modeling species’ geographic distributions. Ecological Informatics 19 (2014), 10–15.", "doc_id": "luccioni2025b", "page": 16, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "REFERENCES\n\n[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.\n\n[2] Mohamed Abdalla, Jan Philip Wahle, Terry Ruas, Aurélie Névél, Fanny Ducel, Saif M Mohammad, and Karën Fort. 2023. The elephant in the room: Analyzing the presence of big tech in natural language processing research. arXiv preprint arXiv:2305.02797 (2023).\n\n[3] AI Safety Summit. 2023. The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023. (2023). https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit\n\n[4] Amazon Web Services. 2021. Sustainability in the Cloud. https://sustainability.aboutamazon.com/environment/the-cloud.\n\n[5] Daniel Andler. 2023. Intelligence artificielle, intelligence humaine: le double énigme. Gallimard, Paris.\n\n[6] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. Machine bias. In Ethics of data and analytics. Auerbach Publications, 254–264.\n\n[7] Thomas Aquinas. 1702. Summa theologica. J. Mentelin.\n\n[8] Aristotle. 350. Nicomachean Ethics. Original work published in 350 B.C.E.\n\n[9] Hutan Ashrafian. 2015. Intelligent robots must uphold human rights. Nature 519 (2015), 391. https://doi.org/10.1038/519391a\n\n[10] Carolyn Ashurst, Solon Barocas, Rosie Campbell, and Deborah Raji. 2022. Disentangling the Components of Ethical Research in Machine Learning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 2057–2068. https://doi.org/10.1145/3531146.3533781\n\n[11] Ricardo Baeza-Yates and Zeinab Liaghat. 2017. Quality-efficiency trade-offs in machine learning for text processing. In 2017 IEEE international conference on big data (big data). IEEE, 897–904.\n\n[12] Edward B Barbier. 1987. The concept of sustainable economic development. Environmental conservation 14, 2 (1987), 101–110.\n\n[13] Mohini Bariya, Genevieve Flaspohler, Ngoran Clare-Joyce, and Margaret Odero. 2023. Topology Estimation from Voltage Edge Sensing for Resource-Constrained Grids. Tackling Climate Change with Machine Learning Workshop - ICLR 2023 (2023).\n\n[14] Vita Santa Barletta, Danilo Caivano, Domenico Gigante, and Azzurra Ragone. 2023. A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. In Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering. 358–367.\n\n[15] Jan Beck, Marianne Böller, Andreas Erhardt, and Wolfgang Schwanghart. 2014. Spatial bias in the GBIF database and its effect on modeling species’ geographic distributions. Ecological Informatics 19 (2014), 10–15.", "original_types": ["text", "header"], "id": 1334}
{"type": "section", "content": "[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610–623. https://doi.org/10.1145/3442188.3445922\n\n[17] Ruha Benjamin. 2023. Race after technology. In Social Theory Re-Wired. Routledge, 405–415.\n\n[18] Jeremy Bentham. 1789. An Introduction to the Principles of Morals and Legislation. T. Payne and Son.\n\n[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michael, Lennart Heim, Xueyun Luo, and Neil Thompson. 2024. The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny? arXiv preprint arXiv:2401.02452 (2024).\n\n[20] Joseph R Biden. 2023. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. (2023).\n\n[21] Elettra Bietti. 2020. From Ethics Washing to Ethics Bashing: A View on Tech Ethics from within Moral Philosophy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* '20). Association for Computing Machinery, New York, NY, USA, 210–219. https://doi.org/10.1145/3351095.3372860\n\n[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590 [cs.LG]\n\n[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S. Brown, Johnathan Flowers, Anthony Ventresque, and Christopher L. Dancy. 2022. The forgotten margins of AI ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 948–958.\n\n[24] Joshua Blumenstock. 2018. Don’t forget people in the use of big data for development.\n\n[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van Wynsberghe. 2022. From an Ethics of Carefulness to an Ethics of Desirability: Going Beyond Current Ethics Approaches to Sustainable AI. Sustainability 14, 8 (2022). https://doi.org/10.3390/su14084472\n\n[26] Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holistic evaluation of language models. Annals of the New York Academy of Sciences 1525, 1 (2023), 140–146.\n\n[27] Mioara Borza. 2014. The connection between efficiency and sustainability—a theoretical approach. Procedia Economics and Finance 15 (2014), 1355–1363.\n\n[28] Andrew Brennan and Norva Y. S. Lo. 2022. Environmental Ethics. In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.\n\n[29] Benedetta Brevini. 2020. Black boxes, not green: Mythologizing artificial intelligence and omitting the environment. Big Data & Society 7, 2 (2020), 2053951720935141. https://doi.org/10.1177/2053951720935141", "doc_id": "luccioni2025b", "page": 16, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610–623. https://doi.org/10.1145/3442188.3445922\n\n[17] Ruha Benjamin. 2023. Race after technology. In Social Theory Re-Wired. Routledge, 405–415.\n\n[18] Jeremy Bentham. 1789. An Introduction to the Principles of Morals and Legislation. T. Payne and Son.\n\n[19] Tamay Besiroglu, Sage Andrus Bergerson, Amelia Michael, Lennart Heim, Xueyun Luo, and Neil Thompson. 2024. The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny? arXiv preprint arXiv:2401.02452 (2024).\n\n[20] Joseph R Biden. 2023. Executive order on the safe, secure, and trustworthy development and use of artificial intelligence. (2023).\n\n[21] Elettra Bietti. 2020. From Ethics Washing to Ethics Bashing: A View on Tech Ethics from within Moral Philosophy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* '20). Association for Computing Machinery, New York, NY, USA, 210–219. https://doi.org/10.1145/3351095.3372860\n\n[22] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590 [cs.LG]\n\n[23] Abeba Birhane, Elayne Ruane, Thomas Laurent, Matthew S. Brown, Johnathan Flowers, Anthony Ventresque, and Christopher L. Dancy. 2022. The forgotten margins of AI ethics. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 948–958.\n\n[24] Joshua Blumenstock. 2018. Don’t forget people in the use of big data for development.\n\n[25] Larissa Bolte, Tijs Vandemeulebroucke, and Aimee van Wynsberghe. 2022. From an Ethics of Carefulness to an Ethics of Desirability: Going Beyond Current Ethics Approaches to Sustainable AI. Sustainability 14, 8 (2022). https://doi.org/10.3390/su14084472\n\n[26] Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holistic evaluation of language models. Annals of the New York Academy of Sciences 1525, 1 (2023), 140–146.\n\n[27] Mioara Borza. 2014. The connection between efficiency and sustainability—a theoretical approach. Procedia Economics and Finance 15 (2014), 1355–1363.\n\n[28] Andrew Brennan and Norva Y. S. Lo. 2022. Environmental Ethics. In The Stanford Encyclopedia of Philosophy (Summer 2022 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.\n\n[29] Benedetta Brevini. 2020. Black boxes, not green: Mythologizing artificial intelligence and omitting the environment. Big Data & Society 7, 2 (2020), 2053951720935141. https://doi.org/10.1177/2053951720935141", "original_types": ["text"], "id": 1335}
{"type": "section", "content": "[30] Benedetta Brevini. 2023. Myths, techno solutionism and artificial intelligence: reclaiming AI materiality and its massive environmental costs. In Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing, 869–877.\n\n[31] Alexander EI Brownlee, Jason Adair, Saemundur O Haraldsson, and John Jabbo. 2021. Exploring the accuracy–energy trade-off in machine learning. In 2021 IEEE/ACM International Workshop on Genetic Improvement (GI). IEEE, 11–18.", "doc_id": "luccioni2025b", "page": 16, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[30] Benedetta Brevini. 2023. Myths, techno solutionism and artificial intelligence: reclaiming AI materiality and its massive environmental costs. In Handbook of Critical Studies of Artificial Intelligence. Edward Elgar Publishing, 869–877.\n\n[31] Alexander EI Brownlee, Jason Adair, Saemundur O Haraldsson, and John Jabbo. 2021. Exploring the accuracy–energy trade-off in machine learning. In 2021 IEEE/ACM International Workshop on Genetic Improvement (GI). IEEE, 11–18.", "original_types": ["text"], "id": 1336}
{"type": "section", "content": "[32] Robert D. Bullard, Glenn S. Johnson, and Beverly H. Wright. 1997. Confronting Environmental Injustice: It’s the Right Thing to Do. Race, Gender & Class 5, 1 (1997), 63–79.\n\n[33] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, 77–91.\n\n[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Franch, and Justus Bogner. 2023. Exploring the Carbon Footprint of Hugging Face’s ML Models: A Repository Mining Study. arXiv preprint arXiv:2305.11164 (2023).\n\n[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelina Wang. 2021. The limits of global inclusion in AI development. arXiv preprint arXiv:2102.01265 (2021).\n\n[36] Kyla Chasalow and Karen Levy. 2021. Representativeness in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 77–89.\n\n[37] Rachel Chason and Rael Ombuor. 2021. A lack of weather data in Africa is thwarting critical climate research. https://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/\n\n[38] Chinese Data Law Alliance. 2023. Chinese Artificial Intelligence Law, v. 1.0. https://mp.weixin.qq.com/s/85D8TjMkN9Tl-oWjq15JiQ\n\n[39] Concil of EU. 2022. Artificial Intelligence Act: Council calls for promoting safe AI that respects fundamental rights. https://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting-safe-ai-that-respects-fundamental-rights/\n\n[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. 2022. Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1571–1583.\n\n[41] Josh Cowls, Andreas Tsamados, Mariarosaria Taddeo, and Luciano Floridi. 2023. The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.\n\n[42] Kate Crawford and Trevor Paglen. 2021. Excavating AI: The politics of images in machine learning training sets. Ai & Society 36, 4 (2021), 1105–1116.\n\n[43] Rowena Cullen. 2001. Addressing the digital divide. Online information review 25, 5 (2001), 311–320.\n\n[44] Amane Dannouni, Stefan A. Deutscher, Ghita Dezzaz, Adam Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid Maher, David Patterson, Edmond Rhys Jones, Juliet Rothenberg, Hamza Tber, Maud Texier, and Ali Ziat. 2023. Accelerating Climate Action with AI. https://www.gstatic.com/gumdrop/sustainability/accelerating-climate-action-ai.pdf\n\n[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur. 2022. Recommendations for ethical and responsible use of artificial intelligence in digital agriculture. Frontiers in Artificial Intelligence 5 (2022), 884192.\n\n[46] Hannah Davis. 2020. A Dataset is a Worldview. Towards Data Science (2020).", "doc_id": "luccioni2025b", "page": 17, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[32] Robert D. Bullard, Glenn S. Johnson, and Beverly H. Wright. 1997. Confronting Environmental Injustice: It’s the Right Thing to Do. Race, Gender & Class 5, 1 (1997), 63–79.\n\n[33] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, 77–91.\n\n[34] Joel Castaño, Silverio Martínez-Fernández, Xavier Franch, and Justus Bogner. 2023. Exploring the Carbon Footprint of Hugging Face’s ML Models: A Repository Mining Study. arXiv preprint arXiv:2305.11164 (2023).\n\n[35] Alan Chan, Chinasa T Okolo, Zachary Terner, and Angelina Wang. 2021. The limits of global inclusion in AI development. arXiv preprint arXiv:2102.01265 (2021).\n\n[36] Kyla Chasalow and Karen Levy. 2021. Representativeness in statistics, politics, and machine learning. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 77–89.\n\n[37] Rachel Chason and Rael Ombuor. 2021. A lack of weather data in Africa is thwarting critical climate research. https://www.washingtonpost.com/world/2021/09/24/africa-climate-weather-data/\n\n[38] Chinese Data Law Alliance. 2023. Chinese Artificial Intelligence Law, v. 1.0. https://mp.weixin.qq.com/s/85D8TjMkN9Tl-oWjq15JiQ\n\n[39] Concil of EU. 2022. Artificial Intelligence Act: Council calls for promoting safe AI that respects fundamental rights. https://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting-safe-ai-that-respects-fundamental-rights/\n\n[40] Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. 2022. Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1571–1583.\n\n[41] Josh Cowls, Andreas Tsamados, Mariarosaria Taddeo, and Luciano Floridi. 2023. The AI gambit: leveraging artificial intelligence to combat climate change—opportunities, challenges, and recommendations. Ai & Society (2023), 1–25.\n\n[42] Kate Crawford and Trevor Paglen. 2021. Excavating AI: The politics of images in machine learning training sets. Ai & Society 36, 4 (2021), 1105–1116.\n\n[43] Rowena Cullen. 2001. Addressing the digital divide. Online information review 25, 5 (2001), 311–320.\n\n[44] Amane Dannouni, Stefan A. Deutscher, Ghita Dezzaz, Adam Elman, Antonia Gawel, Marsden Hanna, Andrew Hyland, Amjad Kharij, Hamid Maher, David Patterson, Edmond Rhys Jones, Juliet Rothenberg, Hamza Tber, Maud Texier, and Ali Ziat. 2023. Accelerating Climate Action with AI. https://www.gstatic.com/gumdrop/sustainability/accelerating-climate-action-ai.pdf\n\n[45] Rozita Dara, Seyed Mehdi Hazrati Fard, and Jasmin Kaur. 2022. Recommendations for ethical and responsible use of artificial intelligence in digital agriculture. Frontiers in Artificial Intelligence 5 (2022), 884192.\n\n[46] Hannah Davis. 2020. A Dataset is a Worldview. Towards Data Science (2020).", "original_types": ["text"], "id": 1337}
{"type": "section", "content": "[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, 248–255.\n\n[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, et al. 2023. Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 375–386.\n\n[49] Fatma Denton. 2002. Climate change vulnerability, impacts, and adaptation: Why does gender matter? Gender & Development 10, 2 (2002), 10–20.\n\n[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\n[51] Nicholas Diakopoulos. 2020. Accountability, transparency, and algorithms. The Oxford handbook of ethics of AI 17, 4 (2020), 197.\n\n[52] Roel Dobbe and Meredith Whittaker. 2019. AI and climate change: how they’re connected, and what we can do about it. AI Now Institute 17 (2019).\n\n[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.\n\n[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 (2021).\n\n[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma. 2018. Neo-colonialism and Millennium Development Goals (MDGs) in Africa: A blend of an old wine in a new bottle. African Journal of Science, Technology, Innovation and Development 10, 3 (2018), 355–366.\n\n[56] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 82, 19 pages. https://doi.org/10.1145/3411764.3445188\n\n[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 (2023).\n\n[58] Virginia Eubanks. 2018. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press.", "doc_id": "luccioni2025b", "page": 17, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[47] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, 248–255.\n\n[48] Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, et al. 2023. Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society. 375–386.\n\n[49] Fatma Denton. 2002. Climate change vulnerability, impacts, and adaptation: Why does gender matter? Gender & Development 10, 2 (2002), 10–20.\n\n[50] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\n[51] Nicholas Diakopoulos. 2020. Accountability, transparency, and algorithms. The Oxford handbook of ethics of AI 17, 4 (2020), 197.\n\n[52] Roel Dobbe and Meredith Whittaker. 2019. AI and climate change: how they’re connected, and what we can do about it. AI Now Institute 17 (2019).\n\n[53] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of AI in cloud instances. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.\n\n[54] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758 (2021).\n\n[55] Anuoluwapo Abosede Durokifa and Edwin Chikata Ijeoma. 2018. Neo-colonialism and Millennium Development Goals (MDGs) in Africa: A blend of an old wine in a new bottle. African Journal of Science, Technology, Innovation and Development 10, 3 (2018), 355–366.\n\n[56] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, and Justin D. Weisz. 2021. Expanding Explainability: Towards Social Transparency in AI Systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 82, 19 pages. https://doi.org/10.1145/3411764.3445188\n\n[57] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 (2023).\n\n[58] Virginia Eubanks. 2018. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press.", "original_types": ["text"], "id": 1338}
{"type": "section", "content": "[59] Executive Office of the President, Office of Management and Budget. 2024. Memorandum for the heads of executive departments and agencies on Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence. https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf", "doc_id": "luccioni2025b", "page": 17, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[59] Executive Office of the President, Office of Management and Budget. 2024. Memorandum for the heads of executive departments and agencies on Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence. https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf", "original_types": ["text"], "id": 1339}
{"type": "section", "content": "[60] Florian Eyert and Paola Lopez. 2023. Rethinking Transparency as a Communicative Constellation. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 444–454. https://doi.org/10.1145/3593013.3594010\n\n[61] Sophia Falk and Aimee van Wynsberghe. 2023. Challenging AI for Sustainability: what ought it mean? AI and Ethics (2023), 1–11.\n\n[62] Alec Feinberg. 2022. Solar Geoengineering Modeling and Applications for Mitigating Global Warming: Assessing Key Parameters and the Urban Heat Island Influence. Frontiers in Climate 4 (2022), 870071.\n\n[63] H. Felzmann, E. Fosch-Villaronga, C. Lutz, et al. 2020. Towards Transparency by Design for Artificial Intelligence. Science and Engineering Ethics 26 (2020), 3333–3361. https://doi.org/10.1007/s11948-020-00276-4\n\n[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, et al. 2018. AI4People—an ethical framework for a good AI society: opportunities, risks, principles, and recommendations. Minds and machines 28 (2018), 689–707.\n\n[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2021. The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136–143.\n\n[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).\n\n[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.10256836\n\n[68] Timnit Gebru. 2019. Oxford handbook on AI ethics book chapter on race and gender. arXiv preprint arXiv:1908.06165 (2019).\n\n[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n\n[70] Sergio Genovesi and Julia Maria Mönig. 2022. Acknowledging Sustainability in the Framework of Ethical Certification for AI. Sustainability 14, 7 (2022). https://doi.org/10.3390/su14074157\n\n[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, Irene Y Chen, and Rajesh Ranganath. 2020. A review of challenges and opportunities in machine learning for health. AMIA Summits on Translational Science Proceedings 2020 (2020), 191.", "doc_id": "luccioni2025b", "page": 18, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[60] Florian Eyert and Paola Lopez. 2023. Rethinking Transparency as a Communicative Constellation. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 444–454. https://doi.org/10.1145/3593013.3594010\n\n[61] Sophia Falk and Aimee van Wynsberghe. 2023. Challenging AI for Sustainability: what ought it mean? AI and Ethics (2023), 1–11.\n\n[62] Alec Feinberg. 2022. Solar Geoengineering Modeling and Applications for Mitigating Global Warming: Assessing Key Parameters and the Urban Heat Island Influence. Frontiers in Climate 4 (2022), 870071.\n\n[63] H. Felzmann, E. Fosch-Villaronga, C. Lutz, et al. 2020. Towards Transparency by Design for Artificial Intelligence. Science and Engineering Ethics 26 (2020), 3333–3361. https://doi.org/10.1007/s11948-020-00276-4\n\n[64] Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, et al. 2018. AI4People—an ethical framework for a good AI society: opportunities, risks, principles, and recommendations. Minds and machines 28 (2018), 689–707.\n\n[65] Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2021. The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136–143.\n\n[66] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).\n\n[67] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. https://doi.org/10.5281/zenodo.10256836\n\n[68] Timnit Gebru. 2019. Oxford handbook on AI ethics book chapter on race and gender. arXiv preprint arXiv:1908.06165 (2019).\n\n[69] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.\n\n[70] Sergio Genovesi and Julia Maria Mönig. 2022. Acknowledging Sustainability in the Framework of Ethical Certification for AI. Sustainability 14, 7 (2022). https://doi.org/10.3390/su14074157\n\n[71] Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, Irene Y Chen, and Rajesh Ranganath. 2020. A review of challenges and opportunities in machine learning for health. AMIA Summits on Translational Science Proceedings 2020 (2020), 191.", "original_types": ["text"], "id": 1340}
{"type": "section", "content": "[72] Amandeep S Gill and Stefan Germann. 2022. Conceptual and normative approaches to AI governance for a global digital ecosystem supportive of the UN Sustainable Development Goals (SDGs). AI and Ethics 2, 2 (2022), 293–301.\n\n[73] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862 (2019).\n\n[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT press.\n\n[75] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon.\n\n[76] Green Software Foundation. 2023. Can AI be Truly Green? https://greensoftware.foundation/articles/can-ai-truly-be-green\n\n[77] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destructions. https://www.greenpeace.org/usa/reports/oil-in-the-cloud/\n\n[78] Odd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the art: Reproducibility in artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.\n\n[79] Thilo Hagendorff. 2022. A virtue-based framework to support putting AI ethics into practice. Philosophy & Technology 35, 3 (2022), 55.\n\n[80] Benjamin Haibe-Kains et al. 2020. Transparency and reproducibility in artificial intelligence. Nature 586, 7829 (2020), E14–E16.\n\n[81] Karen Hao. 2022. Artificial intelligence is creating a new colonial world order. MIT Technology Review (2022).\n\n[82] Karen Hao. 2022. A new vision of artificial intelligence for the people. MIT Technology Review (2022).\n\n[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Nagel. 2023. Beware of sustainable AI! Uses and abuses of a worthy goal. AI and Ethics (2023), 1–12.\n\n[84] Martin Hilbert. 2016. Big data for development: A review of promises and challenges. Development Policy Review 34, 1 (2016), 135–174.\n\n[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, et al. 2023. International institutions for advanced AI. arXiv preprint arXiv:2307.04699 (2023).\n\n[86] Mél Hogan. 2018. Big data ecologies. Ephemera 18, 3 (2018), 631.\n\n[87] Wayne Holmes and Ilkka Tuomi. 2022. State of the art and practice in AI in education. European Journal of Education 57, 4 (2022), 542–570.\n\n[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).\n\n[89] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)Fairness: Lessons for Machine Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* '19). Association for Computing Machinery, New York, NY, USA, 49–58. https://doi.org/10.1145/3287560.3287600\n\n[90] Strategic Imperatives. 1987. Report of the World Commission on Environment and Development: Our common future. Accessed Feb 10 (1987), 1–300.", "doc_id": "luccioni2025b", "page": 18, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[72] Amandeep S Gill and Stefan Germann. 2022. Conceptual and normative approaches to AI governance for a global digital ecosystem supportive of the UN Sustainable Development Goals (SDGs). AI and Ethics 2, 2 (2022), 293–301.\n\n[73] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862 (2019).\n\n[74] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT press.\n\n[75] Google. 2022. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon.\n\n[76] Green Software Foundation. 2023. Can AI be Truly Green? https://greensoftware.foundation/articles/can-ai-truly-be-green\n\n[77] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil Profit from Climate Destructions. https://www.greenpeace.org/usa/reports/oil-in-the-cloud/\n\n[78] Odd Erik Gundersen and Sigbjørn Kjensmo. 2018. State of the art: Reproducibility in artificial intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.\n\n[79] Thilo Hagendorff. 2022. A virtue-based framework to support putting AI ethics into practice. Philosophy & Technology 35, 3 (2022), 55.\n\n[80] Benjamin Haibe-Kains et al. 2020. Transparency and reproducibility in artificial intelligence. Nature 586, 7829 (2020), E14–E16.\n\n[81] Karen Hao. 2022. Artificial intelligence is creating a new colonial world order. MIT Technology Review (2022).\n\n[82] Karen Hao. 2022. A new vision of artificial intelligence for the people. MIT Technology Review (2022).\n\n[83] Jan-Christoph Heilinger, Hendrik Kempt, and Saskia Nagel. 2023. Beware of sustainable AI! Uses and abuses of a worthy goal. AI and Ethics (2023), 1–12.\n\n[84] Martin Hilbert. 2016. Big data for development: A review of promises and challenges. Development Policy Review 34, 1 (2016), 135–174.\n\n[85] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, et al. 2023. International institutions for advanced AI. arXiv preprint arXiv:2307.04699 (2023).\n\n[86] Mél Hogan. 2018. Big data ecologies. Ephemera 18, 3 (2018), 631.\n\n[87] Wayne Holmes and Ilkka Tuomi. 2022. State of the art and practice in AI in education. European Journal of Education 57, 4 (2022), 542–570.\n\n[88] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).\n\n[89] Ben Hutchinson and Margaret Mitchell. 2019. 50 Years of Test (Un)Fairness: Lessons for Machine Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* '19). Association for Computing Machinery, New York, NY, USA, 49–58. https://doi.org/10.1145/3287560.3287600\n\n[90] Strategic Imperatives. 1987. Report of the World Commission on Environment and Development: Our common future. Accessed Feb 10 (1987), 1–300.", "original_types": ["text"], "id": 1341}
{"type": "section", "content": "[91] Innovation, Science and Economic Development Canada. 2023. The Artificial Intelligence and Data Act (AIDA) – Companion document. https://ISED-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document\n\n[92] Yacine Jernite et al. 2022. Data governance in the age of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM\n\n[93] W Stanley Jevons. 1866. The coal question. In The Economics of Population. Routledge, 193–204\n\n[94] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature machine intelligence 1, 9 (2019), 389–399\n\n[95] LN Joppa, Brian O’Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoffmann, James EM Watson, Stuart HM Butchart, Malika Virah-Sawmy, Benjamin S Halpern, et al. 2016. Filling in biodiversity threat gaps. Science 352, 6284 (2016), 416–418\n\n[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527\n\n[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Megan Mills-Novoa, Elizabeth Shoffner, and Katherine Siegel. 2022. Biases and limitations of Global Forest Change and author-generated land cover maps in detecting deforestation in the Amazon. PLoS One 17, 7 (2022), e0268970\n\n[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Foster. 2021. Reduced, reused and recycled: The life of a dataset in machine learning research. arXiv preprint arXiv:2112.01716 (2021)\n\n[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning. PMLR, 5637–5664\n\n[100] Julian Kuehnert, Deborah McGlynn, Sekou L. Remy, Aisha Walcott-Bryant, and Anne Jones. 2022. Surrogate Ensemble Forecasting for Dynamic Climate Impact Models. arXiv:2204.05795 [cs.LG]\n\n[101] Maciej Kuziemski and Gianluca Misuraca. 2020. AI governance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings. Telecommunications policy 44, 6 (2020), 101976\n\n[102] Travis LaCroix and Alexandra Sasha Luccioni. 2022. Metaethical perspectives on’Benchmarking’AI ethics. arXiv preprint arXiv:2204.05151 (2022)\n\n[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto. 2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 3206–3219. https://doi.org/10.18653/v1/2023.eacl-main.234", "doc_id": "luccioni2025b", "page": 19, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[91] Innovation, Science and Economic Development Canada. 2023. The Artificial Intelligence and Data Act (AIDA) – Companion document. https://ISED-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document\n\n[92] Yacine Jernite et al. 2022. Data governance in the age of large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. ACM\n\n[93] W Stanley Jevons. 1866. The coal question. In The Economics of Population. Routledge, 193–204\n\n[94] Anna Jobin, Marcello Ienca, and Effy Vayena. 2019. The global landscape of AI ethics guidelines. Nature machine intelligence 1, 9 (2019), 389–399\n\n[95] LN Joppa, Brian O’Connor, Piero Visconti, Cathy Smith, Jonas Geldmann, Michael Hoffmann, James EM Watson, Stuart HM Butchart, Malika Virah-Sawmy, Benjamin S Halpern, et al. 2016. Filling in biodiversity threat gaps. Science 352, 6284 (2016), 416–418\n\n[96] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig, and David Rolnick. 2022. Aligning artificial intelligence with climate change mitigation. Nature Climate Change 12, 6 (2022), 518–527\n\n[97] Eva Kinnebrew, Jose I Ochoa-Brito, Matthew French, Megan Mills-Novoa, Elizabeth Shoffner, and Katherine Siegel. 2022. Biases and limitations of Global Forest Change and author-generated land cover maps in detecting deforestation in the Amazon. PLoS One 17, 7 (2022), e0268970\n\n[98] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Foster. 2021. Reduced, reused and recycled: The life of a dataset in machine learning research. arXiv preprint arXiv:2112.01716 (2021)\n\n[99] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. 2021. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning. PMLR, 5637–5664\n\n[100] Julian Kuehnert, Deborah McGlynn, Sekou L. Remy, Aisha Walcott-Bryant, and Anne Jones. 2022. Surrogate Ensemble Forecasting for Dynamic Climate Impact Models. arXiv:2204.05795 [cs.LG]\n\n[101] Maciej Kuziemski and Gianluca Misuraca. 2020. AI governance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings. Telecommunications policy 44, 6 (2020), 101976\n\n[102] Travis LaCroix and Alexandra Sasha Luccioni. 2022. Metaethical perspectives on’Benchmarking’AI ethics. arXiv preprint arXiv:2204.05151 (2022)\n\n[103] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto. 2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 3206–3219. https://doi.org/10.18653/v1/2023.eacl-main.234", "original_types": ["text"], "id": 1342}
{"type": "section", "content": "[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675–1684\n\n[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2019. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 131–138\n\n[106] Julian Lamont (Ed.). 2017. Distributive Justice. Routledge\n\n[107] S. Larsson and F. Heintz. 2020. Transparency in artificial intelligence. Internet Policy Review 9, 2 (2020). https://doi.org/10.14763/2020.2.1469\n\n[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elisabeth Cross, Alessandro Parente, and Ricardo Vinuesa. 2023. Improving aircraft performance using machine learning: a review. Aerospace Science and Technology (2023), 108354\n\n[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva. 2019. Ethical framework for designing autonomous intelligent systems. Journal of Open Innovation: Technology, Market, and Complexity 5, 1 (2019), 18\n\n[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Kaipulaumakaniolono Baker, Scott Benesiinaabandan, Michelle Brown, Melanie Cheung, Meredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artificial intelligence position paper. (2020)\n\n[111] Chenyang Li. 2013. The Confucian philosophy of harmony. Vol. 10. Routledge\n\n[112] Mochen Liao, Kai Lan, and Yuan Yao. 2022. Sustainability implications of artificial intelligence in the chemical industry: A conceptual framework. Journal of industrial ecology 26, 1 (2022), 164–182\n\n[113] Zachary C Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31–57\n\n[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)\n\n[115] Kirsten Lloyd. 2018. Bias amplification in artificial intelligence systems. arXiv preprint arXiv:1809.07842 (2018)\n\n[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023)\n\n[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2023. Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv:2311.16863 [cs.LG]\n\n[118] Alexandra Sasha Luccioni and Anna Rogers. 2023. Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice. arXiv preprint arXiv:2308.07120 (2023)\n\n[119] Alexandra Sasha Luccioni and David Rolnick. 2023. Bugs in the data: How ImageNet misrepresents biodiversity. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 14382–14390", "doc_id": "luccioni2025b", "page": 19, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[104] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675–1684\n\n[105] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2019. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 131–138\n\n[106] Julian Lamont (Ed.). 2017. Distributive Justice. Routledge\n\n[107] S. Larsson and F. Heintz. 2020. Transparency in artificial intelligence. Internet Policy Review 9, 2 (2020). https://doi.org/10.14763/2020.2.1469\n\n[108] Soledad Le Clainche, Esteban Ferrer, Sam Gibson, Elisabeth Cross, Alessandro Parente, and Ricardo Vinuesa. 2023. Improving aircraft performance using machine learning: a review. Aerospace Science and Technology (2023), 108354\n\n[109] Jaana Leikas, Raija Koivisto, and Nadezhda Gotcheva. 2019. Ethical framework for designing autonomous intelligent systems. Journal of Open Innovation: Technology, Market, and Complexity 5, 1 (2019), 18\n\n[110] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Kaipulaumakaniolono Baker, Scott Benesiinaabandan, Michelle Brown, Melanie Cheung, Meredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artificial intelligence position paper. (2020)\n\n[111] Chenyang Li. 2013. The Confucian philosophy of harmony. Vol. 10. Routledge\n\n[112] Mochen Liao, Kai Lan, and Yuan Yao. 2022. Sustainability implications of artificial intelligence in the chemical industry: A conceptual framework. Journal of industrial ecology 26, 1 (2022), 164–182\n\n[113] Zachary C Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31–57\n\n[114] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)\n\n[115] Kirsten Lloyd. 2018. Bias amplification in artificial intelligence systems. arXiv preprint arXiv:1809.07842 (2018)\n\n[116] Alexandra Sasha Luccioni and Alex Hernandez-Garcia. 2023. Counting carbon: A survey of factors influencing the emissions of machine learning. arXiv preprint arXiv:2302.08476 (2023)\n\n[117] Alexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2023. Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv:2311.16863 [cs.LG]\n\n[118] Alexandra Sasha Luccioni and Anna Rogers. 2023. Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice. arXiv preprint arXiv:2308.07120 (2023)\n\n[119] Alexandra Sasha Luccioni and David Rolnick. 2023. Bugs in the data: How ImageNet misrepresents biodiversity. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 14382–14390", "original_types": ["text"], "id": 1343}
{"type": "section", "content": "[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025)", "doc_id": "luccioni2025b", "page": 19, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[120] Alexandra Sasha Luccioni, Emma Strubell, and Kate Crawford. 2025. From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate. arXiv preprint arXiv:2501.16548 (2025)", "original_types": ["text"], "id": 1344}
{"type": "section", "content": "[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language model. arXiv preprint arXiv:2211.02001 (2022).\n\n[122] Federica Lucivero. 2020. Big data, big waste? A reflection on the environmental sustainability of big data initiatives. Science and engineering ethics 26, 2 (2020), 1009–1030.\n\n[123] Amanda H Lynch and Siri Veland. 2018. Urgency in the Anthropocene. MIT Press.\n\n[124] Mirca Madianou. 2021. Nonhuman humanitarianism: when’AI for good’can be harmful. Information, Communication & Society 24, 6 (2021), 850–868.\n\n[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi, Tee Garnett, et al. 2023. What’s fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 1505–1519.\n\n[126] Duncan P McLaren. 2018. Whose climate and whose ethics? Conceptions of justice in solar geoengineering modelling. Energy research & social science 44 (2018), 209–221.\n\n[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR) 54, 6 (2021), 1–35.\n\n[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature 624, 7990 (2023), 80–85.\n\n[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. 2021. Algorithmic Impact Assessments and Accountability: The Co-Construction of Impacts. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 735–746. https://doi.org/10.1145/3442188.3445935\n\n[130] Rachel Metz. 2024. Google’s Emissions Shot Up 48% Over Five Years Due to AI. Bloomberg (2024).\n\n[131] Thaddeus Metz and Scott C. Miller. 2016. Relational ethics. In The international encyclopedia of ethics. 1–10.\n\n[132] Milagros Miceli, Julian Posada, and Tianling Yang. 2022. Studying up machine learning data: Why talk about bias when we mean power? Proceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.\n\n[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija. 2019. Ethical AI for the Governance of the Society: Challenges and Opportunities. In CEUR Workshop Proceedings, Vol. 2505. 20–26.\n\n[134] John Stuart Mill. 1863. Utilitarianism. Parker, Son, and Bourn.\n\n[135] David Miller and Michael Walzer (Eds.). 1995. Pluralism, Justice, and Equality. Oxford University Press.", "doc_id": "luccioni2025b", "page": 20, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[121] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the carbon footprint of BLOOM, a 176B parameter language model. arXiv preprint arXiv:2211.02001 (2022).\n\n[122] Federica Lucivero. 2020. Big data, big waste? A reflection on the environmental sustainability of big data initiatives. Science and engineering ethics 26, 2 (2020), 1009–1030.\n\n[123] Amanda H Lynch and Siri Veland. 2018. Urgency in the Anthropocene. MIT Press.\n\n[124] Mirca Madianou. 2021. Nonhuman humanitarianism: when’AI for good’can be harmful. Information, Communication & Society 24, 6 (2021), 850–868.\n\n[125] Melissa Mccradden, Oluwadara Odusi, Shalmali Joshi, Ismail Akrout, Kagiso Ndlovu, Ben Glocker, Gabriel Maicas, Xiaoxuan Liu, Mjaye Mazwi, Tee Garnett, et al. 2023. What’s fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 1505–1519.\n\n[126] Duncan P McLaren. 2018. Whose climate and whose ethics? Conceptions of justice in solar geoengineering modelling. Energy research & social science 44 (2018), 209–221.\n\n[127] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR) 54, 6 (2021), 1–35.\n\n[128] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature 624, 7990 (2023), 80–85.\n\n[129] Jacob Metcalf, Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, and Madeleine Clare Elish. 2021. Algorithmic Impact Assessments and Accountability: The Co-Construction of Impacts. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 735–746. https://doi.org/10.1145/3442188.3445935\n\n[130] Rachel Metz. 2024. Google’s Emissions Shot Up 48% Over Five Years Due to AI. Bloomberg (2024).\n\n[131] Thaddeus Metz and Scott C. Miller. 2016. Relational ethics. In The international encyclopedia of ethics. 1–10.\n\n[132] Milagros Miceli, Julian Posada, and Tianling Yang. 2022. Studying up machine learning data: Why talk about bias when we mean power? Proceedings of the ACM on Human-Computer Interaction 6, GROUP (2022), 1–14.\n\n[133] Nieminen Mika, Gotcheva Nadezhda, Leikas Jaana, and K Raija. 2019. Ethical AI for the Governance of the Society: Challenges and Opportunities. In CEUR Workshop Proceedings, Vol. 2505. 20–26.\n\n[134] John Stuart Mill. 1863. Utilitarianism. Parker, Son, and Bourn.\n\n[135] David Miller and Michael Walzer (Eds.). 1995. Pluralism, Justice, and Equality. Oxford University Press.", "original_types": ["text"], "id": 1345}
{"type": "section", "content": "[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Emily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Jamie Morgenstern. 2020. Diversity and inclusion metrics in subset selection. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 117–123.\n\n[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.\n\n[138] Shakir Mohamed, Marie-Therese Png, and William Isaac. 2020. Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology 33 (2020), 659–684.\n\n[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. 2023. Auditing large language models: a three-layered approach. AI and Ethics (2023), 1–31.\n\n[140] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.\n\n[141] Steven Gonzalez Monserrate. 2022. The cloud is material: On the environmental impacts of computation and data storage. (2022).\n\n[142] L. Munn. 2022. The Uselessness of AI Ethics. AI Ethics (2022). https://doi.org/10.1007/s43681-022-00209-w\n\n[143] Luke Munn. 2023. The five tests: designing and evaluating AI according to indigenous Māori principles. AI & SOCIETY (2023), 1–9.\n\n[144] Patrick Murmann and Simone Fischer-Hübner. 2017. Usable transparency enhancing tools: A literature review. (2017).\n\n[145] Mechthild Nagel. 2022. Ludic ubuntu ethics: Decolonizing justice. Taylor & Francis.\n\n[146] National Academies of Sciences, Engineering and Medicine. 2019. Reproducibility and replicability in science. (2019).\n\n[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing Information Disclosures to Support Meaningful Transparency and Accountability. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 679–690. https://doi.org/10.1145/3531146.3533133\n\n[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Luke Abraham, John Pyle, and Apostolos Voulgarakis. 2018. Using machine learning to build temperature-based ozone parameterizations for climate sensitivity simulations. Environmental Research Letters 13, 10 (2018), 104016.\n\n[149] Victor Ordonez, Taylor Dunn, and Eric Noll. 2023. OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks:‘A little bit scared of this’. (2023). https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshapessociety-acknowledges/story\n\n[150] Friederike Otto. 2023. Without Warning: A Lack of Weather Stations Is Costing African Lives. https://e360.yale.edu/features/africa-weather-stations-climate-change\n\n[151] Edward A Page. 2008. Distributing the burdens of climate change. Environmental Politics 17, 4 (2008), 556–575.", "doc_id": "luccioni2025b", "page": 20, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[136] Margaret Mitchell, Dylan Baker, Nyalleng Moorosi, Emily Denton, Ben Hutchinson, Alex Hanna, Timnit Gebru, and Jamie Morgenstern. 2020. Diversity and inclusion metrics in subset selection. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 117–123.\n\n[137] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.\n\n[138] Shakir Mohamed, Marie-Therese Png, and William Isaac. 2020. Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology 33 (2020), 659–684.\n\n[139] Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. 2023. Auditing large language models: a three-layered approach. AI and Ethics (2023), 1–31.\n\n[140] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com.\n\n[141] Steven Gonzalez Monserrate. 2022. The cloud is material: On the environmental impacts of computation and data storage. (2022).\n\n[142] L. Munn. 2022. The Uselessness of AI Ethics. AI Ethics (2022). https://doi.org/10.1007/s43681-022-00209-w\n\n[143] Luke Munn. 2023. The five tests: designing and evaluating AI according to indigenous Māori principles. AI & SOCIETY (2023), 1–9.\n\n[144] Patrick Murmann and Simone Fischer-Hübner. 2017. Usable transparency enhancing tools: A literature review. (2017).\n\n[145] Mechthild Nagel. 2022. Ludic ubuntu ethics: Decolonizing justice. Taylor & Francis.\n\n[146] National Academies of Sciences, Engineering and Medicine. 2019. Reproducibility and replicability in science. (2019).\n\n[147] Chris Norval, Kristin Cornelius, Jennifer Cobbe, and Jatinder Singh. 2022. Disclosure by Design: Designing Information Disclosures to Support Meaningful Transparency and Accountability. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 679–690. https://doi.org/10.1145/3531146.3533133\n\n[148] Peer Nowack, Peter Braesicke, Joanna Haigh, Nathan Luke Abraham, John Pyle, and Apostolos Voulgarakis. 2018. Using machine learning to build temperature-based ozone parameterizations for climate sensitivity simulations. Environmental Research Letters 13, 10 (2018), 104016.\n\n[149] Victor Ordonez, Taylor Dunn, and Eric Noll. 2023. OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks:‘A little bit scared of this’. (2023). https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshapessociety-acknowledges/story\n\n[150] Friederike Otto. 2023. Without Warning: A Lack of Weather Stations Is Costing African Lives. https://e360.yale.edu/features/africa-weather-stations-climate-change\n\n[151] Edward A Page. 2008. Distributing the burdens of climate change. Environmental Politics 17, 4 (2008), 556–575.", "original_types": ["text"], "id": 1346}
{"type": "section", "content": "[152] European Parliament. 2023. Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI. https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai", "doc_id": "luccioni2025b", "page": 20, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[152] European Parliament. 2023. Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI. https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai", "original_types": ["text"], "id": 1347}
{"type": "section", "content": "[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jernite, and Margaret Mitchell. 2023. Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, <conf-loc>) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 343–354. https://doi.org/10.1145/3593013.3594002\n\n[155] Erich Prem. 2023. From ethical AI frameworks to tools: a review of approaches. AI and Ethics 3, 3 (2023), 699–716.\n\n[156] Edward Raff. 2019. A Step Toward Quantifying Independently Reproducible Machine Learning Research. arXiv:1909.06674 [cs.LG]\n\n[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. 2021. AI and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 (2021).\n\n[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buolamwini. 2023. Change from the outside: Towards credible third-party audits of ai systems. Missing links in AI governance (2023), 5.\n\n[159] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, et al. 2020. Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, 33–44.\n\n[160] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Social-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic Audits. In 2023 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3593013.3594014\n\n[161] Akshat Rathi and Dina Bass. 2024. Microsoft’s AI Push Imperils Climate Goal as Carbon Emissions Jump 30%. Bloomberg (2024).\n\n[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640 [cs.CV]\n\n[163] Jennifer Rhee. 2018. The robotic imaginary: The human and the price of dehumanized labor. U of Minnesota Press.\n\n[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386 (2016).\n\n[165] Paola Ricaurte. 2022. Ethics for the majority world: AI and the question of violence at scale. Media, Culture & Society 44, 4 (2022), 726–745.\n\n[166] Anna Rogers. 2021. Changing the world by changing the data. arXiv preprint arXiv:2105.13947 (2021).", "doc_id": "luccioni2025b", "page": 21, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[153] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\n\n[154] Giada Pistilli, Carlos Muñoz Ferrandis, Yacine Jernite, and Margaret Mitchell. 2023. Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, <conf-loc>) (FAccT '23). Association for Computing Machinery, New York, NY, USA, 343–354. https://doi.org/10.1145/3593013.3594002\n\n[155] Erich Prem. 2023. From ethical AI frameworks to tools: a review of approaches. AI and Ethics 3, 3 (2023), 699–716.\n\n[156] Edward Raff. 2019. A Step Toward Quantifying Independently Reproducible Machine Learning Research. arXiv:1909.06674 [cs.LG]\n\n[157] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. 2021. AI and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366 (2021).\n\n[158] Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buolamwini. 2023. Change from the outside: Towards credible third-party audits of ai systems. Missing links in AI governance (2023), 5.\n\n[159] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, et al. 2020. Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, 33–44.\n\n[160] Bogdana Rakova and Roel Dobbe. 2023. Algorithms as Social-Ecological-Technological Systems: an Environmental Justice Lens on Algorithmic Audits. In 2023 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3593013.3594014\n\n[161] Akshat Rathi and Dina Bass. 2024. Microsoft’s AI Push Imperils Climate Goal as Carbon Emissions Jump 30%. Bloomberg (2024).\n\n[162] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640 [cs.CV]\n\n[163] Jennifer Rhee. 2018. The robotic imaginary: The human and the price of dehumanized labor. U of Minnesota Press.\n\n[164] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386 (2016).\n\n[165] Paola Ricaurte. 2022. Ethics for the majority world: AI and the question of violence at scale. Media, Culture & Society 44, 4 (2022), 726–745.\n\n[166] Anna Rogers. 2021. Changing the world by changing the data. arXiv preprint arXiv:2105.13947 (2021).", "original_types": ["text"], "id": 1348}
{"type": "section", "content": "[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. The Dollar Street dataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\n\n[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\n[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684–10695.\n\n[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Elizabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin, Hannah Tait Neufeld, Andrew Nixon, et al. 2019. Automated pastures and the digital divide: How agricultural technologies are shaping labour and rural communities. Journal of Rural Studies 68 (2019), 112–122.\n\n[171] Mark Ryan. 2022. The social and ethical impacts of artificial intelligence in agriculture: mapping the agricultural AI literature. AI & SOCIETY (2022), 1–13.\n\n[172] Salesforce. 2024. Sustainable AI Policy Principles. https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-sustainable-ai-policy-principles.pdf\n\n[173] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1–15.\n\n[174] Aaron Sankin and Surya Mattu. 2023. Predictive Policing Software Terrible At Predicting Crimes. https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes.\n\n[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C. Parkes, and Yang Liu. 2019. How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, HI, USA) (AIES '19). Association for Computing Machinery, New York, NY, USA, 99–106. https://doi.org/10.1145/3306618.3314248\n\n[176] David Schlosberg. 2012. Climate Justice and Capabilities: A Framework for Adaptation Policy. Ethics & International Affairs 26, 4 (2012), 445–461.\n\n[177] David Schlosberg and Lisette B Collins. 2014. From environmental to climate justice: climate change and the discourse of environmental justice. Wiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.\n\n[178] Jurgen Schmidhuber. 1991. Neural Sequence Chunkers. Technical Report FKI-148-91 (1991).", "doc_id": "luccioni2025b", "page": 21, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[167] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. The Dollar Street dataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\n\n[168] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2022. Tackling climate change with machine learning. ACM Computing Surveys (CSUR) 55, 2 (2022), 1–96.\n\n[169] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684–10695.\n\n[170] Sarah Rotz, Evan Gravely, Ian Mosby, Emily Duncan, Elizabeth Finnis, Mervyn Horgan, Joseph LeBlanc, Ralph Martin, Hannah Tait Neufeld, Andrew Nixon, et al. 2019. Automated pastures and the digital divide: How agricultural technologies are shaping labour and rural communities. Journal of Rural Studies 68 (2019), 112–122.\n\n[171] Mark Ryan. 2022. The social and ethical impacts of artificial intelligence in agriculture: mapping the agricultural AI literature. AI & SOCIETY (2022), 1–13.\n\n[172] Salesforce. 2024. Sustainable AI Policy Principles. https://www.salesforce.com/content/dam/web/en_us/www/documents/company/sustainability/salesforce-sustainable-ai-policy-principles.pdf\n\n[173] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1–15.\n\n[174] Aaron Sankin and Surya Mattu. 2023. Predictive Policing Software Terrible At Predicting Crimes. https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes.\n\n[175] Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C. Parkes, and Yang Liu. 2019. How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (Honolulu, HI, USA) (AIES '19). Association for Computing Machinery, New York, NY, USA, 99–106. https://doi.org/10.1145/3306618.3314248\n\n[176] David Schlosberg. 2012. Climate Justice and Capabilities: A Framework for Adaptation Policy. Ethics & International Affairs 26, 4 (2012), 445–461.\n\n[177] David Schlosberg and Lisette B Collins. 2014. From environmental to climate justice: climate change and the discourse of environmental justice. Wiley Interdisciplinary Reviews: Climate Change 5, 3 (2014), 359–374.\n\n[178] Jurgen Schmidhuber. 1991. Neural Sequence Chunkers. Technical Report FKI-148-91 (1991).", "original_types": ["text"], "id": 1349}
{"type": "section", "content": "[179] Christian Schroeder de Witt and Thomas Hornigold. 2019. Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. arXiv e-prints (2019), arXiv–1905.\n\n[180] Paul Schütze. 2024. The Problem of Sustainable AI: A Critical Assessment of an Emerging Phenomenon. Weizenbaum Journal of the Digital Society 4, 1 (2024).\n\n[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.", "doc_id": "luccioni2025b", "page": 21, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[179] Christian Schroeder de Witt and Thomas Hornigold. 2019. Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. arXiv e-prints (2019), arXiv–1905.\n\n[180] Paul Schütze. 2024. The Problem of Sustainable AI: A Critical Assessment of an Emerging Phenomenon. Weizenbaum Journal of the Digital Society 4, 1 (2024).\n\n[181] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.", "original_types": ["text"], "id": 1350}
{"type": "section", "content": "[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalleng Moorosi, and Richard Klein. 2021. Constructing a visual dataset to study the effects of spatial apartheid in South Africa. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2).\n\n[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency. 59–68.\n\n[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinck, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes. 2023. Limits to the accurate and generalizable use of soundscapes to monitor biodiversity. Nature Ecology & Evolution 7, 9 (2023), 1373–1378.\n\n[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay Chowdary. 2020. Machine learning applications for precision agriculture: A comprehensive review. IEEE Access 9 (2020), 4843–4873.\n\n[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romain Bosc, and Li Wenwei. 2017. Digital infrastructure: Overcoming the digital divide in emerging economies. G20 Insights 3 (2017), 1–36.\n\n[187] Ahmed AH Siddig. 2019. Why is biodiversity data-deficiency an ongoing conservation dilemma in Africa? Journal for Nature Conservation 50 (2019), 125719.\n\n[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Real ML: Recognizing, exploring, and articulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 587–597.\n\n[189] Irene Solaiman. 2023. The gradient of generative AI release: Methods and considerations. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 111–122.\n\n[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and Apostol Vassilev. 2023. Evaluating the Social Impact of Generative AI Systems in Systems and Society. arXiv:2306.05949 [cs.CY]\n\n[191] Maddie Stone. 2024. Microsoft employees spent years fighting the tech giant’s oil ties. Now, they’re speaking out. https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/\n\n[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1–9.", "doc_id": "luccioni2025b", "page": 22, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[182] Raesetje Sefala, Timnit Gebru, Luzango Mfupe, Nyalleng Moorosi, and Richard Klein. 2021. Constructing a visual dataset to study the effects of spatial apartheid in South Africa. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2).\n\n[183] Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency. 59–68.\n\n[184] Sarab S Sethi, Avery Bick, Robert M Ewers, Holger Klinck, Vijay Ramesh, Mao-Ning Tuanmu, and David A Coomes. 2023. Limits to the accurate and generalizable use of soundscapes to monitor biodiversity. Nature Ecology & Evolution 7, 9 (2023), 1373–1378.\n\n[185] Abhinav Sharma, Arpit Jain, Prateek Gupta, and Vinay Chowdary. 2020. Machine learning applications for precision agriculture: A comprehensive review. IEEE Access 9 (2020), 4843–4873.\n\n[186] Ben Shenglin, Felice Simonelli, Zhang Ruidong, Romain Bosc, and Li Wenwei. 2017. Digital infrastructure: Overcoming the digital divide in emerging economies. G20 Insights 3 (2017), 1–36.\n\n[187] Ahmed AH Siddig. 2019. Why is biodiversity data-deficiency an ongoing conservation dilemma in Africa? Journal for Nature Conservation 50 (2019), 125719.\n\n[188] Jessie J Smith, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Real ML: Recognizing, exploring, and articulating limitations of machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 587–597.\n\n[189] Irene Solaiman. 2023. The gradient of generative AI release: Methods and considerations. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 111–122.\n\n[190] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, and Apostol Vassilev. 2023. Evaluating the Social Impact of Generative AI Systems in Systems and Society. arXiv:2306.05949 [cs.CY]\n\n[191] Maddie Stone. 2024. Microsoft employees spent years fighting the tech giant’s oil ties. Now, they’re speaking out. https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/\n\n[192] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n\n[193] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1–9.", "original_types": ["text"], "id": 1351}
{"type": "section", "content": "[194] Sy Taffel, Laura Bedford, and Monique Mann. 2019. Ecocide isn’t ethical: Political ecology and capitalist AI ethics. ECONOMIES OF VIRTUE 20 (2019), 58.\n\n[195] Petros Terzis. 2020. Onward for the Freedom of Others: Marching beyond the AI Ethics. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3351095.3373152\n\n[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre. 2017. Taxonomic bias in biodiversity data and societal preferences. Scientific reports 7, 1 (2017), 9132.\n\n[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakalembe, and Hannah Kerner. 2021. CropHarvest: A global dataset for crop-type classification. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=JtjzUXPEaCu\n\n[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshava, Patricia Nying’uro, Herbert Misiani, Jemimah G Ongoma, Yoav Yair, Yacob Mulugetta, and Amadou T Gaye. 2023. How to reduce Africa’s undue exposure to climate risks. Nature 620, 7974 (2023), 488–491.\n\n[199] UN General Assembly. 2015. Transforming our world : the 2030 Agenda for Sustainable Development. https://www.refworld.org/legal/resolution/unga/2015/en/111816\n\n[200] UNESCO. 2021. Recommendation on the ethics of artificial intelligence.\n\n[201] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. 2020. Towards debiasing NLU models from unknown biases. arXiv preprint arXiv:2009.12303 (2020).\n\n[202] I. van de Poel. 2020. Embedding Values in Artificial Intelligence (AI) Systems. Minds & Machines 30 (2020), 385–409. https://doi.org/10.1007/s11023-020-09537-4\n\n[203] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. AI and Ethics 1, 3 (2021), 213–218.\n\n[204] GaÃ§l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker. 2024. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI. arXiv preprint arXiv:2409.14160 (2024).\n\n[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexandre Polvora. 2020. Societal and ethical impacts of artificial intelligence: Critical notes on European policy frameworks. Telecommunications Policy 44, 6 (2020), 101961.\n\n[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature communications 11, 1 (2020), 1–10.\n\n[207] Johannes M Waldmueller. 2015. Agriculture, knowledge and the ‘colonial matrix of power’: approaching sustainabilities from the Global South. Journal of Global Ethics 11, 3 (2015), 294–302.", "doc_id": "luccioni2025b", "page": 22, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[194] Sy Taffel, Laura Bedford, and Monique Mann. 2019. Ecocide isn’t ethical: Political ecology and capitalist AI ethics. ECONOMIES OF VIRTUE 20 (2019), 58.\n\n[195] Petros Terzis. 2020. Onward for the Freedom of Others: Marching beyond the AI Ethics. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20). Association for Computing Machinery, New York, NY, USA, 220–229. https://doi.org/10.1145/3351095.3373152\n\n[196] Julien Troudet, Philippe Grandcolas, Amandine Blin, Régine Vignes-Lebbe, and Frédéric Legendre. 2017. Taxonomic bias in biodiversity data and societal preferences. Scientific reports 7, 1 (2017), 9132.\n\n[197] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakalembe, and Hannah Kerner. 2021. CropHarvest: A global dataset for crop-type classification. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=JtjzUXPEaCu\n\n[198] Asaf Tzachor, Catherine E Richards, Masilin Gudoshava, Patricia Nying’uro, Herbert Misiani, Jemimah G Ongoma, Yoav Yair, Yacob Mulugetta, and Amadou T Gaye. 2023. How to reduce Africa’s undue exposure to climate risks. Nature 620, 7974 (2023), 488–491.\n\n[199] UN General Assembly. 2015. Transforming our world : the 2030 Agenda for Sustainable Development. https://www.refworld.org/legal/resolution/unga/2015/en/111816\n\n[200] UNESCO. 2021. Recommendation on the ethics of artificial intelligence.\n\n[201] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. 2020. Towards debiasing NLU models from unknown biases. arXiv preprint arXiv:2009.12303 (2020).\n\n[202] I. van de Poel. 2020. Embedding Values in Artificial Intelligence (AI) Systems. Minds & Machines 30 (2020), 385–409. https://doi.org/10.1007/s11023-020-09537-4\n\n[203] Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. AI and Ethics 1, 3 (2021), 213–218.\n\n[204] GaÃ§l Varoquaux, Alexandra Sasha Luccioni, and Meredith Whittaker. 2024. Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI. arXiv preprint arXiv:2409.14160 (2024).\n\n[205] Lucia Vesnic-Alujevic, Susana Nascimento, and Alexandre Polvora. 2020. Societal and ethical impacts of artificial intelligence: Critical notes on European policy frameworks. Telecommunications Policy 44, 6 (2020), 101961.\n\n[206] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. Nature communications 11, 1 (2020), 1–10.\n\n[207] Johannes M Waldmueller. 2015. Agriculture, knowledge and the ‘colonial matrix of power’: approaching sustainabilities from the Global South. Journal of Global Ethics 11, 3 (2015), 294–302.", "original_types": ["text"], "id": 1352}
{"type": "section", "content": "[208] J. Walmsley. 2021. Artificial Intelligence and the Value of Transparency. AI & Society 36 (2021), 585–595. https://doi.org/10.1007/s00146-020-01066-z\n\n[209] Laura Weidinger, Kevin R. McKee, Richard Everett, Saffron Huang, Tina O. Zhu, Martin J. Chadwick, Christopher Summerfield, and Iason Gabriel. 2023. Using the Veil of Ignorance to align AI systems with principles of justice. Proceedings of the National Academy of Sciences 120, 18 (2023), e2213709120. https://doi.org/10.1073/pnas.2213709120 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120\n\n[210] Craig R White, Dustin J Marshall, Steven L Chown, Susana Clusella-Trullas, Steven J Portugal, Craig E Franklin, and Frank Seebacher. 2021. Geographical bias in physiological data limits predictions of global change impacts. Functional Ecology 35, 7 (2021), 1572–1578.\n\n[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru. 2022. The exploited labor behind artificial intelligence. Noema Magazine 13 (2022).\n\n[212] T. Wischmeyer. 2020. Artificial Intelligence and Transparency: Opening the Black Box. In Regulating Artificial Intelligence. Springer, 75–101.\n\n[213] Robert Wolfe and Aylin Caliskan. 2022. American== white in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 800–812.\n\n[214] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n\n[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[216] D. Zeng. 2015. AI Ethics: Science Fiction Meets Technological Reality. IEEE Intelligent Systems 30, 03 (may 2015), 2–5. https://doi.org/10.1109/MIS.2015.53\n\n[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. 2019. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning. PMLR, 7472–7482.\n\n[218] Aram Ziai. 2016. Development discourse and global history: From colonialism to the sustainable development goals. Taylor & Francis.\n\n[219] Ofer Zwikael and John Smyrk. 2015. Project governance: Balancing control and trust in dealing with risk. International Journal of Project Management 33, 4 (2015), 852–862.", "doc_id": "luccioni2025b", "page": 23, "url": "https://arxiv.org/pdf/2504.00797", "embedded_text": "[208] J. Walmsley. 2021. Artificial Intelligence and the Value of Transparency. AI & Society 36 (2021), 585–595. https://doi.org/10.1007/s00146-020-01066-z\n\n[209] Laura Weidinger, Kevin R. McKee, Richard Everett, Saffron Huang, Tina O. Zhu, Martin J. Chadwick, Christopher Summerfield, and Iason Gabriel. 2023. Using the Veil of Ignorance to align AI systems with principles of justice. Proceedings of the National Academy of Sciences 120, 18 (2023), e2213709120. https://doi.org/10.1073/pnas.2213709120 arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2213709120\n\n[210] Craig R White, Dustin J Marshall, Steven L Chown, Susana Clusella-Trullas, Steven J Portugal, Craig E Franklin, and Frank Seebacher. 2021. Geographical bias in physiological data limits predictions of global change impacts. Functional Ecology 35, 7 (2021), 1572–1578.\n\n[211] Adrienne Williams, Milagros Miceli, and Timnit Gebru. 2022. The exploited labor behind artificial intelligence. Noema Magazine 13 (2022).\n\n[212] T. Wischmeyer. 2020. Artificial Intelligence and Transparency: Opening the Black Box. In Regulating Artificial Intelligence. Springer, 75–101.\n\n[213] Robert Wolfe and Aylin Caliskan. 2022. American== white in multimodal language-and-image ai. In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. 800–812.\n\n[214] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n\n[215] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).\n\n[216] D. Zeng. 2015. AI Ethics: Science Fiction Meets Technological Reality. IEEE Intelligent Systems 30, 03 (may 2015), 2–5. https://doi.org/10.1109/MIS.2015.53\n\n[217] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. 2019. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning. PMLR, 7472–7482.\n\n[218] Aram Ziai. 2016. Development discourse and global history: From colonialism to the sustainable development goals. Taylor & Francis.\n\n[219] Ofer Zwikael and John Smyrk. 2015. Project governance: Balancing control and trust in dealing with risk. International Journal of Project Management 33, 4 (2015), 852–862.", "original_types": ["text"], "id": 1353}
{"type": "section", "content": "Abstract\n\nLarge language models (LLMs) are increasingly integrated into many online services, yet they remain cost-prohibitive to deploy due to the requirement of expensive GPU instances. Prior work has addressed the high cost of LLM serving by improving the inference engine, but less attention has been given to selecting the most cost-efficient GPU type(s) for a specific LLM service. There is a large and growing landscape of GPU types and, within these options, higher cost does not always lead to increased performance. Instead, through a comprehensive investigation, we find that three key LLM service characteristics (request size, request rate, SLO) strongly influence GPU cost efficiency, and differing GPU types are most cost efficient for differing LLM service settings. As a result, the most cost-efficient allocation for a given service is typically a mix of heterogeneous GPU types. Based on this analysis, we introduce Mélange, a GPU allocation framework that navigates these diverse LLM service characteristics and heterogeneous GPU option space to automatically and efficiently derive the minimal-cost GPU allocation for a given LLM service. We formulate the GPU allocation task as a cost-aware bin packing problem where GPUs are bins and items are slices of the service workload. Our formulation’s constraints account for a service’s unique characteristics, allowing Mélange to be flexible to support diverse service settings and heterogeneity-aware to adapt the GPU allocation to a specific service. Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.", "doc_id": "griggs2024", "page": 1, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Abstract\n\nLarge language models (LLMs) are increasingly integrated into many online services, yet they remain cost-prohibitive to deploy due to the requirement of expensive GPU instances. Prior work has addressed the high cost of LLM serving by improving the inference engine, but less attention has been given to selecting the most cost-efficient GPU type(s) for a specific LLM service. There is a large and growing landscape of GPU types and, within these options, higher cost does not always lead to increased performance. Instead, through a comprehensive investigation, we find that three key LLM service characteristics (request size, request rate, SLO) strongly influence GPU cost efficiency, and differing GPU types are most cost efficient for differing LLM service settings. As a result, the most cost-efficient allocation for a given service is typically a mix of heterogeneous GPU types. Based on this analysis, we introduce Mélange, a GPU allocation framework that navigates these diverse LLM service characteristics and heterogeneous GPU option space to automatically and efficiently derive the minimal-cost GPU allocation for a given LLM service. We formulate the GPU allocation task as a cost-aware bin packing problem where GPUs are bins and items are slices of the service workload. Our formulation’s constraints account for a service’s unique characteristics, allowing Mélange to be flexible to support diverse service settings and heterogeneity-aware to adapt the GPU allocation to a specific service. Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.", "original_types": ["text", "header"], "id": 1354}
{"type": "section", "content": "a wide array of choices with varying performance specifications and on-demand cloud costs. Within these hardware options, higher cost does not always lead to increased performance. To investigate this phenomenon further, we examine GPU cost efficiency, defined based on common pricing models [34] as the number of input and output tokens processed per dollar cost (T/$) of on-demand cloud GPUs. We find that GPU cost efficiency is determined by three key LLM service characteristics:\n\n1. Request Size: An LLM request’s size is made up of its input and output token lengths. For small request sizes, lower-end GPUs generally produce greater T/$ than high-end GPUs.\n\n2. Request Rate: To maximize utilization, provisioned GPU capacity should align with request volume. At low request rates, services can reduce costs by right-sizing from expensive high-end GPUs to cheap low-end GPUs. Further, leveraging a mix of GPU types facilitates finer-grained resource scaling to better match request volume.\n\n3. Service-level Objective: Services typically establish latency SLOs to ensure service quality. Because low-end GPUs generally incur higher latency than high-end GPUs, high-end GPUs are required for stringent SLOs while low-end GPUs can reduce costs in loose-SLO settings.\n\nConsider a GPU allocation strategy that integrates each of the three observations above: high-cost A100 GPUs handle large requests and meet stringent SLOs, but lower-cost A10G GPUs serve smaller requests (1) and looser SLOs (3) at higher T/$. Then, during periods of low service activity, the service right-sizes to the even-cheaper L4 GPU to maintain service availability at lowest cost (2). Consequently, we find that GPU heterogeneity presents opportunities for increasing GPU cost efficiency, but such opportunities are highly dependent on LLM service characteristics. The key challenge, then, is creating a GPU allocation framework that can navigate the diversity of LLM services (request sizes, request rates, latency SLOs) and GPU types to find the optimal GPU allocation.", "doc_id": "griggs2024", "page": 2, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "a wide array of choices with varying performance specifications and on-demand cloud costs. Within these hardware options, higher cost does not always lead to increased performance. To investigate this phenomenon further, we examine GPU cost efficiency, defined based on common pricing models [34] as the number of input and output tokens processed per dollar cost (T/$) of on-demand cloud GPUs. We find that GPU cost efficiency is determined by three key LLM service characteristics:\n\n1. Request Size: An LLM request’s size is made up of its input and output token lengths. For small request sizes, lower-end GPUs generally produce greater T/$ than high-end GPUs.\n\n2. Request Rate: To maximize utilization, provisioned GPU capacity should align with request volume. At low request rates, services can reduce costs by right-sizing from expensive high-end GPUs to cheap low-end GPUs. Further, leveraging a mix of GPU types facilitates finer-grained resource scaling to better match request volume.\n\n3. Service-level Objective: Services typically establish latency SLOs to ensure service quality. Because low-end GPUs generally incur higher latency than high-end GPUs, high-end GPUs are required for stringent SLOs while low-end GPUs can reduce costs in loose-SLO settings.\n\nConsider a GPU allocation strategy that integrates each of the three observations above: high-cost A100 GPUs handle large requests and meet stringent SLOs, but lower-cost A10G GPUs serve smaller requests (1) and looser SLOs (3) at higher T/$. Then, during periods of low service activity, the service right-sizes to the even-cheaper L4 GPU to maintain service availability at lowest cost (2). Consequently, we find that GPU heterogeneity presents opportunities for increasing GPU cost efficiency, but such opportunities are highly dependent on LLM service characteristics. The key challenge, then, is creating a GPU allocation framework that can navigate the diversity of LLM services (request sizes, request rates, latency SLOs) and GPU types to find the optimal GPU allocation.", "original_types": ["text"], "id": 1355}
{"type": "figure", "content": "Figure 1: Mélange framework.", "doc_id": "griggs2024", "page": 2, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 1: Mélange framework.", "id": 1356}
{"type": "section", "content": "2 Related Work\n\n2.1 LLM Inference Optimization\n\nA significant body of research has focused on optimizing LLM inference efficiency. One stream concentrates on memory optimization, particularly through improved key-value cache reuse [56] and management strategies [19]. Another avenue seeks to minimize latency, such as scheduling optimization [51, 1, 46], speculative decoding [20, 18], kernel optimization [8, 40] and early exiting [41, 59]. Additional optimizations include quantization [10, 21, 49, 50] and sparsification [9, 52]. Instead of altering inference logic, our work assumes a fixed inference engine configuration and concentrates on reducing LLM deployment costs by choosing cost-effective GPU instance types.\n\n2.2 Machine Learning with Cloud Resources\n\nRecent studies have explored various strategies for reducing the cost of machine learning (ML) inference or training. Several focus on utilizing spot instances [42, 12, 53, 11], which is complementary to our work. Other work targets deployment on heterogeneous resources [5, 6, 30, 26, 27], but focuses primarily on model training rather than serving. Also, lveraging serverless instances for inference cost reduction has been examined in [2]. Nonetheless, these prior work predominantly concentrate on machine learning prior to the advent of LLMs, which we show to have unique characteristics that significantly impact cost efficiency. More recent studies, such as [25, 15], focus on LLMs, but they propose strategies for reducing costs via optimal migration plans and parallelism with heterogeneous resources. They do not identify key LLM service characteristics that impact cost efficiency and consider them in GPU deployment, which our work highlights. Another line of work [58, 36] explores splitting LLM inference into its two phases (prefill and decode) and performing the two phases on separate nodes, perhaps with different GPU types. Our work shows that, even within a phase, the best GPU type can change based on LLM service specifications.\n\n3 Background\n\n3.1 LLM Request Size Variance", "doc_id": "griggs2024", "page": 3, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "2 Related Work\n\n2.1 LLM Inference Optimization\n\nA significant body of research has focused on optimizing LLM inference efficiency. One stream concentrates on memory optimization, particularly through improved key-value cache reuse [56] and management strategies [19]. Another avenue seeks to minimize latency, such as scheduling optimization [51, 1, 46], speculative decoding [20, 18], kernel optimization [8, 40] and early exiting [41, 59]. Additional optimizations include quantization [10, 21, 49, 50] and sparsification [9, 52]. Instead of altering inference logic, our work assumes a fixed inference engine configuration and concentrates on reducing LLM deployment costs by choosing cost-effective GPU instance types.\n\n2.2 Machine Learning with Cloud Resources\n\nRecent studies have explored various strategies for reducing the cost of machine learning (ML) inference or training. Several focus on utilizing spot instances [42, 12, 53, 11], which is complementary to our work. Other work targets deployment on heterogeneous resources [5, 6, 30, 26, 27], but focuses primarily on model training rather than serving. Also, lveraging serverless instances for inference cost reduction has been examined in [2]. Nonetheless, these prior work predominantly concentrate on machine learning prior to the advent of LLMs, which we show to have unique characteristics that significantly impact cost efficiency. More recent studies, such as [25, 15], focus on LLMs, but they propose strategies for reducing costs via optimal migration plans and parallelism with heterogeneous resources. They do not identify key LLM service characteristics that impact cost efficiency and consider them in GPU deployment, which our work highlights. Another line of work [58, 36] explores splitting LLM inference into its two phases (prefill and decode) and performing the two phases on separate nodes, perhaps with different GPU types. Our work shows that, even within a phase, the best GPU type can change based on LLM service specifications.\n\n3 Background\n\n3.1 LLM Request Size Variance", "original_types": ["text", "header"], "id": 1357}
{"type": "figure", "content": "Figure 2: Request latency of different input/output lengths on A100-80G.", "doc_id": "griggs2024", "page": 3, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 2: Request latency of different input/output lengths on A100-80G.", "id": 1358}
{"type": "section", "content": "4. GPU Cost Efficiency Analysis\n\nIn this section, we analyze GPU cost efficiency for LLM services by serving Llama2-7b on NVIDIA A100 [32] and A10G [31] as a representative example. We show that GPU cost efficiency is influenced by three key LLM service characteristics: request size (§ 4.2), latency SLO (§ 4.3), and request rate (§ 4.4). For each characteristic, we demonstrate opportunities to exploit the heterogeneity of GPU types to increase cost efficiency and reduce deployment cost. Each plot is tagged with the request size, request rate, and SLO used to generate the plot. We use vLLM-0.2.7 as the serving engine [19].\n\n4.1 Definitions\n\nService-level Objective (SLO). SLOs are performance targets that define the acceptable quality of service, and a specific SLO varies according to the service’s interactivity needs. As in prior work [19, 58, 51], we use the average Time Per Output Token (TPOT) as our SLO. TPOT is determined by dividing request latency by the number of generated tokens. SLOs are application dependent: in-line code editors (e.g., GitHub Copilot [28]) require tight latency deadlines to suggest real-time code additions, whereas summarization services may permit additional processing time. There are other common definitions of SLO, such as time to first token and request latency, and Mélange is flexible to support these and other alternative definitions of SLO.\n\nCost Efficiency Metric. We use tokens per dollar (T/$) to measure GPU cost efficiency, calculated by summing input and output tokens and dividing the total by the GPU’s on-demand rental cost for a given time period. Cost models are orthogonal to Mélange; we chose this cost model for its simplicity, but cost efficiency can be computed with alternative formulations without affecting Mélange’s efficacy. In general, we derive T/$ by finding the input and output token rates while at the highest GPU saturation for which TPOT still meets a specified SLO.", "doc_id": "griggs2024", "page": 4, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "4. GPU Cost Efficiency Analysis\n\nIn this section, we analyze GPU cost efficiency for LLM services by serving Llama2-7b on NVIDIA A100 [32] and A10G [31] as a representative example. We show that GPU cost efficiency is influenced by three key LLM service characteristics: request size (§ 4.2), latency SLO (§ 4.3), and request rate (§ 4.4). For each characteristic, we demonstrate opportunities to exploit the heterogeneity of GPU types to increase cost efficiency and reduce deployment cost. Each plot is tagged with the request size, request rate, and SLO used to generate the plot. We use vLLM-0.2.7 as the serving engine [19].\n\n4.1 Definitions\n\nService-level Objective (SLO). SLOs are performance targets that define the acceptable quality of service, and a specific SLO varies according to the service’s interactivity needs. As in prior work [19, 58, 51], we use the average Time Per Output Token (TPOT) as our SLO. TPOT is determined by dividing request latency by the number of generated tokens. SLOs are application dependent: in-line code editors (e.g., GitHub Copilot [28]) require tight latency deadlines to suggest real-time code additions, whereas summarization services may permit additional processing time. There are other common definitions of SLO, such as time to first token and request latency, and Mélange is flexible to support these and other alternative definitions of SLO.\n\nCost Efficiency Metric. We use tokens per dollar (T/$) to measure GPU cost efficiency, calculated by summing input and output tokens and dividing the total by the GPU’s on-demand rental cost for a given time period. Cost models are orthogonal to Mélange; we chose this cost model for its simplicity, but cost efficiency can be computed with alternative formulations without affecting Mélange’s efficacy. In general, we derive T/$ by finding the input and output token rates while at the highest GPU saturation for which TPOT still meets a specified SLO.", "original_types": ["text", "header"], "id": 1359}
{"type": "figure", "content": "Figure 4: (a) depicts the absolute batch sizes of A10G and A100 serving Llama2-7b at maximum saturation, (b) reports the same batch sizes divided by GPU cost, plotting with respect to A10G.", "doc_id": "griggs2024", "page": 5, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 4: (a) depicts the absolute batch sizes of A10G and A100 serving Llama2-7b at maximum saturation, (b) reports the same batch sizes divided by GPU cost, plotting with respect to A10G.", "id": 1360}
{"type": "figure", "content": "Figure 5: Comparison of L4, A10G, A100, and H100. Tile colors indicates the GPU with greatest T/$. (a) tile values are the T/$ %-increase of the best GPU compared to the second best for that tile. (b) compares the best GPU to the worst GPU. In black boxes, only A100 and H100 are compared.", "doc_id": "griggs2024", "page": 5, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 5: Comparison of L4, A10G, A100, and H100. Tile colors indicates the GPU with greatest T/$. (a) tile values are the T/$ %-increase of the best GPU compared to the second best for that tile. (b) compares the best GPU to the worst GPU. In black boxes, only A100 and H100 are compared.", "id": 1361}
{"type": "section", "content": "A10G exhibits up to 2.6× greater T/$ than A100. Conversely, for larger request sizes, A100 achieves up to 1.5× the cost efficiency of A10G.\n\nWe extend this exploration to show the separate impacts of input and output lengths on T/$ (Fig. 3b). Each dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and larger sizes are best served on A100. Note that the difference can be significant, as using a single GPU type to serve requests across the entire request size space misses opportunities to produce up to 72% more output tokens for the same cost. This reveals the opportunity to use a mix of GPU types to serve requests for which they are most cost effective.\n\nSource of Cost Efficiency Gains: To isolate how request size influences relative cost efficiency, we examine request size’s effects on batch size, which serves as a proxy for throughput. Fig. 4 depicts absolute batch sizes and batch sizes normalized by instance cost of each GPU at maximum saturation.\n\nA10G and A100 have similar cost-normalized batch sizes at 250 input/output tokens, but as the request size increases to 2K input/output tokens, A10G’s absolute batch size decreases by 9× whereas A100’s only decreases by 6× due to its superior memory size and bandwidth. As a result, A100’s cost efficiency advantage over A10G increases with the increase in request size. In contrast, reducing the size from 250 to 25 input/output tokens expands A10G’s batch size by 15.2×, whereas A100’s growth is 5.89×.Because A100’s batch sizes are larger, A100 is more significantly constrained by per-request latency overheads (e.g., due to interference of prefill and decode [14]) As a result, A10G’s cost-normalized batch size exceeds A100’s at short request lengths, leading to greater overall T/$.\n\nOther Hardware and Model Size We extend our analysis to more GPU types and a larger model variant (Llama2-70b). Fig. 5 depicts the relative cost efficiency across four GPU types. Once again, as request sizes increase, we observe a progression of the most cost efficient GPU from lower-end to higher-end GPUs, matching our observations above. Similar trends are observed in the larger Llama2-70B model when comparing H100 and A100 GPUs, as detailed in Fig. 8.\n\nKey Takeaways: There is no universally most cost-efficient GPU for a given LLM. Instead, GPU cost efficiency is highly dependent on request sizes. Lower-end GPUs are more cost-effective for small", "doc_id": "griggs2024", "page": 5, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "A10G exhibits up to 2.6× greater T/$ than A100. Conversely, for larger request sizes, A100 achieves up to 1.5× the cost efficiency of A10G.\n\nWe extend this exploration to show the separate impacts of input and output lengths on T/$ (Fig. 3b). Each dimension influences cost efficiency similarly: smaller sizes are best served on A10G, and larger sizes are best served on A100. Note that the difference can be significant, as using a single GPU type to serve requests across the entire request size space misses opportunities to produce up to 72% more output tokens for the same cost. This reveals the opportunity to use a mix of GPU types to serve requests for which they are most cost effective.\n\nSource of Cost Efficiency Gains: To isolate how request size influences relative cost efficiency, we examine request size’s effects on batch size, which serves as a proxy for throughput. Fig. 4 depicts absolute batch sizes and batch sizes normalized by instance cost of each GPU at maximum saturation.\n\nA10G and A100 have similar cost-normalized batch sizes at 250 input/output tokens, but as the request size increases to 2K input/output tokens, A10G’s absolute batch size decreases by 9× whereas A100’s only decreases by 6× due to its superior memory size and bandwidth. As a result, A100’s cost efficiency advantage over A10G increases with the increase in request size. In contrast, reducing the size from 250 to 25 input/output tokens expands A10G’s batch size by 15.2×, whereas A100’s growth is 5.89×.Because A100’s batch sizes are larger, A100 is more significantly constrained by per-request latency overheads (e.g., due to interference of prefill and decode [14]) As a result, A10G’s cost-normalized batch size exceeds A100’s at short request lengths, leading to greater overall T/$.\n\nOther Hardware and Model Size We extend our analysis to more GPU types and a larger model variant (Llama2-70b). Fig. 5 depicts the relative cost efficiency across four GPU types. Once again, as request sizes increase, we observe a progression of the most cost efficient GPU from lower-end to higher-end GPUs, matching our observations above. Similar trends are observed in the larger Llama2-70B model when comparing H100 and A100 GPUs, as detailed in Fig. 8.\n\nKey Takeaways: There is no universally most cost-efficient GPU for a given LLM. Instead, GPU cost efficiency is highly dependent on request sizes. Lower-end GPUs are more cost-effective for small", "original_types": ["text"], "id": 1362}
{"type": "section", "content": "SLO and Cost Efficiency\n\nrequest sizes whereas higher-end GPUs are best for large request sizes. These findings generalize to settings with more GPU types and larger model sizes.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "SLO and Cost Efficiency\n\nrequest sizes whereas higher-end GPUs are best for large request sizes. These findings generalize to settings with more GPU types and larger model sizes.", "original_types": ["text", "header"], "id": 1363}
{"type": "figure", "content": "Figure 6: T/$ comparison between A10G and A100 across a range of TPOT SLO parameters.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 6: T/$ comparison between A10G and A100 across a range of TPOT SLO parameters.", "id": 1364}
{"type": "figure", "content": "Figure 7: Relative increase in T/$ when combining SLO and request size.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 7: Relative increase in T/$ when combining SLO and request size.", "id": 1365}
{"type": "section", "content": "In this section, we examine the impact of TPOT SLOs on GPU cost efficiency and highlight the joint effects of SLO and request size.\n\nExperiment: We serve Llama2-7b on A10G and A100 and measure T/$ by maximally saturating each GPU while keeping TPOT below SLO, repeating this across several TPOT deadlines ( Fig. 6). Under tight SLO constraints (<60ms), A100 demonstrates significantly greater T/$ than A10G (2×). A10G’s higher processing latency restricts the throughput it can achieve within a tight TPOT deadline, while A100 maintains much higher throughput even at low latency. However, as the SLO gradually loosens (60-160ms), A10G’s higher latency is less problematic, dramatically increasing its T/$ and surpassing that of A100 (by > 40%). Importantly, this example uses a small request size (64 input/output tokens), which was shown in § 4.2 to be best served on A10G. However, a tight SLO degrades A10G’s cost efficiency much more severely than A100’s and pushes the advantage to A100, exemplifying the tight interplay between SLO and request size explored further below.\n\nSLO and Request Size Interplay: Fig. 7 presents relative cost efficiency between A10G and A100 for a broad range of TPOT SLOs and request sizes. At tight SLOs (40-60ms), A100 always has higher T/$ (up to 2×). At 80ms, A10G begins showing modest benefit over A100 for small request sizes. Finally, at 100-160ms, A10G demonstrates much greater T/$ advantage over A100 for the same request sizes (up to 1.5×), yet A100 is always more cost efficient for larger requests. As demonstrated, a modification to TPOT SLO shifts the boundary within the request size space between which different GPU types are most cost effective and significantly influences the magnitude of cost efficiency differences between GPUs. As a result, both request size and SLO must be considered in tandem when determining cost efficiency.\n\nKey Takeaways: To meet strict SLOs, expensive GPUs are necessary due to the higher latency of cheaper GPUs. However, as SLO is loosened, lower-end GPUs can be used to cut deployment costs.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "In this section, we examine the impact of TPOT SLOs on GPU cost efficiency and highlight the joint effects of SLO and request size.\n\nExperiment: We serve Llama2-7b on A10G and A100 and measure T/$ by maximally saturating each GPU while keeping TPOT below SLO, repeating this across several TPOT deadlines ( Fig. 6). Under tight SLO constraints (<60ms), A100 demonstrates significantly greater T/$ than A10G (2×). A10G’s higher processing latency restricts the throughput it can achieve within a tight TPOT deadline, while A100 maintains much higher throughput even at low latency. However, as the SLO gradually loosens (60-160ms), A10G’s higher latency is less problematic, dramatically increasing its T/$ and surpassing that of A100 (by > 40%). Importantly, this example uses a small request size (64 input/output tokens), which was shown in § 4.2 to be best served on A10G. However, a tight SLO degrades A10G’s cost efficiency much more severely than A100’s and pushes the advantage to A100, exemplifying the tight interplay between SLO and request size explored further below.\n\nSLO and Request Size Interplay: Fig. 7 presents relative cost efficiency between A10G and A100 for a broad range of TPOT SLOs and request sizes. At tight SLOs (40-60ms), A100 always has higher T/$ (up to 2×). At 80ms, A10G begins showing modest benefit over A100 for small request sizes. Finally, at 100-160ms, A10G demonstrates much greater T/$ advantage over A100 for the same request sizes (up to 1.5×), yet A100 is always more cost efficient for larger requests. As demonstrated, a modification to TPOT SLO shifts the boundary within the request size space between which different GPU types are most cost effective and significantly influences the magnitude of cost efficiency differences between GPUs. As a result, both request size and SLO must be considered in tandem when determining cost efficiency.\n\nKey Takeaways: To meet strict SLOs, expensive GPUs are necessary due to the higher latency of cheaper GPUs. However, as SLO is loosened, lower-end GPUs can be used to cut deployment costs.", "original_types": ["text"], "id": 1366}
{"type": "figure", "content": "Figure 8: T/$ comparison between H100x2 and A100x2 serving Llama2-70b.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 8: T/$ comparison between H100x2 and A100x2 serving Llama2-70b.", "id": 1367}
{"type": "figure", "content": "Figure 9: GPU on-demand cost for three GPU provisioning strategies.", "doc_id": "griggs2024", "page": 6, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 9: GPU on-demand cost for three GPU provisioning strategies.", "id": 1368}
{"type": "section", "content": "4.4 Request Rate and Cost Efficiency\n\nIn this section, we investigate the relationship between request rate and GPU cost efficiency.\n\nExperiment: Fig. 9 illustrates the cost of serving Llama2-7b at a range of request rates using three strategies: A10G-only, A100-only, or a mix of both. The y-axis is absolute cost instead of T/$ because each provisioning strategy serves the same request rates and thus the same number of tokens; only the cost varies across strategies.\n\nAs request rate increases, A100-only is increasingly more cost effective than A10G-only. This is because the requests are of size [1000 in tokens, 250 out tokens], which § 4.2 shows is more cost effective on A100. However, A10G-only still presents benefits at low request rates (0-1 req/s). Periods of idleness or low activity are common in real-world services [38], and the service should right-size to a cheaper GPU (here, A10G) when a higher-end GPU (here, A100) is drastically underutilized.\n\nMixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs consistently yields the lowest deployment cost. Because A100s have such large capacity, scaling with only A100s is coarse-grained and often leads to underutilized resources. Instead, A10Gs and A100s can be mixed such that A100s satisfy the bulk of the service demands, while A10Gs handle the remaining load at reduced cost. Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.\n\nKey Takeaways: During low activity periods, LLM services should right-size to cheaper low-end GPUs. Provisioning a mix of GPU types enables finer-grained resource scaling, which better aligns the allocated GPU capacity with request load. This increases GPU utilization and consistently achieves lowest serving cost.\n\n5 Mélange: Automating Cost-Efficient GPU Selection\n\nBuilding on the observations in § 4 that request size, request rate, and SLO all jointly determine GPU cost efficiency, we present Mélange, an allocation framework that considers each of these three dimensions in-tandem to derive the minimal-cost GPU allocation that meets an LLM service’s request load while adhering to SLO constraints. Fig. 1 depicts the Mélange framework. Mélange flexibly supports any GPU type (1a) and LLM service definition (1b), uses a one-time offline profiling step to measure GPU performance (2), formulates the task of GPU allocation as a bin packing problem (3), then computes the minimal-cost GPU allocation (4).\n\n5.1 Problem Formulation", "doc_id": "griggs2024", "page": 7, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "4.4 Request Rate and Cost Efficiency\n\nIn this section, we investigate the relationship between request rate and GPU cost efficiency.\n\nExperiment: Fig. 9 illustrates the cost of serving Llama2-7b at a range of request rates using three strategies: A10G-only, A100-only, or a mix of both. The y-axis is absolute cost instead of T/$ because each provisioning strategy serves the same request rates and thus the same number of tokens; only the cost varies across strategies.\n\nAs request rate increases, A100-only is increasingly more cost effective than A10G-only. This is because the requests are of size [1000 in tokens, 250 out tokens], which § 4.2 shows is more cost effective on A100. However, A10G-only still presents benefits at low request rates (0-1 req/s). Periods of idleness or low activity are common in real-world services [38], and the service should right-size to a cheaper GPU (here, A10G) when a higher-end GPU (here, A100) is drastically underutilized.\n\nMixing GPU Types: The hybrid approach of serving the model on both A10G and A100 GPUs consistently yields the lowest deployment cost. Because A100s have such large capacity, scaling with only A100s is coarse-grained and often leads to underutilized resources. Instead, A10Gs and A100s can be mixed such that A100s satisfy the bulk of the service demands, while A10Gs handle the remaining load at reduced cost. Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.\n\nKey Takeaways: During low activity periods, LLM services should right-size to cheaper low-end GPUs. Provisioning a mix of GPU types enables finer-grained resource scaling, which better aligns the allocated GPU capacity with request load. This increases GPU utilization and consistently achieves lowest serving cost.\n\n5 Mélange: Automating Cost-Efficient GPU Selection\n\nBuilding on the observations in § 4 that request size, request rate, and SLO all jointly determine GPU cost efficiency, we present Mélange, an allocation framework that considers each of these three dimensions in-tandem to derive the minimal-cost GPU allocation that meets an LLM service’s request load while adhering to SLO constraints. Fig. 1 depicts the Mélange framework. Mélange flexibly supports any GPU type (1a) and LLM service definition (1b), uses a one-time offline profiling step to measure GPU performance (2), formulates the task of GPU allocation as a bin packing problem (3), then computes the minimal-cost GPU allocation (4).\n\n5.1 Problem Formulation", "original_types": ["text", "header"], "id": 1369}
{"type": "section", "content": "We begin by defining the key terms utilized in our problem formulation and solution. An LLM service workload is characterized by its overall request rate along with a distribution of input and output sizes. A distribution of request sizes is used rather than fixed values due to the inherent variability of LLM request sizes. Specifically, a workload is a histogram where each bucket corresponds to a range of request sizes and a bucket’s value is the request rate of requests within the bucket’s size range. The service cost is computed by summing the hourly on-demand cloud renatl rates for each of the selected GPUs. We define SLO based on average TPOT, however, Mélange can be extended to other definitions of SLO such as time to first token (TTFT).\n\nProblem Definition: Given a workload, GPU costs, and SLO requirements, our objective is to provision GPUs that can minimize deployment cost while adhering to latency SLO constraints.\n\n5.2 Inputs\n\nMélange takes as input the set of available GPU types (1a) and the LLM service definition (1b) made up of the workload profile and SLO. Each of these inputs can be modified, such as adding a new hardware accelerator or redefining SLO based on end-to-end request latency, and Mélange’s downstream components still derive the minimal-cost allocation. Due to the large diversity of hardware accelerators and LLM services, Mélange’s extensibility is critical for usability.", "doc_id": "griggs2024", "page": 7, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "We begin by defining the key terms utilized in our problem formulation and solution. An LLM service workload is characterized by its overall request rate along with a distribution of input and output sizes. A distribution of request sizes is used rather than fixed values due to the inherent variability of LLM request sizes. Specifically, a workload is a histogram where each bucket corresponds to a range of request sizes and a bucket’s value is the request rate of requests within the bucket’s size range. The service cost is computed by summing the hourly on-demand cloud renatl rates for each of the selected GPUs. We define SLO based on average TPOT, however, Mélange can be extended to other definitions of SLO such as time to first token (TTFT).\n\nProblem Definition: Given a workload, GPU costs, and SLO requirements, our objective is to provision GPUs that can minimize deployment cost while adhering to latency SLO constraints.\n\n5.2 Inputs\n\nMélange takes as input the set of available GPU types (1a) and the LLM service definition (1b) made up of the workload profile and SLO. Each of these inputs can be modified, such as adding a new hardware accelerator or redefining SLO based on end-to-end request latency, and Mélange’s downstream components still derive the minimal-cost allocation. Due to the large diversity of hardware accelerators and LLM services, Mélange’s extensibility is critical for usability.", "original_types": ["text", "header"], "id": 1370}
{"type": "section", "content": "Offline Profiling\n\nA one-time offline profiling step (2) is required to measure the performance of each GPU. For each request size bucket in the workload histogram, we gradually increase the request rate until the GPU is saturated. We record per-request TTFT and TPOT as the request rate is increased, which are sufficient metrics to capture the timing behavior of a request end-to-end [22]. Then, given an SLO, Mélange can quickly find the maximum throughput each GPU achieves across request sizes while adhering to the SLO. Empirically, the one-time profiling is not time-consuming (<1hr).\n\nAllocation Algorithm\n\nThe allocation algorithm’s (3) objective is to map the workload to a minimal-cost set of GPUs that are constrained by adhering to SLO. Our insight is that this task can be formulated as a cost-aware variant of the bin packing problem. Mélange partitions workload buckets into smaller slices for fine-grained packing, then assigns the slices (items) to GPUs (bins). We first define a slice (§ 5.4.1), compute the load of a slice (§ 5.4.2), then create the ILP formulation (§ 5.4.3).\n\nRequest Buckets and Slices\n\nA workload histogram has two dimensions, input length and output length, and each histogram bucket’s value is the aggregate request rate for requests within the bucket’s size range. We further break each bucket down into slices for finer-grained bin packing. A parameter, slice factor, indicates the number of slices that each bucket is divided into. In a setting with a slice factor of 8 and a bucket with a request rate of 4, the bucket would be segmented into 8 slices each corresponding to a request rate of 0.5 requests/s. The slice factor can be tuned to reach the desired balance between granularity and solution complexity, but we have not found overall performance to be sensitive to slice factor.\n\nLoad\n\nThe solver requires an estimate of the load of each slice to ensure that a GPU’s capacity is not exceeded and SLO is not violated. The load of a slice with request size s and rate r on GPU G is calculated as \\(\\frac{r}{MaxTput(G, s, SLO)}\\), where MaxTput(G, s, SLO) is the maximum request/s G can achieve for requests of size s while adhering to SLO. For instance, if MaxTput(G, s, SLO) = 10 reqs/s and r = 1, the load is calculated as 1/10 = 0.1 . Each GPU’s maximum capacity is defined as 1. This approximation allows us to calculate the aggregate load of slices with differing sizes and rates. Based on offline profiling, we compute MaxTput(G, s, SLO) for each bucket in the workload histogram.\n\nILP Formulation", "doc_id": "griggs2024", "page": 8, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Offline Profiling\n\nA one-time offline profiling step (2) is required to measure the performance of each GPU. For each request size bucket in the workload histogram, we gradually increase the request rate until the GPU is saturated. We record per-request TTFT and TPOT as the request rate is increased, which are sufficient metrics to capture the timing behavior of a request end-to-end [22]. Then, given an SLO, Mélange can quickly find the maximum throughput each GPU achieves across request sizes while adhering to the SLO. Empirically, the one-time profiling is not time-consuming (<1hr).\n\nAllocation Algorithm\n\nThe allocation algorithm’s (3) objective is to map the workload to a minimal-cost set of GPUs that are constrained by adhering to SLO. Our insight is that this task can be formulated as a cost-aware variant of the bin packing problem. Mélange partitions workload buckets into smaller slices for fine-grained packing, then assigns the slices (items) to GPUs (bins). We first define a slice (§ 5.4.1), compute the load of a slice (§ 5.4.2), then create the ILP formulation (§ 5.4.3).\n\nRequest Buckets and Slices\n\nA workload histogram has two dimensions, input length and output length, and each histogram bucket’s value is the aggregate request rate for requests within the bucket’s size range. We further break each bucket down into slices for finer-grained bin packing. A parameter, slice factor, indicates the number of slices that each bucket is divided into. In a setting with a slice factor of 8 and a bucket with a request rate of 4, the bucket would be segmented into 8 slices each corresponding to a request rate of 0.5 requests/s. The slice factor can be tuned to reach the desired balance between granularity and solution complexity, but we have not found overall performance to be sensitive to slice factor.\n\nLoad\n\nThe solver requires an estimate of the load of each slice to ensure that a GPU’s capacity is not exceeded and SLO is not violated. The load of a slice with request size s and rate r on GPU G is calculated as \\(\\frac{r}{MaxTput(G, s, SLO)}\\), where MaxTput(G, s, SLO) is the maximum request/s G can achieve for requests of size s while adhering to SLO. For instance, if MaxTput(G, s, SLO) = 10 reqs/s and r = 1, the load is calculated as 1/10 = 0.1 . Each GPU’s maximum capacity is defined as 1. This approximation allows us to calculate the aggregate load of slices with differing sizes and rates. Based on offline profiling, we compute MaxTput(G, s, SLO) for each bucket in the workload histogram.\n\nILP Formulation", "original_types": ["text", "header"], "id": 1371}
{"type": "section", "content": "We formulate the ILP with two decision variables. First, let A be a matrix \\(\\{0, 1\\}^{N \times M}\\), where N is the number of slices, and M is the number of GPU types. A_{i, j} = 1\\) if slice i is assigned to GPU type j, and 0 otherwise. The second decision variable, B, is a vector \\(\\mathbb{Z}_{\\geq 0}^{M}\\) of non-negative integers, where \\(B_{j}\\) specifies the number of GPUs of type j to be allocated. L is a matrix of size \\(N \times M\\) where \\(L_{i, j} \\in [0, 1]\\) is the fractional load of slice i on GPU type j. L is computed offline by the process described in § 5.4.2. \\(c_{j}\\) denotes the cost of GPU type j.\n\nOur objective is to minimize the total GPU allocation cost:\n\n\\(\\arg \\min_{B} (\\sum_{j=1}^{M} B_{j} \\cdot c_{j})\n\n\\(\\forall i \\in \\{1, \\ldots, N\\}, \\quad \\sum_{j=1}^{M} A_{i, j} = 1\n\n\\(\\forall j \\in \\{1, \\ldots, M\\}, \\quad \\sum_{i=1}^{N} A_{i, j} \\cdot L_{i, j} \\leq B_{j}\n\n\\(\\forall i, \\forall j, \\quad A_{i, j} \\in \\{0, 1\\}\n\n\\(\\forall j \\in \\{1, \\ldots, M\\}, \\quad B_{j} \\geq 0\n\nThe solution is computed using an off-the-shelf solver [29]. Upon solution, the decision variable B holds the minimal-cost GPU allocation (4) that meets the workload demand and adheres to SLO.", "doc_id": "griggs2024", "page": 8, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "We formulate the ILP with two decision variables. First, let A be a matrix \\(\\{0, 1\\}^{N \times M}\\), where N is the number of slices, and M is the number of GPU types. A_{i, j} = 1\\) if slice i is assigned to GPU type j, and 0 otherwise. The second decision variable, B, is a vector \\(\\mathbb{Z}_{\\geq 0}^{M}\\) of non-negative integers, where \\(B_{j}\\) specifies the number of GPUs of type j to be allocated. L is a matrix of size \\(N \times M\\) where \\(L_{i, j} \\in [0, 1]\\) is the fractional load of slice i on GPU type j. L is computed offline by the process described in § 5.4.2. \\(c_{j}\\) denotes the cost of GPU type j.\n\nOur objective is to minimize the total GPU allocation cost:\n\n\\(\\arg \\min_{B} (\\sum_{j=1}^{M} B_{j} \\cdot c_{j})\n\n\\(\\forall i \\in \\{1, \\ldots, N\\}, \\quad \\sum_{j=1}^{M} A_{i, j} = 1\n\n\\(\\forall j \\in \\{1, \\ldots, M\\}, \\quad \\sum_{i=1}^{N} A_{i, j} \\cdot L_{i, j} \\leq B_{j}\n\n\\(\\forall i, \\forall j, \\quad A_{i, j} \\in \\{0, 1\\}\n\n\\(\\forall j \\in \\{1, \\ldots, M\\}, \\quad B_{j} \\geq 0\n\nThe solution is computed using an off-the-shelf solver [29]. Upon solution, the decision variable B holds the minimal-cost GPU allocation (4) that meets the workload demand and adheres to SLO.", "original_types": ["text"], "id": 1372}
{"type": "section", "content": "6 Evaluation\n\nWe assess Mélange’s performance across diverse hardware, request sizes, rates, and SLOs. Mélange consistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies, and the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n\n6.1 Experiment Setup\n\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications, with details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To determine the GPU cost, we select the lowest on-demand price available from major cloud providers (AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to the pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons, we normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM 0.2.7 [19].", "doc_id": "griggs2024", "page": 9, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "6 Evaluation\n\nWe assess Mélange’s performance across diverse hardware, request sizes, rates, and SLOs. Mélange consistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies, and the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n\n6.1 Experiment Setup\n\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications, with details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To determine the GPU cost, we select the lowest on-demand price available from major cloud providers (AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to the pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons, we normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM 0.2.7 [19].", "original_types": ["text", "header"], "id": 1373}
{"type": "table", "content": "Table 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\n```markdown\n| Type | L4 | A10G (PCIe) | A100-80G (SXM) | H100 (SXM) |\n|------|----|------------|--------------|------------|\n| On-demand Price ($/h) | 0.7 | 1.01       | 3.67         | 7.516^4    |\n| Instance Provider | GCP | AWS        | Azure        | RunPod     |\n| Instance Name | g2-standard-4 | g5.xlarge | NC24ads_A100_v4/N.A. | N.A.      |\n| Memory (GB) | 24 | 24         | 80           | 80         |\n| Memory Bandwidth (GB/s) | 300 | 600        | 1935         | 3350       |\n| FP16 (TFLOPS) | 242 | 125        | 312           | 1979       |\n```", "doc_id": "griggs2024", "page": 9, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\n```markdown\n| Type | L4 | A10G (PCIe) | A100-80G (SXM) | H100 (SXM) |\n|------|----|------------|--------------|------------|\n| On-demand Price ($/h) | 0.7 | 1.01       | 3.67         | 7.516^4    |\n| Instance Provider | GCP | AWS        | Azure        | RunPod     |\n| Instance Name | g2-standard-4 | g5.xlarge | NC24ads_A100_v4/N.A. | N.A.      |\n| Memory (GB) | 24 | 24         | 80           | 80         |\n| Memory Bandwidth (GB/s) | 300 | 600        | 1935         | 3350       |\n| FP16 (TFLOPS) | 242 | 125        | 312           | 1979       |\n```", "id": 1374}
{"type": "section", "content": "Datasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios. For short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context tasks (document summarization) we use the PubMed dataset [7], and for a mixed-context-length setting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed. The input and output length distributions are shown in Fig. 10. We follow standard LLM inference benchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift responses are essential, and 120ms where longer response times are acceptable. Both selected SLOs surpass the average human reading speed, ensuring the SLOs satisfy practical user experience.", "doc_id": "griggs2024", "page": 9, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Datasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios. For short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context tasks (document summarization) we use the PubMed dataset [7], and for a mixed-context-length setting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed. The input and output length distributions are shown in Fig. 10. We follow standard LLM inference benchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift responses are essential, and 120ms where longer response times are acceptable. Both selected SLOs surpass the average human reading speed, ensuring the SLOs satisfy practical user experience.", "original_types": ["header"], "id": 1375}
{"type": "figure", "content": "Figure 10: Dataset input and output length distributions.", "doc_id": "griggs2024", "page": 9, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 10: Dataset input and output length distributions.", "id": 1376}
{"type": "section", "content": "Mélange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length ranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of 60 · 8 = 480 slices.\n\nBaselines. We compare Mélange to allocations that use a single GPU type. To derive baseline allocations, we use Mélange’s ILP formulation (§ 5.4.3) but restrict the solver to a single GPU type.", "doc_id": "griggs2024", "page": 9, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Mélange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length ranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of 60 · 8 = 480 slices.\n\nBaselines. We compare Mélange to allocations that use a single GPU type. To derive baseline allocations, we use Mélange’s ILP formulation (§ 5.4.3) but restrict the solver to a single GPU type.", "original_types": ["header"], "id": 1377}
{"type": "section", "content": "6.2 Cost Savings Analysis\n\nWe compare the deployment costs of Mélange to the single-GPU-type baselines across datasets and SLOs. Fig. 11 displays costs normalized against the cost of Mélange (purple dotted lines), and the detailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only baselines are only included for the Arena dataset because the PubMed and Mixed datasets contain large requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in Mélange’s allocation but are limited to serving requests smaller than 12,000 tokens. We now discuss each dataset in detail:", "doc_id": "griggs2024", "page": 10, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "6.2 Cost Savings Analysis\n\nWe compare the deployment costs of Mélange to the single-GPU-type baselines across datasets and SLOs. Fig. 11 displays costs normalized against the cost of Mélange (purple dotted lines), and the detailed GPU allocations and cost savings are included in App. C. The A10G-only and L4-only baselines are only included for the Arena dataset because the PubMed and Mixed datasets contain large requests that exceed A10G and L4’s GPU memory capacity. L4 and A10G are included in Mélange’s allocation but are limited to serving requests smaller than 12,000 tokens. We now discuss each dataset in detail:", "original_types": ["text", "header"], "id": 1378}
{"type": "figure", "content": "Figure 11: Deployment cost across different datasets and SLOs.", "doc_id": "griggs2024", "page": 10, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 11: Deployment cost across different datasets and SLOs.", "id": 1379}
{"type": "section", "content": "6.3 SLO Satisfaction\n\nNext, we assess Mélange adherence to TPOT SLOs. We provision cloud GPU instances based on Mélange’s allocation for each dataset and SLO at a rate of 4 req/s. We deploy Llama-2-7b on each GPU and sample requests randomly from the chosen dataset to serve 2K total requests. We record the average TPOT for each request.", "doc_id": "griggs2024", "page": 11, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "6.3 SLO Satisfaction\n\nNext, we assess Mélange adherence to TPOT SLOs. We provision cloud GPU instances based on Mélange’s allocation for each dataset and SLO at a rate of 4 req/s. We deploy Llama-2-7b on each GPU and sample requests randomly from the chosen dataset to serve 2K total requests. We record the average TPOT for each request.", "original_types": ["text", "header"], "id": 1380}
{"type": "figure", "content": "Figure 12: Mélange TPOT CDFs.", "doc_id": "griggs2024", "page": 11, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Figure 12: Mélange TPOT CDFs.", "id": 1381}
{"type": "section", "content": "Fig. 12 presents CDFs of the observed per-request average TPOTs across experiments. With an SLO of 120ms, over 99.95% of requests met SLO. When the SLO was tightened to 40ms, 99.5% of requests met SLO. These results validate Mélange’s ability to choose GPU allocations that meet workload demand, however, we recognize that services may require even higher SLO adherence, so we investigated the source of SLO violations in our experiment.\n\n6.4 Solver Time\n\nWe detail the solver execution time in Tab. 2. Across all datasets and request rates, the solver’s execution time remains under 1.2 seconds, which is negligible compared to service lifetime. We observe a modest increase in solver time with higher request volumes due to greater complexity in slice assignment. However, this increase is empirically sub-linear relative to the increase in request rate, and the solver’s execution time remains practical.\n\n7 Limitations and Conclusion\n\nLimitations. Mélange derives the optimal GPU allocation for a fixed workload distribution and request rate, but does not address other deployment challenges such as GPU unavailability or auto-scaling for dynamic request rates and size distributions. Mélange is only intended to make allocation decisions, a key component to be plugged into a broader serving system that handles these deployment challenges. Given the vast number of LLM deployment configurations (quantization and compression, disaggregated prefill, speculative decoding), we have not exhaustively evaluated each setting. We expect, however, that Mélange’s framework is flexible to support each of these settings.\n\nConclusion. We introduce Mélange, a framework for deriving the minimal-cost GPU allocation for a given LLM service. Mélange is based on our analysis of GPU cost efficiency, which identifies three key service characteristics (request sizes, request rates, and SLOs) as significant influences on cost efficiency. We formulate the GPU allocation task as a cost-aware bin packing problem that accounts for each service characteristic, enabling flexibility and heterogeneity-awareness. In evaluations on a range of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates significant reductions in deployment costs (up to 77%) while providing high SLO attainment.", "doc_id": "griggs2024", "page": 11, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Fig. 12 presents CDFs of the observed per-request average TPOTs across experiments. With an SLO of 120ms, over 99.95% of requests met SLO. When the SLO was tightened to 40ms, 99.5% of requests met SLO. These results validate Mélange’s ability to choose GPU allocations that meet workload demand, however, we recognize that services may require even higher SLO adherence, so we investigated the source of SLO violations in our experiment.\n\n6.4 Solver Time\n\nWe detail the solver execution time in Tab. 2. Across all datasets and request rates, the solver’s execution time remains under 1.2 seconds, which is negligible compared to service lifetime. We observe a modest increase in solver time with higher request volumes due to greater complexity in slice assignment. However, this increase is empirically sub-linear relative to the increase in request rate, and the solver’s execution time remains practical.\n\n7 Limitations and Conclusion\n\nLimitations. Mélange derives the optimal GPU allocation for a fixed workload distribution and request rate, but does not address other deployment challenges such as GPU unavailability or auto-scaling for dynamic request rates and size distributions. Mélange is only intended to make allocation decisions, a key component to be plugged into a broader serving system that handles these deployment challenges. Given the vast number of LLM deployment configurations (quantization and compression, disaggregated prefill, speculative decoding), we have not exhaustively evaluated each setting. We expect, however, that Mélange’s framework is flexible to support each of these settings.\n\nConclusion. We introduce Mélange, a framework for deriving the minimal-cost GPU allocation for a given LLM service. Mélange is based on our analysis of GPU cost efficiency, which identifies three key service characteristics (request sizes, request rates, and SLOs) as significant influences on cost efficiency. We formulate the GPU allocation task as a cost-aware bin packing problem that accounts for each service characteristic, enabling flexibility and heterogeneity-awareness. In evaluations on a range of GPUs, request sizes, request rates, and latency SLOs, Mélange consistently demonstrates significant reductions in deployment costs (up to 77%) while providing high SLO attainment.", "original_types": ["text", "header"], "id": 1382}
{"type": "section", "content": "References\n\n[1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369, 2023.\n\n[2] Ahsan Ali, Riccardo Pincioli, Feng Yan, and Evgenia Smirni. Optimizing inference serving on serverless platforms. Proceedings of the VLDB Endowment, 15(10), 2022.\n\n[3] AnyScale. Anyscale: Llmperf leaderboard. https://github.com/ray-project/llmperf-leaderboard, 2024. [Accessed 13-03-2024].\n\n[4] AWS. Ai accelerator-aws trainium. https://aws.amazon.com/machine-learning/trainium/, 2020. [Accessed 14-03-2024].\n\n[5] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models. arXiv preprint arXiv:2209.01188, 2022.\n\n[6] Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, and Srinidhi Viswanatha. Balancing efficiency and fairness in heterogeneous gpu clusters for deep learning. In Proceedings of the Fifteenth European Conference on Computer Systems, pages 1–16, 2020.\n\n[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.\n\n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.\n\n[9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.\n\n[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\n[11] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R Das. Cocktail: A multidimensional optimization for model serving in cloud. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), pages 1041–1057, 2022.\n\n[12] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, and Phillip B Gibbons. Tributary: spot-dancing for elastic services with latency {SLOs}. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pages 1–14, 2018.\n\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n\n[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024.", "doc_id": "griggs2024", "page": 12, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "References\n\n[1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369, 2023.\n\n[2] Ahsan Ali, Riccardo Pincioli, Feng Yan, and Evgenia Smirni. Optimizing inference serving on serverless platforms. Proceedings of the VLDB Endowment, 15(10), 2022.\n\n[3] AnyScale. Anyscale: Llmperf leaderboard. https://github.com/ray-project/llmperf-leaderboard, 2024. [Accessed 13-03-2024].\n\n[4] AWS. Ai accelerator-aws trainium. https://aws.amazon.com/machine-learning/trainium/, 2020. [Accessed 14-03-2024].\n\n[5] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models. arXiv preprint arXiv:2209.01188, 2022.\n\n[6] Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, and Srinidhi Viswanatha. Balancing efficiency and fairness in heterogeneous gpu clusters for deep learning. In Proceedings of the Fifteenth European Conference on Computer Systems, pages 1–16, 2020.\n\n[7] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.\n\n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.\n\n[9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.\n\n[10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\n[11] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R Das. Cocktail: A multidimensional optimization for model serving in cloud. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), pages 1041–1057, 2022.\n\n[12] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R Ganger, and Phillip B Gibbons. Tributary: spot-dancing for elastic services with latency {SLOs}. In 2018 USENIX Annual Technical Conference (USENIX ATC 18), pages 1–14, 2018.\n\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n\n[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024.", "original_types": ["text", "header"], "id": 1383}
{"type": "section", "content": "[15] Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang Yuan. Hexgen: Generative inference of foundation model over heterogeneous decentralized environment. arXiv preprint arXiv:2311.11514, 2023.\n\n[16] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during generative inference for higher throughput. Advances in Neural Information Processing Systems, 36, 2024.", "doc_id": "griggs2024", "page": 12, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[15] Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, and Binhang Yuan. Hexgen: Generative inference of foundation model over heterogeneous decentralized environment. arXiv preprint arXiv:2311.11514, 2023.\n\n[16] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during generative inference for higher throughput. Advances in Neural Information Processing Systems, 36, 2024.", "original_types": ["text"], "id": 1384}
{"type": "section", "content": "[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1–12, 2017.\n\n[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36, 2024.\n\n[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023.\n\n[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.\n\n[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n\n[22] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes: Defining and enhancing quality-of-experience in llm-based text streaming services. arXiv preprint arXiv:2404.16283, 2024.\n\n[23] Liang Luo, Peter West, Pratyush Patel, Arvind Krishnamurthy, and Luis Ceze. Srifty: Swift and thrifty distributed neural network training on the cloud. Proceedings of Machine Learning and Systems, 4:833–847, 2022.\n\n[24] Yusuf Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web, 2023. Accessed: 2024-02-21.\n\n[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia. Spotserve: Serving generative large language models on preemptible instances. arXiv preprint arXiv:2311.15566, 2023.\n\n[26] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia. Sdpipe: A semi-decentralized framework for heterogeneity-aware pipeline-parallel training. Proceedings of the VLDB Endowment, 16(9):2354–2363, 2023.\n\n[27] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. Galvatron: Efficient transformer training over multiple gpus using automatic parallelism. arXiv preprint arXiv:2211.13878, 2022.\n\n[28] Microsoft. Copilot, 2023. Accessed: 2024-02-21.\n\n[29] Stuart Mitchell. PuLP: A linear programming toolkit for python. https://github.com/coin-or/pulp, 2023. Accessed: 2024-02-25.\n\n[30] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei Zaharia. {Heterogeneity-Aware} cluster scheduling policies for deep learning workloads. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 481–498, 2020.\n\n[31] Nvidia. A10 gpu spec, 2024. Accessed: 2024-03-10.", "doc_id": "griggs2024", "page": 13, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[17] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1–12, 2017.\n\n[18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36, 2024.\n\n[19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023.\n\n[20] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.\n\n[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n\n[22] Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes: Defining and enhancing quality-of-experience in llm-based text streaming services. arXiv preprint arXiv:2404.16283, 2024.\n\n[23] Liang Luo, Peter West, Pratyush Patel, Arvind Krishnamurthy, and Luis Ceze. Srifty: Swift and thrifty distributed neural network training on the cloud. Proceedings of Machine Learning and Systems, 4:833–847, 2022.\n\n[24] Yusuf Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web, 2023. Accessed: 2024-02-21.\n\n[25] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin Cui, and Zhihao Jia. Spotserve: Serving generative large language models on preemptible instances. arXiv preprint arXiv:2311.15566, 2023.\n\n[26] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia. Sdpipe: A semi-decentralized framework for heterogeneity-aware pipeline-parallel training. Proceedings of the VLDB Endowment, 16(9):2354–2363, 2023.\n\n[27] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. Galvatron: Efficient transformer training over multiple gpus using automatic parallelism. arXiv preprint arXiv:2211.13878, 2022.\n\n[28] Microsoft. Copilot, 2023. Accessed: 2024-02-21.\n\n[29] Stuart Mitchell. PuLP: A linear programming toolkit for python. https://github.com/coin-or/pulp, 2023. Accessed: 2024-02-25.\n\n[30] Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar Phanishayee, and Matei Zaharia. {Heterogeneity-Aware} cluster scheduling policies for deep learning workloads. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 481–498, 2020.\n\n[31] Nvidia. A10 gpu spec, 2024. Accessed: 2024-03-10.", "original_types": ["text"], "id": 1385}
{"type": "section", "content": "[32] Nvidia. A100 gpu spec, 2024. Accessed: 2024-03-10.\n\n[33] Nvidia. Gpus, 2024. Accessed: 2024-03-10.\n\n[34] OpenAI. Chatgpt, 2022. Accessed: 2024-02-21.\n\n[35] OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.", "doc_id": "griggs2024", "page": 13, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[32] Nvidia. A100 gpu spec, 2024. Accessed: 2024-03-10.\n\n[33] Nvidia. Gpus, 2024. Accessed: 2024-03-10.\n\n[34] OpenAI. Chatgpt, 2022. Accessed: 2024-02-21.\n\n[35] OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.", "original_types": ["text"], "id": 1386}
{"type": "section", "content": "[36] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. arXiv preprint arXiv:2311.18677, 2023.\n\n[37] Elizabeth Reid. Supercharging search with generative ai, 2023. Accessed: 2024-02-21.\n\n[38] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. {INFaaS}: Automated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 397–411, 2021.\n\n[39] RunPod. Runpod, 2024. Accessed: 2024-02-24.\n\n[40] FlashInfer team. Accelerating self-attentions for llm serving with flashinfer, 2024. Accessed: 2024-02-24.\n\n[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR), pages 2464–2469. IEEE, 2016.\n\n[42] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang, Ravi Netravali, and Guoqing Harry Xu. Bamboo: Making preemptible instances resilient for affordable training of large {DNNs}. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 497–513, 2023.\n\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[45] Abhi Venigalla. Databricks: Training llms at scale with amd mi250 gpus. https://www.databricks.com/blog/training-llms-scale-amd-mi250-gpus, 2023. [Accessed 14-03-2024].\n\n[46] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920, 2023.\n\n[47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\n\n[48] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. An empirical study on challenging math problem solving with gpt-4. In ArXiv preprint arXiv:2306.01337, 2023.\n\n[49] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023.", "doc_id": "griggs2024", "page": 14, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[36] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. arXiv preprint arXiv:2311.18677, 2023.\n\n[37] Elizabeth Reid. Supercharging search with generative ai, 2023. Accessed: 2024-02-21.\n\n[38] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. {INFaaS}: Automated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 397–411, 2021.\n\n[39] RunPod. Runpod, 2024. Accessed: 2024-02-24.\n\n[40] FlashInfer team. Accelerating self-attentions for llm serving with flashinfer, 2024. Accessed: 2024-02-24.\n\n[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR), pages 2464–2469. IEEE, 2016.\n\n[42] John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang, Ravi Netravali, and Guoqing Harry Xu. Bamboo: Making preemptible instances resilient for affordable training of large {DNNs}. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 497–513, 2023.\n\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n\n[45] Abhi Venigalla. Databricks: Training llms at scale with amd mi250 gpus. https://www.databricks.com/blog/training-llms-scale-amd-mi250-gpus, 2023. [Accessed 14-03-2024].\n\n[46] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920, 2023.\n\n[47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\n\n[48] Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. An empirical study on challenging math problem solving with gpt-4. In ArXiv preprint arXiv:2306.01337, 2023.\n\n[49] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023.", "original_types": ["text"], "id": 1387}
{"type": "section", "content": "[50] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168–27183, 2022.\n\n[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n\n[52] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020.", "doc_id": "griggs2024", "page": 14, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[50] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168–27183, 2022.\n\n[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538, 2022.\n\n[52] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020.", "original_types": ["text"], "id": 1388}
{"type": "section", "content": "[53] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. {MArk}: Exploiting cloud services for {Cost-Effective},{SLO-Aware} machine learning inference serving. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pages 1049–1062, 2019.\n\n[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n[56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large language models using sglang. arXiv preprint arXiv:2312.07104, 2023.\n\n[57] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline. Advances in Neural Information Processing Systems, 36, 2024.\n\n[58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. arXiv preprint arXiv:2401.09670, 2024.\n\n[59] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33:18330–18341, 2020.\n\n[60] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao. Towards optimal caching and model selection for large model inference. Advances in Neural Information Processing Systems, 36, 2024.\n\nExperiment Setup\n\nA.1 Dataset\n\nWe test Mélange’s performance on three different datasets listed below:", "doc_id": "griggs2024", "page": 15, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "[53] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. {MArk}: Exploiting cloud services for {Cost-Effective},{SLO-Aware} machine learning inference serving. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pages 1049–1062, 2019.\n\n[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024.\n\n[55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n\n[56] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large language models using sglang. arXiv preprint arXiv:2312.07104, 2023.\n\n[57] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline. Advances in Neural Information Processing Systems, 36, 2024.\n\n[58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. arXiv preprint arXiv:2401.09670, 2024.\n\n[59] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33:18330–18341, 2020.\n\n[60] Banghua Zhu, Ying Sheng, Lianmin Zheng, Clark Barrett, Michael Jordan, and Jiantao Jiao. Towards optimal caching and model selection for large model inference. Advances in Neural Information Processing Systems, 36, 2024.\n\nExperiment Setup\n\nA.1 Dataset\n\nWe test Mélange’s performance on three different datasets listed below:", "original_types": ["text", "header"], "id": 1389}
{"type": "section", "content": "B Solver Time\n\nWe present the solver execution time from each experiment in Table 2.", "doc_id": "griggs2024", "page": 16, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "B Solver Time\n\nWe present the solver execution time from each experiment in Table 2.", "original_types": ["text", "header"], "id": 1390}
{"type": "table", "content": "Table 3: Instance allocations for the short-context Arena dataset, SLO=120ms.\nRate (req/s) | Solver | L4 | A10G | A100 | H100 | Norm. Cost ($/hr) | Savings\n\n1 | Mélange | 1 | 1 | 1 | 1 | 1.71 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 77.25%\nA100-only | 1 | 1 | 1 | 1 | 3.67 | 53.41%\nA10G-only | 2 | 2 | 2 | 2 | 2.02 | 15.35%\nL4-only | 3 | 3 | 3 | 3 | 2.1 | 18.57%\n\n2 | Mélange | 2 | 1 | 1 | 1 | 2.41 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 67.94%\nA100-only | 1 | 1 | 1 | 1 | 3.67 | 34.33%\nA10G-only | 3 | 3 | 3 | 3 | 3.03 | 20.46%\nL4-only | 5 | 5 | 5 | 5 | 3.5 | 31.14%\n\n4 | Mélange | 1 | 1 | 1 | 1 | 4.37 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 41.86%\nA100-only | 1 | 1 | 1 | 1 | 7.34 | 40.46%\nA10G-only | 6 | 6 | 6 | 6 | 6.06 | 27.89%\nL4-only | 9 | 9 | 9 | 9 | 6.3 | 30.63%\n\n8 | Mélange | 1 | 3 | 1 | 2 | 7.4 | N/A\nH100-only | 1 | 3 | 1 | 2 | 15.032 | 50.77%\nA100-only | 1 | 3 | 1 | 2 | 11.01 | 32.79%\nA10G-only | 11 | 11 | 11 | 11 | 11.1 | 33.39%\nL4-only | 17 | 17 | 17 | 17 | 11.9 | 37.82%\n\n16 | Mélange | 2 | 2 | 3 | 4 | 14.43 | N/A\nH100-only | 2 | 2 | 3 | 4 | 30.064 | 52.00%\nA100-only | 2 | 2 | 3 | 4 | 22.02 | 34.47%\nA10G-only | 20 | 20 | 20 | 20 | 20.2 | 28.56%\nL4-only | 33 | 33 | 33 | 33 | 23.1 | 37.53%\n\n32 | Mélange | 2 | 6 | 5 | 8 | 25.81 | N/A\nH100-only | 2 | 6 | 5 | 8 | 60.128 | 57.07%\nA100-only | 2 | 6 | 5 | 8 | 33.03 | 21.86%\nA10G-only | 39 | 39 | 39 | 39 | 39.39 | 34.48%\nL4-only | 65 | 65 | 65 | 65 | 45.5 | 43.27%", "doc_id": "griggs2024", "page": 17, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 3: Instance allocations for the short-context Arena dataset, SLO=120ms.\nRate (req/s) | Solver | L4 | A10G | A100 | H100 | Norm. Cost ($/hr) | Savings\n\n1 | Mélange | 1 | 1 | 1 | 1 | 1.71 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 77.25%\nA100-only | 1 | 1 | 1 | 1 | 3.67 | 53.41%\nA10G-only | 2 | 2 | 2 | 2 | 2.02 | 15.35%\nL4-only | 3 | 3 | 3 | 3 | 2.1 | 18.57%\n\n2 | Mélange | 2 | 1 | 1 | 1 | 2.41 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 67.94%\nA100-only | 1 | 1 | 1 | 1 | 3.67 | 34.33%\nA10G-only | 3 | 3 | 3 | 3 | 3.03 | 20.46%\nL4-only | 5 | 5 | 5 | 5 | 3.5 | 31.14%\n\n4 | Mélange | 1 | 1 | 1 | 1 | 4.37 | N/A\nH100-only | 1 | 1 | 1 | 1 | 7.516 | 41.86%\nA100-only | 1 | 1 | 1 | 1 | 7.34 | 40.46%\nA10G-only | 6 | 6 | 6 | 6 | 6.06 | 27.89%\nL4-only | 9 | 9 | 9 | 9 | 6.3 | 30.63%\n\n8 | Mélange | 1 | 3 | 1 | 2 | 7.4 | N/A\nH100-only | 1 | 3 | 1 | 2 | 15.032 | 50.77%\nA100-only | 1 | 3 | 1 | 2 | 11.01 | 32.79%\nA10G-only | 11 | 11 | 11 | 11 | 11.1 | 33.39%\nL4-only | 17 | 17 | 17 | 17 | 11.9 | 37.82%\n\n16 | Mélange | 2 | 2 | 3 | 4 | 14.43 | N/A\nH100-only | 2 | 2 | 3 | 4 | 30.064 | 52.00%\nA100-only | 2 | 2 | 3 | 4 | 22.02 | 34.47%\nA10G-only | 20 | 20 | 20 | 20 | 20.2 | 28.56%\nL4-only | 33 | 33 | 33 | 33 | 23.1 | 37.53%\n\n32 | Mélange | 2 | 6 | 5 | 8 | 25.81 | N/A\nH100-only | 2 | 6 | 5 | 8 | 60.128 | 57.07%\nA100-only | 2 | 6 | 5 | 8 | 33.03 | 21.86%\nA10G-only | 39 | 39 | 39 | 39 | 39.39 | 34.48%\nL4-only | 65 | 65 | 65 | 65 | 45.5 | 43.27%", "id": 1391}
{"type": "table", "content": "Table 4: Instance allocations for the long-context PubMed dataset, SLO=120ms.\nMarkdown representation of the table", "doc_id": "griggs2024", "page": 18, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 4: Instance allocations for the long-context PubMed dataset, SLO=120ms.\nMarkdown representation of the table", "id": 1392}
{"type": "table", "content": "Table 5: Instance allocations for the mixed context dataset, SLO=120ms.\nMarkdown representation of the table", "doc_id": "griggs2024", "page": 18, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 5: Instance allocations for the mixed context dataset, SLO=120ms.\nMarkdown representation of the table", "id": 1393}
{"type": "table", "content": "Table 6: Instance allocations for the short-context Arena dataset, SLO=40ms.\n```markdown\n| Rate | Solver | L4 | A10G | A100 | H100 | Norm. Cost ($/hr) | Savings |\n|------|-------|----|------|------|------|------------------|--------|\n| 1    | Mélange | 2  | 1    | 1    | 1    | 2.41             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 67.94% |\n|      | A100-only | 1  | 1    | 1    | 1    | 3.67             | 34.33% |\n|      | A10G-only | 3  | 1    | 1    | 1    | 3.03             | 20.46% |\n|      | L4-only   | 5  | 1    | 1    | 1    | 3.5              | 31.14% |\n\n| 2    | Mélange | 1  | 1    | 1    | 1    | 3.67             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 51.17% |\n|      | A100-only | 1  | 1    | 1    | 1    | 3.67             | 0.00%  |\n|      | A10G-only | 5  | 1    | 1    | 1    | 5.05             | 27.33% |\n|      | L4-only   | 9  | 1    | 1    | 1    | 6.3              | 41.75% |\n\n| 4    | Mélange | 1  | 1    | 1    | 1    | 5.38             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 28.42% |\n|      | A100-only | 1  | 1    | 1    | 2    | 7.34             | 26.70% |\n|      | A10G-only | 10 | 1    | 1    | 1    | 10.1             | 46.73% |\n|      | L4-only   | 17 | 1    | 1    | 1    | 11.9             | 54.79% |\n\n| 8    | Mélange | 1  | 1    | 2    | 3    | 9.05             | N/A    |\n|      | H100-only | 1  | 1    | 2    | 3    | 15.032           | 39.80% |\n|      | A100-only | 1  | 1    | 3    | 3    | 11.01            | 17.80% |\n|      | A10G-only | 16 | 1    | 1    | 1    | 16.16            | 44.00% |\n|      | L4-only   | 34 | 1    | 1    | 1    | 23.8             | 61.97% |\n\n| 16   | Mélange | 6  | 3    | 4    | 4    | 17.07            | N/A    |\n|      | H100-only | 3  | 6    | 6    | 6    | 30.064           | 43.22% |\n|      | A100-only | 6  | 6    | 6    | 6    | 22.02            | 22.48% |\n|      | A10G-only | 40 | 6    | 6    | 6    | 40.4             | 57.75% |\n|      | L4-only   | 68 | 6    | 6    | 6    | 47.6             | 64.14% |\n\n| 32   | Mélange | 8  | 6    | 7    | 7    | 30.1             | N/A    |\n|      | H100-only | 6  | 6    | 9    | 9    | 52.612           | 42.79% |\n|      | A100-only | 6  | 6    | 9    | 9    | 33.03            | 8.87%  |\n|      | A10G-only | 80 | 6    | 9    | 9    | 80.8             | 62.75% |\n|      | L4-only   | 135| 6    | 9    | 9    | 94.5             | 68.15% |\n```", "doc_id": "griggs2024", "page": 19, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 6: Instance allocations for the short-context Arena dataset, SLO=40ms.\n```markdown\n| Rate | Solver | L4 | A10G | A100 | H100 | Norm. Cost ($/hr) | Savings |\n|------|-------|----|------|------|------|------------------|--------|\n| 1    | Mélange | 2  | 1    | 1    | 1    | 2.41             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 67.94% |\n|      | A100-only | 1  | 1    | 1    | 1    | 3.67             | 34.33% |\n|      | A10G-only | 3  | 1    | 1    | 1    | 3.03             | 20.46% |\n|      | L4-only   | 5  | 1    | 1    | 1    | 3.5              | 31.14% |\n\n| 2    | Mélange | 1  | 1    | 1    | 1    | 3.67             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 51.17% |\n|      | A100-only | 1  | 1    | 1    | 1    | 3.67             | 0.00%  |\n|      | A10G-only | 5  | 1    | 1    | 1    | 5.05             | 27.33% |\n|      | L4-only   | 9  | 1    | 1    | 1    | 6.3              | 41.75% |\n\n| 4    | Mélange | 1  | 1    | 1    | 1    | 5.38             | N/A    |\n|      | H100-only | 1  | 1    | 1    | 1    | 7.516            | 28.42% |\n|      | A100-only | 1  | 1    | 1    | 2    | 7.34             | 26.70% |\n|      | A10G-only | 10 | 1    | 1    | 1    | 10.1             | 46.73% |\n|      | L4-only   | 17 | 1    | 1    | 1    | 11.9             | 54.79% |\n\n| 8    | Mélange | 1  | 1    | 2    | 3    | 9.05             | N/A    |\n|      | H100-only | 1  | 1    | 2    | 3    | 15.032           | 39.80% |\n|      | A100-only | 1  | 1    | 3    | 3    | 11.01            | 17.80% |\n|      | A10G-only | 16 | 1    | 1    | 1    | 16.16            | 44.00% |\n|      | L4-only   | 34 | 1    | 1    | 1    | 23.8             | 61.97% |\n\n| 16   | Mélange | 6  | 3    | 4    | 4    | 17.07            | N/A    |\n|      | H100-only | 3  | 6    | 6    | 6    | 30.064           | 43.22% |\n|      | A100-only | 6  | 6    | 6    | 6    | 22.02            | 22.48% |\n|      | A10G-only | 40 | 6    | 6    | 6    | 40.4             | 57.75% |\n|      | L4-only   | 68 | 6    | 6    | 6    | 47.6             | 64.14% |\n\n| 32   | Mélange | 8  | 6    | 7    | 7    | 30.1             | N/A    |\n|      | H100-only | 6  | 6    | 9    | 9    | 52.612           | 42.79% |\n|      | A100-only | 6  | 6    | 9    | 9    | 33.03            | 8.87%  |\n|      | A10G-only | 80 | 6    | 9    | 9    | 80.8             | 62.75% |\n|      | L4-only   | 135| 6    | 9    | 9    | 94.5             | 68.15% |\n```", "id": 1394}
{"type": "table", "content": "Table 7: Instance allocations for the long-context PubMed dataset, SLO=40ms.\nMarkdown representation of the table", "doc_id": "griggs2024", "page": 20, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 7: Instance allocations for the long-context PubMed dataset, SLO=40ms.\nMarkdown representation of the table", "id": 1395}
{"type": "table", "content": "Table 8: Instance allocations for the mixed long/short context dataset, SLO=40ms.\nMarkdown representation of the table", "doc_id": "griggs2024", "page": 20, "url": "https://arxiv.org/pdf/2404.14527", "embedded_text": "Table 8: Instance allocations for the mixed long/short context dataset, SLO=40ms.\nMarkdown representation of the table", "id": 1396}
{"type": "section", "content": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The models are publicly available at https://github.com/myshell-ai/JetMoE.\n\n1. Introduction", "doc_id": "shen2024", "page": 1, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The models are publicly available at https://github.com/myshell-ai/JetMoE.\n\n1. Introduction", "original_types": ["text", "header"], "id": 1397}
{"type": "section", "content": "Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to developing powerful and accessible AI. Although modern LLMs have surpassed human performance on some tasks, they remain inefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman et al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters during inference and training, which are referred to as dense models. Considering the substantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer et al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling parameter scaling while keeping computational costs modest. Recent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at scaling language models to a substantial size, accompanied by remarkable performance, such as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org, 2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent performance, they are not truly open-sourced as the training recipes are not published and may contain proprietary datasets inaccessible outside of large corporations. The open-source community has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but its performance is only on par with weak dense models with similar activation parameters, such as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).", "doc_id": "shen2024", "page": 1, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to developing powerful and accessible AI. Although modern LLMs have surpassed human performance on some tasks, they remain inefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman et al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters during inference and training, which are referred to as dense models. Considering the substantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer et al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling parameter scaling while keeping computational costs modest. Recent applications of MoE architectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at scaling language models to a substantial size, accompanied by remarkable performance, such as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org, 2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent performance, they are not truly open-sourced as the training recipes are not published and may contain proprietary datasets inaccessible outside of large corporations. The open-source community has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but its performance is only on par with weak dense models with similar activation parameters, such as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).", "original_types": ["text"], "id": 1398}
{"type": "section", "content": "JetMoE", "doc_id": "shen2024", "page": 2, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE", "original_types": ["header"], "id": 1399}
{"type": "figure", "content": "Figure 1: JetMoE architecture", "doc_id": "shen2024", "page": 2, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Figure 1: JetMoE architecture", "id": 1400}
{"type": "section", "content": "To facilitate future efforts on open foundation models, particularly MoE models, we introduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al., 2023) that extends the concept of sparse activation to both the attention and feed-forward layers. Unlike prior works that only apply sparse activation to the feed-forward layer, JetMoE-8B leverages sparse activation in both components to further reduce computational costs while maintaining performance.\n\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B outperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat model, demonstrating that LLM training can be much more cost-effective than generally thought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.\n\nThe key advantages of JetMoE-8B include:\n\n• {'type': 'text', 'content': 'Openness and academia-friendly: JetMoE-8B is trained using only public datasets and open-source training code, making it accessible to many academia research settings. The model can also be finetuned with limited compute budgets (e.g., consumer-grade GPUs).'}\n• {'type': 'text', 'content': 'Sparse activation on both attention and feed-forward layers, which significantly reduces training and inference costs. We also propose to share the kv projection in attention experts to improve training stability.'}\n• {'type': 'text', 'content': 'Comprehensive open-source data mixture, which ensures high-quality training using only open-source datasets.'}\n\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, benefiting the broader AI research community. To foster collaboration and further advancements, we have detailed all the training parameters and data mixture in this report.\n\nModel Architecture\n\n2.1 Mixture of Experts\n\nA Mixture of Experts (MoE) layer comprises N modules f1, . . . , fN and a router g(e | x). Given an input x to the MoE layer, the router predicts a probability distribution over the N", "doc_id": "shen2024", "page": 2, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "To facilitate future efforts on open foundation models, particularly MoE models, we introduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al., 2023) that extends the concept of sparse activation to both the attention and feed-forward layers. Unlike prior works that only apply sparse activation to the feed-forward layer, JetMoE-8B leverages sparse activation in both components to further reduce computational costs while maintaining performance.\n\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B outperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat model, demonstrating that LLM training can be much more cost-effective than generally thought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.\n\nThe key advantages of JetMoE-8B include:\n\n• {'type': 'text', 'content': 'Openness and academia-friendly: JetMoE-8B is trained using only public datasets and open-source training code, making it accessible to many academia research settings. The model can also be finetuned with limited compute budgets (e.g., consumer-grade GPUs).'}\n• {'type': 'text', 'content': 'Sparse activation on both attention and feed-forward layers, which significantly reduces training and inference costs. We also propose to share the kv projection in attention experts to improve training stability.'}\n• {'type': 'text', 'content': 'Comprehensive open-source data mixture, which ensures high-quality training using only open-source datasets.'}\n\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, benefiting the broader AI research community. To foster collaboration and further advancements, we have detailed all the training parameters and data mixture in this report.\n\nModel Architecture\n\n2.1 Mixture of Experts\n\nA Mixture of Experts (MoE) layer comprises N modules f1, . . . , fN and a router g(e | x). Given an input x to the MoE layer, the router predicts a probability distribution over the N", "original_types": ["section", "text", "header", "list"], "id": 1401}
{"type": "section", "content": "JetMoE\n\nmodules. Of these, we select the top k experts. When k < N , we are using a Sparse Mixture of Experts (SMoE, Shazeer et al. 2017). In this JetMoE, we use a linear layer to model the router\n\ns = W_rtr x,\n\ng(e | x) = {softmax (Topk (s))_i, s_i ∈ Topk (s) 0, s_i ∉ Topk (s)\n\nwhere W_rtr is the expert embedding matrix of shape (N, D_emb), Topk is the operator that select the top k logits from s. The final output of the SMoE is then given by\n\ny = ∑_e=1^N g(e | x) · f_e(x)\n\nWhen g(e | x) = 0, f_e(x) will not need to be evaluated, thus reducing computation cost during training and inference.\n\nFollowing the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-attention and Feed-forward layers (FFD) with SMoE layer. This is different from most opensource MoE models (Dai et al., 2024; Xue et al., 2024), that only replace FFD layers.\n\n2.2 FeedFoward Expert\n\nEach FFD expert is a standard 2-layer MLP with hidden state size D_ffd:\n\nf_mlp(x) = W_outσ(W_in x)\n\nWhere W_out is the output projection matrix of shape (D_emb, D_ffd), W_in in the input projection matrix of shape (2D_ffd, D_emb), σ is the SwiGLU activation function.\n\n2.3 Attention Expert\n\nZhang et al. (2022) propose the Mixture of Attention heads (MoA), which extends SMOEs to attention mechanisms. We adapt MoA for our purposes, generalizing it to allow for multiple heads per expert and introducing RoPE relative positioning into the attention computation.\n\nIn JetMoE, each attention expert e is composed of four R^{D_emb × D_att} matrix: W_q^e, W_k^e, W_v^e, W_o^e, where D_att = H × D_head, H is the number of attention head inside each attention experts, D_head is the dimension of each attention head. Among these matrices, W_q^e and W_o^e are owned by each expert, but W_k and W_v are shared across experts to improve the training and inference efficiency.\n\nGiven an input vector sequence x, we first projected it to key vectors k and value vectors v using the shared key and value projection matrices:\n\nk = W_k x\n\nv = W_v x\n\nInside expert e, we project x into the query vectors q_e, apply standard multi-head attention with RoPE (Su et al., 2024), and project the attention output back to the input space:\n\nq_e = W_q^e x\n\na_e = MHA(q_e, k, v)\n\no_e = W_o^e a\n\nBy introducing the MoA, we can scale up the attention layer with more attention experts while maintaining the same amount of computation. Such that the attention layer will not become a performance bottleneck, while we scale up the MLP layers.", "doc_id": "shen2024", "page": 3, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE\n\nmodules. Of these, we select the top k experts. When k < N , we are using a Sparse Mixture of Experts (SMoE, Shazeer et al. 2017). In this JetMoE, we use a linear layer to model the router\n\ns = W_rtr x,\n\ng(e | x) = {softmax (Topk (s))_i, s_i ∈ Topk (s) 0, s_i ∉ Topk (s)\n\nwhere W_rtr is the expert embedding matrix of shape (N, D_emb), Topk is the operator that select the top k logits from s. The final output of the SMoE is then given by\n\ny = ∑_e=1^N g(e | x) · f_e(x)\n\nWhen g(e | x) = 0, f_e(x) will not need to be evaluated, thus reducing computation cost during training and inference.\n\nFollowing the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-attention and Feed-forward layers (FFD) with SMoE layer. This is different from most opensource MoE models (Dai et al., 2024; Xue et al., 2024), that only replace FFD layers.\n\n2.2 FeedFoward Expert\n\nEach FFD expert is a standard 2-layer MLP with hidden state size D_ffd:\n\nf_mlp(x) = W_outσ(W_in x)\n\nWhere W_out is the output projection matrix of shape (D_emb, D_ffd), W_in in the input projection matrix of shape (2D_ffd, D_emb), σ is the SwiGLU activation function.\n\n2.3 Attention Expert\n\nZhang et al. (2022) propose the Mixture of Attention heads (MoA), which extends SMOEs to attention mechanisms. We adapt MoA for our purposes, generalizing it to allow for multiple heads per expert and introducing RoPE relative positioning into the attention computation.\n\nIn JetMoE, each attention expert e is composed of four R^{D_emb × D_att} matrix: W_q^e, W_k^e, W_v^e, W_o^e, where D_att = H × D_head, H is the number of attention head inside each attention experts, D_head is the dimension of each attention head. Among these matrices, W_q^e and W_o^e are owned by each expert, but W_k and W_v are shared across experts to improve the training and inference efficiency.\n\nGiven an input vector sequence x, we first projected it to key vectors k and value vectors v using the shared key and value projection matrices:\n\nk = W_k x\n\nv = W_v x\n\nInside expert e, we project x into the query vectors q_e, apply standard multi-head attention with RoPE (Su et al., 2024), and project the attention output back to the input space:\n\nq_e = W_q^e x\n\na_e = MHA(q_e, k, v)\n\no_e = W_o^e a\n\nBy introducing the MoA, we can scale up the attention layer with more attention experts while maintaining the same amount of computation. Such that the attention layer will not become a performance bottleneck, while we scale up the MLP layers.", "original_types": ["text", "header"], "id": 1402}
{"type": "section", "content": "2.4 Load Balancing during Pretraining\n\nTo avoid the SMoE repeatedly using the same module and wasting the extra capacity in the other modules, it requires various load balancing losses to regulate the training of the router (Shazeer et al., 2017; Fedus et al., 2021). In the training of JetMoE, we use the frequency-based auxiliary loss introduced in Fedus et al. (2021)\n\nloss_b = N ∑_{i=1}^{N} f_i P_i\n\nwhere N is the number of experts, f_i is the fraction of tokens dispatched to expert i, and P_i is the fraction of the router probability allocated for expert i. To improve the training stability, we also use the router z-loss introduced in Zoph et al. (2022):\n\nloss_z = 1/B ∑_{i=1}^{B} \\left( log ∑_{j=1}^{N} exp(x_j^i) \\right)^2\n\nwhere B is the number of tokens, x is the logits given by router. The final training loss will be the weighted sum of three losses:\n\nloss = loss_{lm} + αloss_b + βloss_z\n\nwhere α is the weight for load balancing loss and β is the weight for z-loss.", "doc_id": "shen2024", "page": 4, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "2.4 Load Balancing during Pretraining\n\nTo avoid the SMoE repeatedly using the same module and wasting the extra capacity in the other modules, it requires various load balancing losses to regulate the training of the router (Shazeer et al., 2017; Fedus et al., 2021). In the training of JetMoE, we use the frequency-based auxiliary loss introduced in Fedus et al. (2021)\n\nloss_b = N ∑_{i=1}^{N} f_i P_i\n\nwhere N is the number of experts, f_i is the fraction of tokens dispatched to expert i, and P_i is the fraction of the router probability allocated for expert i. To improve the training stability, we also use the router z-loss introduced in Zoph et al. (2022):\n\nloss_z = 1/B ∑_{i=1}^{B} \\left( log ∑_{j=1}^{N} exp(x_j^i) \\right)^2\n\nwhere B is the number of tokens, x is the logits given by router. The final training loss will be the weighted sum of three losses:\n\nloss = loss_{lm} + αloss_b + βloss_z\n\nwhere α is the weight for load balancing loss and β is the weight for z-loss.", "original_types": ["text", "header", "equation"], "id": 1403}
{"type": "section", "content": "3.2 Synthetic Datasets\n\nOpenHermes 2.5 is a large-scale, diverse, high-quality compilation of open-source and custom synthetic datasets (Teknium, 2023). It contains 1 million primarily synthetically generated instruction and chat samples, following a ShareGPT structure. The dataset is compiled from sources including Airoboros 2.2 (Durbin, 2023), CamelAI domain expert datasets (Li et al., 2023a), ChatBot Arena (GPT-4 Only) (Zheng et al., 2024a), Collective Cognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol Instruct 70K and 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-LLM (Peng et al., 2023), GPTTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023), MetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023; Mukherjee et al., 2023; Lian et al., 2023), Platypus (Lee et al., 2024; Lightman et al., 2023; Wang et al., 2023b), ShareGPT (GPT4-Only) (lm sys, 2023), and Unnatural Instructions GPT4 (Peng et al., 2023).\n\nUltraTextbooks is a comprehensive collection of high-quality synthetic and human-written textbooks (Locutusque, 2024). The composition of the dataset incorporating multiple sources such as nampdn-ai/mini-peS2o, open-phi/programming_books_llama, open-phi/textbooks,nampdn-ai/tiny-strange-textbooks, and a select high-quality web collection from math-ai/AutoMathText.\n\nUltraChat 200k is a filtered subset of the UltraChat dataset, which consists of 1.4M dialogues generated by ChatGPT (Ding et al., 2023; Tunstall et al., 2023b). The subset was created by selecting a smaller portion of the data, truecasing the text to fix grammatical errors, and removing dialogues where the assistant inappropriately claims to lack emotions or opinions.", "doc_id": "shen2024", "page": 5, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "3.2 Synthetic Datasets\n\nOpenHermes 2.5 is a large-scale, diverse, high-quality compilation of open-source and custom synthetic datasets (Teknium, 2023). It contains 1 million primarily synthetically generated instruction and chat samples, following a ShareGPT structure. The dataset is compiled from sources including Airoboros 2.2 (Durbin, 2023), CamelAI domain expert datasets (Li et al., 2023a), ChatBot Arena (GPT-4 Only) (Zheng et al., 2024a), Collective Cognition (09-11-2023) (CollectiveCognition, 2023), CoT Alpaca GPT4 (Si et al., 2023), Evol Instruct 70K and 140K (Xu et al., 2023a), Glaive Code Assistant (glaiveai, 2023), GPT4-LLM (Peng et al., 2023), GPTTeacher (Teknium1, 2023), Medical Tasks (CogStack, 2023), MetaMath 40k (Yu et al., 2023), SlimOrca 550K (Longpre et al., 2023; Mukherjee et al., 2023; Lian et al., 2023), Platypus (Lee et al., 2024; Lightman et al., 2023; Wang et al., 2023b), ShareGPT (GPT4-Only) (lm sys, 2023), and Unnatural Instructions GPT4 (Peng et al., 2023).\n\nUltraTextbooks is a comprehensive collection of high-quality synthetic and human-written textbooks (Locutusque, 2024). The composition of the dataset incorporating multiple sources such as nampdn-ai/mini-peS2o, open-phi/programming_books_llama, open-phi/textbooks,nampdn-ai/tiny-strange-textbooks, and a select high-quality web collection from math-ai/AutoMathText.\n\nUltraChat 200k is a filtered subset of the UltraChat dataset, which consists of 1.4M dialogues generated by ChatGPT (Ding et al., 2023; Tunstall et al., 2023b). The subset was created by selecting a smaller portion of the data, truecasing the text to fix grammatical errors, and removing dialogues where the assistant inappropriately claims to lack emotions or opinions.", "original_types": ["text", "header"], "id": 1404}
{"type": "section", "content": "Model Pretraining\n\nInfrastructures\n\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate Megablock (Gale et al., 2023) for MoE support. We further modified the training framework to support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we choose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert parallelism for model parallel during training. This is mainly due to two reasons. First, Sparse MoE models usually have a narrower hidden state compared to standard transformer models. Thus, the communication cost for pipeline parallelism is smaller. Second, we use the dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could cause load unbalance across experts. Thus, using expert parallel will cause an unbalanced load across devices and result in inefficient training. Pipeline parallelism could avoid this slowdown because it computes all the experts inside a layer on the same device. We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks. Infiniband is used for fast communication between nodes.\n\nHyper-parameters", "doc_id": "shen2024", "page": 6, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Model Pretraining\n\nInfrastructures\n\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate Megablock (Gale et al., 2023) for MoE support. We further modified the training framework to support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we choose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert parallelism for model parallel during training. This is mainly due to two reasons. First, Sparse MoE models usually have a narrower hidden state compared to standard transformer models. Thus, the communication cost for pipeline parallelism is smaller. Second, we use the dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could cause load unbalance across experts. Thus, using expert parallel will cause an unbalanced load across devices and result in inefficient training. Pipeline parallelism could avoid this slowdown because it computes all the experts inside a layer on the same device. We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks. Infiniband is used for fast communication between nodes.\n\nHyper-parameters", "original_types": ["section", "text", "header"], "id": 1405}
{"type": "table", "content": "Table 1: JetMoE-8B hyperparameters.\n| P_total | P_active | n_layers | D_model | N_experts | Top-k | n_kv_heads | D_head | D_mlp |\n|---------|---------|---------|---------|----------|-------|------------|--------|--------|\n| 8B      | 2B      | 24      | 2048    | 8        | 2     | 16         | 128    | 5632   |", "doc_id": "shen2024", "page": 6, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Table 1: JetMoE-8B hyperparameters.\n| P_total | P_active | n_layers | D_model | N_experts | Top-k | n_kv_heads | D_head | D_mlp |\n|---------|---------|---------|---------|----------|-------|------------|--------|--------|\n| 8B      | 2B      | 24      | 2048    | 8        | 2     | 16         | 128    | 5632   |", "id": 1406}
{"type": "section", "content": "The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer. Such that the model has approximately two times the computation compared to a 1B model. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss is set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\n\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maximum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We employ the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al. (2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted by W, representing the number of steps at the end of the warmup stage), the stable training stage (denoted by S), and the annealing stage (denoted by D):\n\nlr(s) = \\begin{cases} \\frac{s}{W} * \\eta, & s < W \\\\ \\eta, & W < s < S \\\\ f(s - S) * \\eta, & S < s < S + D \\end{cases}\n\nwhere 0 < f(s - S) \\leq 1 is a decreasing function of s, and \\eta is the maximum learning rate. In our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250 billion tokens. The initial and final learning rates are set to 10% of the maximum learning rate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n\nTraining Data Mixture\n\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and Gemma (Team et al., 2024), we increase the weight of high-quality data during the learning rate decay phase. The training process is divided into two phases:\n\nPhase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath.", "doc_id": "shen2024", "page": 6, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B transformer language model. We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer. Such that the model has approximately two times the computation compared to a 1B model. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss is set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\n\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maximum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We employ the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al. (2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted by W, representing the number of steps at the end of the warmup stage), the stable training stage (denoted by S), and the annealing stage (denoted by D):\n\nlr(s) = \\begin{cases} \\frac{s}{W} * \\eta, & s < W \\\\ \\eta, & W < s < S \\\\ f(s - S) * \\eta, & S < s < S + D \\end{cases}\n\nwhere 0 < f(s - S) \\leq 1 is a decreasing function of s, and \\eta is the maximum learning rate. In our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250 billion tokens. The initial and final learning rates are set to 10% of the maximum learning rate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n\nTraining Data Mixture\n\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and Gemma (Team et al., 2024), we increase the weight of high-quality data during the learning rate decay phase. The training process is divided into two phases:\n\nPhase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Starcoder, The Pile, peS2o from Dolma, and OpenWebMath.", "original_types": ["section", "text", "equation", "list"], "id": 1407}
{"type": "section", "content": "Phase 2 (decay learning rate): We include additional high-quality data to further improve the model's performance.\n\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note that given the limited computing budget available, our data mixture might not be ideal. However, it serves as a good starting point for training JetMoE-8B and can be further optimized in future iterations.", "doc_id": "shen2024", "page": 7, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Phase 2 (decay learning rate): We include additional high-quality data to further improve the model's performance.\n\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note that given the limited computing budget available, our data mixture might not be ideal. However, it serves as a good starting point for training JetMoE-8B and can be further optimized in future iterations.", "original_types": ["text", "header"], "id": 1408}
{"type": "figure", "content": "Figure 2: Pretraining data mixture", "doc_id": "shen2024", "page": 7, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Figure 2: Pretraining data mixture", "id": 1409}
{"type": "table", "content": "Table 2: Detailed data mixture for Phase 2\nThe table provides a detailed breakdown of the data mixture for Phase 2. The categories include NL pretraining data, NL SFT data, Textbook, Code pretraining data, Code SFT data, and Math data. The percentages for each category are as follows: NL pretraining data (Refinedweb: 39.8%, Pile_Wikipedia: 6.7%, Pile_StackExchange: 4.8%, Pile_arXiv: 1.0%, Pile_remaining: 5.1%, Dolma_peS2o: 1.0%), NL SFT data (xp3x, OpenAssistant, OpenHermes UltraChat, Oasst-octopack: 7.3%), Textbook (UltraTextbooks: 4.8%), Code pretraining data (Starcoder Github: 19.6%), Code SFT data (Magicoder-OSS, Magicoder-Evol Code-290k-ShareGPT, CommitPackFT Evol-Code Alpaca: 3.8%), and Math data (Open-web-math, algebraic-stack TemplateGSM, StackMathQA: 5.8%).", "doc_id": "shen2024", "page": 7, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Table 2: Detailed data mixture for Phase 2\nThe table provides a detailed breakdown of the data mixture for Phase 2. The categories include NL pretraining data, NL SFT data, Textbook, Code pretraining data, Code SFT data, and Math data. The percentages for each category are as follows: NL pretraining data (Refinedweb: 39.8%, Pile_Wikipedia: 6.7%, Pile_StackExchange: 4.8%, Pile_arXiv: 1.0%, Pile_remaining: 5.1%, Dolma_peS2o: 1.0%), NL SFT data (xp3x, OpenAssistant, OpenHermes UltraChat, Oasst-octopack: 7.3%), Textbook (UltraTextbooks: 4.8%), Code pretraining data (Starcoder Github: 19.6%), Code SFT data (Magicoder-OSS, Magicoder-Evol Code-290k-ShareGPT, CommitPackFT Evol-Code Alpaca: 3.8%), and Math data (Open-web-math, algebraic-stack TemplateGSM, StackMathQA: 5.8%).", "id": 1410}
{"type": "section", "content": "Data Distillation\n\nFor a set of seed prompts \\{x_j^0\\}_{j=1}^J, generate responses y_j^0 using the teacher model \\pi_T, and refine instructions to obtain C = \\{(x_j, y_j)\\}_{j=1}^J.\n\nInstruction Tuning\n\nThe student model \\pi_{dSFT} is trained by maximizing the likelihood of the responses given the instructions:\n\n\\pi_{dSFT} = \\arg\\max_{\\pi} \\sum_{(x, y) \\in C} \\log \\pi(y|x).\n\nNote that the expectation for the likelihood function is approximated by using the arithmetic mean over a batch of training samples.\n\nDistilled Direct Preference Optimization (dDPO)\n\ndDPO refines the dSFT model by incorporating preferences from an aligned teacher model into the training process. It optimizes a reward function that reflects these preferences, aiming to align the student model's outputs with the desired outcomes based on the static preference dataset.\n\nKL-Constrained Optimization\n\nThe foundation of dDPO lies in the KL-constrained optimization, which derives the optimal policy \\pi_r^* that maximizes expected rewards while minimizing divergence from a baseline policy \\pi_0 (Wang et al., 2023a):\n\n\\pi_r^*(y|x) := \\arg\\max_{\\pi} \\mathbb{E}_{x \\sim d_0} \\left[ \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} [r(x, y)] - \\eta \\text{KL}(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\right]\n\nwhere \\eta is a regularization parameter that balances maximizing the reward function r(x, y) and adhering to the baseline policy \\pi_0.\n\nPreference-Driven Reward Function\n\ndDPO incorporates a reward function that reflects preferences from an aligned teacher model:\n\nr^*(x, y) = \\eta \\log \\left( \\frac{\\pi^*(y|x)}{\\pi_{dSFT}(y|x)} \\right) + \\eta \\log Z(x),\n\nquantifying the preference for producing response y given input x relative to the dSFT model's baseline probability. \\eta scales the reward's influence, and Z(x) ensures normalization.\n\nOptimization Objective\n\nThe objective for aligning \\pi_\\theta with the teacher model's preferences is:\n\n\\pi_\\theta = \\arg\\max_{\\pi} \\sum_{(x, y_w, y_l) \\in D} \\log \\sigma \\left( \\eta \\log \\frac{\\pi(y_w|x)}{\\pi_{dSFT}(y_w|x)} - \\eta \\log \\frac{\\pi(y_l|x)}{\\pi_{dSFT}(y_l|x)} \\right),\n\nwhere D comprises instruction-response pairs, with y_w and y_l indicating preferred and less preferred responses respectively, scored by the teacher model.\n\nOffline DPO (Rafailov et al., 2023) directly optimizes language model policies using static preference data, providing stable learning and simpler tuning compared to Reinforcement learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2023). However, it faces challenges with distribution shifts between the dataset and the evolving policy. Online and iterative DPO variants address this issue at the cost of increased computational complexity (Xu et al., 2023b; Guo et al., 2024b; Xiong et al., 2024).\n\nAlignment details\n\nOur alginment framework is based on Alignment Handbook (Tunstall et al.", "doc_id": "shen2024", "page": 8, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Data Distillation\n\nFor a set of seed prompts \\{x_j^0\\}_{j=1}^J, generate responses y_j^0 using the teacher model \\pi_T, and refine instructions to obtain C = \\{(x_j, y_j)\\}_{j=1}^J.\n\nInstruction Tuning\n\nThe student model \\pi_{dSFT} is trained by maximizing the likelihood of the responses given the instructions:\n\n\\pi_{dSFT} = \\arg\\max_{\\pi} \\sum_{(x, y) \\in C} \\log \\pi(y|x).\n\nNote that the expectation for the likelihood function is approximated by using the arithmetic mean over a batch of training samples.\n\nDistilled Direct Preference Optimization (dDPO)\n\ndDPO refines the dSFT model by incorporating preferences from an aligned teacher model into the training process. It optimizes a reward function that reflects these preferences, aiming to align the student model's outputs with the desired outcomes based on the static preference dataset.\n\nKL-Constrained Optimization\n\nThe foundation of dDPO lies in the KL-constrained optimization, which derives the optimal policy \\pi_r^* that maximizes expected rewards while minimizing divergence from a baseline policy \\pi_0 (Wang et al., 2023a):\n\n\\pi_r^*(y|x) := \\arg\\max_{\\pi} \\mathbb{E}_{x \\sim d_0} \\left[ \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} [r(x, y)] - \\eta \\text{KL}(\\pi(\\cdot|x) \\| \\pi_0(\\cdot|x)) \\right]\n\nwhere \\eta is a regularization parameter that balances maximizing the reward function r(x, y) and adhering to the baseline policy \\pi_0.\n\nPreference-Driven Reward Function\n\ndDPO incorporates a reward function that reflects preferences from an aligned teacher model:\n\nr^*(x, y) = \\eta \\log \\left( \\frac{\\pi^*(y|x)}{\\pi_{dSFT}(y|x)} \\right) + \\eta \\log Z(x),\n\nquantifying the preference for producing response y given input x relative to the dSFT model's baseline probability. \\eta scales the reward's influence, and Z(x) ensures normalization.\n\nOptimization Objective\n\nThe objective for aligning \\pi_\\theta with the teacher model's preferences is:\n\n\\pi_\\theta = \\arg\\max_{\\pi} \\sum_{(x, y_w, y_l) \\in D} \\log \\sigma \\left( \\eta \\log \\frac{\\pi(y_w|x)}{\\pi_{dSFT}(y_w|x)} - \\eta \\log \\frac{\\pi(y_l|x)}{\\pi_{dSFT}(y_l|x)} \\right),\n\nwhere D comprises instruction-response pairs, with y_w and y_l indicating preferred and less preferred responses respectively, scored by the teacher model.\n\nOffline DPO (Rafailov et al., 2023) directly optimizes language model policies using static preference data, providing stable learning and simpler tuning compared to Reinforcement learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2023). However, it faces challenges with distribution shifts between the dataset and the evolving policy. Online and iterative DPO variants address this issue at the cost of increased computational complexity (Xu et al., 2023b; Guo et al., 2024b; Xiong et al., 2024).\n\nAlignment details\n\nOur alginment framework is based on Alignment Handbook (Tunstall et al.", "original_types": ["text", "header", "equation"], "id": 1411}
{"type": "section", "content": "JetMoE\n\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al., 2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023). Chat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a learning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\n\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback dataset (Cui et al., 2023), which contains binary preference labels indicating the preferred response between two options. The key hyperparameters for dDPO are a learning rate of 5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n\nEvaluation", "doc_id": "shen2024", "page": 9, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE\n\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al., 2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023). Chat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a learning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\n\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback dataset (Cui et al., 2023), which contains binary preference labels indicating the preferred response between two options. The key hyperparameters for dDPO are a learning rate of 5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n\nEvaluation", "original_types": ["text", "header"], "id": 1412}
{"type": "table", "content": "Table 3: OpenLLM leaderboard and code benchmarks results from four different models.\n| Model | LLaMA2 | DeepseekMoE | Gemma | JetMoE |\n| --- | --- | --- | --- | --- |\n| \\# Total Params | 7B | 16B | 2B | 8B |\n| \\# Activate Params | 7B | 2.8B | 2B | 2.2B |\n| \\# Training tokens | 2T | 2T | 2T | 1.25T |\n\n| ARC-challenge | 53.1 | 53.2 | 48.4 | 48.7 |\n| Hellaswag | 78.6 | 79.8 | 71.8 | 80.5 |\n| MMLU | 46.9 | 46.3 | 41.8 | 49.2 |\n| TruthfulQA | 38.8 | 36.1 | 33.1 | 41.7 |\n| WinoGrande | 74.0 | 73.7 | 66.3 | 70.2 |\n| GSM8k | 14.5 | 17.3 | 16.9 | 27.8 |\n\n| OpenLLM Leaderboard Avg. | 51.0 | 51.1 | 46.4 | 53.0 |\n\n| MBPP (Pass@1) | 20.8 | 34.0 | 28.0 | 34.2 |\n| HumanEval (Pass@1) | 12.8 | 25.0 | 24.4 | 14.6 |\n\n| All Avg. | 45.5 | 47.3 | 43.2 | 47.6 |", "doc_id": "shen2024", "page": 9, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Table 3: OpenLLM leaderboard and code benchmarks results from four different models.\n| Model | LLaMA2 | DeepseekMoE | Gemma | JetMoE |\n| --- | --- | --- | --- | --- |\n| \\# Total Params | 7B | 16B | 2B | 8B |\n| \\# Activate Params | 7B | 2.8B | 2B | 2.2B |\n| \\# Training tokens | 2T | 2T | 2T | 1.25T |\n\n| ARC-challenge | 53.1 | 53.2 | 48.4 | 48.7 |\n| Hellaswag | 78.6 | 79.8 | 71.8 | 80.5 |\n| MMLU | 46.9 | 46.3 | 41.8 | 49.2 |\n| TruthfulQA | 38.8 | 36.1 | 33.1 | 41.7 |\n| WinoGrande | 74.0 | 73.7 | 66.3 | 70.2 |\n| GSM8k | 14.5 | 17.3 | 16.9 | 27.8 |\n\n| OpenLLM Leaderboard Avg. | 51.0 | 51.1 | 46.4 | 53.0 |\n\n| MBPP (Pass@1) | 20.8 | 34.0 | 28.0 | 34.2 |\n| HumanEval (Pass@1) | 12.8 | 25.0 | 24.4 | 14.6 |\n\n| All Avg. | 45.5 | 47.3 | 43.2 | 47.6 |", "id": 1413}
{"type": "section", "content": "JetMoE", "doc_id": "shen2024", "page": 10, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE", "original_types": ["header"], "id": 1414}
{"type": "table", "content": "Table 4: MT-Bench score comparison of various models\n| Model | MT-Bench Score |\n|---|---|\n| GPT-4 | 9.014 |\n| GPT-3.5-turbo | 7.995 |\n| Claude-v1 | 7.923 |\n| JetMoE-8B-chat | 6.681 |\n| Llama-2-13b-chat | 6.650 |\n| Vicuna-13b-v1.3 | 6.413 |\n| Wizardlm-13b | 6.353 |\n| Llama-2-7b-chat | 6.269 |\n", "doc_id": "shen2024", "page": 10, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Table 4: MT-Bench score comparison of various models\n| Model | MT-Bench Score |\n|---|---|\n| GPT-4 | 9.014 |\n| GPT-3.5-turbo | 7.995 |\n| Claude-v1 | 7.923 |\n| JetMoE-8B-chat | 6.681 |\n| Llama-2-13b-chat | 6.650 |\n| Vicuna-13b-v1.3 | 6.413 |\n| Wizardlm-13b | 6.353 |\n| Llama-2-7b-chat | 6.269 |\n", "id": 1415}
{"type": "figure", "content": "Figure 3: MT-Bench radar figure", "doc_id": "shen2024", "page": 10, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Figure 3: MT-Bench radar figure", "id": 1416}
{"type": "section", "content": "JetMoE\n\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational costs while maintaining strong performance across a wide range of tasks.\n\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets, JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to other open-source chatbots.\n\nWe provide detailed training parameters and data mixture information to encourage reproducibility and enable researchers to build upon our work. JetMoE-8B represents a significant step forward in the development of open-source, efficient, and high-performing language models, contributing to the democratization of advanced language technologies.\n\nAcknowledgments\n\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data mixture. We also express our gratitude to Exabits for their assistance in setting up the GPU clusters, and to Lepton AI for their support in setting up the chat demo.\n\nReferences\n\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/SystemChat.\n\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT.\n\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation, 2024.\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2023.\n\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. A framework for the evaluation of code generation models. https://github.com/bigcode-project/bigcode-evaluation-harness, 2022.\n\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432–7439, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.", "doc_id": "shen2024", "page": 11, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "JetMoE\n\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational costs while maintaining strong performance across a wide range of tasks.\n\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets, JetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leaderboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to other open-source chatbots.\n\nWe provide detailed training parameters and data mixture information to encourage reproducibility and enable researchers to build upon our work. JetMoE-8B represents a significant step forward in the development of open-source, efficient, and high-performing language models, contributing to the democratization of advanced language technologies.\n\nAcknowledgments\n\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data mixture. We also express our gratitude to Exabits for their assistance in setting up the GPU clusters, and to Lepton AI for their support in setting up the chat demo.\n\nReferences\n\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/SystemChat.\n\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT.\n\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation, 2024.\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2023.\n\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. A framework for the evaluation of code generation models. https://github.com/bigcode-project/bigcode-evaluation-harness, 2022.\n\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432–7439, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.", "original_types": ["text", "header"], "id": 1417}
{"type": "section", "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,", "doc_id": "shen2024", "page": 11, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,", "original_types": ["text"], "id": 1418}
{"type": "section", "content": "Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/\n\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\nCogStack. OpenGPT: A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs). https://github.com/CogStack/OpenGPT, 2023.\n\nCollectiveCognition. Collective cognition chatgpt conversations, 2023. URL https://huggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22.\n\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\n\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024.\n\nLuigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multi-turn conversations for efficient llm training. arXiv preprint arXiv:(coming soon), 2023. URL https://huggingface.co/datasets/LDJnr/Capybara.\n\nDatabricks. Dbrx: Resources and code examples. https://github.com/databricks/dbrx, 2024.\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023.", "doc_id": "shen2024", "page": 12, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/\n\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\nCogStack. OpenGPT: A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs). https://github.com/CogStack/OpenGPT, 2023.\n\nCollectiveCognition. Collective cognition chatgpt conversations, 2023. URL https://huggingface.co/datasets/CollectiveCognition/chats-data-2023-09-22.\n\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\n\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024.\n\nLuigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multi-turn conversations for efficient llm training. arXiv preprint arXiv:(coming soon), 2023. URL https://huggingface.co/datasets/LDJnr/Capybara.\n\nDatabricks. Dbrx: Resources and code examples. https://github.com/databricks/dbrx, 2024.\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023.", "original_types": ["text"], "id": 1419}
{"type": "section", "content": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.\n\nJon Durbin. airoboros: Customizable implementation of the self-instruct paper. https://github.com/jondurbin/airoboros, 2023.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.", "doc_id": "shen2024", "page": 12, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.\n\nJon Durbin. airoboros: Customizable implementation of the self-instruct paper. https://github.com/jondurbin/airoboros, 2023.\n\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.", "original_types": ["text"], "id": 1420}
{"type": "section", "content": "Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5, 2023.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama.\n\nglaiveai. Glaive-code-assistant, 2023. URL https://huggingface.co/datasets/glaiveai/glaive-code-assistant.\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024a.\n\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct language model alignment from online ai feedback, 2024b.\n\nHorace He and Shangdi Yu. Transcending runtime-memory tradeoffs in checkpointing by being fusion aware. Proceedings of Machine Learning and Systems, 5, 2023.\n\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.", "doc_id": "shen2024", "page": 13, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5, 2023.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nXinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama.\n\nglaiveai. Glaive-code-assistant, 2023. URL https://huggingface.co/datasets/glaiveai/glaive-code-assistant.\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024a.\n\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct language model alignment from online ai feedback, 2024b.\n\nHorace He and Shangdi Yu. Transcending runtime-memory tradeoffs in checkpointing by being fusion aware. Proceedings of Machine Learning and Systems, 5, 2023.\n\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.", "original_types": ["text"], "id": 1421}
{"type": "section", "content": "LAION-AI. Open-Assistant: A chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically. https://github.com/LAION-AI/Open-Assistant, 2023.\n\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms, 2024.\n\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.", "doc_id": "shen2024", "page": 13, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "LAION-AI. Open-Assistant: A chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically. https://github.com/LAION-AI/Open-Assistant, 2023.\n\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms, 2024.\n\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a.", "original_types": ["text"], "id": 1422}
{"type": "section", "content": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-hao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-tasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023b.\n\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. URL https://https://huggingface.co/Open-Orca/SlimOrca.\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. preprint arXiv:2305.20050, 2023.\n\nlm sys. FastChat: An open platform for training, serving, and evaluating large language model based chatbots. https://github.com/lm-sys/FastChat, 2023.\n\nLocutusque. Ultratextbooks, 2024. URL https://huggingface.co/datasets/Locutusque/UltraTextbooks.\n\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023.\n\nArindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024.\n\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023a.", "doc_id": "shen2024", "page": 14, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-hao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-tasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023b.\n\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. URL https://https://huggingface.co/Open-Orca/SlimOrca.\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. preprint arXiv:2305.20050, 2023.\n\nlm sys. FastChat: An open platform for training, serving, and evaluating large language model based chatbots. https://github.com/lm-sys/FastChat, 2023.\n\nLocutusque. Ultratextbooks, 2024. URL https://huggingface.co/datasets/Locutusque/UltraTextbooks.\n\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.\n\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct, 2023.\n\nArindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024.\n\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023a.", "original_types": ["text"], "id": 1423}
{"type": "section", "content": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-gru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2023b.\n\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.", "doc_id": "shen2024", "page": 14, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xian-gru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2023b.\n\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.", "original_types": ["text"], "id": 1424}
{"type": "section", "content": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–15, 2021.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n\nBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models, 2024.\n\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023.\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.\n\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattaflori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.", "doc_id": "shen2024", "page": 15, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–15, 2021.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n\nBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models, 2024.\n\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023.\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.\n\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattaflori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.", "original_types": ["text"], "id": 1425}
{"type": "section", "content": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.", "doc_id": "shen2024", "page": 15, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.", "original_types": ["text"], "id": 1426}
{"type": "section", "content": "Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan. Moduleformer: Learning modular large language models from uncurated data. arXiv preprint arXiv:2306.04640, 2023.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nQingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical study of instruction-tuning large language models in chinese, 2023.\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024.\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.\n\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.\n\nTeknium1. GPTeacher: A collection of modular datasets generated by GPT-4. https://github.com/teknium1/GPTeacher, 2023.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github.com/huggingface/alignment-handbook, 2023a.\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023b.", "doc_id": "shen2024", "page": 16, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan. Moduleformer: Learning modular large language models from uncurated data. arXiv preprint arXiv:2306.04640, 2023.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n\nQingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical study of instruction-tuning large language models in chinese, 2023.\n\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024.\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.\n\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.\n\nTeknium1. GPTeacher: A collection of modular datasets generated by GPT-4. https://github.com/teknium1/GPTeacher, 2023.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alexander M. Rush, and Thomas Wolf. The alignment handbook. https://github.com/huggingface/alignment-handbook, 2023a.\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023b.", "original_types": ["text"], "id": 1427}
{"type": "section", "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nChaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints, 2023a.\n\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2023b.", "doc_id": "shen2024", "page": 16, "url": "https://arxiv.org/pdf/2404.07413", "embedded_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nChaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints, 2023a.\n\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models, 2023b.", "original_types": ["text"], "id": 1428}
{"type": "section", "content": "Comparing energy consumption and accuracy in text classification inference\n\nJohannes Zschache and Tilman Hartwig\n\nApplication Lab for AI and Big Data, German Environment Agency, Alte Messe 6, Leipzig, 04103, Saxony, Germany.\n\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de; Contributing authors: johannes.zschache@uba.de;\n\nAbstract\n\nThe increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.\n\nKeywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI", "doc_id": "zschache2025", "page": 1, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Comparing energy consumption and accuracy in text classification inference\n\nJohannes Zschache and Tilman Hartwig\n\nApplication Lab for AI and Big Data, German Environment Agency, Alte Messe 6, Leipzig, 04103, Saxony, Germany.\n\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de; Contributing authors: johannes.zschache@uba.de;\n\nAbstract\n\nThe increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.\n\nKeywords: NLP, Large Language Model, Resource Efficiency, Sustainable AI", "original_types": ["text", "header"], "id": 1429}
{"type": "section", "content": "development has been enabled by the Transformer architecture (Vaswani et al., 2017) and exemplified by the emergence of large-scale models such as GPT-3 (Brown et al., 2020), which have significantly advanced task performance. However, this progress has come at a cost: the escalating energy demands of AI systems pose significant environmental and computational challenges. Data centers that support AI computations are major electricity consumers, often dependent on fossil fuels, thereby contributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al., 2025). This increasing energy demand challenges global climate objectives such as the Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable Development Goals (SDGs), specifically Goal 13 on climate action (United Nations, 2015b). Consequently, designing energy-efficient AI systems is imperative for aligning technological advancements with sustainability goals. Moreover, beyond sustainability, energy-efficient models offer additional advantages, including reduced hardware requirements, lower financial costs, and faster inference times.\n\nWhen evaluating machine learning models, most studies concentrate on the quality of the model responses by tracking e.g. the accuracy, the RMSE, or other measures. And even if the energy consumption is taken into account, prior research has mainly focused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in real world deployments, has received comparatively less attention. However, energy efficiency during the operational phase is an increasingly relevant topic as LLM applications become ubiquitous and LLM models are trained to use additional test-time compute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this gap, we present a systematic study on the energy consumption of language models during inference, providing actionable insights for balancing accuracy with efficiency.\n\nA particularly popular machine learning task is text categorization, a task that lightweight models have been shown to handle effectively. For instance, Joulin et al. (2017) show that a simple classifier built on word embeddings is often as accurate as deep learning classifiers. Despite this, some authors argue for the use of pre-trained LLMs for text classification because it reduces the need for model training and simplifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials promote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023), further encouraging their use even when more efficient alternatives exist. In order to justify the usage of LLM in relatively simple tasks such as text categorization, we advocate a consequent comparison of a model’s response quality to its energy efficiency.", "doc_id": "zschache2025", "page": 2, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "development has been enabled by the Transformer architecture (Vaswani et al., 2017) and exemplified by the emergence of large-scale models such as GPT-3 (Brown et al., 2020), which have significantly advanced task performance. However, this progress has come at a cost: the escalating energy demands of AI systems pose significant environmental and computational challenges. Data centers that support AI computations are major electricity consumers, often dependent on fossil fuels, thereby contributing to greenhouse gas emissions (Lacoste et al., 2019; Axenbeck et al., 2025). This increasing energy demand challenges global climate objectives such as the Paris Agreement (United Nations, 2015a) and the United Nations’ Sustainable Development Goals (SDGs), specifically Goal 13 on climate action (United Nations, 2015b). Consequently, designing energy-efficient AI systems is imperative for aligning technological advancements with sustainability goals. Moreover, beyond sustainability, energy-efficient models offer additional advantages, including reduced hardware requirements, lower financial costs, and faster inference times.\n\nWhen evaluating machine learning models, most studies concentrate on the quality of the model responses by tracking e.g. the accuracy, the RMSE, or other measures. And even if the energy consumption is taken into account, prior research has mainly focused on the training phase (Strubell et al., 2019; Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023). The inference phase, which is repeatedly executed in real world deployments, has received comparatively less attention. However, energy efficiency during the operational phase is an increasingly relevant topic as LLM applications become ubiquitous and LLM models are trained to use additional test-time compute to improve performance (OpenAI, 2024; DeepSeek-AI, 2025). Addressing this gap, we present a systematic study on the energy consumption of language models during inference, providing actionable insights for balancing accuracy with efficiency.\n\nA particularly popular machine learning task is text categorization, a task that lightweight models have been shown to handle effectively. For instance, Joulin et al. (2017) show that a simple classifier built on word embeddings is often as accurate as deep learning classifiers. Despite this, some authors argue for the use of pre-trained LLMs for text classification because it reduces the need for model training and simplifies data preprocessing (Wang et al., 2024). Additionally, popular software tutorials promote LLMs for classification tasks (LangChain Team, 2023; Lamini Team, 2023), further encouraging their use even when more efficient alternatives exist. In order to justify the usage of LLM in relatively simple tasks such as text categorization, we advocate a consequent comparison of a model’s response quality to its energy efficiency.", "original_types": ["text"], "id": 1430}
{"type": "section", "content": "Given a practical use case that is occurring in public administration, our study empirically analyzes trade-offs between model accuracy and energy consumption across various language models and hardware configurations. We find that the best performing model is energy efficient while LLMs show higher energy usage with lower accuracy. Generally, we see significant variability in inference energy consumption, influenced by model type, model size, and hardware specifications. Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime. This makes the duration of computations a valuable proxy measure for energy consumption in settings where the latter cannot be traced. Our findings have implications for researchers, industry practitioners, and policymakers advocating for sustainable", "doc_id": "zschache2025", "page": 2, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Given a practical use case that is occurring in public administration, our study empirically analyzes trade-offs between model accuracy and energy consumption across various language models and hardware configurations. We find that the best performing model is energy efficient while LLMs show higher energy usage with lower accuracy. Generally, we see significant variability in inference energy consumption, influenced by model type, model size, and hardware specifications. Additionally, the energy consumption during inference is shown to highly correlate with the model’s runtime. This makes the duration of computations a valuable proxy measure for energy consumption in settings where the latter cannot be traced. Our findings have implications for researchers, industry practitioners, and policymakers advocating for sustainable", "original_types": ["text"], "id": 1431}
{"type": "section", "content": "2 Previous research\n\nResearch on the environmental impact of machine learning (ML) has primarily focused on the energy consumption and carbon emissions produced during the training phase of large-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes (their measurements include thousands of hyperparameter tuning jobs, which makes it difficult to disentangle model-inherent efficiency from experimental setup). This seminal work spurred further investigations into the environmental costs of training neural networks, including large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023; Patterson et al., 2022).\n\nWhile training remains a significant contributor to energy consumption, recent studies have begun to focus on the inference phase. Samsi et al. (2023) highlighted the substantial energy demands of LLM inference but did not explore the relationship between energy consumption and task-specific performance. Liu et al. (2022) underscore the importance of evaluating NLP models not just on efficiency metrics but also on accuracy by introducing the Efficient Language Understanding Evaluation (ELUE) benchmark. ELUE aims to establish a Pareto frontier that balances performance and efficiency. It includes various language understanding tasks, facilitating fair and comprehensive comparisons among models. However, the framework adopts number of parameters and FLOPs as the metrics for model efficiency, disregarding hardware specific factors. Similarly, Chien et al. (2023) estimate the energy consumption associated with the inference phase of generative AI applications based on the output word count and several assumptions about the application such as the number of FLOPS per inference and the sampling rate.", "doc_id": "zschache2025", "page": 3, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "2 Previous research\n\nResearch on the environmental impact of machine learning (ML) has primarily focused on the energy consumption and carbon emissions produced during the training phase of large-scale models. Most famously, Strubell et al. (2019) quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes (their measurements include thousands of hyperparameter tuning jobs, which makes it difficult to disentangle model-inherent efficiency from experimental setup). This seminal work spurred further investigations into the environmental costs of training neural networks, including large language models (Patterson et al., 2021; Luccioni and Hernandez-Garcia, 2023; Patterson et al., 2022).\n\nWhile training remains a significant contributor to energy consumption, recent studies have begun to focus on the inference phase. Samsi et al. (2023) highlighted the substantial energy demands of LLM inference but did not explore the relationship between energy consumption and task-specific performance. Liu et al. (2022) underscore the importance of evaluating NLP models not just on efficiency metrics but also on accuracy by introducing the Efficient Language Understanding Evaluation (ELUE) benchmark. ELUE aims to establish a Pareto frontier that balances performance and efficiency. It includes various language understanding tasks, facilitating fair and comprehensive comparisons among models. However, the framework adopts number of parameters and FLOPs as the metrics for model efficiency, disregarding hardware specific factors. Similarly, Chien et al. (2023) estimate the energy consumption associated with the inference phase of generative AI applications based on the output word count and several assumptions about the application such as the number of FLOPS per inference and the sampling rate.", "original_types": ["text", "header"], "id": 1432}
{"type": "section", "content": "3 Data and methods\n\nOur experiments are inspired by an occasionally occurring use case in public administration: the management of objections that are submitted by the population. Due to a potentially very large amount of submissions, an automatic preprocessing of the objections is of high value. One of the possible steps of an automated workflow is to categorize each submission for optimal forwarding to the responsible department. The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is given by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains the category of the text. We scraped the dataset from the website and restricted it to entries for which the topic occurs at least 10 times. The remaining 378 entries were split into half: 189 entries for training and 189 entries for testing. This unusual 50:50 split was done so that the test set should be sufficiently representative by containing enough examples of each of the 14 categories. Each of the following experiments was repeated 10 times with different train-test-splits. To increase comparability, every experiment was run with the same 10 train-test-splits. An experiment run consists of a training phase and a testing phase. Since large language models have been argued to be applicable to text categorization without training (zero-shot), we omit the training phase for these models and apply LLMs without fine-tuning. We report the energy consumption and accuracy only for the test phase as averages over all runs.", "doc_id": "zschache2025", "page": 4, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "3 Data and methods\n\nOur experiments are inspired by an occasionally occurring use case in public administration: the management of objections that are submitted by the population. Due to a potentially very large amount of submissions, an automatic preprocessing of the objections is of high value. One of the possible steps of an automated workflow is to categorize each submission for optimal forwarding to the responsible department. The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is given by the column ‘Beitrag’ (input). The column ‘Themenkomplex’ (topic) contains the category of the text. We scraped the dataset from the website and restricted it to entries for which the topic occurs at least 10 times. The remaining 378 entries were split into half: 189 entries for training and 189 entries for testing. This unusual 50:50 split was done so that the test set should be sufficiently representative by containing enough examples of each of the 14 categories. Each of the following experiments was repeated 10 times with different train-test-splits. To increase comparability, every experiment was run with the same 10 train-test-splits. An experiment run consists of a training phase and a testing phase. Since large language models have been argued to be applicable to text categorization without training (zero-shot), we omit the training phase for these models and apply LLMs without fine-tuning. We report the energy consumption and accuracy only for the test phase as averages over all runs.", "original_types": ["text", "header"], "id": 1433}
{"type": "section", "content": "3.2 Large language models\n\nLarge language models (LLMs) were applied without training (zero-shot) using the test set only. Table 1 gives the names and sources of the models used. The LLMs were selected by the following criteria:\n\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\n\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architecture that includes next to Transformer also Mamba layers (a state-space model). The Deepseek distillations (DS) were added to include models with reasoning capabilities (test-time compute).", "doc_id": "zschache2025", "page": 5, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "3.2 Large language models\n\nLarge language models (LLMs) were applied without training (zero-shot) using the test set only. Table 1 gives the names and sources of the models used. The LLMs were selected by the following criteria:\n\n• availability on Huggingface\n• support of german language\n• capability of processing the dspy-prompt (see appendix A)\n\nAdditionally, Jamba Mini 1.5 was chosen as model with an alternative architecture that includes next to Transformer also Mamba layers (a state-space model). The Deepseek distillations (DS) were added to include models with reasoning capabilities (test-time compute).", "original_types": ["text", "header", "list"], "id": 1434}
{"type": "table", "content": "Table 1 Selection of large language models\nLlama 3.1 8B | https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B | https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B | https://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B | https://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini | https://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE | https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5 | https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "doc_id": "zschache2025", "page": 5, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table 1 Selection of large language models\nLlama 3.1 8B | https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\nLlama 3.1 70B | https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\nQwen 2.5 7B | https://huggingface.co/Qwen/Qwen2-7B-Instruct\nQwen 2.5 72B | https://huggingface.co/Qwen/Qwen2-72B-Instruct\nPhi 3.5 Mini | https://huggingface.co/microsoft/Phi-3.5-mini-instruct\nPhi 3.5 MoE | https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\nJamba Mini 1.5 | https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini\nDS Qwen 14B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\nDS Qwen 32B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\nDS Llama 8B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\nDS Llama 70B | https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "id": 1435}
{"type": "section", "content": "differences - especially GPU tensor core capabilities - affect the inference speed and power usage. A diversity in computational infrastructure is crucial for generalizing findings across different environments and ensuring the validity and replicability of experimental results in machine learning research. Furthermore, insights gained from using multiple platforms contribute to optimizing resource allocation strategies and improving cost-effectiveness in large-scale machine learning projects.", "doc_id": "zschache2025", "page": 6, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "differences - especially GPU tensor core capabilities - affect the inference speed and power usage. A diversity in computational infrastructure is crucial for generalizing findings across different environments and ensuring the validity and replicability of experimental results in machine learning research. Furthermore, insights gained from using multiple platforms contribute to optimizing resource allocation strategies and improving cost-effectiveness in large-scale machine learning projects.", "original_types": ["text"], "id": 1436}
{"type": "table", "content": "Table 2: HPC Resources\n| Cluster | Capella | Paula | Clara |\n| --- | --- | --- | --- |\n| HPC center | TUD Dresden University of Technology | Leipzig University | Leipzig University |\n| number of nodes | 144 | 12 | 6 |\n| CPU per node | 2 x AMD (32 cores) 2.7GHz | 2 x AMD (64 cores) 2.0GHz | 1 x AMD (32 cores) 2.0GHz |\n| RAM per node | 768 GB | 1 TB | 512 GB |\n| GPU per node | 4 x NVIDIA H100 (94GB) | 8 x NVIDIA A30 (24 GB) | 4 x NVIDIA V100 (32GB) |\n| single GPU max power consumption | 700W | 165W | 250W |\n", "doc_id": "zschache2025", "page": 6, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table 2: HPC Resources\n| Cluster | Capella | Paula | Clara |\n| --- | --- | --- | --- |\n| HPC center | TUD Dresden University of Technology | Leipzig University | Leipzig University |\n| number of nodes | 144 | 12 | 6 |\n| CPU per node | 2 x AMD (32 cores) 2.7GHz | 2 x AMD (64 cores) 2.0GHz | 1 x AMD (32 cores) 2.0GHz |\n| RAM per node | 768 GB | 1 TB | 512 GB |\n| GPU per node | 4 x NVIDIA H100 (94GB) | 8 x NVIDIA A30 (24 GB) | 4 x NVIDIA V100 (32GB) |\n| single GPU max power consumption | 700W | 165W | 250W |\n", "id": 1437}
{"type": "section", "content": "LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations. If a model is too large to be deployed on a single GPU, the model weights are distributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing nodes are needed, the model is split into two parts and executed sequentially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model part on the first node and then the model part on the second node.", "doc_id": "zschache2025", "page": 6, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations. If a model is too large to be deployed on a single GPU, the model weights are distributed over multiple GPUs, which allow for a parallel computation of the activations (c.f. tensor model parallelism (TMP) in Bai et al., 2024, pp.16). In cases where two computing nodes are needed, the model is split into two parts and executed sequentially (c.f. pipeline model parallelism (PMP) in Bai et al., 2024, p.17): first the model part on the first node and then the model part on the second node.", "original_types": ["text"], "id": 1438}
{"type": "section", "content": "4 Results\n\nFor each model, we report accuracy, energy consumption, and inference duration. The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded. One inference run involves classifying 189 text samples from a test set. All tables and figures present the average results over 10 runs on different test sets, with the same 10 test sets used for each model. Measurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both energy consumption and duration (logarithmically scaled to base 10).", "doc_id": "zschache2025", "page": 7, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "4 Results\n\nFor each model, we report accuracy, energy consumption, and inference duration. The energy consumption and duration were measured only for the inference step, i.e., after the model and data were already loaded. One inference run involves classifying 189 text samples from a test set. All tables and figures present the average results over 10 runs on different test sets, with the same 10 test sets used for each model. Measurement variance was generally low: < 0.002 for accuracy, and < 0.2 dex for both energy consumption and duration (logarithmically scaled to base 10).", "original_types": ["text", "header"], "id": 1439}
{"type": "figure", "content": "Figure 1: Title...", "doc_id": "zschache2025", "page": 7, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Figure 1: Title...", "id": 1440}
{"type": "section", "content": "Figure 1 illustrates the trade-off between energy consumption and accuracy across all models. For these experiments, a single node of the Capella system was used. The minimum number of H100 GPUs required varies by model (see Table B1).\n\nThe highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Notably, even the most energy-efficient model - a linear model with TF-IDF features - outperformed several large language models (LLMs). Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive reasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs while consuming significantly more energy and taking longer to complete inference.", "doc_id": "zschache2025", "page": 7, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Figure 1 illustrates the trade-off between energy consumption and accuracy across all models. For these experiments, a single node of the Capella system was used. The minimum number of H100 GPUs required varies by model (see Table B1).\n\nThe highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Notably, even the most energy-efficient model - a linear model with TF-IDF features - outperformed several large language models (LLMs). Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points. Deepseek models, despite their extensive reasoning processes during inference, exhibit lower accuracy than non-reasoning LLMs while consuming significantly more energy and taking longer to complete inference.", "original_types": ["text"], "id": 1441}
{"type": "figure", "content": "Fig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single node). The energy consumption for the same task spans over six orders of magnitude with traditional models being the most energy-efficient models and reasoning models are most energy-consuming. The best model for this specific task is a traditional model (Linear Embedding) with moderate energy consumption.", "doc_id": "zschache2025", "page": 8, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 1 Accuracy-energy-trade-off of all models for the inference task on the Capella system (single node). The energy consumption for the same task spans over six orders of magnitude with traditional models being the most energy-efficient models and reasoning models are most energy-consuming. The best model for this specific task is a traditional model (Linear Embedding) with moderate energy consumption.", "id": 1442}
{"type": "figure", "content": "Fig. 2 Energy consumption of all models for the inference task on the Capella system (single node)", "doc_id": "zschache2025", "page": 9, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 2 Energy consumption of all models for the inference task on the Capella system (single node)", "id": 1443}
{"type": "figure", "content": "Fig. 3 Effects of the number of GPUs on the runtime and consumed energy (Capella, single node). Deepseek models are not shown.", "doc_id": "zschache2025", "page": 9, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 3 Effects of the number of GPUs on the runtime and consumed energy (Capella, single node). Deepseek models are not shown.", "id": 1444}
{"type": "figure", "content": "Fig. 4 Comparison single node vs. double node deployment (Capella).", "doc_id": "zschache2025", "page": 10, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 4 Comparison single node vs. double node deployment (Capella).", "id": 1445}
{"type": "section", "content": "text is generated. For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.", "doc_id": "zschache2025", "page": 10, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "text is generated. For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.", "original_types": ["text"], "id": 1446}
{"type": "figure", "content": "Fig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment.", "doc_id": "zschache2025", "page": 10, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 5 Comparison of different GPU cards: four exemplary LLMs. Single node deployment.", "id": 1447}
{"type": "section", "content": "all experiments conducted on a single node of the Capella cluster. When controlling for the number of GPUs used for model deployment, the relation between duration and energy is approximately linear. Therefore, the duration appears to serve as a good proxy for the energy consumed.", "doc_id": "zschache2025", "page": 11, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "all experiments conducted on a single node of the Capella cluster. When controlling for the number of GPUs used for model deployment, the relation between duration and energy is approximately linear. Therefore, the duration appears to serve as a good proxy for the energy consumed.", "original_types": ["text"], "id": 1448}
{"type": "figure", "content": "Fig. 6 Plotting the relationship between duration and energy consumption (single node on Capella). The lines are added by running a linear regression model.", "doc_id": "zschache2025", "page": 11, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 6 Plotting the relationship between duration and energy consumption (single node on Capella). The lines are added by running a linear regression model.", "id": 1449}
{"type": "section", "content": "This relationship suggests that, under fixed hardware conditions, monitoring the duration of computations provides an efficient means of estimating energy usage with minimal additional measurement overhead.", "doc_id": "zschache2025", "page": 12, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "This relationship suggests that, under fixed hardware conditions, monitoring the duration of computations provides an efficient means of estimating energy usage with minimal additional measurement overhead.", "original_types": ["text"], "id": 1450}
{"type": "table", "content": "Table 3\nLinear regression of energy consumption on duration (table format by Hlavac, 2022). The numbers (coefficients) give the estimated effects of each predictor on the dependent variable. A positive coefficient means the variable increases the outcome, while a negative coefficient means it decreases the outcome. The standard error (in parenthesis) estimates the variability of the coefficient estimate. The p-value (given by the asterisks) indicates whether the predictor is statistically significant (different from zero).", "doc_id": "zschache2025", "page": 12, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table 3\nLinear regression of energy consumption on duration (table format by Hlavac, 2022). The numbers (coefficients) give the estimated effects of each predictor on the dependent variable. A positive coefficient means the variable increases the outcome, while a negative coefficient means it decreases the outcome. The standard error (in parenthesis) estimates the variability of the coefficient estimate. The p-value (given by the asterisks) indicates whether the predictor is statistically significant (different from zero).", "id": 1451}
{"type": "section", "content": "We would like to mention the limitations of our study, which also point to the areas of future research. First, while traditional models were trained on approximately 200 examples, the large language models (LLMs) were applied in a zero-shot setting, meaning they had no access to labeled examples. Previous research has shown that few-shot prompting - where representative examples are included in the prompt - can improve performance (Brown et al., 2020). For the present study, we kept the prompt", "doc_id": "zschache2025", "page": 12, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "We would like to mention the limitations of our study, which also point to the areas of future research. First, while traditional models were trained on approximately 200 examples, the large language models (LLMs) were applied in a zero-shot setting, meaning they had no access to labeled examples. Previous research has shown that few-shot prompting - where representative examples are included in the prompt - can improve performance (Brown et al., 2020). For the present study, we kept the prompt", "original_types": ["text"], "id": 1452}
{"type": "section", "content": "as simple as possible (see section A). But in an actual application, we would add background information about the data and the categories. In general, prompt engineering, the addition of representative examples to the prompt, or even fine-tuning an LLM could yield higher accuracy rates. On the other hand, energy efficiency in LLMs can be improved through model quantization, which reduces computational demands by compressing model parameters (Jacob et al., 2017).\n\nSecond, we do not account for the energy costs associated with training the traditional models because it is infeasible to compare them to the training costs of LLMs. The LLMs used in this study were pre-trained by external organizations and made publicly available. As a result, the energy costs of training are distributed among all users, making it difficult to estimate per-user energy consumption. Even if training energy costs for an LLM were known, the number of users remains uncertain. Additionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also contribute to energy consumption. Deploying an LLM on a dedicated server (e.g., using vLLM) requires setup time and additional energy. Beyond inference, significant time and computational resources are also required for development tasks, including data processing, testing different models and prompts, parameter tuning, and debugging - workloads that apply to both traditional models and LLMs. The measurement of additional related energy consumptions (such as network traffic or disk storage) is beyond the scope of this paper.\n\nThird, energy consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine’s total energy use (Bouza et al., 2023). However, it does not allow for precise measurement of energy consumption at the level of individual processes. Moreover, power intake was recorded at 15-second intervals, meaning the accuracy of energy estimates improves with longer-running processes. Another limitation of CodeCarbon is that RAM energy consumption is approximated at 0.375W per GB of memory used. While the Running Average Power Limit (RAPL) framework can directly measure RAM power consumption, it is not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#issuecomment-2589805160). Additionally, in shared computing environments such as high-performance computing (HPC) clusters, measurements may be affected by other users’ activities. Especially when an LLM was deployed across multiple nodes, variations in network traffic at different times may have influenced energy measurements. A more precise assessment of energy efficiency would benefit from using dedicated computing resources with physical wattmeters and high-resolution energy measurement tools(e.g. Ilsche et al., 2019).", "doc_id": "zschache2025", "page": 13, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "as simple as possible (see section A). But in an actual application, we would add background information about the data and the categories. In general, prompt engineering, the addition of representative examples to the prompt, or even fine-tuning an LLM could yield higher accuracy rates. On the other hand, energy efficiency in LLMs can be improved through model quantization, which reduces computational demands by compressing model parameters (Jacob et al., 2017).\n\nSecond, we do not account for the energy costs associated with training the traditional models because it is infeasible to compare them to the training costs of LLMs. The LLMs used in this study were pre-trained by external organizations and made publicly available. As a result, the energy costs of training are distributed among all users, making it difficult to estimate per-user energy consumption. Even if training energy costs for an LLM were known, the number of users remains uncertain. Additionally, hosting LLMs (e.g., on Hugging Face) and managing network traffic also contribute to energy consumption. Deploying an LLM on a dedicated server (e.g., using vLLM) requires setup time and additional energy. Beyond inference, significant time and computational resources are also required for development tasks, including data processing, testing different models and prompts, parameter tuning, and debugging - workloads that apply to both traditional models and LLMs. The measurement of additional related energy consumptions (such as network traffic or disk storage) is beyond the scope of this paper.\n\nThird, energy consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine’s total energy use (Bouza et al., 2023). However, it does not allow for precise measurement of energy consumption at the level of individual processes. Moreover, power intake was recorded at 15-second intervals, meaning the accuracy of energy estimates improves with longer-running processes. Another limitation of CodeCarbon is that RAM energy consumption is approximated at 0.375W per GB of memory used. While the Running Average Power Limit (RAPL) framework can directly measure RAM power consumption, it is not supported on all CPUs (https://github.com/mlco2/codecarbon/issues/717#issuecomment-2589805160). Additionally, in shared computing environments such as high-performance computing (HPC) clusters, measurements may be affected by other users’ activities. Especially when an LLM was deployed across multiple nodes, variations in network traffic at different times may have influenced energy measurements. A more precise assessment of energy efficiency would benefit from using dedicated computing resources with physical wattmeters and high-resolution energy measurement tools(e.g. Ilsche et al., 2019).", "original_types": ["text"], "id": 1453}
{"type": "section", "content": "In the following, we assess further limitations of the present study in more detail. More specifically, we address our focus on a single dataset in section 5.1 and the limitation to the text categorisation task in section 5.2. Subsequently, we contextualise our work in the broader context of planet-centered LLMs (section 5.3).", "doc_id": "zschache2025", "page": 13, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "In the following, we assess further limitations of the present study in more detail. More specifically, we address our focus on a single dataset in section 5.1 and the limitation to the text categorisation task in section 5.2. Subsequently, we contextualise our work in the broader context of planet-centered LLMs (section 5.3).", "original_types": ["text"], "id": 1454}
{"type": "section", "content": "Based on popularity and had to meet two criteria: suitability for text classification and inclusion of two columns - text and label. To maintain comparability with our initial analysis, we randomly sampled 200 training examples and 200 test examples from each dataset. Using a slightly larger training set might have provided an advantage to traditional models, as the LLMs were applied in a zero-shot setting without fine-tuning. Each model experiment was repeated 10 times with different samples, ensuring that each model was tested on the same 10 sets.", "doc_id": "zschache2025", "page": 14, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Based on popularity and had to meet two criteria: suitability for text classification and inclusion of two columns - text and label. To maintain comparability with our initial analysis, we randomly sampled 200 training examples and 200 test examples from each dataset. Using a slightly larger training set might have provided an advantage to traditional models, as the LLMs were applied in a zero-shot setting without fine-tuning. Each model experiment was repeated 10 times with different samples, ensuring that each model was tested on the same 10 sets.", "original_types": ["text"], "id": 1455}
{"type": "table", "content": "Table 4 Selection of datasets for text classification tasks.\n| Name | Classification Task | ID on https://huggingface.co/datasets | \n| --- | --- | --- | \n| news | news topics: World, Sports, Business, Sci/Tech | fancyzhx/ag_news | \n| yelp | sentiment: 1-5 stars | Yelp/yelp_review_full | \n| tomatoes | sentiment: pos, neg | cornell-movie-review-data/rotten_tomatoes | \n| emotion | emotion: anger, fear, joy, love, sadness, surprise | dair-ai/emotion |", "doc_id": "zschache2025", "page": 14, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table 4 Selection of datasets for text classification tasks.\n| Name | Classification Task | ID on https://huggingface.co/datasets | \n| --- | --- | --- | \n| news | news topics: World, Sports, Business, Sci/Tech | fancyzhx/ag_news | \n| yelp | sentiment: 1-5 stars | Yelp/yelp_review_full | \n| tomatoes | sentiment: pos, neg | cornell-movie-review-data/rotten_tomatoes | \n| emotion | emotion: anger, fear, joy, love, sadness, surprise | dair-ai/emotion |", "id": 1456}
{"type": "figure", "content": "Fig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the Linear Embedding model was added for comparison), Capella system, single node. See Tables B4 and B5 for results of all models.", "doc_id": "zschache2025", "page": 15, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Fig. 7 Accuracy-energy-trade-off of the best models for the inference task on different datasets (the Linear Embedding model was added for comparison), Capella system, single node. See Tables B4 and B5 for results of all models.", "id": 1457}
{"type": "section", "content": "5.3 Further Requirements of Planet-Centered LLMs\n\nWhile energy consumption and the associated carbon footprint remain crucial considerations for sustainable AI, truly planet-centered LLMs must meet a broader set of requirements that go beyond mere efficiency. These include other limited resources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical considerations, and technical adaptability to ensure responsible and sustainable AI deployment.", "doc_id": "zschache2025", "page": 16, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "5.3 Further Requirements of Planet-Centered LLMs\n\nWhile energy consumption and the associated carbon footprint remain crucial considerations for sustainable AI, truly planet-centered LLMs must meet a broader set of requirements that go beyond mere efficiency. These include other limited resources (water, rare-earth metals, landuse,...), transparency, accessibility, ethical considerations, and technical adaptability to ensure responsible and sustainable AI deployment.", "original_types": ["text", "header"], "id": 1458}
{"type": "section", "content": "long-term sustainability. Future AI systems should prioritize modularity and adaptability, enabling efficient integration into diverse infrastructures without excessive resource demands.\n\nThe relevance and fairness of AI-generated outputs depend on the quality and recency of training data. Stale or biased datasets can lead to misleading results and reinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In particular, the presence of toxic content or hate speech in training data can result in models generating harmful or discriminatory outputs, which poses serious challenges for their deployment in sensitive contexts such as education, healthcare, or public administration. Moreover, safety concerns—such as the risk of models producing factually incorrect, manipulative, or otherwise harmful content—are especially critical in public-sector applications, where accountability and trust are paramount (Weidinger et al., 2021). Addressing these challenges requires robust bias-mitigation strategies and transparent documentation of model behavior.\n\nTo align with global sustainability and ethical AI principles, future research should emphasize the development of adaptable, transparent, and energy-efficient LLMs. By integrating principles of openness, fairness, and regulatory compliance, we can foster AI systems that not only minimize environmental impact but also promote responsible and equitable usage across sectors.\n\nAcknowledgements. We gratefully acknowledge the support provided by the Federal Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV). Additionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their support and inspiration.\n\nThis work was supported by high-performance computer time and resources from the Center for Information Services and High Performance Computing (ZIH) of TUD Dresden University of Technology and the systems for scientific computing of Leipzig University. We thank the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.\n\nThe tool ChatGPT (OpenAI) was used to revise the text of the paper.\n\nAuthor contribution statements. T.H. conceived the study, initiated the project, led the research effort, and contributed to the literature review and manuscript writing. J.Z. designed and implemented the experiments, developed the codebase, conducted data analysis, and contributed to drafting the manuscript.\n\nCompeting interests. There are no competing interests.\n\nAvailability of data and code. All underlying data will be shared upon reasonable request to the corresponding author. The source code will be made public.", "doc_id": "zschache2025", "page": 17, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "long-term sustainability. Future AI systems should prioritize modularity and adaptability, enabling efficient integration into diverse infrastructures without excessive resource demands.\n\nThe relevance and fairness of AI-generated outputs depend on the quality and recency of training data. Stale or biased datasets can lead to misleading results and reinforce harmful stereotypes (Bender et al., 2021; Gehman et al., 2020). In particular, the presence of toxic content or hate speech in training data can result in models generating harmful or discriminatory outputs, which poses serious challenges for their deployment in sensitive contexts such as education, healthcare, or public administration. Moreover, safety concerns—such as the risk of models producing factually incorrect, manipulative, or otherwise harmful content—are especially critical in public-sector applications, where accountability and trust are paramount (Weidinger et al., 2021). Addressing these challenges requires robust bias-mitigation strategies and transparent documentation of model behavior.\n\nTo align with global sustainability and ethical AI principles, future research should emphasize the development of adaptable, transparent, and energy-efficient LLMs. By integrating principles of openness, fairness, and regulatory compliance, we can foster AI systems that not only minimize environmental impact but also promote responsible and equitable usage across sectors.\n\nAcknowledgements. We gratefully acknowledge the support provided by the Federal Ministry for the Environment, Nature Conservation and Nuclear Safety (BMUV). Additionally, we thank colleagues from Z 2.3 and the entire AI-Lab team for their support and inspiration.\n\nThis work was supported by high-performance computer time and resources from the Center for Information Services and High Performance Computing (ZIH) of TUD Dresden University of Technology and the systems for scientific computing of Leipzig University. We thank the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI Dresden/Leipzig) for their support in the acquisition process.\n\nThe tool ChatGPT (OpenAI) was used to revise the text of the paper.\n\nAuthor contribution statements. T.H. conceived the study, initiated the project, led the research effort, and contributed to the literature review and manuscript writing. J.Z. designed and implemented the experiments, developed the codebase, conducted data analysis, and contributed to drafting the manuscript.\n\nCompeting interests. There are no competing interests.\n\nAvailability of data and code. All underlying data will be shared upon reasonable request to the corresponding author. The source code will be made public.", "original_types": ["text"], "id": 1459}
{"type": "section", "content": "Appendix A  LLM prompt\n\nFor the zero-shot classification, we prompted the LLM with the following instruction (originally in German):\n\nClassify the text as one of the following categories:\n\n– <category 1>\n\n– <category 2>\n\n– …\n\nThe categories were a fixed set of 14 options that occurred in the training as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive Störungszonen’, ‘Öffentlichkeitsbeteiligung’, ‘Kristalllingestein’, ‘FEP/Szenarien/Entwicklungen des Endlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler Lagerung’, ‘Datenverfügbarkeit’, ‘Modellierung’, ‘Referenzdatensätze’, ‘Bereitstellung der Daten’, ‘Ausschlusskriterien’.\n\nSince we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the final prompt was automatically extended to the following:\n\n– role: system\n\ncontent: |\n\nYour input fields are:\n\n1. ‘text’ (str)\n\nYour output fields are:\n\n1. ‘category’ (str)\n\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## text ## ]]\n\n{text}\n\n[[ ## category ## ]]\n\n{category}\n\n[[ ## completed ## ]]\n\nIn adhering to this structure, your objective is:\n\nClassify the text as one of the following categories:\n\n– <category 1>\n\n– <category 2>\n\n– …\n\n– role: user\n\ncontent: |\n\n[[ ## text ## ]]\n\n<text>\n\nRespond with the corresponding output fields, starting with the field ‘[[ ## category ## ]]’, and then ending with the marker for ‘[[ ## completed ## ]]’.", "doc_id": "zschache2025", "page": 18, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Appendix A  LLM prompt\n\nFor the zero-shot classification, we prompted the LLM with the following instruction (originally in German):\n\nClassify the text as one of the following categories:\n\n– <category 1>\n\n– <category 2>\n\n– …\n\nThe categories were a fixed set of 14 options that occurred in the training as well as the test dataset: ‘geoWK’, ‘Tongestein’, ‘Aktive Störungszonen’, ‘Öffentlichkeitsbeteiligung’, ‘Kristalllingestein’, ‘FEP/Szenarien/Entwicklungen des Endlagersystems’, ‘Anwendung geoWK’, ‘Mindestanforderungen’, ‘Steinsalz in steiler Lagerung’, ‘Datenverfügbarkeit’, ‘Modellierung’, ‘Referenzdatensätze’, ‘Bereitstellung der Daten’, ‘Ausschlusskriterien’.\n\nSince we deployed the dspy framework (https://dspy.ai/) to query the LLMs, the final prompt was automatically extended to the following:\n\n– role: system\n\ncontent: |\n\nYour input fields are:\n\n1. ‘text’ (str)\n\nYour output fields are:\n\n1. ‘category’ (str)\n\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## text ## ]]\n\n{text}\n\n[[ ## category ## ]]\n\n{category}\n\n[[ ## completed ## ]]\n\nIn adhering to this structure, your objective is:\n\nClassify the text as one of the following categories:\n\n– <category 1>\n\n– <category 2>\n\n– …\n\n– role: user\n\ncontent: |\n\n[[ ## text ## ]]\n\n<text>\n\nRespond with the corresponding output fields, starting with the field ‘[[ ## category ## ]]’, and then ending with the marker for ‘[[ ## completed ## ]]’.", "original_types": ["text", "header"], "id": 1460}
{"type": "section", "content": "Appendix B  Tables", "doc_id": "zschache2025", "page": 19, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Appendix B  Tables", "original_types": ["header"], "id": 1461}
{"type": "table", "content": "Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs\nModel GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)\nLinear BoW 1 <0.01 0.43 0.01 139.96\nLinear Tf-idf 1 <0.01 0.41 0.01 43.72\nLinear Embedding 1 0.12 0.57 1.64 259.41\nXGBoost BoW 1 <0.01 0.35 0.01 63.32\nXGBoost Tf-idf 1 <0.01 0.47 0.01 67.77\nXGBoost Embedding 1 0.21 0.47 2.87 259.94\nLlama 3.1 8B 1 5.86 0.35 36.88 572.49\nLlama 3.1 70B 2 48.60 0.48 161.59 1082.82\nQwen 2.5 7B 1 5.58 0.45 36.28 553.84\nQwen 2.5 72B 2 48.66 0.51 164.44 1065.31\nPhi 3.5 Mini 1 5.74 0.30 41.45 498.46\nPhi 3.5 MoE 2 11.00 0.40 55.51 713.34\nJamba Mini 1.5 2 17.42 0.34 78.61 797.94\nDS Llama 8B 1 79.64 0.37 517.83 553.67\nDS Llama 70B 2 702.06 0.46 2543.47 993.68\nDS Qwen 14B 1 155.20 0.39 981.35 569.33\nDS Qwen 32B 1 373.56 0.45 2255.99 596.11", "doc_id": "zschache2025", "page": 19, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs\nModel GPUs Energy (Wh) Accuracy Duration (s) Average Power (W)\nLinear BoW 1 <0.01 0.43 0.01 139.96\nLinear Tf-idf 1 <0.01 0.41 0.01 43.72\nLinear Embedding 1 0.12 0.57 1.64 259.41\nXGBoost BoW 1 <0.01 0.35 0.01 63.32\nXGBoost Tf-idf 1 <0.01 0.47 0.01 67.77\nXGBoost Embedding 1 0.21 0.47 2.87 259.94\nLlama 3.1 8B 1 5.86 0.35 36.88 572.49\nLlama 3.1 70B 2 48.60 0.48 161.59 1082.82\nQwen 2.5 7B 1 5.58 0.45 36.28 553.84\nQwen 2.5 72B 2 48.66 0.51 164.44 1065.31\nPhi 3.5 Mini 1 5.74 0.30 41.45 498.46\nPhi 3.5 MoE 2 11.00 0.40 55.51 713.34\nJamba Mini 1.5 2 17.42 0.34 78.61 797.94\nDS Llama 8B 1 79.64 0.37 517.83 553.67\nDS Llama 70B 2 702.06 0.46 2543.47 993.68\nDS Qwen 14B 1 155.20 0.39 981.35 569.33\nDS Qwen 32B 1 373.56 0.45 2255.99 596.11", "id": 1462}
{"type": "table", "content": "Table B2 Comparison single vs. double node deployment, Capella system\nModel Duration (s) Energy consumed (Wh)\nsingle double ratio single double ratio\nLlama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95\nQwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97\nJamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71\nDS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71", "doc_id": "zschache2025", "page": 19, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B2 Comparison single vs. double node deployment, Capella system\nModel Duration (s) Energy consumed (Wh)\nsingle double ratio single double ratio\nLlama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95\nQwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97\nJamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71\nDS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71", "id": 1463}
{"type": "table", "content": "Table B3 Comparison of different GPU cards, single node deployment.\nModel Duration (s) Energy consumed (Wh)\nA30 V100 H100 A30 V100 H100\nLlama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86\nQwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58\nPhi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74\nPhi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04\nDS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64\nDS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58\nDS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58", "doc_id": "zschache2025", "page": 19, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B3 Comparison of different GPU cards, single node deployment.\nModel Duration (s) Energy consumed (Wh)\nA30 V100 H100 A30 V100 H100\nLlama 3.1 8B 20.78 27.52 36.88 2.91 2.88 5.86\nQwen 2.5 7B 19.58 24.64 36.28 2.87 2.63 5.58\nPhi 3.5 Mini 19.18 25.02 41.45 2.65 2.50 5.74\nPhi 3.5 MoE 77.60 32.53 45.93 17.77 6.04 15.04\nDS Llama 8B 1210.90 1439.58 517.83 175.83 137.90 79.64\nDS Qwen 14B 1348.09 1736.21 624.38 254.01 230.72 157.58\nDS Qwen 32B 1688.23 2192.53 806.68 444.67 457.60 378.58", "id": 1464}
{"type": "table", "content": "Table B4 Measurements of all models for the inference task on the news and yelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset news yelp\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.65 <0.01 0.36\nLinear Tf-idf <0.01 0.65 <0.01 0.34\nLinear Embedding <0.01 0.83 0.04 0.43\nXGBoost BoW <0.01 0.48 <0.01 0.31\nXGBoost Tf-idf <0.01 0.52 <0.01 0.29\nXGBoost Embedding 0.03 0.74 0.01 0.40\nLlama 3.1 8B 4.31 0.71 4.73 0.58\nLlama 3.1 70B 34.15 0.88 36.71 0.67\nQwen 2.5 7B 4.21 0.01 4.52 0.60\nQwen 2.5 72B 33.75 0.79 38.20 0.68\nPhi 3.5 Mini 3.30 0.53 15.55 0.58\nPhi 3.5 MoE 8.53 0.78 8.32 0.58\nJamba Mini 1.5 9.34 0.78 11.45 0.56\nDS Llama 8B 60.58 0.82 97.18 0.62\nDS Llama 70B 483.73 0.83 707.03 0.67\nDS Qwen 14B 113.81 0.83 177.41 0.63\nDS Qwen 32B 271.92 0.83 358.62 0.63", "doc_id": "zschache2025", "page": 20, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B4 Measurements of all models for the inference task on the news and yelp datasets, Capella system, single node, shown are averages over 10 runs\nDataset news yelp\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.65 <0.01 0.36\nLinear Tf-idf <0.01 0.65 <0.01 0.34\nLinear Embedding <0.01 0.83 0.04 0.43\nXGBoost BoW <0.01 0.48 <0.01 0.31\nXGBoost Tf-idf <0.01 0.52 <0.01 0.29\nXGBoost Embedding 0.03 0.74 0.01 0.40\nLlama 3.1 8B 4.31 0.71 4.73 0.58\nLlama 3.1 70B 34.15 0.88 36.71 0.67\nQwen 2.5 7B 4.21 0.01 4.52 0.60\nQwen 2.5 72B 33.75 0.79 38.20 0.68\nPhi 3.5 Mini 3.30 0.53 15.55 0.58\nPhi 3.5 MoE 8.53 0.78 8.32 0.58\nJamba Mini 1.5 9.34 0.78 11.45 0.56\nDS Llama 8B 60.58 0.82 97.18 0.62\nDS Llama 70B 483.73 0.83 707.03 0.67\nDS Qwen 14B 113.81 0.83 177.41 0.63\nDS Qwen 32B 271.92 0.83 358.62 0.63", "id": 1465}
{"type": "table", "content": "Table B5 Measurements of all models for the inference task on the tomatoes and emotion datasets, Capella system, single node, shown are averages over 10 runs\nDataset tomatoes emotion\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.59 <0.01 0.36\nLinear Tf-idf <0.01 0.59 <0.01 0.40\nLinear Embedding 0.01 0.79 <0.01 0.59\nXGBoost BoW <0.01 0.54 <0.01 0.30\nXGBoost Tf-idf <0.01 0.55 <0.01 0.33\nXGBoost Embedding <0.01 0.76 <0.01 0.53\nLlama 3.1 8B 4.12 0.87 4.46 0.56\nLlama 3.1 70B 32.07 0.91 34.12 0.58\nQwen 2.5 7B 4.04 0.73 4.17 0.37\nQwen 2.5 72B 33.25 0.91 34.81 0.58\nPhi 3.5 Mini 7.20 0.87 5.13 0.53\nPhi 3.5 MoE 7.72 0.89 8.82 0.60\nJamba Mini 1.5 8.37 0.91 10.22 0.56\nDS Llama 8B 72.15 0.83 81.82 0.60\nDS Llama 70B 510.86 0.90 670.40 0.61\nDS Qwen 14B 134.02 0.89 148.20 0.60\nDS Qwen 32B 246.48 0.89 323.48 0.60", "doc_id": "zschache2025", "page": 20, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B5 Measurements of all models for the inference task on the tomatoes and emotion datasets, Capella system, single node, shown are averages over 10 runs\nDataset tomatoes emotion\nModel Energy (Wh) Accuracy Energy (Wh) Accuracy\nLinear BoW <0.01 0.59 <0.01 0.36\nLinear Tf-idf <0.01 0.59 <0.01 0.40\nLinear Embedding 0.01 0.79 <0.01 0.59\nXGBoost BoW <0.01 0.54 <0.01 0.30\nXGBoost Tf-idf <0.01 0.55 <0.01 0.33\nXGBoost Embedding <0.01 0.76 <0.01 0.53\nLlama 3.1 8B 4.12 0.87 4.46 0.56\nLlama 3.1 70B 32.07 0.91 34.12 0.58\nQwen 2.5 7B 4.04 0.73 4.17 0.37\nQwen 2.5 72B 33.25 0.91 34.81 0.58\nPhi 3.5 Mini 7.20 0.87 5.13 0.53\nPhi 3.5 MoE 7.72 0.89 8.82 0.60\nJamba Mini 1.5 8.37 0.91 10.22 0.56\nDS Llama 8B 72.15 0.83 81.82 0.60\nDS Llama 70B 510.86 0.90 670.40 0.61\nDS Qwen 14B 134.02 0.89 148.20 0.60\nDS Qwen 32B 246.48 0.89 323.48 0.60", "id": 1466}
{"type": "table", "content": "Table B6 Linear regression of energy consumption on duration for the datasets of section 5.1 (table format by Hlavac, 2022).\nDependent variable: Energy\n\n| tomatoes | emotion | news | yelp |\n|----------|---------|------|------|\n| Duration | 0.040*** | 0.043*** | 0.052*** | 0.045*** \n|          | (0.002)  | (0.002)  | (0.003)  | (0.003)  \n|----------|---------|------|------|\n| GPUs     | -0.079  | -0.052  | 0.536  | 0.810   \n|          | (0.950) | (1.011) | (1.470) | (1.545) \n|----------|---------|------|------|\n| Duration:GPUs | 0.122*** | 0.120*** | 0.115*** | 0.120*** \n|          | (0.001)  | (0.001)  | (0.002)  | (0.002)  \n|----------|---------|------|------|\n| Constant  | -0.397  | -0.464  | -1.300 | -1.773  \n|          | (1.290) | (1.372) | (1.985) | (2.103) \n\nObservations | 17 | 17 | 17 | 17 \nR² | 1.000 | 1.000 | 1.000 | 1.000 \nAdjusted R² | 1.000 | 1.000 | 1.000 | 1.000 \n\n*Note: *p<0.1; **p<0.05; ***p<0.01", "doc_id": "zschache2025", "page": 21, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Table B6 Linear regression of energy consumption on duration for the datasets of section 5.1 (table format by Hlavac, 2022).\nDependent variable: Energy\n\n| tomatoes | emotion | news | yelp |\n|----------|---------|------|------|\n| Duration | 0.040*** | 0.043*** | 0.052*** | 0.045*** \n|          | (0.002)  | (0.002)  | (0.003)  | (0.003)  \n|----------|---------|------|------|\n| GPUs     | -0.079  | -0.052  | 0.536  | 0.810   \n|          | (0.950) | (1.011) | (1.470) | (1.545) \n|----------|---------|------|------|\n| Duration:GPUs | 0.122*** | 0.120*** | 0.115*** | 0.120*** \n|          | (0.001)  | (0.001)  | (0.002)  | (0.002)  \n|----------|---------|------|------|\n| Constant  | -0.397  | -0.464  | -1.300 | -1.773  \n|          | (1.290) | (1.372) | (1.985) | (2.103) \n\nObservations | 17 | 17 | 17 | 17 \nR² | 1.000 | 1.000 | 1.000 | 1.000 \nAdjusted R² | 1.000 | 1.000 | 1.000 | 1.000 \n\n*Note: *p<0.1; **p<0.05; ***p<0.01", "id": 1467}
{"type": "section", "content": "transformingourworld\n\nStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep Learning in NLP (2019). https://arxiv.org/abs/1906.02243\n\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon Emissions and Large Neural Network Training (2021). https://arxiv.org/abs/2104.10350\n\nLuccioni, A.S., Hernandez-Garcia, A.: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning (2023). https://arxiv.org/abs/2302.08476\n\nOpenAI: Learning to reason with LLMs (2024). https://openai.com/index/learning-to-reason-with-llms/\n\nDeepSeek-AI: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025). https://arxiv.org/abs/2501.12948\n\nJoulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for efficient text classification. In: Lapata, M., Blunsom, P., Koller, A. (eds.) Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 427–431. Association for Computational Linguistics, Valencia, Spain (2017). https://aclanthology.org/E17-2068/\n\nWang, Z., Pang, Y., Lin, Y., Zhu, X.: Adaptable and Reliable Text Classification using Large Language Models (2024). https://arxiv.org/abs/2405.10523\n\nLangChain Team: Classification Tutorial – LangChain Documentation. https://python.langchain.com/docs/tutorials/classification/. Accessed: 2025-02-23 (2023)\n\nLamini Team: CAT Documentation – Lamini. https://docs.lamini.ai/cat/. Accessed: 2025-02-23 (2023)\n\nKaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning artificial intelligence with climate change mitigation 12(6), 518–527 https://doi.org/10.1038/s41558-022-01377-7\n\nLuccioni, A.S., Strubell, E., Crawford, K.: From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate (2025). https://arxiv.org/abs/2501.16548\n\nPatterson, D., Gonzalez, J., Hölzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink (2022). https://arxiv.org/abs/2204.05149\n\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W., Kepner, J., Tiwari, D., Gadepally, V.: From Words to Watts: Benchmarking the", "doc_id": "zschache2025", "page": 22, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "transformingourworld\n\nStrubell, E., Ganesh, A., McCallum, A.: Energy and Policy Considerations for Deep Learning in NLP (2019). https://arxiv.org/abs/1906.02243\n\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon Emissions and Large Neural Network Training (2021). https://arxiv.org/abs/2104.10350\n\nLuccioni, A.S., Hernandez-Garcia, A.: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning (2023). https://arxiv.org/abs/2302.08476\n\nOpenAI: Learning to reason with LLMs (2024). https://openai.com/index/learning-to-reason-with-llms/\n\nDeepSeek-AI: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025). https://arxiv.org/abs/2501.12948\n\nJoulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for efficient text classification. In: Lapata, M., Blunsom, P., Koller, A. (eds.) Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 427–431. Association for Computational Linguistics, Valencia, Spain (2017). https://aclanthology.org/E17-2068/\n\nWang, Z., Pang, Y., Lin, Y., Zhu, X.: Adaptable and Reliable Text Classification using Large Language Models (2024). https://arxiv.org/abs/2405.10523\n\nLangChain Team: Classification Tutorial – LangChain Documentation. https://python.langchain.com/docs/tutorials/classification/. Accessed: 2025-02-23 (2023)\n\nLamini Team: CAT Documentation – Lamini. https://docs.lamini.ai/cat/. Accessed: 2025-02-23 (2023)\n\nKaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D.: Aligning artificial intelligence with climate change mitigation 12(6), 518–527 https://doi.org/10.1038/s41558-022-01377-7\n\nLuccioni, A.S., Strubell, E., Crawford, K.: From Efficiency Gains to Rebound Effects: The Problem of Jevons’ Paradox in AI’s Polarized Environmental Debate (2025). https://arxiv.org/abs/2501.16548\n\nPatterson, D., Gonzalez, J., Hölzle, U., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink (2022). https://arxiv.org/abs/2204.05149\n\nSamsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W., Kepner, J., Tiwari, D., Gadepally, V.: From Words to Watts: Benchmarking the", "original_types": ["text"], "id": 1468}
{"type": "section", "content": "Energy Costs of Large Language Model Inference (2023). https://arxiv.org/abs/2310.03003\n\nLiu, X., Sun, T., He, J., Wu, J., Wu, L., Zhang, X., Jiang, H., Cao, Z., Huang, X., Qiu, X.: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (2022). https://arxiv.org/abs/2110.07038\n\nChien, A.A., Lin, L., Nguyen, H., Rao, V., Sharma, T., Wijayawardana, R.: Reducing the carbon impact of generative ai inference (today and in 2035). In: Proceedings of the 2nd Workshop on Sustainable Computer Systems. HotCarbon '23. Association for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3604930.3605705 . https://doi.org/10.1145/3604930.3605705\n\nWang, A., Wolf, T.: Overview of the sustainlp 2020 shared task. In: SUSTAINLP (2020). https://api.semanticscholar.org/CorpusID:226283937\n\nAlizadeh, N., Belchev, B., Saurabh, N., Kelbert, P., Castor, F.: Language Models in Software Development Tasks: An Experimental Analysis of Energy and Accuracy (2025). https://arxiv.org/abs/2412.00329\n\nLuccioni, S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the cost of ai deployment? In: Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. FAccT '24, pp. 85–99. Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/3630106.3658542\n\nBai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M., Zhang, Y., Song, X., Yang, C., Cheng, Y., Zhao, L.: Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models (2024). https://arxiv.org/abs/2401.00625\n\nAnthony, L.F.W., Kanding, B., Selvan, R.: Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models (2020). https://arxiv.org/abs/2007.03051\n\nHenderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., Pineau, J.: Towards the systematic reporting of the energy and carbon footprints of machine learning. J. Mach. Learn. Res. 21(1) (2020)\n\nBouza, L., Bugeau, A., Lannelongue, L.: How to estimate carbon footprint when training deep learning models? a guide and review. Environmental Research Communications 5(11), 115014 (2023) https://doi.org/10.1088/2515-7620/acf81b\n\nHlavac, M.: stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3 (2022). https://CRAN.R-project.org/package=stargazer\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,", "doc_id": "zschache2025", "page": 23, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Energy Costs of Large Language Model Inference (2023). https://arxiv.org/abs/2310.03003\n\nLiu, X., Sun, T., He, J., Wu, J., Wu, L., Zhang, X., Jiang, H., Cao, Z., Huang, X., Qiu, X.: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (2022). https://arxiv.org/abs/2110.07038\n\nChien, A.A., Lin, L., Nguyen, H., Rao, V., Sharma, T., Wijayawardana, R.: Reducing the carbon impact of generative ai inference (today and in 2035). In: Proceedings of the 2nd Workshop on Sustainable Computer Systems. HotCarbon '23. Association for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3604930.3605705 . https://doi.org/10.1145/3604930.3605705\n\nWang, A., Wolf, T.: Overview of the sustainlp 2020 shared task. In: SUSTAINLP (2020). https://api.semanticscholar.org/CorpusID:226283937\n\nAlizadeh, N., Belchev, B., Saurabh, N., Kelbert, P., Castor, F.: Language Models in Software Development Tasks: An Experimental Analysis of Energy and Accuracy (2025). https://arxiv.org/abs/2412.00329\n\nLuccioni, S., Jernite, Y., Strubell, E.: Power hungry processing: Watts driving the cost of ai deployment? In: Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. FAccT '24, pp. 85–99. Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/3630106.3658542\n\nBai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M., Zhang, Y., Song, X., Yang, C., Cheng, Y., Zhao, L.: Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models (2024). https://arxiv.org/abs/2401.00625\n\nAnthony, L.F.W., Kanding, B., Selvan, R.: Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models (2020). https://arxiv.org/abs/2007.03051\n\nHenderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D., Pineau, J.: Towards the systematic reporting of the energy and carbon footprints of machine learning. J. Mach. Learn. Res. 21(1) (2020)\n\nBouza, L., Bugeau, A., Lannelongue, L.: How to estimate carbon footprint when training deep learning models? a guide and review. Environmental Research Communications 5(11), 115014 (2023) https://doi.org/10.1088/2515-7620/acf81b\n\nHlavac, M.: stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3 (2022). https://CRAN.R-project.org/package=stargazer\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,", "original_types": ["text"], "id": 1469}
{"type": "section", "content": "G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp. 1877–1901. Curran Associates, Inc., ??? (2020)\n\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (2017). https://arxiv.org/abs/1712.05877\n\nIlsche, T., Hackenberg, D., Schöne, R., Bielert, M., Höpfner, F., E. Nagel, W.: Metricq: A scalable infrastructure for processing high-resolution time series data. In: 2019 IEEE/ACM Industry/University Joint International Workshop on Data-center Automation, Analytics, and Control (DAAC), pp. 7–12 (2019). https://doi.org/10.1109/DAAC49578.2019.00007\n\nClavié, B., Cooper, N., Warner, B.: It’s All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers (2025). https://arxiv.org/abs/2502.03793\n\nSavvov, S.: Your Company Needs Small Language Models (2024). https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9/\n\nWei, L., Ying, Z., He, M., Chen, Y., Yang, Q., Hong, Y., Lu, J., Li, X., Huang, W., Chen, Y.: An adapted large language model facilitates multiple medical tasks in diabetes care (2024). https://arxiv.org/abs/2409.13191\n\nLu, Y., Yao, B., Zhang, S., Wang, Y., Zhang, P., Lu, T., Li, T.J.-J., Wang, D.: Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks (2023). https://arxiv.org/abs/2311.09825\n\nZhan, X., Goyal, A., Chen, Y., Chandrasekharan, E., Saha, K.: SLM-Mod: Small Language Models Surpass LLMs at Content Moderation (2025). https://arxiv.org/abs/2410.13155\n\nRaji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving face: Investigating the ethical concerns of facial recognition auditing. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (2020)\n\nWei, J., Bosma, M., Zhao, V.Y., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (2021)", "doc_id": "zschache2025", "page": 24, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp. 1877–1901. Curran Associates, Inc., ??? (2020)\n\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko, D.: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (2017). https://arxiv.org/abs/1712.05877\n\nIlsche, T., Hackenberg, D., Schöne, R., Bielert, M., Höpfner, F., E. Nagel, W.: Metricq: A scalable infrastructure for processing high-resolution time series data. In: 2019 IEEE/ACM Industry/University Joint International Workshop on Data-center Automation, Analytics, and Control (DAAC), pp. 7–12 (2019). https://doi.org/10.1109/DAAC49578.2019.00007\n\nClavié, B., Cooper, N., Warner, B.: It’s All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers (2025). https://arxiv.org/abs/2502.03793\n\nSavvov, S.: Your Company Needs Small Language Models (2024). https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9/\n\nWei, L., Ying, Z., He, M., Chen, Y., Yang, Q., Hong, Y., Lu, J., Li, X., Huang, W., Chen, Y.: An adapted large language model facilitates multiple medical tasks in diabetes care (2024). https://arxiv.org/abs/2409.13191\n\nLu, Y., Yao, B., Zhang, S., Wang, Y., Zhang, P., Lu, T., Li, T.J.-J., Wang, D.: Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks (2023). https://arxiv.org/abs/2311.09825\n\nZhan, X., Goyal, A., Chen, Y., Chandrasekharan, E., Saha, K.: SLM-Mod: Small Language Models Surpass LLMs at Content Moderation (2025). https://arxiv.org/abs/2410.13155\n\nRaji, I.D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., Denton, E.: Saving face: Investigating the ethical concerns of facial recognition auditing. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (2020)\n\nWei, J., Bosma, M., Zhao, V.Y., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (2021)", "original_types": ["text"], "id": 1470}
{"type": "section", "content": "Luccioni, A.S., Viguier, S., Ligozat, A.-L.: Estimating the carbon footprint of bloom, a 176b parameter language model. J. Mach. Learn. Res. 24(1) (2023)\n\nGehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: Realtoxicityprompts: Evaluating neural toxic degeneration in language models. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (2020)\n\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021)", "doc_id": "zschache2025", "page": 25, "url": "https://arxiv.org/pdf/2508.14170 ", "embedded_text": "Luccioni, A.S., Viguier, S., Ligozat, A.-L.: Estimating the carbon footprint of bloom, a 176b parameter language model. J. Mach. Learn. Res. 24(1) (2023)\n\nGehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: Realtoxicityprompts: Evaluating neural toxic degeneration in language models. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (2020)\n\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021)", "original_types": ["text"], "id": 1471}
{"type": "section", "content": "Abstract\n\nRecent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.", "doc_id": "strubell2019", "page": 1, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Abstract\n\nRecent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.", "original_types": ["text", "header"], "id": 1472}
{"type": "table", "content": "Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.\nAir travel, 1 passenger, NY<>SF & 1984\nHuman life, avg, 1 year & 11,023\nAmerican life, avg, 1 year & 36,156\nCar, avg incl. fuel, 1 lifetime & 126,000\n", "doc_id": "strubell2019", "page": 1, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.\nAir travel, 1 passenger, NY<>SF & 1984\nHuman life, avg, 1 year & 11,023\nAmerican life, avg, 1 year & 36,156\nCar, avg incl. fuel, 1 lifetime & 126,000\n", "id": 1473}
{"type": "section", "content": "NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.\nEven when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy is not currently derived from carbon-neural sources in many locations, and (2) when renewable energy is available, it is still limited to the equipment we have to produce and store it, and energy spent training a neural network might better be allocated to heating a family’s home. It is estimated that we must cut carbon emissions by half over the next decade to deter escalating rates of natural disaster, and based on the estimated CO2 emissions listed in Table 1,", "doc_id": "strubell2019", "page": 1, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.\nEven when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy is not currently derived from carbon-neural sources in many locations, and (2) when renewable energy is available, it is still limited to the equipment we have to produce and store it, and energy spent training a neural network might better be allocated to heating a family’s home. It is estimated that we must cut carbon emissions by half over the next decade to deter escalating rates of natural disaster, and based on the estimated CO2 emissions listed in Table 1,", "original_types": ["text"], "id": 1474}
{"type": "section", "content": "Methods\n\nTo quantify the computational and environmental cost of training deep neural network models for NLP, we perform an analysis of the energy required to train a variety of popular off-the-shelf NLP models, as well as a case study of the complete sum of resources required to develop LISA (Strubell et al., 2018), a state-of-the-art NLP model from EMNLP 2018, including all tuning and experimentation.", "doc_id": "strubell2019", "page": 2, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Methods\n\nTo quantify the computational and environmental cost of training deep neural network models for NLP, we perform an analysis of the energy required to train a variety of popular off-the-shelf NLP models, as well as a case study of the complete sum of resources required to develop LISA (Strubell et al., 2018), a state-of-the-art NLP model from EMNLP 2018, including all tuning and experimentation.", "original_types": ["text", "header"], "id": 1475}
{"type": "table", "content": "Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States,4 China5 and Germany (Burger, 2019).", "doc_id": "strubell2019", "page": 2, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States,4 China5 and Germany (Burger, 2019).", "id": 1476}
{"type": "section", "content": "We estimate the total time expected for models to train to completion using training times and hardware reported in the original papers. We then calculate the power consumption in kilowatt-hours (kWh) as follows. Let \\( p_c \\) be the average power draw (in watts) from all CPU sockets during training, let \\( p_r \\) be the average power draw from all DRAM (main memory) sockets, let \\( p_g \\) be the average power draw of a GPU during training, and let \\( g \\) be the number of GPUs used to train. We estimate total power consumption as combined GPU, CPU and DRAM consumption, then multiply this by Power Usage Effectiveness (PUE), which accounts for the additional energy required to support the compute infrastructure (mainly cooling). We use a PUE coefficient of 1.58, the 2018 global average for data centers (Ascierto, 2018). Then the total power \\( p_t \\) required at a given instance during training is given by:", "doc_id": "strubell2019", "page": 2, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "We estimate the total time expected for models to train to completion using training times and hardware reported in the original papers. We then calculate the power consumption in kilowatt-hours (kWh) as follows. Let \\( p_c \\) be the average power draw (in watts) from all CPU sockets during training, let \\( p_r \\) be the average power draw from all DRAM (main memory) sockets, let \\( p_g \\) be the average power draw of a GPU during training, and let \\( g \\) be the number of GPUs used to train. We estimate total power consumption as combined GPU, CPU and DRAM consumption, then multiply this by Power Usage Effectiveness (PUE), which accounts for the additional energy required to support the compute infrastructure (mainly cooling). We use a PUE coefficient of 1.58, the 2018 global average for data centers (Ascierto, 2018). Then the total power \\( p_t \\) required at a given instance during training is given by:", "original_types": ["text"], "id": 1477}
{"type": "section", "content": "2.1 Models\n\nWe analyze four models, the computational requirements of which we describe below. All models have code freely available online, which we used out-of-the-box. For more details on the models themselves, please refer to the original papers. Transformer. The Transformer model (Vaswani et al., 2017) is an encoder-decoder architecture primarily recognized for efficient and accurate machine translation. The encoder and decoder each consist of 6 stacked layers of multi-head self-attention. Vaswani et al. (2017) report that the Transformer base model (65M parameters) was trained on 8 NVIDIA P100 GPUs for 12 hours, and the Transformer big model (213M parameters) was trained for 3.5 days (84 hours; 300k steps). This model is also the basis for recent work on neural architecture search (NAS) for machine translation and language modeling (So et al., 2019), and the NLP pipeline that we study in more detail in §4.2 (Strubell et al., 2018). So et al. (2019) report that their full architecture search ran for a total of 979M training steps, and that their base model requires 10 hours to train for 300k steps on one TPUv2 core. This equates to 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs. ELMo. The ELMo model (Peters et al., 2018) is based on stacked LSTMs and provides rich word representations in context by pre-training on a large amount of data using a language modeling objective. Replacing context-independent pre-trained word embeddings with ELMo has been shown to increase performance on downstream tasks such as named entity recognition, semantic role labeling, and coreference. Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). BERT. The BERT model (Devlin et al., 2019) provides a Transformer-based architecture for building contextual representations similar to ELMo, but trained with a different language modeling objective. BERT substantially improves accuracy on tasks requiring sentence-level representations such as question answering and natural language inference. Devlin et al. (2019) report that the BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours). NVIDIA reports that they can train a BERT model in 3.3 days (79.2 hours) using 4 DGX-2H servers, totaling 64 Tesla V100 GPUs (Forster et al., 2019). GPT-2. This model is the latest edition of OpenAI’s GPT general-purpose token encoder, also based on Transformer-style self-attention and trained with a language modeling objective (Radford et al., 2019). By training a very large model on massive data, Radford et al. (2019) show high zero-shot performance on question answering and language modeling benchmarks. The large model described in Radford et al. (2019) has 1542M parameters and is reported to require 1 week (168 hours) of training on 32 TPUv3 chips. 6\n\n3 Related work", "doc_id": "strubell2019", "page": 3, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "2.1 Models\n\nWe analyze four models, the computational requirements of which we describe below. All models have code freely available online, which we used out-of-the-box. For more details on the models themselves, please refer to the original papers. Transformer. The Transformer model (Vaswani et al., 2017) is an encoder-decoder architecture primarily recognized for efficient and accurate machine translation. The encoder and decoder each consist of 6 stacked layers of multi-head self-attention. Vaswani et al. (2017) report that the Transformer base model (65M parameters) was trained on 8 NVIDIA P100 GPUs for 12 hours, and the Transformer big model (213M parameters) was trained for 3.5 days (84 hours; 300k steps). This model is also the basis for recent work on neural architecture search (NAS) for machine translation and language modeling (So et al., 2019), and the NLP pipeline that we study in more detail in §4.2 (Strubell et al., 2018). So et al. (2019) report that their full architecture search ran for a total of 979M training steps, and that their base model requires 10 hours to train for 300k steps on one TPUv2 core. This equates to 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs. ELMo. The ELMo model (Peters et al., 2018) is based on stacked LSTMs and provides rich word representations in context by pre-training on a large amount of data using a language modeling objective. Replacing context-independent pre-trained word embeddings with ELMo has been shown to increase performance on downstream tasks such as named entity recognition, semantic role labeling, and coreference. Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). BERT. The BERT model (Devlin et al., 2019) provides a Transformer-based architecture for building contextual representations similar to ELMo, but trained with a different language modeling objective. BERT substantially improves accuracy on tasks requiring sentence-level representations such as question answering and natural language inference. Devlin et al. (2019) report that the BERT base model (110M parameters) was trained on 16 TPU chips for 4 days (96 hours). NVIDIA reports that they can train a BERT model in 3.3 days (79.2 hours) using 4 DGX-2H servers, totaling 64 Tesla V100 GPUs (Forster et al., 2019). GPT-2. This model is the latest edition of OpenAI’s GPT general-purpose token encoder, also based on Transformer-style self-attention and trained with a language modeling objective (Radford et al., 2019). By training a very large model on massive data, Radford et al. (2019) show high zero-shot performance on question answering and language modeling benchmarks. The large model described in Radford et al. (2019) has 1542M parameters and is reported to require 1 week (168 hours) of training on 32 TPUv3 chips. 6\n\n3 Related work", "original_types": ["text", "header"], "id": 1478}
{"type": "section", "content": "There is some precedent for work characterizing the computational requirements of training and inference in modern neural network architectures in the computer vision community. Li et al. (2016) present a detailed study of the energy use required for training and inference in popular convolutional models for image classification in computer vision, including fine-grained analysis comparing different neural network layer types. Canziani et al. (2016) assess image classification model accuracy as a function of model size and gigaflops required during inference. They also measure average power draw required during inference on GPUs as a function of batch size. Neither work analyzes the recurrent and self-attention models that have become commonplace in NLP, nor do they extrapolate power to estimates of carbon and dollar cost of training. Analysis of hyperparameter tuning has been performed in the context of improved algorithms for hyperparameter search (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). To our knowledge there exists to date no analysis of the computation required for R&D and hyperparameter tuning of neural network models in NLP.", "doc_id": "strubell2019", "page": 3, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "There is some precedent for work characterizing the computational requirements of training and inference in modern neural network architectures in the computer vision community. Li et al. (2016) present a detailed study of the energy use required for training and inference in popular convolutional models for image classification in computer vision, including fine-grained analysis comparing different neural network layer types. Canziani et al. (2016) assess image classification model accuracy as a function of model size and gigaflops required during inference. They also measure average power draw required during inference on GPUs as a function of batch size. Neither work analyzes the recurrent and self-attention models that have become commonplace in NLP, nor do they extrapolate power to estimates of carbon and dollar cost of training. Analysis of hyperparameter tuning has been performed in the context of improved algorithms for hyperparameter search (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). To our knowledge there exists to date no analysis of the computation required for R&D and hyperparameter tuning of neural network models in NLP.", "original_types": ["text"], "id": 1479}
{"type": "section", "content": "Experimental results\n\n4.1 Cost of training\n\nTable 3 lists CO2 emissions and estimated cost of training the models described in §2.1. Of note is that TPUs are more cost-efficient than GPUs on workloads that make sense for that hardware (e.g. BERT). We also see that models emit substantial carbon emissions; training BERT on GPU is roughly equivalent to a trans-American flight. So et al. (2019) report that NAS achieves a new state-of-the-art BLEU score of 29.7 for English to German machine translation, an increase of just 0.1 BLEU at the cost of at least $150k in on-demand compute time and non-trivial carbon emissions.\n\n4.2 Cost of development: Case study\n\nTo quantify the computational requirements of R&D for a new model we study the logs of all training required to develop Linguistically-Informed Self-Attention (Strubell et al., 2018), a multi-task model that performs part-of-speech tagging, labeled dependency parsing, predicate detection and semantic role labeling. This model makes for an interesting case study as a representative NLP pipeline and as a Best Long Paper at EMNLP. Model training associated with the project spanned a period of 172 days (approx. 6 months). During that time 123 small hyperparameter grid searches were performed, resulting in 4789 jobs in total. Jobs varied in length ranging from a minimum of 3 minutes, indicating a crash, to a maximum of 9 days, with an average job length of 52 hours. All training was done on a combination of NVIDIA Titan X (72%) and M40 (28%) GPUs. The sum GPU time required for the project totaled 9998 days (27 years). This averages to about 60 GPUs running constantly throughout the 6 month duration of the project. Table 4 lists upper and lower bounds of the estimated cost in terms of Google Cloud compute and raw electricity required to develop and deploy this model. We see that while training a single model is relatively inexpensive, the cost of tuning a model for a new dataset, which we estimate here to require 24 jobs, or performing the full R&D required to develop this model, quickly becomes extremely expensive.\n\nConclusions\n\nAuthors should report training time and sensitivity to hyperparameters. Our experiments suggest that it would be beneficial to directly compare different models to perform a cost-benefit (accuracy) analysis. To address this, when proposing a model that is meant to be re-trained for downstream use, such as retraining on a new domain or fine-tuning on a new task, authors should report training time and computational resources required, as well as model sensitivity to hyperparameters. This will enable direct comparison across models, allowing subsequent consumers of these models to accurately assess whether the required computational resources", "doc_id": "strubell2019", "page": 4, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Experimental results\n\n4.1 Cost of training\n\nTable 3 lists CO2 emissions and estimated cost of training the models described in §2.1. Of note is that TPUs are more cost-efficient than GPUs on workloads that make sense for that hardware (e.g. BERT). We also see that models emit substantial carbon emissions; training BERT on GPU is roughly equivalent to a trans-American flight. So et al. (2019) report that NAS achieves a new state-of-the-art BLEU score of 29.7 for English to German machine translation, an increase of just 0.1 BLEU at the cost of at least $150k in on-demand compute time and non-trivial carbon emissions.\n\n4.2 Cost of development: Case study\n\nTo quantify the computational requirements of R&D for a new model we study the logs of all training required to develop Linguistically-Informed Self-Attention (Strubell et al., 2018), a multi-task model that performs part-of-speech tagging, labeled dependency parsing, predicate detection and semantic role labeling. This model makes for an interesting case study as a representative NLP pipeline and as a Best Long Paper at EMNLP. Model training associated with the project spanned a period of 172 days (approx. 6 months). During that time 123 small hyperparameter grid searches were performed, resulting in 4789 jobs in total. Jobs varied in length ranging from a minimum of 3 minutes, indicating a crash, to a maximum of 9 days, with an average job length of 52 hours. All training was done on a combination of NVIDIA Titan X (72%) and M40 (28%) GPUs. The sum GPU time required for the project totaled 9998 days (27 years). This averages to about 60 GPUs running constantly throughout the 6 month duration of the project. Table 4 lists upper and lower bounds of the estimated cost in terms of Google Cloud compute and raw electricity required to develop and deploy this model. We see that while training a single model is relatively inexpensive, the cost of tuning a model for a new dataset, which we estimate here to require 24 jobs, or performing the full R&D required to develop this model, quickly becomes extremely expensive.\n\nConclusions\n\nAuthors should report training time and sensitivity to hyperparameters. Our experiments suggest that it would be beneficial to directly compare different models to perform a cost-benefit (accuracy) analysis. To address this, when proposing a model that is meant to be re-trained for downstream use, such as retraining on a new domain or fine-tuning on a new task, authors should report training time and computational resources required, as well as model sensitivity to hyperparameters. This will enable direct comparison across models, allowing subsequent consumers of these models to accurately assess whether the required computational resources", "original_types": ["text", "header", "subheader"], "id": 1480}
{"type": "section", "content": "Academic researchers need equitable access to computation resources.\n\nRecent advances in available compute come at a high price not attainable to all who desire access. Most of the models studied in this paper were developed outside academia; recent improvements in state-of-the-art accuracy are possible thanks to industry access to large-scale compute. Limiting this style of research to industry labs hurts the NLP research community in many ways. First, it stifles creativity. Researchers with good ideas but without access to large-scale compute will simply not be able to execute their ideas, instead constrained to focus on different problems. Second, it prohibits certain types of research on the basis of access to financial resources. This even more deeply promotes the already problematic “rich get richer” cycle of research funding, where groups that are already successful and thus well-funded tend to receive more funding due to their existing accomplishments. Third, the prohibitive start-up cost of building in-house resources forces resource-poor groups to rely on cloud compute services such as AWS, Google Cloud and Microsoft Azure. While these services provide valuable, flexible, and often relatively environmentally friendly compute resources, it is more cost effective for academic researchers, who often work for non-profit educational institutions and whose research is funded by government entities, to pool resources to build shared compute centers at the level of funding agencies, such as the U.S. National Science Foundation. For example, an off-the-shelf GPU server containing 8 NVIDIA 1080 Ti GPUs and supporting hardware can be purchased for approximately $20,000 USD. At that cost, the hardware required to develop the model in our case study (approximately 58 GPUs for 172 days) would cost $145,000 USD plus electricity, about half the estimated cost to use on-demand cloud GPUs. Unlike money spent on cloud compute, however, that invested in centralized resources would continue to pay off as resources are shared across many projects. A government-funded academic compute cloud would provide equitable access to all researchers.\n\nResearchers should prioritize computationally efficient hardware and algorithms.", "doc_id": "strubell2019", "page": 5, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "Academic researchers need equitable access to computation resources.\n\nRecent advances in available compute come at a high price not attainable to all who desire access. Most of the models studied in this paper were developed outside academia; recent improvements in state-of-the-art accuracy are possible thanks to industry access to large-scale compute. Limiting this style of research to industry labs hurts the NLP research community in many ways. First, it stifles creativity. Researchers with good ideas but without access to large-scale compute will simply not be able to execute their ideas, instead constrained to focus on different problems. Second, it prohibits certain types of research on the basis of access to financial resources. This even more deeply promotes the already problematic “rich get richer” cycle of research funding, where groups that are already successful and thus well-funded tend to receive more funding due to their existing accomplishments. Third, the prohibitive start-up cost of building in-house resources forces resource-poor groups to rely on cloud compute services such as AWS, Google Cloud and Microsoft Azure. While these services provide valuable, flexible, and often relatively environmentally friendly compute resources, it is more cost effective for academic researchers, who often work for non-profit educational institutions and whose research is funded by government entities, to pool resources to build shared compute centers at the level of funding agencies, such as the U.S. National Science Foundation. For example, an off-the-shelf GPU server containing 8 NVIDIA 1080 Ti GPUs and supporting hardware can be purchased for approximately $20,000 USD. At that cost, the hardware required to develop the model in our case study (approximately 58 GPUs for 172 days) would cost $145,000 USD plus electricity, about half the estimated cost to use on-demand cloud GPUs. Unlike money spent on cloud compute, however, that invested in centralized resources would continue to pay off as resources are shared across many projects. A government-funded academic compute cloud would provide equitable access to all researchers.\n\nResearchers should prioritize computationally efficient hardware and algorithms.", "original_types": ["text", "header"], "id": 1481}
{"type": "section", "content": "We recommend a concerted effort by industry and academia to promote research of more computationally efficient algorithms, as well as hardware that requires less energy. An effort can also be made in terms of software. There is already a precedent for NLP software packages prioritizing efficient models. An additional avenue through which NLP and machine learning software developers could aid in reducing the energy associated with model tuning is by providing easy-to-use APIs implementing more efficient alternatives to brute-force grid search for hyperparameter tuning, e.g. random or Bayesian hyperparameter search techniques (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). While software packages implementing these techniques do exist,10 they are rarely employed in practice for tuning NLP models. This is likely because their interoperability with popular deep learning frameworks such as PyTorch and TensorFlow is not optimized, i.e. there are not simple examples of how to tune TensorFlow Estimators using Bayesian search. Integrating these tools into the workflows with which NLP researchers and practitioners are already familiar could have notable impact on the cost of developing and tuning in NLP.", "doc_id": "strubell2019", "page": 5, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "We recommend a concerted effort by industry and academia to promote research of more computationally efficient algorithms, as well as hardware that requires less energy. An effort can also be made in terms of software. There is already a precedent for NLP software packages prioritizing efficient models. An additional avenue through which NLP and machine learning software developers could aid in reducing the energy associated with model tuning is by providing easy-to-use APIs implementing more efficient alternatives to brute-force grid search for hyperparameter tuning, e.g. random or Bayesian hyperparameter search techniques (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). While software packages implementing these techniques do exist,10 they are rarely employed in practice for tuning NLP models. This is likely because their interoperability with popular deep learning frameworks such as PyTorch and TensorFlow is not optimized, i.e. there are not simple examples of how to tune TensorFlow Estimators using Bayesian search. Integrating these tools into the workflows with which NLP researchers and practitioners are already familiar could have notable impact on the cost of developing and tuning in NLP.", "original_types": ["text"], "id": 1482}
{"type": "section", "content": "References\n\nRhonda Ascierto. 2018. Uptime Institute Global Data Center Survey. Technical report, Uptime Institute.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference for Learning Representations (ICLR), San Diego, California, USA.\n\nJames Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281–305.\n\nJames S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pages 2546–2554.\n\nBruno Burger. 2019. Net Public Electricity Generation in Germany in 2018. Technical report, Fraunhofer Institute for Solar Energy Systems ISE.\n\nAlfredo Canziani, Adam Paszke, and Eugenio Culurciello. 2016. An analysis of deep neural network models for practical applications.\n\nGary Cook, Jude Lee, Tamina Tsai, Ada Kongn, John Deans, Brian Johnson, Elizabeth Jardim, and Brian Johnson. 2017. Clicking Clean: Who is winning the race to build a green internet? Technical report, Greenpeace.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.\n\nTimothy Dozat and Christopher D. Manning. 2017. Deep biaffine attention for neural dependency parsing. In ICLR.\n\nEPA. 2018. Emissions & Generation Resource Integrated Database (eGRID). Technical report, U.S. Environmental Protection Agency.\n\nChristopher Forster, Thor Johnsen, Swetha Mandava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie Bernauer, Allison Gray, Sharan Chetlur, and Raul Puri. 2019. BERT Meets GPUs. Technical report, NVIDIA AI.\n\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong. 2016. Evaluating the energy efficiency of deep convolutional neural networks on cpus and gpus. 2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom), pages 477–484.\n\nThang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421. Association for Computational Linguistics.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959.", "doc_id": "strubell2019", "page": 6, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "References\n\nRhonda Ascierto. 2018. Uptime Institute Global Data Center Survey. Technical report, Uptime Institute.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference for Learning Representations (ICLR), San Diego, California, USA.\n\nJames Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281–305.\n\nJames S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pages 2546–2554.\n\nBruno Burger. 2019. Net Public Electricity Generation in Germany in 2018. Technical report, Fraunhofer Institute for Solar Energy Systems ISE.\n\nAlfredo Canziani, Adam Paszke, and Eugenio Culurciello. 2016. An analysis of deep neural network models for practical applications.\n\nGary Cook, Jude Lee, Tamina Tsai, Ada Kongn, John Deans, Brian Johnson, Elizabeth Jardim, and Brian Johnson. 2017. Clicking Clean: Who is winning the race to build a green internet? Technical report, Greenpeace.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.\n\nTimothy Dozat and Christopher D. Manning. 2017. Deep biaffine attention for neural dependency parsing. In ICLR.\n\nEPA. 2018. Emissions & Generation Resource Integrated Database (eGRID). Technical report, U.S. Environmental Protection Agency.\n\nChristopher Forster, Thor Johnsen, Swetha Mandava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie Bernauer, Allison Gray, Sharan Chetlur, and Raul Puri. 2019. BERT Meets GPUs. Technical report, NVIDIA AI.\n\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong. 2016. Evaluating the energy efficiency of deep convolutional neural networks on cpus and gpus. 2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom), pages 477–484.\n\nThang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421. Association for Computational Linguistics.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959.", "original_types": ["text", "header"], "id": 1483}
{"type": "section", "content": "David R. So, Chen Liang, and Quoc V. Le. 2019. The evolved transformer. In Proceedings of the 36th International Conference on Machine Learning (ICML).\n\nEmma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-Informed Self-Attention for Semantic Role Labeling. In Conference on Empirical Methods in Natural Language Processing (EMNLP), Brussels, Belgium.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In 31st Conference on Neural Information Processing Systems (NIPS).", "doc_id": "strubell2019", "page": 6, "url": "https://arxiv.org/pdf/1906.02243", "embedded_text": "David R. So, Chen Liang, and Quoc V. Le. 2019. The evolved transformer. In Proceedings of the 36th International Conference on Machine Learning (ICML).\n\nEmma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-Informed Self-Attention for Semantic Role Labeling. In Conference on Empirical Methods in Natural Language Processing (EMNLP), Brussels, Belgium.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In 31st Conference on Neural Information Processing Systems (NIPS).", "original_types": ["text"], "id": 1484}
{"type": "section", "content": "THE RISING COSTS OF TRAINING FRONTIER AI MODELS\n\nThe costs of training frontier AI models have grown dramatically in recent years, but there is limited public data on the magnitude and growth of these expenses. This paper develops a detailed cost model to address this gap, estimating training costs using three approaches that account for hardware, energy, cloud rental, and staff expenses. The analysis reveals that the amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI: 2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%). If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.\n\n1. Introduction\n\nThe large and growing cost of training state-of-the-art AI models has become an important issue in the field of AI [1]. Improving AI capabilities demand exponential increases in computing power, as evidenced by both economic analysis [2] and the discovery of empirical scaling laws, which show that model performance improves with more parameters and training data [3, 4]. Dario Amodei, CEO of the AI lab Anthropic, has stated that frontier AI developers are likely to spend close to a billion dollars on a single training run this year, and up to ten billion-dollar training runs in the next two years [5]. Given this trend, some innovations, particularly those requiring large-scale training, may become inaccessible to all but the most well-funded organizations.\n\nAlthough it is widely known that training the largest ML models is expensive, until recently there were few concrete estimates of training costs in the public domain. In collaboration with Epoch AI, the 2024 AI Index presented one of the most comprehensive datasets to date, estimating the costs of training runs based on cloud rental prices [6]. We build on that work with a more in-depth account of hardware, energy and R&D staff costs for both training runs and experiments, as well as a more detailed analysis of how costs are increasing over time. To our knowledge, our study is the most thorough analysis of model development costs to date.", "doc_id": "cottier2024", "page": 1, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "THE RISING COSTS OF TRAINING FRONTIER AI MODELS\n\nThe costs of training frontier AI models have grown dramatically in recent years, but there is limited public data on the magnitude and growth of these expenses. This paper develops a detailed cost model to address this gap, estimating training costs using three approaches that account for hardware, energy, cloud rental, and staff expenses. The analysis reveals that the amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI: 2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%). If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.\n\n1. Introduction\n\nThe large and growing cost of training state-of-the-art AI models has become an important issue in the field of AI [1]. Improving AI capabilities demand exponential increases in computing power, as evidenced by both economic analysis [2] and the discovery of empirical scaling laws, which show that model performance improves with more parameters and training data [3, 4]. Dario Amodei, CEO of the AI lab Anthropic, has stated that frontier AI developers are likely to spend close to a billion dollars on a single training run this year, and up to ten billion-dollar training runs in the next two years [5]. Given this trend, some innovations, particularly those requiring large-scale training, may become inaccessible to all but the most well-funded organizations.\n\nAlthough it is widely known that training the largest ML models is expensive, until recently there were few concrete estimates of training costs in the public domain. In collaboration with Epoch AI, the 2024 AI Index presented one of the most comprehensive datasets to date, estimating the costs of training runs based on cloud rental prices [6]. We build on that work with a more in-depth account of hardware, energy and R&D staff costs for both training runs and experiments, as well as a more detailed analysis of how costs are increasing over time. To our knowledge, our study is the most thorough analysis of model development costs to date.", "original_types": ["text", "header"], "id": 1485}
{"type": "section", "content": "Our methods are built upon a comprehensive database of notable machine learning models [7], and informed by interviews with industry experts. We consider three complementary approaches to measuring the cost of frontier models. The first approach estimates the hardware capital expenses (CapEx) amortized over the final training run, along with the cost of hardware energy consumption. By considering AI accelerator chips, other server hardware, networking hardware, and energy separately, this approach can provide more accurate training costs. We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. Among frontier models, defined as models within the top 10 most compute-intensive models when they are released, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).", "doc_id": "cottier2024", "page": 1, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Our methods are built upon a comprehensive database of notable machine learning models [7], and informed by interviews with industry experts. We consider three complementary approaches to measuring the cost of frontier models. The first approach estimates the hardware capital expenses (CapEx) amortized over the final training run, along with the cost of hardware energy consumption. By considering AI accelerator chips, other server hardware, networking hardware, and energy separately, this approach can provide more accurate training costs. We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. Among frontier models, defined as models within the top 10 most compute-intensive models when they are released, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).", "original_types": ["text"], "id": 1486}
{"type": "figure", "content": "Figure 1: Amortized hardware and energy cost to train frontier AI models over time", "doc_id": "cottier2024", "page": 2, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 1: Amortized hardware and energy cost to train frontier AI models over time", "id": 1487}
{"type": "section", "content": "2.2 Amortizing the cost of hardware for training\n\nTo estimate the cost of hardware for a training run, we first calculated the cost to acquire the necessary accelerator chips (GPUs/TPUs), servers, and networking hardware. This involved looking up historical prices for GPUs, or estimating production costs for TPUs. Further details are provided in Appendix A.1.\n\nHardware normally remains available for future use after a training run finishes, but its value depreciates over time due to hardware progress. We amortized the cost of a training run based on this depreciation. Specifically, we depreciated the value of hardware at a rate of \\( r = 0.14 \\) orders of magnitude per year, based on the growth rate of ML GPU price-performance [9].\n\nTo get the value of the hardware at the start of training, we used the following formula:\n\nStart value per chip = \\(\\frac{\text{Acquisition cost per chip}}{\\exp\\left(\\left[\text{Training start date} - \text{Hardware availability date}\right] \\cdot r \\ln 10\right)}\\)\n\nwhere the difference in dates is in units of years. For example, if the training run starts one year after hardware is acquired, the start value is approximately 72% of the acquisition cost. We neglected the impact of hardware failures on depreciation, as the effect seemed small compared to hardware progress. We provide evidence for that in Appendix A.3.\n\nAfter finding the initial value of the hardware, the amortized cost of the training run is then the portion of value that is lost during the training run. However, the training time is often difficult to determine, due to a lack of public information. More often, we were able to estimate the number of chip-hours: the product of the training time and the number of chips. So we substituted chip-hours for the training time and the number of chips, using a linear approximation. This led to our final formula for amortized training cost:\n\nAmortized training cost ≈ Start value per chip × \\(\\frac{\text{Training chip-hours}}{(365 \times 24) \text{ hours/year}} \times r \\ln 10\\)\n\nUp until Section 3.5, our results only account for the chip-hours of the final training run. In Section 3.5, we scale up the chip-hours to account for all experiments towards developing an AI model. Although the amortized cost model involves several estimates and approximations, our results are robust to reasonable changes in the method (see Appendix A.3 for further methodological details and Appendix B.3 for the sensitivity analysis).", "doc_id": "cottier2024", "page": 3, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "2.2 Amortizing the cost of hardware for training\n\nTo estimate the cost of hardware for a training run, we first calculated the cost to acquire the necessary accelerator chips (GPUs/TPUs), servers, and networking hardware. This involved looking up historical prices for GPUs, or estimating production costs for TPUs. Further details are provided in Appendix A.1.\n\nHardware normally remains available for future use after a training run finishes, but its value depreciates over time due to hardware progress. We amortized the cost of a training run based on this depreciation. Specifically, we depreciated the value of hardware at a rate of \\( r = 0.14 \\) orders of magnitude per year, based on the growth rate of ML GPU price-performance [9].\n\nTo get the value of the hardware at the start of training, we used the following formula:\n\nStart value per chip = \\(\\frac{\text{Acquisition cost per chip}}{\\exp\\left(\\left[\text{Training start date} - \text{Hardware availability date}\right] \\cdot r \\ln 10\right)}\\)\n\nwhere the difference in dates is in units of years. For example, if the training run starts one year after hardware is acquired, the start value is approximately 72% of the acquisition cost. We neglected the impact of hardware failures on depreciation, as the effect seemed small compared to hardware progress. We provide evidence for that in Appendix A.3.\n\nAfter finding the initial value of the hardware, the amortized cost of the training run is then the portion of value that is lost during the training run. However, the training time is often difficult to determine, due to a lack of public information. More often, we were able to estimate the number of chip-hours: the product of the training time and the number of chips. So we substituted chip-hours for the training time and the number of chips, using a linear approximation. This led to our final formula for amortized training cost:\n\nAmortized training cost ≈ Start value per chip × \\(\\frac{\text{Training chip-hours}}{(365 \times 24) \text{ hours/year}} \times r \\ln 10\\)\n\nUp until Section 3.5, our results only account for the chip-hours of the final training run. In Section 3.5, we scale up the chip-hours to account for all experiments towards developing an AI model. Although the amortized cost model involves several estimates and approximations, our results are robust to reasonable changes in the method (see Appendix A.3 for further methodological details and Appendix B.3 for the sensitivity analysis).", "original_types": ["text", "header"], "id": 1488}
{"type": "section", "content": "2.5 Total amortized model development cost\n\nAlthough the final training run ultimately determines an AI model’s capabilities and impact, the research and development surrounding it is crucial. We therefore used a third approach that considers all of the compute that went into model development, as well as the cost of R&D staff developing the model. Since this approach was more time-intensive, and relied on having a list of contributors to estimate R&D staff cost, we applied it to just four models: GPT-3, OPT-175B, GPT-4, and Gemini Ultra.\n\nTo estimate the compute cost over model development—including experiments, failed attempts, evaluation and fine-tuning—we applied a multiplicative factor to the final training run compute. We estimated this factor based on evidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta. Appendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.\n\n2.5.1 R&D staff costs\n\nResearch and development (R&D) staff costs are an often-neglected component of the total cost of developing ML models. These costs include the salaries and equity compensation of the researchers, engineers, and managers in the project, but excludes operations staff and data center employees. We set out to better quantify these costs for a few selected models to see how significant they are relative to the hardware costs.\n\nWe estimated total annual compensation of R&D personnel by multiplying the estimated full-time equivalent workload per contributor by their compensation and by the total time spent on model development. Since these parameters were all quite uncertain, we sampled from log-normal distributions over each parameter.\n\nFor full-time equivalent workers, we were informed by the type and number of contributors listed on the research paper. For all models except Gemini Ultra, we sampled full-time equivalent workloads from a 90% credible interval of 5% to 80% FTE for each contributor, resulting in a median of 20%. For Gemini Ultra, we used different workloads for each type of contributor listed [10, pp. 66–69].\n\nFor compensation, we were informed by company-specific data from https://www.levels.fyi/ and https://aipaygrad.es/. From levels.fyi, we used data for Google Software Engineers from level 3 to level 8. From aipaygrad.es, we used the overall statistics for all companies and all roles (researchers, engineers and managers). After averaging the two sources, base salaries were modeled with a 90% CI of $140K to $160K, and equity with a 90% CI of $35K to $490K. We applied an overhead factor of 1.25x to 1.4x to base salaries to account for taxes and benefits [11], resulting in total compensation with a 90% CI of $210K to $690K and a median of $330K.", "doc_id": "cottier2024", "page": 4, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "2.5 Total amortized model development cost\n\nAlthough the final training run ultimately determines an AI model’s capabilities and impact, the research and development surrounding it is crucial. We therefore used a third approach that considers all of the compute that went into model development, as well as the cost of R&D staff developing the model. Since this approach was more time-intensive, and relied on having a list of contributors to estimate R&D staff cost, we applied it to just four models: GPT-3, OPT-175B, GPT-4, and Gemini Ultra.\n\nTo estimate the compute cost over model development—including experiments, failed attempts, evaluation and fine-tuning—we applied a multiplicative factor to the final training run compute. We estimated this factor based on evidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta. Appendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.\n\n2.5.1 R&D staff costs\n\nResearch and development (R&D) staff costs are an often-neglected component of the total cost of developing ML models. These costs include the salaries and equity compensation of the researchers, engineers, and managers in the project, but excludes operations staff and data center employees. We set out to better quantify these costs for a few selected models to see how significant they are relative to the hardware costs.\n\nWe estimated total annual compensation of R&D personnel by multiplying the estimated full-time equivalent workload per contributor by their compensation and by the total time spent on model development. Since these parameters were all quite uncertain, we sampled from log-normal distributions over each parameter.\n\nFor full-time equivalent workers, we were informed by the type and number of contributors listed on the research paper. For all models except Gemini Ultra, we sampled full-time equivalent workloads from a 90% credible interval of 5% to 80% FTE for each contributor, resulting in a median of 20%. For Gemini Ultra, we used different workloads for each type of contributor listed [10, pp. 66–69].\n\nFor compensation, we were informed by company-specific data from https://www.levels.fyi/ and https://aipaygrad.es/. From levels.fyi, we used data for Google Software Engineers from level 3 to level 8. From aipaygrad.es, we used the overall statistics for all companies and all roles (researchers, engineers and managers). After averaging the two sources, base salaries were modeled with a 90% CI of $140K to $160K, and equity with a 90% CI of $35K to $490K. We applied an overhead factor of 1.25x to 1.4x to base salaries to account for taxes and benefits [11], resulting in total compensation with a 90% CI of $210K to $690K and a median of $330K.", "original_types": ["text", "header"], "id": 1489}
{"type": "section", "content": "Actual staff compensation may vary significantly between AI labs. The chosen estimate of compensation may be particularly unreliable for small and early companies, such as OpenAI in its earlier years, where there are many uncertainties about how to value equity compensation. However, these numbers serve as a reasonable baseline, and our estimates provide a useful starting point to analyze R&D labor costs.", "doc_id": "cottier2024", "page": 4, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Actual staff compensation may vary significantly between AI labs. The chosen estimate of compensation may be particularly unreliable for small and early companies, such as OpenAI in its earlier years, where there are many uncertainties about how to value equity compensation. However, these numbers serve as a reasonable baseline, and our estimates provide a useful starting point to analyze R&D labor costs.", "original_types": ["text"], "id": 1490}
{"type": "section", "content": "3 Results\n\n3.1 Amortized training costs of frontier models have grown by 2.4x per year since 2016\n\nThe amortized training costs of frontier models have increased by a factor of 2.4x per year since 2016. This is the result of the preferred amortized hardware CapEex + energy approach, shown in Figure 2. Table 1 compares this to the cloud approach, which yields a similar growth rate of 2.5× per year. The growth rate is also similar if we vary hardware depreciation or training start date within reasonable limits (see Appendix B.4). However, the growth rate rises to 2.9x per year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.", "doc_id": "cottier2024", "page": 5, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "3 Results\n\n3.1 Amortized training costs of frontier models have grown by 2.4x per year since 2016\n\nThe amortized training costs of frontier models have increased by a factor of 2.4x per year since 2016. This is the result of the preferred amortized hardware CapEex + energy approach, shown in Figure 2. Table 1 compares this to the cloud approach, which yields a similar growth rate of 2.5× per year. The growth rate is also similar if we vary hardware depreciation or training start date within reasonable limits (see Appendix B.4). However, the growth rate rises to 2.9x per year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.", "original_types": ["text", "header"], "id": 1491}
{"type": "table", "content": "Table 1: Cost growth rates based on log-linear regression, for different cost estimation approaches. All approaches select the top 10 most compute intensive models at the time of model release. N refers to the number of relevant observations. Based on a two-sided t-test adjusted for correlation of residuals, the growth rates for amortized hardware capex + energy and cloud are not significantly different (p = 0.13). However, when the costs of models trained with estimated TPU production costs are excluded, the growth rate rises significantly to 2.9x per year (p < 0.01). OOMs/year: orders of magnitude per year. Square brackets: 90% confidence interval.", "doc_id": "cottier2024", "page": 5, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Table 1: Cost growth rates based on log-linear regression, for different cost estimation approaches. All approaches select the top 10 most compute intensive models at the time of model release. N refers to the number of relevant observations. Based on a two-sided t-test adjusted for correlation of residuals, the growth rates for amortized hardware capex + energy and cloud are not significantly different (p = 0.13). However, when the costs of models trained with estimated TPU production costs are excluded, the growth rate rises significantly to 2.9x per year (p < 0.01). OOMs/year: orders of magnitude per year. Square brackets: 90% confidence interval.", "id": 1492}
{"type": "table", "content": "Amortized hardware and energy cost to train frontier AI models over time", "doc_id": "cottier2024", "page": 5, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Amortized hardware and energy cost to train frontier AI models over time", "id": 1493}
{"type": "figure", "content": "Figure 2: (Reproduction of Figure 1 for convenience.) Amortized hardware cost plus energy cost for the final training run of frontier models. The selected models are among the top 10 most compute-intensive for their time. Amortized hardware costs are the product of training chip-hours and a depreciated hardware cost, with 23% overhead added for cluster-level networking. Open circles indicate costs which used an estimated production cost of Google TPU hardware. These costs are generally more uncertain than the others, which used actual price data rather than estimates.", "doc_id": "cottier2024", "page": 5, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 2: (Reproduction of Figure 1 for convenience.) Amortized hardware cost plus energy cost for the final training run of frontier models. The selected models are among the top 10 most compute-intensive for their time. Amortized hardware costs are the product of training chip-hours and a depreciated hardware cost, with 23% overhead added for cluster-level networking. Open circles indicate costs which used an estimated production cost of Google TPU hardware. These costs are generally more uncertain than the others, which used actual price data rather than estimates.", "id": 1494}
{"type": "section", "content": "Estimating costs from cloud rental prices, although less representative of actual costs, has the advantage of simplicity. The cloud cost approach also helps to check the robustness of the amortized hardware CapEx + energy approach. Figure 3 shows the trend of cloud compute cost to train models among the top 10 most compute-intensive as of their release. Note that some of these estimates previously appeared in the 2024 AI Index report [6]. We find that the cost of training models based on cloud rental prices has grown by 2.5× per year since 2016, with a 90% CI of 2.1× to 3.1×.", "doc_id": "cottier2024", "page": 5, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Estimating costs from cloud rental prices, although less representative of actual costs, has the advantage of simplicity. The cloud cost approach also helps to check the robustness of the amortized hardware CapEx + energy approach. Figure 3 shows the trend of cloud compute cost to train models among the top 10 most compute-intensive as of their release. Note that some of these estimates previously appeared in the 2024 AI Index report [6]. We find that the cost of training models based on cloud rental prices has grown by 2.5× per year since 2016, with a 90% CI of 2.1× to 3.1×.", "original_types": ["text"], "id": 1495}
{"type": "section", "content": "Cloud compute cost to train frontier AI models over time", "doc_id": "cottier2024", "page": 6, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Cloud compute cost to train frontier AI models over time", "original_types": ["header"], "id": 1496}
{"type": "figure", "content": "Figure 3: Estimated cloud compute costs for the final training run of frontier models. The selected models are among the top 10 most compute-intensive for their time. The costs are the product of the number of training chip-hours and a historical cloud rental price.", "doc_id": "cottier2024", "page": 6, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 3: Estimated cloud compute costs for the final training run of frontier models. The selected models are among the top 10 most compute-intensive for their time. The costs are the product of the number of training chip-hours and a historical cloud rental price.", "id": 1497}
{"type": "section", "content": "This is consistent with the amortized hardware CapEx + energy approach, as shown in Table 1. This shows that our trend estimates are robust to two different ways of estimating prices per chip-hour.\n\nOverall, these results suggest that the cloud approach is valid for estimating growth rates in compute costs, and has the advantage of simplicity. However, public cloud rental prices are less reliable for individual model costs when the model developer owns the hardware or has a special partnership with a cloud provider.\n\nThe trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027\n\nThe growth rate in training cost indicates how rapidly AI investment is scaling. We can use this growth rate to extrapolate the cost of the largest training run. Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M. GPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.\n\nWhether this cost is justified hinges on how profitable the resulting AI model is—but parts of the AI industry believe it is worthwhile. The CEO of the AI lab Anthropic has claimed that close to a billion dollars will already be spent on a single training run in 2024 (implying an amortized cost), which is even sooner than the historical trend suggests [5].\n\nHardware acquisition costs are one to two orders of magnitude higher than amortized costs\n\nIt’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime of the hardware, and the acquisition cost of purchasing that hardware outright. The choice of which cost to consider depends on the purpose of the analysis. Amortized costs are more relevant for understanding the economics of training and deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and financial risks involved in developing such models.\n\nTo illustrate the difference between amortized hardware cost and acquisition cost, Figure 4 shows the acquisition costs we were able to estimate using hardware purchase prices and training hardware quantities. Since this is the up-front cost of acquiring the hardware, the costs are one to two orders of magnitude larger than amortized hardware costs. For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. The ratio between the two depends on when and for how long the model is trained.\n\nBased on 40 estimates of acquisition cost, we find a growth rate of 2.5× per year (90% CI: 2.1×, 3.0×). This is slightly faster than the rate of 2× per year suggested by amortized costs (2.4× per year) divided by training times (1.2× per", "doc_id": "cottier2024", "page": 6, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "This is consistent with the amortized hardware CapEx + energy approach, as shown in Table 1. This shows that our trend estimates are robust to two different ways of estimating prices per chip-hour.\n\nOverall, these results suggest that the cloud approach is valid for estimating growth rates in compute costs, and has the advantage of simplicity. However, public cloud rental prices are less reliable for individual model costs when the model developer owns the hardware or has a special partnership with a cloud provider.\n\nThe trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027\n\nThe growth rate in training cost indicates how rapidly AI investment is scaling. We can use this growth rate to extrapolate the cost of the largest training run. Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M. GPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.\n\nWhether this cost is justified hinges on how profitable the resulting AI model is—but parts of the AI industry believe it is worthwhile. The CEO of the AI lab Anthropic has claimed that close to a billion dollars will already be spent on a single training run in 2024 (implying an amortized cost), which is even sooner than the historical trend suggests [5].\n\nHardware acquisition costs are one to two orders of magnitude higher than amortized costs\n\nIt’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime of the hardware, and the acquisition cost of purchasing that hardware outright. The choice of which cost to consider depends on the purpose of the analysis. Amortized costs are more relevant for understanding the economics of training and deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and financial risks involved in developing such models.\n\nTo illustrate the difference between amortized hardware cost and acquisition cost, Figure 4 shows the acquisition costs we were able to estimate using hardware purchase prices and training hardware quantities. Since this is the up-front cost of acquiring the hardware, the costs are one to two orders of magnitude larger than amortized hardware costs. For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. The ratio between the two depends on when and for how long the model is trained.\n\nBased on 40 estimates of acquisition cost, we find a growth rate of 2.5× per year (90% CI: 2.1×, 3.0×). This is slightly faster than the rate of 2× per year suggested by amortized costs (2.4× per year) divided by training times (1.2× per", "original_types": ["text", "header"], "id": 1498}
{"type": "section", "content": "3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips\n\nBreaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips. The rest of the server (including markup) makes up 29% of the cost, while cluster level interconnect makes up 17%. Energy makes up the remainder of costs, averaging 9% but varying across models. Although this is a small fraction, it corresponds to rapid growth in energy use and power requirements over time. The trend in power requirements is provided in Appendix C. Note that this breakdown does not include all costs associated with an AI supercomputer. Other costs include the data center infrastructure besides servers and networking, as well as data center personnel and maintenance.", "doc_id": "cottier2024", "page": 7, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips\n\nBreaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips. The rest of the server (including markup) makes up 29% of the cost, while cluster level interconnect makes up 17%. Energy makes up the remainder of costs, averaging 9% but varying across models. Although this is a small fraction, it corresponds to rapid growth in energy use and power requirements over time. The trend in power requirements is provided in Appendix C. Note that this breakdown does not include all costs associated with an AI supercomputer. Other costs include the data center infrastructure besides servers and networking, as well as data center personnel and maintenance.", "original_types": ["text", "header"], "id": 1499}
{"type": "figure", "content": "Figure 5: The percentage of the amortized hardware CapEx + energy estimates made up by different hardware and energy costs. Note that the breakdown across models is approximate. Cluster-level interconnect is assumed to be a constant 19% fraction of the cluster CapEx, and the proportion of server components is based on only three comparisons between NVIDIA DGX server prices and single GPU prices (see Appendix A.1 for details). The energy costs are more specific, varying with the number of training chip-hours and the hardware (see Appendix A.4).", "doc_id": "cottier2024", "page": 8, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 5: The percentage of the amortized hardware CapEx + energy estimates made up by different hardware and energy costs. Note that the breakdown across models is approximate. Cluster-level interconnect is assumed to be a constant 19% fraction of the cluster CapEx, and the proportion of server components is based on only three comparisons between NVIDIA DGX server prices and single GPU prices (see Appendix A.1 for details). The energy costs are more specific, varying with the number of training chip-hours and the hardware (see Appendix A.4).", "id": 1500}
{"type": "section", "content": "8", "doc_id": "cottier2024", "page": 8, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "8", "original_types": ["text"], "id": 1501}
{"type": "figure", "content": "Figure 6: (a) Breakdown of total amortized model development costs for selected models. Hardware costs are amortized to the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based on median estimates.", "doc_id": "cottier2024", "page": 9, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 6: (a) Breakdown of total amortized model development costs for selected models. Hardware costs are amortized to the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based on median estimates.", "id": 1502}
{"type": "section", "content": "GPT-4 at 284 contributors. Though we assumed a very small contribution from the 428 people under the “Contributors” role—a median full-time equivalent of about 1%—the estimate may still be too high.\n\nOn the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while energy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively. Note that while energy consumption is a small fraction of total cost, this doesn’t entail that power requirements are not a challenge in frontier AI development. Regulatory and logistical hurdles to secure power supplies may cause bottlenecks in the coming years, but we leave that topic to future work.", "doc_id": "cottier2024", "page": 9, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "GPT-4 at 284 contributors. Though we assumed a very small contribution from the 428 people under the “Contributors” role—a median full-time equivalent of about 1%—the estimate may still be too high.\n\nOn the compute side, we find that amortized hardware cost makes up 47–64% of the full model development cost, while energy comprises only 2–6%. With equity excluded from R&D costs, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively. Note that while energy consumption is a small fraction of total cost, this doesn’t entail that power requirements are not a challenge in frontier AI development. Regulatory and logistical hurdles to secure power supplies may cause bottlenecks in the coming years, but we leave that topic to future work.", "original_types": ["text"], "id": 1503}
{"type": "section", "content": "4.2 How to estimate training costs\n\nWe used two approaches to estimate the cost of final training runs: the amortized hardware CapEx + energy approach, and the cloud rental price approach. These two approaches produced consistent estimates of the growth rate in training cost over time. However, the approaches diverged on individual costs: the cloud costs were twice as large on average. We recommend using the amortized hardware CapEx + energy approach for frontier models wherever it’s feasible, because it accounts for the lower costs in practice for large training runs, and can be broken down into components.\n\nOur third approach adds the cost of R&D staff, as well as the compute cost of experiments, evaluations, and fine-tuning involved in model development. To our knowledge, we present the first detailed estimates of these costs for GPT-3, OPT-175B and Gemini Ultra. Moreover, our results suggest that R&D staff costs were a major component of total costs for these frontier models. Although this is the most comprehensive of the three approaches, further data collection and evidence on the AI development process are needed before we can recommend it as the default.\n\n4.3 Limitations\n\nWhile our study provides valuable insights into the growth of AI training costs, there are important limitations. The analysis relies on publicly available information, which may lead to biases or gaps in the dataset. Cost estimation methods are subject to uncertainties due to factors such as hardware depreciation rates and pricing dynamics. Moreover, our methods neglect several costs that are potentially significant, including the data center infrastructure apart from the training cluster, and the acquisition of data for model training.\n\nOur results may also have limited generality. The trends observed for the selected frontier models may not generalize to the broader AI landscape, or specific AI domains such as language modeling. Rapid innovation could also lead to large gains in hardware and software efficiency that are difficult to predict from historical data. Further research on all of these unknowns would help refine our insights, and inform evidence-based strategies to respond to growing financial barriers in ML.\n\n5 Conclusion\n\nIn this paper we used three approaches to analyze the cost of training ML models at the frontier. The first two approaches—one based on hardware purchase prices and energy costs, the other based on cloud rental prices—indicate that the amortized cost of compute for these training runs has grown by around 2.4x per year (90% CI: 2.0x to 2.9x) since 2016. This shows the large role of investment in driving AI progress.\n\nBreaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. This motivates further research on the scaling of R&D labor with computing power.", "doc_id": "cottier2024", "page": 10, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "4.2 How to estimate training costs\n\nWe used two approaches to estimate the cost of final training runs: the amortized hardware CapEx + energy approach, and the cloud rental price approach. These two approaches produced consistent estimates of the growth rate in training cost over time. However, the approaches diverged on individual costs: the cloud costs were twice as large on average. We recommend using the amortized hardware CapEx + energy approach for frontier models wherever it’s feasible, because it accounts for the lower costs in practice for large training runs, and can be broken down into components.\n\nOur third approach adds the cost of R&D staff, as well as the compute cost of experiments, evaluations, and fine-tuning involved in model development. To our knowledge, we present the first detailed estimates of these costs for GPT-3, OPT-175B and Gemini Ultra. Moreover, our results suggest that R&D staff costs were a major component of total costs for these frontier models. Although this is the most comprehensive of the three approaches, further data collection and evidence on the AI development process are needed before we can recommend it as the default.\n\n4.3 Limitations\n\nWhile our study provides valuable insights into the growth of AI training costs, there are important limitations. The analysis relies on publicly available information, which may lead to biases or gaps in the dataset. Cost estimation methods are subject to uncertainties due to factors such as hardware depreciation rates and pricing dynamics. Moreover, our methods neglect several costs that are potentially significant, including the data center infrastructure apart from the training cluster, and the acquisition of data for model training.\n\nOur results may also have limited generality. The trends observed for the selected frontier models may not generalize to the broader AI landscape, or specific AI domains such as language modeling. Rapid innovation could also lead to large gains in hardware and software efficiency that are difficult to predict from historical data. Further research on all of these unknowns would help refine our insights, and inform evidence-based strategies to respond to growing financial barriers in ML.\n\n5 Conclusion\n\nIn this paper we used three approaches to analyze the cost of training ML models at the frontier. The first two approaches—one based on hardware purchase prices and energy costs, the other based on cloud rental prices—indicate that the amortized cost of compute for these training runs has grown by around 2.4x per year (90% CI: 2.0x to 2.9x) since 2016. This shows the large role of investment in driving AI progress.\n\nBreaking down the total amortized model development cost for selected frontier models (GPT-3, OPT-175B, GPT-4 and Gemini Ultra), we found that R&D staff are a major component, making up 29–49% of the total. This motivates further research on the scaling of R&D labor with computing power.", "original_types": ["text", "header"], "id": 1504}
{"type": "section", "content": "The rapid exponential growth of costs over eight years suggests that growth is unlikely to stall in the next few years. However, frontier AI labs appear to face non-trivial challenges to scaling further. One such challenge is securing enough power capacity for increasingly large computing clusters. Analyzing potential bottlenecks such as this is an important topic for future work.\n\nThe rapid increase in AI investment is likely to drive major advances in AI capabilities. Given that total model development costs at the frontier are already over $100 million, these advances may only be accessible to the largest companies and government institutions. The concentration of such a powerful technology among a few key players raises questions about responsible development and deployment. Both AI developers and policymakers must engage with these issues and consider the tradeoffs involved. The stakes are high—decisions made now about the governance and trajectory of AI could have profound consequences for society.", "doc_id": "cottier2024", "page": 10, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "The rapid exponential growth of costs over eight years suggests that growth is unlikely to stall in the next few years. However, frontier AI labs appear to face non-trivial challenges to scaling further. One such challenge is securing enough power capacity for increasingly large computing clusters. Analyzing potential bottlenecks such as this is an important topic for future work.\n\nThe rapid increase in AI investment is likely to drive major advances in AI capabilities. Given that total model development costs at the frontier are already over $100 million, these advances may only be accessible to the largest companies and government institutions. The concentration of such a powerful technology among a few key players raises questions about responsible development and deployment. Both AI developers and policymakers must engage with these issues and consider the tradeoffs involved. The stakes are high—decisions made now about the governance and trajectory of AI could have profound consequences for society.", "original_types": ["text"], "id": 1505}
{"type": "section", "content": "Acknowledgements\n\nWe thank Bartosz Podkanowicz for assisting with data collection, Luke Frymire for assisting with ground truth cost verification, and Josh You for copyediting. We thank Igor Molybog, Yafah Edelman and Horace He for helpful conversations about the training process and requirements for creating large ML models. We thank Konstantin Pilz, Yafah Edelman, Tom Davidson, Isabel Juniewicz, Carl Shulman, Jaime Sevilla, Aleksandar Kostovic, Tim Fist, Haydn Belfield, Alan Chan, David Patterson, Mauricio Baker, Erich Grunewald, and Cullen O’Keefe for feedback on drafts. This study was supported by a grant from the AI Index based out of the Stanford Institute for Human-Centered Artificial Intelligence. The cloud compute cost estimates shown here previously appeared in the 2024 Stanford AI Index Report.", "doc_id": "cottier2024", "page": 11, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Acknowledgements\n\nWe thank Bartosz Podkanowicz for assisting with data collection, Luke Frymire for assisting with ground truth cost verification, and Josh You for copyediting. We thank Igor Molybog, Yafah Edelman and Horace He for helpful conversations about the training process and requirements for creating large ML models. We thank Konstantin Pilz, Yafah Edelman, Tom Davidson, Isabel Juniewicz, Carl Shulman, Jaime Sevilla, Aleksandar Kostovic, Tim Fist, Haydn Belfield, Alan Chan, David Patterson, Mauricio Baker, Erich Grunewald, and Cullen O’Keefe for feedback on drafts. This study was supported by a grant from the AI Index based out of the Stanford Institute for Human-Centered Artificial Intelligence. The cloud compute cost estimates shown here previously appeared in the 2024 Stanford AI Index Report.", "original_types": ["text", "header"], "id": 1506}
{"type": "section", "content": "[20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/1691877797487165443, 2024. Accessed: 2024-05-30.\n\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs, 2024.\n\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs, 2024.\n\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and David Patterson. Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 1–14, 2021.\n\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet, 2023.\n\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint arXiv:2104.10350, 2021.\n\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems), 4 2023. Version 01.\n\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a Computer: Designing Warehouse-scale Machines, 2018.\n\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Accessed: 2024-05-30.\n\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI Datacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race, 2024. Accessed: 2024-05-30.\n\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a, 2024. Accessed: 2024-01-15.\n\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.\n\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/, 2023. Accessed: 2024-05-30.", "doc_id": "cottier2024", "page": 12, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "[20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/1691877797487165443, 2024. Accessed: 2024-05-30.\n\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs, 2024.\n\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs, 2024.\n\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and David Patterson. Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 1–14, 2021.\n\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet, 2023.\n\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint arXiv:2104.10350, 2021.\n\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems), 4 2023. Version 01.\n\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a Computer: Designing Warehouse-scale Machines, 2018.\n\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Accessed: 2024-05-30.\n\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI Datacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race, 2024. Accessed: 2024-05-30.\n\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a, 2024. Accessed: 2024-01-15.\n\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.\n\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/, 2023. Accessed: 2024-05-30.", "original_types": ["text"], "id": 1507}
{"type": "section", "content": "[38] Electricity generation, capacity, and sales in the United States. https://web.archive.org/web/20240407085026/https://www.eia.gov/energyexplained/electricity/electricity-in-the-us-top-10.php, 2022. Accessed: 2024-05-29.", "doc_id": "cottier2024", "page": 12, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "[38] Electricity generation, capacity, and sales in the United States. https://web.archive.org/web/20240407085026/https://www.eia.gov/energyexplained/electricity/electricity-in-the-us-top-10.php, 2022. Accessed: 2024-05-29.", "original_types": ["text"], "id": 1508}
{"type": "section", "content": "Training cost estimation\n\nA.1 Hardware acquisition cost\n\nThe frontier AI models in our dataset were trained on clusters of many GPU or TPU chips (or “chips” for short). We set out to estimate the total cost of the chips, servers, and networking equipment in these clusters, which we call the hardware acquisition cost. This cost is calculated as follows:\n\nHardware acquisition cost = Acquisition cost per chip × Number of chips\n\nWhere “Acquisition cost per chip” accounts for the GPU or TPU chip itself, other server costs (CPUs, memory, chip-to-chip networking, and markup), and the cost of server-to-server networking equipment. Table 2 shows how we calculated this quantity depending on what was known about the training hardware.", "doc_id": "cottier2024", "page": 13, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Training cost estimation\n\nA.1 Hardware acquisition cost\n\nThe frontier AI models in our dataset were trained on clusters of many GPU or TPU chips (or “chips” for short). We set out to estimate the total cost of the chips, servers, and networking equipment in these clusters, which we call the hardware acquisition cost. This cost is calculated as follows:\n\nHardware acquisition cost = Acquisition cost per chip × Number of chips\n\nWhere “Acquisition cost per chip” accounts for the GPU or TPU chip itself, other server costs (CPUs, memory, chip-to-chip networking, and markup), and the cost of server-to-server networking equipment. Table 2 shows how we calculated this quantity depending on what was known about the training hardware.", "original_types": ["text", "header", "subheader"], "id": 1509}
{"type": "table", "content": "Table 2: The formula to estimate “Acquisition cost per chip”, depending on what is known about the training hardware.\n| Known information | Formula for “Acquisition cost per chip” |\n|---|---|\n| Single GPU price | GPU chip price × Chip-to-server factor × Server-to-cluster factor |\n| GPU server price | GPU server price / GPUs per server × Server-to-cluster factor |\n| Chips are TPUs | TPU chip cost × Chip-to-server factor × Server-to-cluster factor |\n", "doc_id": "cottier2024", "page": 13, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Table 2: The formula to estimate “Acquisition cost per chip”, depending on what is known about the training hardware.\n| Known information | Formula for “Acquisition cost per chip” |\n|---|---|\n| Single GPU price | GPU chip price × Chip-to-server factor × Server-to-cluster factor |\n| GPU server price | GPU server price / GPUs per server × Server-to-cluster factor |\n| Chips are TPUs | TPU chip cost × Chip-to-server factor × Server-to-cluster factor |\n", "id": 1510}
{"type": "section", "content": "If the training hardware was a GPU, we looked up the earliest known price linked to that GPU. If the price we found was for a single GPU, we multiplied that price by a “chip-to-server” cost factor (detailed later). If the price we found was instead for a DGX server, we divided that price by the number of GPUs per server.3 Finally, if the training hardware was a Google TPU, we used the “Geometric mean” production cost estimate from Table 3. This represents the cost of the TPU chip itself, which we then multiplied by a chip-to-server cost factor.\n\nWe calculated chip-to-server cost factors based on known DGX and single-GPU prices near release, using the formula (DGX cost)/(8×GPU cost). We were able to estimate this for the NVIDIA P100 (1.54×), V100 (1.69×), and A100 (1.66×). For other NVIDIA chips, and for TPUs, we used the mean of these three known factors (1.64×). We assumed that the DGX server prices included the cost of chip-to-chip interconnect switches and transceivers.4 We did not account for financing, i.e. the interest paid on a loan to purchase the hardware up-front.\n\nOnce we had the cost per chip for a single server, we added the cost of server-to-server networking equipment. We used an estimate by Kostovic (forthcoming), based on the reference architecture of the NVIDIA H100 SuperPOD [15]. According to this estimate, approximately 19% of the total cost of the SuperPOD goes towards cluster-level interconnect for configurations with less than 4096 GPUs, and 20% for 4096 GPUs and above, due to an additional third layer of switches. Consistent with these figures, another expert in AI hardware estimated a range of 10% to 20% for A100-based clusters with an Infiniband network [16].\n\nFor simplicity, we assumed that 19% of the hardware acquisition cost was for server-to-server networking equipment. We therefore multiplied the cost per chip for a single server by a “server-to-cluster” factor of 100%/(100%−19%)≈1.23×, resulting in the final “Acquisition cost per chip”. We assumed that the overhead factor is accurate for TPU servers as well as GPU servers, though we have substantial uncertainty about this. In reality, the proportion of costs varies with the cluster architecture and size.\n\nA.2 Cost of Google TPUs\n\nTensor Processing Units are a class of proprietary AI accelerator hardware developed by Google, and used in their internal computing projects and employed in Google Cloud datacenters [17]. These chips are not available for sale, but some of them can be rented on the cloud. Since they have never been sold, there are no available purchase prices, which makes it more difficult to estimate the amortized capital expenses for Google Brain, DeepMind, and other Google labs.", "doc_id": "cottier2024", "page": 13, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "If the training hardware was a GPU, we looked up the earliest known price linked to that GPU. If the price we found was for a single GPU, we multiplied that price by a “chip-to-server” cost factor (detailed later). If the price we found was instead for a DGX server, we divided that price by the number of GPUs per server.3 Finally, if the training hardware was a Google TPU, we used the “Geometric mean” production cost estimate from Table 3. This represents the cost of the TPU chip itself, which we then multiplied by a chip-to-server cost factor.\n\nWe calculated chip-to-server cost factors based on known DGX and single-GPU prices near release, using the formula (DGX cost)/(8×GPU cost). We were able to estimate this for the NVIDIA P100 (1.54×), V100 (1.69×), and A100 (1.66×). For other NVIDIA chips, and for TPUs, we used the mean of these three known factors (1.64×). We assumed that the DGX server prices included the cost of chip-to-chip interconnect switches and transceivers.4 We did not account for financing, i.e. the interest paid on a loan to purchase the hardware up-front.\n\nOnce we had the cost per chip for a single server, we added the cost of server-to-server networking equipment. We used an estimate by Kostovic (forthcoming), based on the reference architecture of the NVIDIA H100 SuperPOD [15]. According to this estimate, approximately 19% of the total cost of the SuperPOD goes towards cluster-level interconnect for configurations with less than 4096 GPUs, and 20% for 4096 GPUs and above, due to an additional third layer of switches. Consistent with these figures, another expert in AI hardware estimated a range of 10% to 20% for A100-based clusters with an Infiniband network [16].\n\nFor simplicity, we assumed that 19% of the hardware acquisition cost was for server-to-server networking equipment. We therefore multiplied the cost per chip for a single server by a “server-to-cluster” factor of 100%/(100%−19%)≈1.23×, resulting in the final “Acquisition cost per chip”. We assumed that the overhead factor is accurate for TPU servers as well as GPU servers, though we have substantial uncertainty about this. In reality, the proportion of costs varies with the cluster architecture and size.\n\nA.2 Cost of Google TPUs\n\nTensor Processing Units are a class of proprietary AI accelerator hardware developed by Google, and used in their internal computing projects and employed in Google Cloud datacenters [17]. These chips are not available for sale, but some of them can be rented on the cloud. Since they have never been sold, there are no available purchase prices, which makes it more difficult to estimate the amortized capital expenses for Google Brain, DeepMind, and other Google labs.", "original_types": ["text", "subheader"], "id": 1511}
{"type": "section", "content": "To estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU manufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end estimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead of co-designing TPUs with Broadcom [18]. The second approach models the equivalent purchase prices of Google TPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We consider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We interpolated hardware costs based on price-performance:\n\nTPU effective cost = GPU cost × TPU performance GPU performance × date adjustment factor\n\nwhere the date adjustment factor adjusts costs compared on different dates to make them comparable, based on the trend that GPU performance per unit cost improves at a rate of 0.14 orders of magnitude per year.\n\nFor the manufacturing cost approach, we estimated the manufacturing cost for the NVIDIA DGX SuperPOD at $8,665 per GPU. This estimate was informed by [19] and [20]—calculations are available in `h100_manufacturing_cost.ipynb`. After converting this to the TPU cost using the above formula, we divided by the average server-to-chip cost ratio of 1.64 that we estimated from NVIDIA GPU prices (see Appendix A.1). The results are listed in Table 3. For the equivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google ML GPUs, listed in Table 4 [21, 22, 23, 24].", "doc_id": "cottier2024", "page": 14, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "To estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU manufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end estimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead of co-designing TPUs with Broadcom [18]. The second approach models the equivalent purchase prices of Google TPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We consider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We interpolated hardware costs based on price-performance:\n\nTPU effective cost = GPU cost × TPU performance GPU performance × date adjustment factor\n\nwhere the date adjustment factor adjusts costs compared on different dates to make them comparable, based on the trend that GPU performance per unit cost improves at a rate of 0.14 orders of magnitude per year.\n\nFor the manufacturing cost approach, we estimated the manufacturing cost for the NVIDIA DGX SuperPOD at $8,665 per GPU. This estimate was informed by [19] and [20]—calculations are available in `h100_manufacturing_cost.ipynb`. After converting this to the TPU cost using the above formula, we divided by the average server-to-chip cost ratio of 1.64 that we estimated from NVIDIA GPU prices (see Appendix A.1). The results are listed in Table 3. For the equivalent GPU price approach, we found the specifications, release dates, and prices of the most similar non-Google ML GPUs, listed in Table 4 [21, 22, 23, 24].", "original_types": ["text"], "id": 1512}
{"type": "table", "content": "Table 3: Cost and performance comparison between Google TPUs and the NVIDIA H100. Performance ratios to the NVIDIA H100 use the same number format and are without sparsity. Our overall estimate of TPU costs is the geometric mean of estimates for the chip manufacturing cost and the price of an equivalent-performance GPU.", "doc_id": "cottier2024", "page": 14, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Table 3: Cost and performance comparison between Google TPUs and the NVIDIA H100. Performance ratios to the NVIDIA H100 use the same number format and are without sparsity. Our overall estimate of TPU costs is the geometric mean of estimates for the chip manufacturing cost and the price of an equivalent-performance GPU.", "id": 1513}
{"type": "table", "content": "Table 4: Comparison of GPU specifications. By interpolation between GPUs, and their price-performance data, we estimate performance-equivalent prices for TPU versions.", "doc_id": "cottier2024", "page": 14, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Table 4: Comparison of GPU specifications. By interpolation between GPUs, and their price-performance data, we estimate performance-equivalent prices for TPU versions.", "id": 1514}
{"type": "section", "content": "As explained above, we consider the manufacturing costs to be low estimates and the equivalent GPU prices to be high-end estimates of the full production cost. To aggregate the two approaches into a final estimate, we took the geometric mean, as shown in Table 3. Each TPU version has an estimated cost (for Google) of about $5,000.", "doc_id": "cottier2024", "page": 14, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "As explained above, we consider the manufacturing costs to be low estimates and the equivalent GPU prices to be high-end estimates of the full production cost. To aggregate the two approaches into a final estimate, we took the geometric mean, as shown in Table 3. Each TPU version has an estimated cost (for Google) of about $5,000.", "original_types": ["text"], "id": 1515}
{"type": "section", "content": "Amortization model\n\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\n\nStart value per chip = Acquisition cost per chip\n\nexp ([Training start date - Hardware availability date] * r ln 10)\n\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware availability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware announcement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU actually being shipped to the buyer. Our results are robust to variations in this buffer time—see Appendix B.4.\n\nFor the training start date, there were a few known cases—for example, GPT-4 finished training in August 2022 [12]. Otherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account for time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the training time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\n\nThe precise way to amortize the training cost through exponential depreciation is:\n\nAmortized training cost = Start value per chip * Number of chips * Depreciation during training\n\n= Start value per chip * Number of chips * (1 - exp [- Training time * r ln 10])\n\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training time or the number of chips separately. This is because chip-hours can also be estimated from training compute in FLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these chip-hour estimates:\n\nAmortized training cost = Start value per chip * Training chip-hours\n\n(365 * 24) hours/year\n\nThis approximation is valid if (Training time) * r ln 10 is small, and this is the case for the training times in our data and our choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 * 0.14 ln(10) ~ = 32% depreciation compared to 1 - exp(-1 * 0.14 ln(10)) ~ = 28% depreciation. This is not a large difference relative to other sources of uncertainty.\n\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a significant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per week occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs defective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.", "doc_id": "cottier2024", "page": 15, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Amortization model\n\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\n\nStart value per chip = Acquisition cost per chip\n\nexp ([Training start date - Hardware availability date] * r ln 10)\n\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware availability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware announcement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU actually being shipped to the buyer. Our results are robust to variations in this buffer time—see Appendix B.4.\n\nFor the training start date, there were a few known cases—for example, GPT-4 finished training in August 2022 [12]. Otherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account for time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the training time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\n\nThe precise way to amortize the training cost through exponential depreciation is:\n\nAmortized training cost = Start value per chip * Number of chips * Depreciation during training\n\n= Start value per chip * Number of chips * (1 - exp [- Training time * r ln 10])\n\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training time or the number of chips separately. This is because chip-hours can also be estimated from training compute in FLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these chip-hour estimates:\n\nAmortized training cost = Start value per chip * Training chip-hours\n\n(365 * 24) hours/year\n\nThis approximation is valid if (Training time) * r ln 10 is small, and this is the case for the training times in our data and our choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 * 0.14 ln(10) ~ = 32% depreciation compared to 1 - exp(-1 * 0.14 ln(10)) ~ = 28% depreciation. This is not a large difference relative to other sources of uncertainty.\n\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a significant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per week occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs defective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.", "original_types": ["text", "header"], "id": 1516}
{"type": "section", "content": "Accounting for compute used throughout model development\n\nIt is important to consider compute used throughout model development. The cost of experiments, evaluations, and fine-tuning reflects actual costs for developers to research and possibly deploy a useful ML model. This compute is not only important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.\n\nOne source of evidence on the allocation of compute is the training of smaller model sizes for a given architecture. For example, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6 × parameters × tokens) [14, Table 2.1]. This shows at least 14% of compute was spent outside the main training run. Similar reasoning for BLOOM reveals about 63% of compute was used on smaller models [25, Table 5].\n\nAnother source of evidence is reports of how compute budgets are allocated. For example, the OPT-175B developers estimated total cost at “roughly 2x higher” than the largest training run [34]. Meanwhile, across Meta’s AI infrastructure, one estimate in the literature suggested a 1:2 ratio between experimentation and training, where training includes additional hyper-parameter tuning and retraining [35].\n\nFor GPT-3, the true ratio is almost certainly higher than 1.14x due to failures and other experiments. We believe the Meta, BLOOM and OPT-175B cases are the more central examples as they account better for all experiments. So a factor close to 2x seems like a reasonable median estimate. On the high end, it’s plausible that several large-scale experiments are necessary before training—say, 4x. We sampled from the range of plausible values using a log-normal distribution. The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.", "doc_id": "cottier2024", "page": 16, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Accounting for compute used throughout model development\n\nIt is important to consider compute used throughout model development. The cost of experiments, evaluations, and fine-tuning reflects actual costs for developers to research and possibly deploy a useful ML model. This compute is not only important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.\n\nOne source of evidence on the allocation of compute is the training of smaller model sizes for a given architecture. For example, smaller versions of GPT-3 used 4.5e22 FLOP (based on compute = 6 × parameters × tokens) [14, Table 2.1]. This shows at least 14% of compute was spent outside the main training run. Similar reasoning for BLOOM reveals about 63% of compute was used on smaller models [25, Table 5].\n\nAnother source of evidence is reports of how compute budgets are allocated. For example, the OPT-175B developers estimated total cost at “roughly 2x higher” than the largest training run [34]. Meanwhile, across Meta’s AI infrastructure, one estimate in the literature suggested a 1:2 ratio between experimentation and training, where training includes additional hyper-parameter tuning and retraining [35].\n\nFor GPT-3, the true ratio is almost certainly higher than 1.14x due to failures and other experiments. We believe the Meta, BLOOM and OPT-175B cases are the more central examples as they account better for all experiments. So a factor close to 2x seems like a reasonable median estimate. On the high end, it’s plausible that several large-scale experiments are necessary before training—say, 4x. We sampled from the range of plausible values using a log-normal distribution. The distribution was defined by a 90% CI of 1.2x to 4x, leading to a median of 2.2x.", "original_types": ["text", "header"], "id": 1517}
{"type": "table", "content": "Table 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger uncertainty due to the additional uncertainty in estimating their equivalent costs.\nHardware acquisition (GPUs) & 0.5x to 2x\nHardware acquisition (TPUs) & 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) & 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) & 0.2x to 5x\n", "doc_id": "cottier2024", "page": 17, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Table 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger uncertainty due to the additional uncertainty in estimating their equivalent costs.\nHardware acquisition (GPUs) & 0.5x to 2x\nHardware acquisition (TPUs) & 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) & 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) & 0.2x to 5x\n", "id": 1518}
{"type": "figure", "content": "Figure 7: Comparison of hardware capex + energy cost regression using different frontier model selection methods. Results are fairly similar across methods, although taking the top 20% of residuals leads to a flatter trend.", "doc_id": "cottier2024", "page": 18, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 7: Comparison of hardware capex + energy cost regression using different frontier model selection methods. Results are fairly similar across methods, although taking the top 20% of residuals leads to a flatter trend.", "id": 1519}
{"type": "section", "content": "Varying N in top-N model selection\n\nWhen selecting frontier models by the top- N method, there is a question of how to choose N. We chose N = 10 to produce a large enough sample size while still focusing on models near the frontier. The estimated growth rate is moderately robust to the choice of N, as it is similar for N = 3, N = 5 and N = 20 (see Figure 8).\n\nVarying the depreciation of hardware value\n\nThe growth in price-performance for ML GPUs running at FP32 precision has been estimated at 0.14 OOMs/year with a 90% CI of 0.10 to 0.18 OOMs/year [9]. Substituting the lower and upper bounds of that CI for the depreciation rate did not significantly change the growth rate of amortized hardware CapEx + energy. For the lower bound of 0.10 OOMs/year, cost estimates decreased by 15% on average, while for the upper bound of 0.18 OOMs/year, cost estimates increased by 10% on average. Note that increasing the depreciation rate has two effects that partially cancel out: 1. the value of hardware at the start of training is smaller, 2. the proportion of value used up by training is larger. We also tested 0.3 OOMs/year as an extreme case, based on a claim that single-GPU inference performance has improved by 1000× in the last decade [37]. This did not significantly change the growth rate either, but it increased costs by an average of 30%.\n\nVarying the time between hardware acquisition and the start of training\n\nWe tested different estimates of the hardware acquisition date relative to the release date, as well as the training start date relative to the model publication date. These dates affect the time over which hardware value depreciates. To make the depreciation times long, we removed the minimum buffer of 90 days between hardware release and hardware acquisition, and pushed the default training start date back by 15 days relative to the publication date. This decreased the estimated costs by 4% on average, and did not change the growth rate. Similarly, we tested a short depreciation time by extending the hardware acquisition buffer time to 180 days and bringing the default training start date forward by 60 days. This increased costs by 10% on average and did not change the growth rate.", "doc_id": "cottier2024", "page": 18, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Varying N in top-N model selection\n\nWhen selecting frontier models by the top- N method, there is a question of how to choose N. We chose N = 10 to produce a large enough sample size while still focusing on models near the frontier. The estimated growth rate is moderately robust to the choice of N, as it is similar for N = 3, N = 5 and N = 20 (see Figure 8).\n\nVarying the depreciation of hardware value\n\nThe growth in price-performance for ML GPUs running at FP32 precision has been estimated at 0.14 OOMs/year with a 90% CI of 0.10 to 0.18 OOMs/year [9]. Substituting the lower and upper bounds of that CI for the depreciation rate did not significantly change the growth rate of amortized hardware CapEx + energy. For the lower bound of 0.10 OOMs/year, cost estimates decreased by 15% on average, while for the upper bound of 0.18 OOMs/year, cost estimates increased by 10% on average. Note that increasing the depreciation rate has two effects that partially cancel out: 1. the value of hardware at the start of training is smaller, 2. the proportion of value used up by training is larger. We also tested 0.3 OOMs/year as an extreme case, based on a claim that single-GPU inference performance has improved by 1000× in the last decade [37]. This did not significantly change the growth rate either, but it increased costs by an average of 30%.\n\nVarying the time between hardware acquisition and the start of training\n\nWe tested different estimates of the hardware acquisition date relative to the release date, as well as the training start date relative to the model publication date. These dates affect the time over which hardware value depreciates. To make the depreciation times long, we removed the minimum buffer of 90 days between hardware release and hardware acquisition, and pushed the default training start date back by 15 days relative to the publication date. This decreased the estimated costs by 4% on average, and did not change the growth rate. Similarly, we tested a short depreciation time by extending the hardware acquisition buffer time to 180 days and bringing the default training start date forward by 60 days. This increased costs by 10% on average and did not change the growth rate.", "original_types": ["text"], "id": 1520}
{"type": "figure", "content": "Figure 8: Comparison of amortized hardware capex + energy regression for varying top-N selection.", "doc_id": "cottier2024", "page": 19, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 8: Comparison of amortized hardware capex + energy regression for varying top-N selection.", "id": 1521}
{"type": "section", "content": "B.5 Excluding equity from R&D staff costs\n\nTo measure the impact of equity on the total amortized model development cost, Figure 9a shows the cost breakdown with equity excluded from the R&D staff cost. The proportion of cost on R&D staff decreases from 29–49% with equity included, to 19–33% with equity excluded.", "doc_id": "cottier2024", "page": 19, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "B.5 Excluding equity from R&D staff costs\n\nTo measure the impact of equity on the total amortized model development cost, Figure 9a shows the cost breakdown with equity excluded from the R&D staff cost. The proportion of cost on R&D staff decreases from 29–49% with equity included, to 19–33% with equity excluded.", "original_types": ["text"], "id": 1522}
{"type": "figure", "content": "Figure 9: (a) Breakdown of total amortized model development costs for selected models, with equity excluded from the R&D staff cost. Hardware costs are amortized to the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based on median estimates.", "doc_id": "cottier2024", "page": 20, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 9: (a) Breakdown of total amortized model development costs for selected models, with equity excluded from the R&D staff cost. Hardware costs are amortized to the total number of chip-hours spent on experiments and training, while R&D staff costs cover the duration of development from initial experiments to publication. Error bars indicate 90% credible intervals, while the main bar values are medians. (b) Costs components as a percentage of the total, based on median estimates.", "id": 1523}
{"type": "figure", "content": "Figure 10: The trend in AI compute cluster power (in kilowatts) required to train frontier models over time. Power is calculated as the product of the number of servers, server TDP, and power usage effectiveness.", "doc_id": "cottier2024", "page": 20, "url": "https://arxiv.org/pdf/2405.21015", "embedded_text": "Figure 10: The trend in AI compute cluster power (in kilowatts) required to train frontier models over time. Power is calculated as the product of the number of servers, server TDP, and power usage effectiveness.", "id": 1524}
